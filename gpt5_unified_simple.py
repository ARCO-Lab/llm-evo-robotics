#!/usr/bin/env python3
"""
GPT-5ç»Ÿä¸€ç­–ç•¥ç®€åŒ–ç‰ˆè®­ç»ƒè„šæœ¬ï¼š
1. Set-Transformeræ¶æ„ï¼Œæ”¯æŒå¯å˜å…³èŠ‚æ•°(2-5)
2. ç»Ÿä¸€å¥–åŠ±å‡½æ•°ï¼Œè·¨Nå¯æ¯”
3. å•ä¸€æ¨¡å‹å¤„ç†æ‰€æœ‰å…³èŠ‚æ•°
"""

import os
import numpy as np
import gymnasium as gym
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.vec_env import DummyVecEnv
import time
import math
import tempfile
from gymnasium.envs.mujoco import MujocoEnv
from gymnasium.spaces import Box

# è®¾ç½®æ¸²æŸ“ç¯å¢ƒå˜é‡
os.environ['MUJOCO_GL'] = 'glfw'
os.environ['MUJOCO_RENDERER'] = 'glfw'

# GPT-5ç»Ÿä¸€ç­–ç•¥å‚æ•°
ALPHA_DISTANCE = 5.0      # è·ç¦»æƒ©ç½šæƒé‡
BETA_CONTROL = 1e-3       # æ§åˆ¶æƒ©ç½šæƒé‡ (é™¤ä»¥N)
GAMMA_SMOOTH = 1e-3       # åŠ¨ä½œå¹³æ»‘æƒ©ç½šæƒé‡ (é™¤ä»¥N)
SUCCESS_THRESHOLD = 0.03  # ç»Ÿä¸€æˆåŠŸé˜ˆå€¼ (3cm)
SUCCESS_REWARD = 5.0      # æˆåŠŸå¥–åŠ±
EPISODE_LENGTH = 200      # ç»Ÿä¸€episodeé•¿åº¦

# ç½‘ç»œå‚æ•°
JOINT_TOKEN_DIM = 10
GLOBAL_TOKEN_DIM = 10
ENCODER_LAYERS = 2
HIDDEN_DIM = 256
ATTENTION_HEADS = 4
DROPOUT = 0.1

# ä»baselineå¤åˆ¶çš„3å…³èŠ‚XMLé…ç½®
def get_3joint_xml():
    """3å…³èŠ‚XMLé…ç½®ï¼ˆç»Ÿä¸€åŠ¨åŠ›å­¦å‚æ•° + è‡ªç¢°æ’æ£€æµ‹ï¼‰"""
    return """
<mujoco model="3joint_reacher">
  <compiler angle="radian" inertiafromgeom="true"/>
  <default>
    <joint armature="1" damping="1" limited="true"/>
    <geom contype="1" conaffinity="1" friction="1 0.1 0.1" rgba="0.7 0.7 0 1" density="1000"/>
  </default>
  <contact>
    <!-- é“¾èŠ‚ä¹‹é—´çš„è‡ªç¢°æ’æ£€æµ‹ -->
    <pair geom1="link0" geom2="link2" condim="3"/>
    <!-- End-effectorä¸æ‰€æœ‰é“¾èŠ‚çš„ç¢°æ’æ£€æµ‹ -->
    <pair geom1="fingertip" geom2="link0" condim="3"/>
    <pair geom1="fingertip" geom2="link1" condim="3"/>
  </contact>
  <option gravity="0 0 -9.81" integrator="RK4" timestep="0.01"/>
  <worldbody>
    <geom conaffinity="0" contype="0" name="ground" pos="0 0 0" rgba="0.9 0.9 0.9 1" size="0.5 0.5 10" type="plane"/>
    <geom conaffinity="0" contype="0" fromto="-.5 -.5 .01 .5 -.5 .01" name="sideS" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto=" .5 -.5 .01 .5  .5 .01" name="sideE" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto="-.5  .5 .01 .5  .5 .01" name="sideN" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto="-.5 -.5 .01 -.5 .5 .01" name="sideW" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto="0 0 0 0 0 0.02" name="root" rgba="0.9 0.4 0.6 1" size=".011" type="cylinder"/>
    <body name="body0" pos="0 0 .01">
      <geom fromto="0 0 0 0.1 0 0" name="link0" rgba="0.0 0.4 0.6 1" size=".01" type="capsule" contype="1" conaffinity="1"/>
      <joint axis="0 0 1" limited="false" name="joint0" pos="0 0 0" type="hinge"/>
      <body name="body1" pos="0.1 0 0">
        <joint axis="0 0 1" limited="true" name="joint1" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
        <geom fromto="0 0 0 0.1 0 0" name="link1" rgba="0.0 0.4 0.6 1" size=".01" type="capsule" contype="2" conaffinity="2"/>
        <body name="body2" pos="0.1 0 0">
          <joint axis="0 0 1" limited="true" name="joint2" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
          <geom fromto="0 0 0 0.1 0 0" name="link2" rgba="0.0 0.4 0.6 1" size=".01" type="capsule" contype="4" conaffinity="4"/>
          <body name="fingertip" pos="0.11 0 0">
            <geom contype="16" conaffinity="16" name="fingertip" pos="0 0 0" rgba="0.0 0.8 0.6 1" size=".01" type="sphere"/>
          </body>
        </body>
      </body>
    </body>
    <body name="target" pos=".2 -.2 .01">
      <joint armature="0" axis="1 0 0" damping="0" limited="true" name="target_x" pos="0 0 0" range="-.5 .5" ref=".2" stiffness="0" type="slide"/>
      <joint armature="0" axis="0 1 0" damping="0" limited="true" name="target_y" pos="0 0 0" range="-.5 .5" ref="-.2" stiffness="0" type="slide"/>
      <geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
    </body>
  </worldbody>
  <actuator>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint0"/>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint1"/>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint2"/>
  </actuator>
</mujoco>
"""

def get_4joint_xml():
    """4å…³èŠ‚XMLé…ç½®ï¼ˆç»Ÿä¸€åŠ¨åŠ›å­¦å‚æ•° + è‡ªç¢°æ’æ£€æµ‹ï¼‰"""
    return """
<mujoco model="4joint_reacher">
  <compiler angle="radian" inertiafromgeom="true"/>
  <default>
    <joint armature="1" damping="1" limited="true"/>
    <geom contype="1" conaffinity="1" friction="1 0.1 0.1" rgba="0.7 0.7 0 1" density="1000"/>
  </default>
  <contact>
    <pair geom1="link0" geom2="link2" condim="3"/>
    <pair geom1="link0" geom2="link3" condim="3"/>
    <pair geom1="link1" geom2="link3" condim="3"/>
  </contact>
  <option gravity="0 0 -9.81" integrator="RK4" timestep="0.01"/>
  <worldbody>
    <geom conaffinity="0" contype="0" name="ground" pos="0 0 0" rgba="0.9 0.9 0.9 1" size="0.6 0.6 10" type="plane"/>
    <geom conaffinity="0" contype="0" fromto="-.6 -.6 .01 .6 -.6 .01" name="sideS" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto=" .6 -.6 .01 .6  .6 .01" name="sideE" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto="-.6  .6 .01 .6  .6 .01" name="sideN" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto="-.6 -.6 .01 -.6 .6 .01" name="sideW" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto="0 0 0 0 0 0.02" name="root" rgba="0.9 0.4 0.6 1" size=".011" type="cylinder"/>
    <body name="body0" pos="0 0 .01">
      <geom fromto="0 0 0 0.08 0 0" name="link0" rgba="0.0 0.4 0.6 1" size=".01" type="capsule" contype="1" conaffinity="1"/>
      <joint axis="0 0 1" limited="false" name="joint0" pos="0 0 0" type="hinge"/>
      <body name="body1" pos="0.08 0 0">
        <joint axis="0 0 1" limited="true" name="joint1" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
        <geom fromto="0 0 0 0.08 0 0" name="link1" rgba="0.0 0.4 0.6 1" size=".01" type="capsule" contype="2" conaffinity="2"/>
        <body name="body2" pos="0.08 0 0">
          <joint axis="0 0 1" limited="true" name="joint2" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
          <geom fromto="0 0 0 0.08 0 0" name="link2" rgba="0.0 0.4 0.6 1" size=".01" type="capsule" contype="4" conaffinity="4"/>
          <body name="body3" pos="0.08 0 0">
            <joint axis="0 0 1" limited="true" name="joint3" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
            <geom fromto="0 0 0 0.08 0 0" name="link3" rgba="0.0 0.4 0.6 1" size=".01" type="capsule" contype="8" conaffinity="8"/>
            <body name="fingertip" pos="0.088 0 0">
              <geom contype="0" conaffinity="0" name="fingertip" pos="0 0 0" rgba="0.0 0.8 0.6 1" size=".01" type="sphere"/>
            </body>
          </body>
        </body>
      </body>
    </body>
    <body name="target" pos=".25 -.25 .01">
      <joint armature="0" axis="1 0 0" damping="0" limited="true" name="target_x" pos="0 0 0" range="-.6 .6" ref=".25" stiffness="0" type="slide"/>
      <joint armature="0" axis="0 1 0" damping="0" limited="true" name="target_y" pos="0 0 0" range="-.6 .6" ref="-.25" stiffness="0" type="slide"/>
      <geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
    </body>
  </worldbody>
  <actuator>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint0"/>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint1"/>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint2"/>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint3"/>
  </actuator>
</mujoco>
"""

class SimpleSetTransformerExtractor(BaseFeaturesExtractor):
    """ç®€åŒ–ç‰ˆSet-Transformerç‰¹å¾æå–å™¨"""
    
    def __init__(self, observation_space: gym.Space, features_dim: int = 256):
        super().__init__(observation_space, features_dim)
        
        # ç®€åŒ–çš„ç‰¹å¾æå–ç½‘ç»œ
        self.joint_encoder = nn.Sequential(
            nn.Linear(4, JOINT_TOKEN_DIM),  # q, q_dot, sin_q, cos_q
            nn.ReLU(),
            nn.LayerNorm(JOINT_TOKEN_DIM)
        )
        
        self.global_encoder = nn.Sequential(
            nn.Linear(4, GLOBAL_TOKEN_DIM),  # ee_x, ee_y, target_x, target_y
            nn.ReLU(),
            nn.LayerNorm(GLOBAL_TOKEN_DIM)
        )
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶
        token_dim = max(JOINT_TOKEN_DIM, GLOBAL_TOKEN_DIM)
        self.attention = nn.MultiheadAttention(token_dim, num_heads=2, dropout=DROPOUT, batch_first=True)
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šç›´æ¥ä»è§‚å¯Ÿç©ºé—´åˆ°ç‰¹å¾ç©ºé—´çš„æ˜ å°„
        obs_dim = observation_space.shape[0]
        self.output_proj = nn.Sequential(
            nn.Linear(obs_dim, HIDDEN_DIM),
            nn.ReLU(),
            nn.Linear(HIDDEN_DIM, features_dim)
        )
        
        print(f"ğŸ”§ SimpleSetTransformerExtractor: tokenç»´åº¦={token_dim}, è¾“å‡ºç»´åº¦={features_dim}")
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ç®€åŒ–çš„å‰å‘ä¼ æ’­ï¼Œé¿å…å¤æ‚çš„å¾ªç¯å’Œæ³¨æ„åŠ›è®¡ç®—
        # ç›´æ¥ä½¿ç”¨MLPå¤„ç†è§‚å¯Ÿï¼Œé¿å…Set-Transformerçš„å¤æ‚è®¡ç®—
        batch_size = observations.shape[0]
        obs_dim = observations.shape[1]
        
        # ğŸ”§ åŠ¨æ€é€‚åº”ä¸åŒè§‚å¯Ÿç»´åº¦ï¼šå¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œå¡«å……æˆ–æˆªæ–­
        expected_dim = self.output_proj[0].in_features
        if obs_dim != expected_dim:
            if obs_dim < expected_dim:
                # å¡«å……é›¶åˆ°æœŸæœ›ç»´åº¦
                padding = torch.zeros(batch_size, expected_dim - obs_dim, device=observations.device)
                observations = torch.cat([observations, padding], dim=1)
            else:
                # æˆªæ–­åˆ°æœŸæœ›ç»´åº¦
                observations = observations[:, :expected_dim]
        
        # ç®€å•çš„ç‰¹å¾æå–ï¼šç›´æ¥å¤„ç†æ•´ä¸ªè§‚å¯Ÿå‘é‡
        features = self.output_proj(observations)
        return features
    
    def forward_original(self, observations: torch.Tensor) -> torch.Tensor:
        batch_size = observations.shape[0]
        obs_dim = observations.shape[1]
        
        # åŠ¨æ€æ£€æµ‹å…³èŠ‚æ•°
        num_joints = (obs_dim - 4) // 3
        num_joints = min(max(num_joints, 2), 5)
        
        batch_features = []
        
        for i in range(batch_size):
            obs = observations[i]
            
            # æå–å…³èŠ‚ç‰¹å¾
            cos_q = obs[:num_joints]
            sin_q = obs[num_joints:2*num_joints]
            q_dot = obs[2*num_joints:3*num_joints]
            
            joint_tokens = []
            for j in range(num_joints):
                joint_feature = torch.tensor([
                    torch.atan2(sin_q[j], cos_q[j]).item(),
                    q_dot[j].item(),
                    sin_q[j].item(),
                    cos_q[j].item()
                ], dtype=torch.float32)
                joint_tokens.append(joint_feature)
            
            joint_tokens = torch.stack(joint_tokens)  # [num_joints, 4]
            joint_encoded = self.joint_encoder(joint_tokens)  # [num_joints, token_dim]
            
            # æå–å…¨å±€ç‰¹å¾
            ee_pos = obs[3*num_joints:3*num_joints+2]
            target_pos = obs[3*num_joints+2:3*num_joints+4]
            global_feature = torch.cat([ee_pos, target_pos])  # [4]
            global_encoded = self.global_encoder(global_feature.unsqueeze(0))  # [1, token_dim]
            
            # ç»„åˆtokens
            all_tokens = torch.cat([joint_encoded, global_encoded], dim=0)  # [num_joints+1, token_dim]
            
            # è‡ªæ³¨æ„åŠ›
            attn_output, _ = self.attention(
                all_tokens.unsqueeze(0), 
                all_tokens.unsqueeze(0), 
                all_tokens.unsqueeze(0)
            )
            attn_output = attn_output.squeeze(0)  # [num_joints+1, token_dim]
            
            # å…¨å±€æ± åŒ–
            pooled = torch.mean(attn_output, dim=0)  # [token_dim]
            batch_features.append(pooled)
        
        batch_features = torch.stack(batch_features)  # [batch_size, token_dim]
        return self.output_proj(batch_features)

# ä»baselineå¤åˆ¶çš„çœŸæ­£3å…³èŠ‚ç¯å¢ƒ
class Sequential3JointReacherEnv(MujocoEnv):
    """çœŸæ­£çš„3å…³èŠ‚Reacherç¯å¢ƒï¼ˆä»baselineå¤åˆ¶ï¼‰"""
    
    def __init__(self, render_mode=None, **kwargs):
        print("ğŸŒŸ Sequential3JointReacherEnv åˆå§‹åŒ–")
        
        self.num_joints = 3
        self.link_lengths = [0.1, 0.1, 0.1]
        self.max_reach = sum(self.link_lengths)
        
        # åˆ›å»ºä¸´æ—¶XMLæ–‡ä»¶
        self.xml_file = tempfile.NamedTemporaryFile(mode='w', suffix='.xml', delete=False)
        self.xml_file.write(get_3joint_xml())
        self.xml_file.flush()
        
        # è®¡ç®—è§‚å¯Ÿç©ºé—´ç»´åº¦
        obs_dim = self.num_joints * 3 + 4  # cos, sin, vel + ee_pos + target_pos
        observation_space = Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float64)
        
        super().__init__(
            self.xml_file.name,
            frame_skip=2,
            observation_space=observation_space,
            render_mode=render_mode,
            width=480,
            height=480
        )
        
        self.step_count = 0
        self.max_episode_steps = EPISODE_LENGTH
        
        print(f"âœ… 3å…³èŠ‚Reacheråˆ›å»ºå®Œæˆ")
        print(f"   é“¾é•¿: {self.link_lengths}")
        print(f"   ğŸ¯ å¯è¾¾åŠå¾„R: {self.max_reach:.3f}")
    
    def step(self, action):
        # ä½¿ç”¨æ ‡å‡†MuJoCoæ­¥éª¤
        self.do_simulation(action, self.frame_skip)
        
        observation = self._get_obs()
        
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåƒæ ‡å‡†Reacherä¸€æ ·åœ¨stepä¸­æ¸²æŸ“
        if self.render_mode == "human":
            self.render()
        
        # è®¡ç®—è·ç¦»
        fingertip_pos = self.get_body_com("fingertip")[:2]
        target_pos = self.get_body_com("target")[:2]
        distance = np.linalg.norm(fingertip_pos - target_pos)
        
        # ä½¿ç”¨GPT-5ç»Ÿä¸€å¥–åŠ±å‡½æ•°
        distance_penalty = -ALPHA_DISTANCE * distance
        control_penalty = -(BETA_CONTROL / self.num_joints) * np.sum(np.square(action))
        
        # ç®€åŒ–å¥–åŠ±ï¼Œæš‚æ—¶ä¸ä½¿ç”¨åŠ¨ä½œå¹³æ»‘
        reward = distance_penalty + control_penalty
        
        terminated = False
        self.step_count += 1
        truncated = self.step_count >= self.max_episode_steps
        
        info = {
            'distance_to_target': distance,
            'is_success': distance < SUCCESS_THRESHOLD,
            'fingertip_pos': fingertip_pos.copy(),
            'target_pos': target_pos.copy()
        }
        
        return observation, reward, terminated, truncated, info
    
    def _get_obs(self):
        # 3å…³èŠ‚çš„è§‚å¯Ÿï¼šcos, sin, vel, fingertip_pos, target_pos
        theta = self.data.qpos.flat[:self.num_joints]
        obs = np.concatenate([
            np.cos(theta),                    # 3ä¸ªcoså€¼
            np.sin(theta),                    # 3ä¸ªsinå€¼
            self.data.qvel.flat[:self.num_joints],  # 3ä¸ªå…³èŠ‚é€Ÿåº¦
            self.get_body_com("fingertip")[:2],     # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½® (x,y)
            self.get_body_com("target")[:2],        # ç›®æ ‡ä½ç½® (x,y)
        ])
        return obs
    
    def reset_model(self):
        # é‡ç½®ç­–ç•¥
        qpos = self.init_qpos + self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq)
        qvel = self.init_qvel.copy()
        
        # åªç»™æœºæ¢°è‡‚å…³èŠ‚æ·»åŠ éšæœºé€Ÿåº¦
        qvel[:self.num_joints] += self.np_random.standard_normal(self.num_joints) * 0.1
        
        # ç”Ÿæˆéšæœºç›®æ ‡ä½ç½®
        max_target_distance = self.max_reach * 0.85
        min_target_distance = 0.05
        
        target_distance = self.np_random.uniform(min_target_distance, max_target_distance)
        target_angle = self.np_random.uniform(-np.pi, np.pi)
        
        target_x = target_distance * np.cos(target_angle)
        target_y = target_distance * np.sin(target_angle)
        qpos[-2:] = [target_x, target_y]
        
        self.set_state(qpos, qvel)
        self.step_count = 0
        return self._get_obs()

# ä»baselineå¤åˆ¶çš„çœŸæ­£4å…³èŠ‚ç¯å¢ƒ
class Sequential4JointReacherEnv(MujocoEnv):
    """çœŸæ­£çš„4å…³èŠ‚Reacherç¯å¢ƒï¼ˆä»baselineå¤åˆ¶ï¼‰"""
    
    def __init__(self, render_mode=None, **kwargs):
        print("ğŸŒŸ Sequential4JointReacherEnv åˆå§‹åŒ–")
        
        self.num_joints = 4
        self.link_lengths = [0.08, 0.08, 0.08, 0.08]
        self.max_reach = sum(self.link_lengths)
        
        # åˆ›å»ºä¸´æ—¶XMLæ–‡ä»¶
        self.xml_file = tempfile.NamedTemporaryFile(mode='w', suffix='.xml', delete=False)
        self.xml_file.write(get_4joint_xml())
        self.xml_file.flush()
        
        # è®¡ç®—è§‚å¯Ÿç©ºé—´ç»´åº¦
        obs_dim = self.num_joints * 3 + 4  # cos, sin, vel + ee_pos + target_pos
        observation_space = Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float64)
        
        super().__init__(
            self.xml_file.name,
            frame_skip=2,
            observation_space=observation_space,
            render_mode=render_mode,
            width=480,
            height=480
        )
        
        self.step_count = 0
        self.max_episode_steps = EPISODE_LENGTH
        
        print(f"âœ… 4å…³èŠ‚Reacheråˆ›å»ºå®Œæˆ")
        print(f"   é“¾é•¿: {self.link_lengths}")
        print(f"   ğŸ¯ å¯è¾¾åŠå¾„R: {self.max_reach:.3f}")
    
    def step(self, action):
        # ä½¿ç”¨æ ‡å‡†MuJoCoæ­¥éª¤
        self.do_simulation(action, self.frame_skip)
        
        observation = self._get_obs()
        
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåƒæ ‡å‡†Reacherä¸€æ ·åœ¨stepä¸­æ¸²æŸ“
        if self.render_mode == "human":
            self.render()
        
        # è®¡ç®—è·ç¦»
        fingertip_pos = self.get_body_com("fingertip")[:2]
        target_pos = self.get_body_com("target")[:2]
        distance = np.linalg.norm(fingertip_pos - target_pos)
        
        # ä½¿ç”¨GPT-5ç»Ÿä¸€å¥–åŠ±å‡½æ•°
        distance_penalty = -ALPHA_DISTANCE * distance
        control_penalty = -(BETA_CONTROL / self.num_joints) * np.sum(np.square(action))
        
        # ç®€åŒ–å¥–åŠ±ï¼Œæš‚æ—¶ä¸ä½¿ç”¨åŠ¨ä½œå¹³æ»‘
        reward = distance_penalty + control_penalty
        
        terminated = False
        self.step_count += 1
        truncated = self.step_count >= self.max_episode_steps
        
        info = {
            'distance_to_target': distance,
            'is_success': distance < SUCCESS_THRESHOLD,
            'fingertip_pos': fingertip_pos.copy(),
            'target_pos': target_pos.copy()
        }
        
        return observation, reward, terminated, truncated, info
    
    def _get_obs(self):
        # 4å…³èŠ‚çš„è§‚å¯Ÿï¼šcos, sin, vel, fingertip_pos, target_pos
        theta = self.data.qpos.flat[:self.num_joints]
        obs = np.concatenate([
            np.cos(theta),                    # 4ä¸ªcoså€¼
            np.sin(theta),                    # 4ä¸ªsinå€¼
            self.data.qvel.flat[:self.num_joints],  # 4ä¸ªå…³èŠ‚é€Ÿåº¦
            self.get_body_com("fingertip")[:2],     # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½® (x,y)
            self.get_body_com("target")[:2],        # ç›®æ ‡ä½ç½® (x,y)
        ])
        return obs
    
    def reset_model(self):
        # é‡ç½®ç­–ç•¥
        qpos = self.init_qpos + self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq)
        qvel = self.init_qvel.copy()
        
        # åªç»™æœºæ¢°è‡‚å…³èŠ‚æ·»åŠ éšæœºé€Ÿåº¦
        qvel[:self.num_joints] += self.np_random.standard_normal(self.num_joints) * 0.1
        
        # ç”Ÿæˆéšæœºç›®æ ‡ä½ç½®
        max_target_distance = self.max_reach * 0.85
        min_target_distance = 0.05
        
        target_distance = self.np_random.uniform(min_target_distance, max_target_distance)
        target_angle = self.np_random.uniform(-np.pi, np.pi)
        
        target_x = target_distance * np.cos(target_angle)
        target_y = target_distance * np.sin(target_angle)
        qpos[-2:] = [target_x, target_y]
        
        self.set_state(qpos, qvel)
        self.step_count = 0
        return self._get_obs()

class UnifiedReacherEnv(gym.Wrapper):
    """ç»Ÿä¸€çš„Reacherç¯å¢ƒï¼Œæ”¯æŒ2-5å…³èŠ‚"""
    
    def __init__(self, base_env, num_joints):
        super().__init__(base_env)
        self.num_joints = num_joints
        self.last_action = None
        self.success_count = 0
        self.success_threshold_steps = 10
        self.step_count = 0
        
        print(f"ğŸ”§ UnifiedReacherEnv: {num_joints}å…³èŠ‚ï¼Œç»Ÿä¸€å¥–åŠ±å‡½æ•°")
    
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.last_action = None
        self.success_count = 0
        self.step_count = 0
        return obs, info
    
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šé™ä½æ¸²æŸ“é¢‘ç‡ï¼Œæ¯5æ­¥æ¸²æŸ“ä¸€æ¬¡
        if hasattr(self.env, 'render_mode') and self.env.render_mode == "human":
            if self.step_count % 5 == 0:  # æ¯5æ­¥æ¸²æŸ“ä¸€æ¬¡ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦
                self.env.render()
        
        # é‡æ–°è®¡ç®—ç»Ÿä¸€å¥–åŠ±
        if 'distance_to_target' in info:
            distance = info['distance_to_target']
            
            # GPT-5ç»Ÿä¸€å¥–åŠ±å‡½æ•°
            distance_penalty = -ALPHA_DISTANCE * distance
            control_penalty = -(BETA_CONTROL / self.num_joints) * np.sum(np.square(action))
            
            smooth_penalty = 0.0
            if self.last_action is not None:
                action_diff = action - self.last_action
                smooth_penalty = -(GAMMA_SMOOTH / self.num_joints) * np.sum(np.square(action_diff))
            
            success_reward = 0.0
            if distance < SUCCESS_THRESHOLD:
                self.success_count += 1
                if self.success_count >= self.success_threshold_steps:
                    success_reward = SUCCESS_REWARD
            else:
                self.success_count = 0
            
            # æ›´æ–°å¥–åŠ±å’ŒæˆåŠŸåˆ¤æ–­
            reward = distance_penalty + control_penalty + smooth_penalty + success_reward
            info['is_success'] = distance < SUCCESS_THRESHOLD
            
            self.last_action = action.copy()
        
        self.step_count += 1
        if self.step_count >= EPISODE_LENGTH:
            truncated = True
        
        return obs, reward, terminated, truncated, info

def create_unified_env(num_joints, render_mode=None):
    """åˆ›å»ºç»Ÿä¸€çš„Reacherç¯å¢ƒ"""
    if num_joints == 2:
        base_env = gym.make('Reacher-v5', render_mode=render_mode)
        env = UnifiedReacherEnv(base_env, num_joints)
    elif num_joints == 3:
        # ğŸ¯ ä½¿ç”¨çœŸæ­£çš„3å…³èŠ‚ç¯å¢ƒï¼
        env = Sequential3JointReacherEnv(render_mode=render_mode)
    elif num_joints == 4:
        # ğŸ¯ ä½¿ç”¨çœŸæ­£çš„4å…³èŠ‚ç¯å¢ƒï¼
        env = Sequential4JointReacherEnv(render_mode=render_mode)
    else:
        # å¯¹äº5å…³èŠ‚ï¼Œæš‚æ—¶ä½¿ç”¨2å…³èŠ‚ç¯å¢ƒ
        print(f"âš ï¸ ç®€åŒ–ç‰ˆï¼š{num_joints}å…³èŠ‚æš‚æ—¶ä½¿ç”¨2å…³èŠ‚ç¯å¢ƒä»£æ›¿")
        base_env = gym.make('Reacher-v5', render_mode=render_mode)
        env = UnifiedReacherEnv(base_env, num_joints)
    
    return Monitor(env)

class RandomJointVecEnv(DummyVecEnv):
    """éšæœºå…³èŠ‚æ•°çš„å‘é‡åŒ–ç¯å¢ƒ"""
    
    def __init__(self, n_envs=4, render_mode=None):
        self.joint_numbers = [2, 3, 4, 5]
        self.joint_probs = [0.25, 0.25, 0.25, 0.25]
        self.render_mode = render_mode
        
        # åˆ›å»ºç¯å¢ƒå‡½æ•°
        def make_env():
            # éšæœºé€‰æ‹©å…³èŠ‚æ•°
            num_joints = np.random.choice(self.joint_numbers, p=self.joint_probs)
            return create_unified_env(num_joints, render_mode=self.render_mode)
        
        env_fns = [make_env for _ in range(n_envs)]
        super().__init__(env_fns)
        
        render_info = f"æ¸²æŸ“={'å¼€å¯' if render_mode else 'å…³é—­'}"
        print(f"ğŸ”§ RandomJointVecEnv: {n_envs}ä¸ªå¹¶è¡Œç¯å¢ƒï¼Œéšæœºå…³èŠ‚æ•°{self.joint_numbers}ï¼Œ{render_info}")

def train_gpt5_unified_model(total_timesteps=50000):
    """è®­ç»ƒGPT-5ç»Ÿä¸€ç­–ç•¥æ¨¡å‹"""
    print("ğŸŒŸ GPT-5ç»Ÿä¸€ç­–ç•¥è®­ç»ƒå¼€å§‹")
    print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {total_timesteps:,}")
    print(f"ğŸ¯ ç»Ÿä¸€æˆåŠŸé˜ˆå€¼: {SUCCESS_THRESHOLD}m")
    print(f"ğŸ¯ æ¸²æŸ“æ¨¡å¼: å¼€å¯ï¼Œå•ç¯å¢ƒè®­ç»ƒç¡®ä¿æµç•…æ€§")
    print("="*60)
    
    # ğŸ¯ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å•ä¸ªç¯å¢ƒè€Œä¸æ˜¯å‘é‡åŒ–ç¯å¢ƒï¼Œç¡®ä¿æ¸²æŸ“æµç•…
    # å‚è€ƒbaselineçš„åšæ³•ï¼Œç›´æ¥åˆ›å»ºå•ä¸ªç¯å¢ƒ
    train_env = create_unified_env(num_joints=4, render_mode='human')  # æµ‹è¯•4å…³èŠ‚
    
    # åˆ›å»ºæ¨¡å‹
    policy_kwargs = {
        'features_extractor_class': SimpleSetTransformerExtractor,
        'features_extractor_kwargs': {'features_dim': HIDDEN_DIM},
    }
    
    model = SAC(
        'MlpPolicy',
        train_env,
        policy_kwargs=policy_kwargs,
        verbose=1,
        learning_rate=3e-4,
        batch_size=256,
        buffer_size=100000,
        learning_starts=1000,
        gamma=0.99,
        tau=0.005,
        ent_coef='auto',
        target_entropy='auto',
        tensorboard_log=None  # ç¦ç”¨tensorboardæ—¥å¿—
    )
    
    print("âœ… GPT-5ç»Ÿä¸€ç­–ç•¥æ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # å¼€å§‹è®­ç»ƒ
    try:
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            log_interval=50,  # ğŸš€ å‡å°‘æ—¥å¿—é¢‘ç‡ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        
        # ä¿å­˜æ¨¡å‹
        model_path = "models/gpt5_unified_4joint_reacher"
        model.save(model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        return model, training_time
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        model_path = "models/gpt5_unified_4joint_reacher_interrupted"
        model.save(model_path)
        return model, 0
    
    finally:
        train_env.close()

def test_gpt5_unified_model(model, n_eval_episodes=20):
    """æµ‹è¯•GPT-5ç»Ÿä¸€ç­–ç•¥æ¨¡å‹"""
    print(f"\nğŸ§ª æµ‹è¯•GPT-5ç»Ÿä¸€ç­–ç•¥æ¨¡å‹")
    print(f"ğŸ“Š æ¯ä¸ªå…³èŠ‚æ•°æµ‹è¯•{n_eval_episodes}ä¸ªepisodes")
    print("-"*40)
    
    all_results = []
    
    # ğŸ”§ æš‚æ—¶åªæµ‹è¯•4å…³èŠ‚ï¼Œå› ä¸ºæ¨¡å‹æ˜¯åœ¨4å…³èŠ‚ä¸Šè®­ç»ƒçš„
    for num_joints in [4]:
        print(f"\nğŸ”§ æµ‹è¯•{num_joints}å…³èŠ‚...")
        
        test_env = create_unified_env(num_joints, render_mode='human')
        
        success_count = 0
        episode_rewards = []
        episode_distances = []
        
        for episode in range(n_eval_episodes):
            obs, info = test_env.reset()
            episode_reward = 0
            episode_success = False
            min_distance = float('inf')
            
            for step in range(EPISODE_LENGTH):
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = test_env.step(action)
                episode_reward += reward
                
                distance = info.get('distance_to_target', float('inf'))
                min_distance = min(min_distance, distance)
                
                if info.get('is_success', False):
                    episode_success = True
                
                if terminated or truncated:
                    break
            
            if episode_success:
                success_count += 1
            
            episode_rewards.append(episode_reward)
            episode_distances.append(min_distance)
        
        success_rate = success_count / n_eval_episodes
        avg_reward = np.mean(episode_rewards)
        avg_distance = np.mean(episode_distances)
        
        result = {
            'num_joints': num_joints,
            'success_rate': success_rate,
            'avg_reward': avg_reward,
            'avg_distance': avg_distance
        }
        all_results.append(result)
        
        print(f"   æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{n_eval_episodes})")
        print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"   å¹³å‡è·ç¦»: {avg_distance:.4f}")
        
        test_env.close()
    
    # æ€»ç»“
    print(f"\nğŸ¯ GPT-5ç»Ÿä¸€ç­–ç•¥æµ‹è¯•æ€»ç»“:")
    print("-"*50)
    for result in all_results:
        print(f"{result['num_joints']}å…³èŠ‚: æˆåŠŸç‡{result['success_rate']:.1%}, å¥–åŠ±{result['avg_reward']:.1f}, è·ç¦»{result['avg_distance']:.3f}")
    
    return all_results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ GPT-5ç»Ÿä¸€ç­–ç•¥ç®€åŒ–ç‰ˆè®­ç»ƒç³»ç»Ÿ")
    print("ğŸ¤– å•ä¸€æ¨¡å‹æ”¯æŒ2-5å…³èŠ‚Reacher")
    print("ğŸ¯ Set-Transformer + ç»Ÿä¸€å¥–åŠ±å‡½æ•°")
    print()
    
    # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
    os.makedirs("models", exist_ok=True)
    os.makedirs("tensorboard_logs", exist_ok=True)
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆå‡å°‘æ­¥æ•°å…ˆæµ‹è¯•æ€§èƒ½ï¼‰
    model, training_time = train_gpt5_unified_model(total_timesteps=50000)
    
    # æµ‹è¯•æ¨¡å‹
    results = test_gpt5_unified_model(model, n_eval_episodes=10)
    
    print(f"\nğŸ‰ GPT-5ç»Ÿä¸€ç­–ç•¥è®­ç»ƒå®Œæˆ!")
    print(f"   è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
    print(f"   æ”¯æŒå…³èŠ‚æ•°: 2-5 (å•ä¸€æ¨¡å‹)")
    
    # åˆ†æç»“æœ
    success_rates = [r['success_rate'] for r in results]
    avg_success_rate = np.mean(success_rates)
    success_std = np.std(success_rates)
    
    print(f"\nğŸ“Š è·¨å…³èŠ‚æ•°ä¸€è‡´æ€§åˆ†æ:")
    print(f"   å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1%}")
    print(f"   æˆåŠŸç‡æ ‡å‡†å·®: {success_std:.3f} (è¶Šå°è¶Šä¸€è‡´)")
    
    if success_std < 0.1:
        print(f"   âœ… è·¨å…³èŠ‚æ•°ä¸€è‡´æ€§å¾ˆå¥½!")
    elif success_std < 0.2:
        print(f"   âš ï¸ è·¨å…³èŠ‚æ•°ä¸€è‡´æ€§ä¸€èˆ¬")
    else:
        print(f"   âŒ è·¨å…³èŠ‚æ•°ä¸€è‡´æ€§è¾ƒå·®")

if __name__ == "__main__":
    main()
