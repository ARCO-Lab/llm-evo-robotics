#!/usr/bin/env python3
"""
ç®€å•çš„Baseline SACè®­ç»ƒ3å…³èŠ‚Reacher
ä½¿ç”¨ç°æœ‰ç¯å¢ƒä½†ç§»é™¤æ‰€æœ‰è‡ªå®šä¹‰ç»„ä»¶
"""

import gymnasium as gym
import numpy as np
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from complete_sequential_training_with_evaluation import create_env

def train_simple_baseline():
    """è®­ç»ƒç®€å•baseline SACï¼Œç§»é™¤æ‰€æœ‰è‡ªå®šä¹‰ç»„ä»¶"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒç®€å•Baseline SAC 3å…³èŠ‚Reacher")
    print("ğŸ“‹ é…ç½®:")
    print("  - ä½¿ç”¨ç°æœ‰3å…³èŠ‚ç¯å¢ƒ")
    print("  - ç§»é™¤è‡ªå®šä¹‰ç‰¹å¾æå–å™¨")
    print("  - ä½¿ç”¨æ ‡å‡†SACé»˜è®¤å‚æ•°")
    print("  - æ ‡å‡†MlpPolicy")
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_env(3, render_mode=None)
    print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"   è§‚å¯Ÿç©ºé—´: {env.observation_space.shape}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space.shape}")
    
    # åˆ›å»ºçº¯baseline SACæ¨¡å‹ - ä¸ä½¿ç”¨ä»»ä½•è‡ªå®šä¹‰ç»„ä»¶
    model = SAC(
        'MlpPolicy',  # æ ‡å‡†MLPç­–ç•¥ï¼Œä¸ä½¿ç”¨è‡ªå®šä¹‰ç‰¹å¾æå–å™¨
        env,
        verbose=1,
        learning_rate=3e-4,    # æ ‡å‡†å­¦ä¹ ç‡
        buffer_size=1000000,   # æ ‡å‡†buffer
        batch_size=256,        # æ ‡å‡†batchå¤§å°
        tau=0.005,            # æ ‡å‡†è½¯æ›´æ–°å‚æ•°
        gamma=0.99,           # æ ‡å‡†æŠ˜æ‰£å› å­
        train_freq=1,         # æ ‡å‡†è®­ç»ƒé¢‘ç‡
        gradient_steps=1,     # æ ‡å‡†æ¢¯åº¦æ­¥æ•°
        # ä¸ä¼ å…¥ä»»ä½•policy_kwargsï¼Œä½¿ç”¨å®Œå…¨é»˜è®¤çš„ç½‘ç»œç»“æ„
    )
    
    print("âœ… ç®€å•Baseline SACæ¨¡å‹åˆ›å»ºå®Œæˆ")
    print("   - ä½¿ç”¨æ ‡å‡†MlpPolicy")
    print("   - é»˜è®¤ç½‘ç»œç»“æ„: [256, 256]")
    print("   - æ— è‡ªå®šä¹‰ç‰¹å¾æå–å™¨")
    print("   - æ‰€æœ‰å‚æ•°ä¸ºSACé»˜è®¤å€¼")
    
    # è®­ç»ƒ
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ10000æ­¥...")
    model.learn(total_timesteps=10000, progress_bar=True)
    
    # ä¿å­˜æ¨¡å‹
    model.save('models/simple_baseline_3joint_sac')
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: models/simple_baseline_3joint_sac.zip")
    
    # æµ‹è¯•
    print("\nğŸ§ª æµ‹è¯•ç®€å•baselineæ¨¡å‹...")
    success_count = 0
    rewards = []
    distances = []
    
    for i in range(10):
        obs, info = env.reset()
        episode_reward = 0
        episode_success = False
        min_distance = float('inf')
        
        target_pos = info.get('target_pos', [0, 0])
        initial_distance = info.get('distance_to_target', 0)
        
        for step in range(100):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            
            distance = info.get('distance_to_target', 0)
            min_distance = min(min_distance, distance)
            
            if info.get('is_success', False):
                episode_success = True
            
            if terminated or truncated:
                break
        
        if episode_success:
            success_count += 1
        
        rewards.append(episode_reward)
        distances.append(min_distance)
        
        target_dist = np.linalg.norm(target_pos)
        improvement = initial_distance - min_distance
        print(f"  Episode {i+1}: ç›®æ ‡è·ç¦»={target_dist:.3f}, æ”¹å–„={improvement:.4f}, å¥–åŠ±={episode_reward:.1f}, æˆåŠŸ={'âœ…' if episode_success else 'âŒ'}")
    
    print(f"\nğŸ“Š ç®€å•Baselineæµ‹è¯•ç»“æœ:")
    print(f"  æˆåŠŸç‡: {success_count/10:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(rewards):.1f}")
    print(f"  å¹³å‡æœ€å°è·ç¦»: {np.mean(distances):.4f}")
    print(f"  å¥–åŠ±èŒƒå›´: [{min(rewards):.1f}, {max(rewards):.1f}]")
    
    env.close()
    return model

def compare_with_custom_model():
    """å¯¹æ¯”è‡ªå®šä¹‰æ¨¡å‹å’Œbaselineæ¨¡å‹"""
    print("\nğŸ” å¯¹æ¯”åˆ†æ:")
    print("=" * 60)
    
    # åŠ è½½å¹¶æµ‹è¯•è‡ªå®šä¹‰æ¨¡å‹
    print("\n1ï¸âƒ£ æµ‹è¯•è‡ªå®šä¹‰æ¨¡å‹ (SpecializedJointExtractor)")
    try:
        from stable_baselines3 import SAC
        custom_model = SAC.load('models/complete_sequential_3joint_reacher.zip')
        env = create_env(3, render_mode=None)
        
        custom_rewards = []
        custom_success = 0
        
        for i in range(5):
            obs, info = env.reset()
            episode_reward = 0
            episode_success = False
            
            for step in range(100):
                action, _ = custom_model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                episode_reward += reward
                
                if info.get('is_success', False):
                    episode_success = True
                
                if terminated or truncated:
                    break
            
            if episode_success:
                custom_success += 1
            custom_rewards.append(episode_reward)
        
        print(f"   è‡ªå®šä¹‰æ¨¡å‹æˆåŠŸç‡: {custom_success/5:.1%}")
        print(f"   è‡ªå®šä¹‰æ¨¡å‹å¹³å‡å¥–åŠ±: {np.mean(custom_rewards):.1f}")
        
        env.close()
        
    except Exception as e:
        print(f"   âŒ è‡ªå®šä¹‰æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•baselineæ¨¡å‹
    print("\n2ï¸âƒ£ æµ‹è¯•Baselineæ¨¡å‹ (æ ‡å‡†MlpPolicy)")
    try:
        baseline_model = SAC.load('models/simple_baseline_3joint_sac.zip')
        env = create_env(3, render_mode=None)
        
        baseline_rewards = []
        baseline_success = 0
        
        for i in range(5):
            obs, info = env.reset()
            episode_reward = 0
            episode_success = False
            
            for step in range(100):
                action, _ = baseline_model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                episode_reward += reward
                
                if info.get('is_success', False):
                    episode_success = True
                
                if terminated or truncated:
                    break
            
            if episode_success:
                baseline_success += 1
            baseline_rewards.append(episode_reward)
        
        print(f"   Baselineæ¨¡å‹æˆåŠŸç‡: {baseline_success/5:.1%}")
        print(f"   Baselineæ¨¡å‹å¹³å‡å¥–åŠ±: {np.mean(baseline_rewards):.1f}")
        
        env.close()
        
    except Exception as e:
        print(f"   âŒ Baselineæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
    
    print("\nğŸ“‹ å¯¹æ¯”æ€»ç»“:")
    print("   å¦‚æœBaselineæ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œè¯´æ˜è‡ªå®šä¹‰ç‰¹å¾æå–å™¨å¯èƒ½æœ‰é—®é¢˜")
    print("   å¦‚æœè‡ªå®šä¹‰æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œè¯´æ˜ç‰¹å¾æå–å™¨æ˜¯æœ‰æ•ˆçš„")

if __name__ == "__main__":
    # è®­ç»ƒbaselineæ¨¡å‹
    train_simple_baseline()
    
    # å¯¹æ¯”åˆ†æ
    compare_with_custom_model()

