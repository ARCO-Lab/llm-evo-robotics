#!/usr/bin/env python3
"""
çœŸæ­£çš„MADDPGå®ç°ï¼š
1. æ¯ä¸ªå…³èŠ‚ç”±ä¸€ä¸ªDDPGæ™ºèƒ½ä½“æ§åˆ¶
2. ä¸­å¿ƒåŒ–è®­ç»ƒï¼Œåˆ†å¸ƒå¼æ‰§è¡Œ
3. æ‰€æœ‰æ™ºèƒ½ä½“åœ¨åŒä¸€ä¸ªç¯å¢ƒä¸­åä½œ
4. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
"""

import os
import numpy as np
import gymnasium as gym
from stable_baselines3 import DDPG
from stable_baselines3.common.noise import NormalActionNoise
from stable_baselines3.common.monitor import Monitor
import time

# è®¾ç½®æ¸²æŸ“ç¯å¢ƒå˜é‡
os.environ['MUJOCO_GL'] = 'glfw'
os.environ['MUJOCO_RENDERER'] = 'glfw'

# å¯¼å…¥ç¯å¢ƒ
from maddpg_complete_sequential_training import (
    create_env, 
    SUCCESS_THRESHOLD_RATIO, 
    SUCCESS_THRESHOLD_2JOINT
)

class TrueMADDPG:
    """çœŸæ­£çš„MADDPGå®ç° - å¤šæ™ºèƒ½ä½“åä½œ"""
    
    def __init__(self, num_joints=3):
        self.num_joints = num_joints
        self.agents = []
        
        print(f"ğŸŒŸ åˆ›å»ºçœŸæ­£çš„MADDPGç³»ç»Ÿ: {num_joints}ä¸ªæ™ºèƒ½ä½“åä½œ")
        
        # åˆ›å»ºä¸»ç¯å¢ƒç”¨äºåä½œè®­ç»ƒå’Œå¯è§†åŒ–
        self.env = create_env(num_joints, render_mode='human', show_position_info=True)
        
        # ä¸ºæ¯ä¸ªå…³èŠ‚åˆ›å»ºç‹¬ç«‹çš„DDPGæ™ºèƒ½ä½“
        for i in range(num_joints):
            print(f"ğŸ¤– åˆ›å»ºæ™ºèƒ½ä½“ {i} (æ§åˆ¶å…³èŠ‚{i})...")
            
            # åˆ›å»ºå•å…³èŠ‚ç¯å¢ƒåŒ…è£…å™¨
            single_env = SingleJointEnvWrapper(self.env, joint_id=i, num_joints=num_joints)
            
            # æ·»åŠ åŠ¨ä½œå™ªå£°
            action_noise = NormalActionNoise(
                mean=np.zeros(1), 
                sigma=0.1 * np.ones(1)
            )
            
            # åˆ›å»ºDDPGæ™ºèƒ½ä½“ï¼ˆå¼€å¯è¯¦ç»†æ—¥å¿—ï¼‰
            agent = DDPG(
                "MlpPolicy",
                single_env,
                learning_rate=1e-3,
                gamma=0.99,
                tau=0.005,
                action_noise=action_noise,
                verbose=1,  # å¼€å¯è¯¦ç»†è¾“å‡º
                device='cpu',
                batch_size=64,
                buffer_size=50000,
                learning_starts=100,
                tensorboard_log=f"./tensorboard_logs/maddpg_agent_{i}/"  # æ·»åŠ TensorBoardæ—¥å¿—
            )
            
            self.agents.append(agent)
            print(f"   âœ… æ™ºèƒ½ä½“ {i} åˆ›å»ºå®Œæˆ")
        
        print(f"ğŸŒŸ MADDPGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ: {num_joints}ä¸ªæ™ºèƒ½ä½“å°†åä½œæ§åˆ¶æœºæ¢°è‡‚")
    
    def train_collaborative(self, total_timesteps=5000):
        """åä½œè®­ç»ƒæ‰€æœ‰æ™ºèƒ½ä½“"""
        print(f"\nğŸ¯ å¼€å§‹MADDPGåä½œè®­ç»ƒ {total_timesteps} æ­¥...")
        print(f"   æ¯ä¸ªæ™ºèƒ½ä½“æ§åˆ¶ä¸€ä¸ªå…³èŠ‚ï¼Œå…±åŒå­¦ä¹ æœ€ä¼˜ç­–ç•¥")
        
        obs, _ = self.env.reset()
        episode_count = 0
        episode_reward = 0
        step_count = 0
        
        # å­˜å‚¨ç»éªŒç”¨äºè®­ç»ƒ
        experiences = []
        
        start_time = time.time()
        
        try:
            while step_count < total_timesteps:
                # ğŸ¤– æ‰€æœ‰æ™ºèƒ½ä½“åŒæ—¶å†³ç­–
                actions = []
                for i, agent in enumerate(self.agents):
                    # æ¯ä¸ªæ™ºèƒ½ä½“åŸºäºå…¨å±€è§‚å¯Ÿåšå†³ç­–
                    action, _ = agent.predict(obs, deterministic=False)
                    actions.append(action[0])  # å–å‡ºæ ‡é‡åŠ¨ä½œ
                
                # ğŸ¯ æ‰§è¡Œè”åˆåŠ¨ä½œ
                joint_action = np.array(actions)
                next_obs, reward, terminated, truncated, info = self.env.step(joint_action)
                episode_reward += reward
                step_count += 1
                
                # ğŸ“ å­˜å‚¨ç»éªŒ
                experiences.append({
                    'obs': obs.copy(),
                    'actions': actions.copy(),
                    'reward': reward,
                    'next_obs': next_obs.copy(),
                    'done': terminated or truncated
                })
                
                # ğŸ“ å®šæœŸè®­ç»ƒæ™ºèƒ½ä½“
                if len(experiences) >= 100 and step_count % 50 == 0:
                    print(f"   ğŸ“š æ­¥æ•° {step_count}: å¼€å§‹è®­ç»ƒæ™ºèƒ½ä½“...")
                    self._train_agents(experiences[-100:])  # ä½¿ç”¨æœ€è¿‘100ä¸ªç»éªŒ
                
                obs = next_obs
                
                # ğŸ”„ é‡ç½®ç¯å¢ƒ
                if terminated or truncated:
                    obs, _ = self.env.reset()
                    episode_count += 1
                    
                    if episode_count % 5 == 0:
                        avg_reward = episode_reward
                        success = info.get('is_success', False) if info else False
                        distance = info.get('distance_to_target', 0) if info else 0
                        print(f"   Episode {episode_count}: å¥–åŠ±={avg_reward:.2f}, è·ç¦»={distance:.4f}, æˆåŠŸ={'âœ…' if success else 'âŒ'}")
                    
                    episode_reward = 0
                
                # ğŸ“Š å®šæœŸè¾“å‡ºè¿›åº¦
                if step_count % 1000 == 0:
                    elapsed_time = time.time() - start_time
                    fps = step_count / elapsed_time
                    print(f"   ğŸš€ æ­¥æ•° {step_count}/{total_timesteps}, FPS: {fps:.1f}, ç”¨æ—¶: {elapsed_time:.1f}s")
        
        except KeyboardInterrupt:
            print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        training_time = time.time() - start_time
        print(f"\nâœ… MADDPGåä½œè®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ¯ æ€»episodes: {episode_count}")
        print(f"ğŸ“Š å¹³å‡FPS: {step_count/training_time:.1f}")
        
        return episode_count
    
    def _train_agents(self, experiences):
        """è®­ç»ƒæ‰€æœ‰æ™ºèƒ½ä½“å¹¶æ˜¾ç¤ºlossä¿¡æ¯"""
        # ç®€åŒ–ç‰ˆè®­ç»ƒï¼šæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹å­¦ä¹ 
        for i, agent in enumerate(self.agents):
            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å‡†å¤‡è®­ç»ƒæ•°æ®
            for exp in experiences[-10:]:  # ä½¿ç”¨æœ€è¿‘10ä¸ªç»éªŒ
                # æ„é€ å•å…³èŠ‚çš„ç»éªŒ
                single_action = [exp['actions'][i]]
                
                # æ·»åŠ åˆ°æ™ºèƒ½ä½“çš„replay buffer
                try:
                    if hasattr(agent.replay_buffer, 'add'):
                        agent.replay_buffer.add(
                            exp['obs'], 
                            single_action, 
                            exp['reward'], 
                            exp['next_obs'], 
                            exp['done'], 
                            [{}]
                        )
                except Exception as e:
                    # å¦‚æœæ·»åŠ ç»éªŒå¤±è´¥ï¼Œè·³è¿‡
                    continue
            
            # è®­ç»ƒæ™ºèƒ½ä½“å¹¶è·å–lossä¿¡æ¯
            try:
                buffer_size = 0
                if hasattr(agent.replay_buffer, 'size'):
                    buffer_size = agent.replay_buffer.size()
                elif hasattr(agent.replay_buffer, '__len__'):
                    buffer_size = len(agent.replay_buffer)
                
                if buffer_size > agent.batch_size:
                    # ä¿å­˜è®­ç»ƒå‰çš„å‚æ•°ä»¥è®¡ç®—losså˜åŒ–
                    old_actor_loss = getattr(agent, '_last_actor_loss', 0.0)
                    old_critic_loss = getattr(agent, '_last_critic_loss', 0.0)
                    
                    # æ‰§è¡Œè®­ç»ƒ
                    agent.train(gradient_steps=1)
                    
                    # å°è¯•è·å–æœ€æ–°çš„lossä¿¡æ¯
                    current_actor_loss = getattr(agent, '_last_actor_loss', old_actor_loss)
                    current_critic_loss = getattr(agent, '_last_critic_loss', old_critic_loss)
                    
                    # æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
                    print(f"     ğŸ¤– Agent {i}: Buffer={buffer_size}, Actor_Loss={current_actor_loss:.4f}, Critic_Loss={current_critic_loss:.4f}")
                    
            except Exception as e:
                # å¦‚æœè®­ç»ƒå¤±è´¥ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                print(f"     âŒ Agent {i} è®­ç»ƒå¤±è´¥: {str(e)[:50]}...")
                continue
    
    def test(self, n_episodes=5):
        """æµ‹è¯•è®­ç»ƒå¥½çš„MADDPGç³»ç»Ÿ"""
        print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•MADDPGç³»ç»Ÿ {n_episodes} episodes...")
        
        success_count = 0
        episode_rewards = []
        episode_distances = []
        
        for episode in range(n_episodes):
            obs, info = self.env.reset()
            episode_reward = 0
            episode_success = False
            min_distance = float('inf')
            
            print(f"\n   ğŸ® Episode {episode+1} å¼€å§‹...")
            
            for step in range(100):  # æ¯ä¸ªepisodeæœ€å¤š100æ­¥
                # ğŸ¤– æ‰€æœ‰æ™ºèƒ½ä½“åä½œå†³ç­–ï¼ˆç¡®å®šæ€§ï¼‰
                actions = []
                for i, agent in enumerate(self.agents):
                    action, _ = agent.predict(obs, deterministic=True)
                    actions.append(action[0])
                
                # ğŸ¯ æ‰§è¡Œè”åˆåŠ¨ä½œ
                joint_action = np.array(actions)
                obs, reward, terminated, truncated, info = self.env.step(joint_action)
                episode_reward += reward
                
                # ğŸ“Š æ£€æŸ¥æˆåŠŸå’Œè·ç¦»
                distance = info.get('distance_to_target', float('inf'))
                min_distance = min(min_distance, distance)
                
                if info.get('is_success', False):
                    episode_success = True
                
                if terminated or truncated:
                    break
            
            if episode_success:
                success_count += 1
            
            episode_rewards.append(episode_reward)
            episode_distances.append(min_distance)
            
            print(f"   ğŸ“Š Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, æœ€å°è·ç¦»={min_distance:.4f}, æˆåŠŸ={'âœ…' if episode_success else 'âŒ'}")
        
        success_rate = success_count / n_episodes
        avg_reward = np.mean(episode_rewards)
        avg_distance = np.mean(episode_distances)
        
        print(f"\nğŸ¯ MADDPGæµ‹è¯•ç»“æœ:")
        print(f"   ğŸ† æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{n_episodes})")
        print(f"   ğŸ’° å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"   ğŸ“ å¹³å‡æœ€å°è·ç¦»: {avg_distance:.4f}")
        
        return {
            'success_rate': success_rate,
            'avg_reward': avg_reward,
            'avg_distance': avg_distance
        }

class SingleJointEnvWrapper(gym.Wrapper):
    """å•å…³èŠ‚ç¯å¢ƒåŒ…è£…å™¨ - ç”¨äºMADDPGä¸­çš„å•ä¸ªæ™ºèƒ½ä½“"""
    
    def __init__(self, env, joint_id, num_joints):
        super().__init__(env)
        self.joint_id = joint_id
        self.num_joints = num_joints
        
        # é‡æ–°å®šä¹‰åŠ¨ä½œç©ºé—´ä¸ºå•å…³èŠ‚
        from gymnasium.spaces import Box
        self.action_space = Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)
        
        # è§‚å¯Ÿç©ºé—´ä¿æŒä¸å˜ï¼ˆå…¨å±€è§‚å¯Ÿï¼‰
        self.observation_space = env.observation_space
        
        # å­˜å‚¨å…¶ä»–æ™ºèƒ½ä½“çš„åŠ¨ä½œ
        self._other_actions = np.zeros(num_joints)
        
        # ç¡®ä¿æœ‰å¿…è¦çš„å±æ€§
        if not hasattr(self, 'metadata'):
            self.metadata = getattr(env, 'metadata', {})
        if not hasattr(self, 'spec'):
            self.spec = getattr(env, 'spec', None)
        
    def step(self, action):
        """å•å…³èŠ‚æ­¥éª¤ - éœ€è¦ä¸å…¶ä»–æ™ºèƒ½ä½“åè°ƒ"""
        # è¿™ä¸ªæ–¹æ³•å®é™…ä¸Šä¸ä¼šè¢«ç›´æ¥è°ƒç”¨
        # å› ä¸ºæˆ‘ä»¬åœ¨MADDPGä¸­ç›´æ¥ä½¿ç”¨åŸç¯å¢ƒ
        full_action = self._other_actions.copy()
        full_action[self.joint_id] = action[0]
        return self.env.step(full_action)
    
    def reset(self, **kwargs):
        """é‡ç½®ç¯å¢ƒ"""
        return self.env.reset(**kwargs)
    
    def render(self, **kwargs):
        """æ¸²æŸ“"""
        return self.env.render(**kwargs)
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        return self.env.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ çœŸæ­£çš„MADDPGåä½œè®­ç»ƒç³»ç»Ÿ")
    print("ğŸ¤– ç­–ç•¥: æ¯ä¸ªå…³èŠ‚ç”±ç‹¬ç«‹æ™ºèƒ½ä½“æ§åˆ¶ï¼Œå¤šæ™ºèƒ½ä½“åä½œå­¦ä¹ ")
    print("ğŸ¯ ç›®æ ‡: 3å…³èŠ‚Reacherä»»åŠ¡")
    print("ğŸ‘€ ç‰¹ç‚¹: å®æ—¶å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹")
    print()
    
    # åˆ›å»ºMADDPGç³»ç»Ÿ
    maddpg = TrueMADDPG(num_joints=3)
    
    # åä½œè®­ç»ƒ
    print("\n" + "="*60)
    episode_count = maddpg.train_collaborative(total_timesteps=3000)  # è¾ƒçŸ­è®­ç»ƒç”¨äºæ¼”ç¤º
    
    # æµ‹è¯•
    print("\n" + "="*60)
    result = maddpg.test(n_episodes=3)
    
    print(f"\nğŸ‰ çœŸæ­£çš„MADDPGè®­ç»ƒå®Œæˆ!")
    print(f"   ğŸ“ˆ è®­ç»ƒepisodes: {episode_count}")
    print(f"   ğŸ† æœ€ç»ˆæˆåŠŸç‡: {result['success_rate']:.1%}")
    print(f"   ğŸ’° å¹³å‡å¥–åŠ±: {result['avg_reward']:.2f}")
    print(f"   ğŸ“ å¹³å‡è·ç¦»: {result['avg_distance']:.4f}")
    
    print(f"\nğŸ” MADDPG vs SAC å¯¹æ¯”:")
    print(f"   âœ… MADDPG: æ¯ä¸ªå…³èŠ‚ç‹¬ç«‹å­¦ä¹ ï¼Œå¯èƒ½æœ‰æ›´å¥½çš„ä¸“ä¸šåŒ–")
    print(f"   âœ… SAC: å•ä¸€ç½‘ç»œå¤„ç†æ‰€æœ‰å…³èŠ‚ï¼Œå¯èƒ½æ›´ç®€å•é«˜æ•ˆ")
    print(f"   ğŸ¯ å»ºè®®: æ¯”è¾ƒä¸¤ç§æ–¹æ³•åœ¨ç›¸åŒè®­ç»ƒæ­¥æ•°ä¸‹çš„è¡¨ç°")

if __name__ == "__main__":
    main()
