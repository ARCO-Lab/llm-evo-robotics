#!/usr/bin/env python3
"""
Link-Aware SAC æ¶æ„ - æ–¹æ¡ˆ A: å…³èŠ‚ç‰¹å¾æ‰©å±•
åŸºäºæˆåŠŸçš„ç®€åŒ–ç‰ˆä¿®å¤æ¶æ„ï¼Œæ‰©å±•å…³èŠ‚ç‰¹å¾ç»´åº¦èåˆ link é•¿åº¦ä¿¡æ¯

æ‰©å±•å†…å®¹ï¼š
1. å…³èŠ‚ç‰¹å¾: [cos, sin, vel] â†’ [cos, sin, vel, link_length]
2. Link-Motion åˆ†ç¦»å¤„ç†
3. å–æ¶ˆè®­ç»ƒæ¸²æŸ“ï¼Œæé«˜æ•ˆç‡
4. ä¿æŒç°æœ‰æ¶æ„çš„æ‰€æœ‰ä¼˜åŠ¿
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import time
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.evaluation import evaluate_policy
from typing import Dict, List, Tuple, Type, Union, Optional, Any
import math

# ============================================================================
# ğŸ§© æ‰©å±• 1: Link-Aware å…³èŠ‚ç¼–ç å™¨ - æ–¹æ¡ˆ A
# ============================================================================

class LinkAwareJointEncoder(nn.Module):
    """
    Link-Aware å…³èŠ‚ç¼–ç å™¨ï¼šèåˆ link é•¿åº¦ä¿¡æ¯
    è¾“å…¥æ ¼å¼ï¼š[cos, sin, vel, link_length] (4ç»´)
    """
    def __init__(self, joint_input_dim: int = 4, joint_feature_dim: int = 64):
        super(LinkAwareJointEncoder, self).__init__()
        self.joint_input_dim = joint_input_dim  # [cos, sin, vel, link_length]
        self.joint_feature_dim = joint_feature_dim
        
        # Link é•¿åº¦ç‰¹å¾å¤„ç† (å‡ ä½•ä¿¡æ¯)
        self.link_processor = nn.Sequential(
            nn.Linear(1, 8),  # link_length â†’ 8ç»´å‡ ä½•ç‰¹å¾
            nn.ReLU(),
            nn.LayerNorm(8)
        )
        
        # è¿åŠ¨ç‰¹å¾å¤„ç† [cos, sin, vel] (è¿åŠ¨ä¿¡æ¯)
        self.motion_processor = nn.Sequential(
            nn.Linear(3, 24),  # [cos, sin, vel] â†’ 24ç»´è¿åŠ¨ç‰¹å¾
            nn.ReLU(),
            nn.LayerNorm(24)
        )
        
        # å‡ ä½•-è¿åŠ¨èåˆå¤„ç†å™¨
        self.fusion_processor = nn.Sequential(
            nn.Linear(32, joint_feature_dim),  # 8(å‡ ä½•) + 24(è¿åŠ¨) = 32 â†’ 64
            nn.ReLU(),
            nn.LayerNorm(joint_feature_dim)
        )
        
        print(f"ğŸ”— LinkAwareJointEncoder: {joint_input_dim} â†’ {joint_feature_dim} (å‡ ä½•+è¿åŠ¨èåˆ)")
    
    def forward(self, joint_features: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç å…³èŠ‚ç‰¹å¾ + link é•¿åº¦
        joint_features: [batch_size, max_joints, 4] - [cos, sin, vel, link_length]
        return: [batch_size, max_joints, joint_feature_dim]
        """
        batch_size, max_joints, _ = joint_features.shape
        
        # åˆ†ç¦»è¿åŠ¨ç‰¹å¾å’Œå‡ ä½•ç‰¹å¾
        motion_features = joint_features[:, :, :3]  # [cos, sin, vel]
        link_lengths = joint_features[:, :, 3:4]    # [link_length]
        
        # é‡å¡‘ä¸º [batch_size * max_joints, feature_dim]
        motion_flat = motion_features.view(-1, 3)
        link_flat = link_lengths.view(-1, 1)
        
        # åˆ†åˆ«å¤„ç†å‡ ä½•å’Œè¿åŠ¨ä¿¡æ¯
        motion_encoded = self.motion_processor(motion_flat)  # [batch_size * max_joints, 24]
        link_encoded = self.link_processor(link_flat)        # [batch_size * max_joints, 8]
        
        # èåˆå‡ ä½•å’Œè¿åŠ¨ç‰¹å¾
        fused_features = torch.cat([motion_encoded, link_encoded], dim=1)  # [batch_size * max_joints, 32]
        joint_encoded = self.fusion_processor(fused_features)  # [batch_size * max_joints, 64]
        
        # é‡å¡‘å› [batch_size, max_joints, joint_feature_dim]
        encoded_features = joint_encoded.view(batch_size, max_joints, self.joint_feature_dim)
        
        return encoded_features

# ============================================================================
# ğŸ§© å¤ç”¨ç°æœ‰çš„ä¼˜ç§€ç»„ä»¶
# ============================================================================

class FixedSelfAttention(nn.Module):
    """
    ä¿®å¤ç‰ˆè‡ªæ³¨æ„åŠ›ï¼šä½¿ç”¨ key_padding_mask è€Œéå¤æ‚çš„ attn_mask
    """
    def __init__(self, feature_dim: int = 64, num_heads: int = 4, dropout: float = 0.0):
        super(FixedSelfAttention, self).__init__()
        self.feature_dim = feature_dim
        self.num_heads = num_heads
        
        assert feature_dim % num_heads == 0, "feature_dim must be divisible by num_heads"
        
        # æ ‡å‡† Multi-Head Attention - å»æ‰ Dropout
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Layer Norm - å»æ‰ Dropout
        self.layer_norm = nn.LayerNorm(feature_dim)
        
        # Feed Forward - å»æ‰ Dropout
        self.feed_forward = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.ReLU(),
            nn.Linear(feature_dim * 2, feature_dim)
        )
        
        self.layer_norm2 = nn.LayerNorm(feature_dim)
        
        print(f"ğŸ§  FixedSelfAttention: {feature_dim}d, {num_heads} heads (å¤ç”¨ä¿®å¤ç‰ˆ)")
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è‡ªæ³¨æ„åŠ›å¤„ç† - ä½¿ç”¨ key_padding_mask
        x: [batch_size, max_joints, feature_dim] (å·² padding)
        mask: [batch_size, max_joints] - True è¡¨ç¤ºæœ‰æ•ˆå…³èŠ‚ï¼ŒFalse è¡¨ç¤º padding
        return: [batch_size, max_joints, feature_dim]
        """
        # å‡†å¤‡ key_padding_mask (True è¡¨ç¤ºè¦å±è”½çš„ä½ç½®)
        key_padding_mask = None
        if mask is not None:
            key_padding_mask = ~mask  # åè½¬ï¼šTrue è¡¨ç¤ºå±è”½ padding ä½ç½®
        
        # Multi-Head Self-Attention - ä½¿ç”¨ key_padding_mask
        attn_output, _ = self.multihead_attn(
            query=x, key=x, value=x,
            key_padding_mask=key_padding_mask,
            need_weights=False
        )
        
        # æ®‹å·®è¿æ¥ + Layer Norm
        x = self.layer_norm(x + attn_output)
        
        # Feed Forward
        ff_output = self.feed_forward(x)
        
        # æ®‹å·®è¿æ¥ + Layer Norm
        x = self.layer_norm2(x + ff_output)
        
        # åº”ç”¨æ©ç åˆ°è¾“å‡º (å°† padding ä½ç½®ç½®é›¶)
        if mask is not None:
            x = x * mask.unsqueeze(-1).float()
        
        return x

class FixedAttentionPooling(nn.Module):
    """
    ä¿®å¤ç‰ˆæ³¨æ„åŠ›æ± åŒ–ï¼šå»æ‰åŒé‡ softmaxï¼Œmask åœ¨ softmax å‰ç”Ÿæ•ˆ
    """
    def __init__(self, input_dim: int = 64, output_dim: int = 128):
        super(FixedAttentionPooling, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        # æ³¨æ„åŠ›æƒé‡è®¡ç®— - å»æ‰ Softmax
        self.score = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.ReLU(),
            nn.Linear(input_dim // 2, 1)
        )
        
        # è¾“å‡ºæŠ•å½± - å»æ‰ Dropout
        self.proj = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.ReLU(),
            nn.LayerNorm(output_dim)
        )
        
        print(f"ğŸ¯ FixedAttentionPooling: {input_dim} â†’ {output_dim} (å¤ç”¨ä¿®å¤ç‰ˆ)")
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        æ³¨æ„åŠ›æ± åŒ– - mask åœ¨ softmax å‰ç”Ÿæ•ˆ
        x: [batch_size, max_joints, input_dim] (å·² padding)
        mask: [batch_size, max_joints] - True è¡¨ç¤ºæœ‰æ•ˆå…³èŠ‚
        return: [batch_size, output_dim]
        """
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        s = self.score(x).squeeze(-1)  # [batch_size, max_joints]
        
        # å…ˆ mask å† softmax
        if mask is not None:
            s = s.masked_fill(~mask, -1e9)  # padding ä½ç½®è®¾ä¸ºæå°å€¼
        
        # å•æ¬¡ softmax
        w = F.softmax(s, dim=1).unsqueeze(-1)  # [batch_size, max_joints, 1]
        
        # åŠ æƒèšåˆ
        pooled = (x * w).sum(dim=1)  # [batch_size, input_dim]
        
        # è¾“å‡ºæŠ•å½±
        output = self.proj(pooled)  # [batch_size, output_dim]
        
        return output

# ============================================================================
# ğŸ§© æ‰©å±• 2: Link-Aware Mask ç³»ç»Ÿ
# ============================================================================

class LinkAwareMaskSystem:
    """
    Link-Aware Mask ç³»ç»Ÿï¼šå¤„ç† link é•¿åº¦ä¿¡æ¯
    """
    
    @staticmethod
    def create_joint_mask(batch_size: int, num_joints: int, max_joints: int, device: torch.device) -> torch.Tensor:
        """
        åˆ›å»ºå…³èŠ‚æ©ç 
        return: [batch_size, max_joints] - True è¡¨ç¤ºæœ‰æ•ˆå…³èŠ‚ï¼ŒFalse è¡¨ç¤º padding
        """
        mask = torch.zeros(batch_size, max_joints, dtype=torch.bool, device=device)
        mask[:, :num_joints] = True
        return mask
    
    @staticmethod
    def parse_observation_with_links(obs: torch.Tensor, num_joints: int, max_joints: int, 
                                   link_lengths: Optional[List[float]] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è§£æè§‚å¯Ÿç©ºé—´å¹¶èåˆ link é•¿åº¦ä¿¡æ¯
        obs: [batch_size, obs_dim]
        link_lengths: [link1_length, link2_length, ...] æˆ– None (ä½¿ç”¨é»˜è®¤å€¼)
        return: (joint_features_with_links, global_features)
        """
        batch_size = obs.size(0)
        device = obs.device
        
        # é»˜è®¤ link é•¿åº¦ (MuJoCo Reacher-v5)
        if link_lengths is None:
            if num_joints == 2:
                link_lengths = [0.1, 0.1]  # MuJoCo Reacher-v5 é»˜è®¤ link é•¿åº¦
            else:
                link_lengths = [0.1] * num_joints  # é€šç”¨é»˜è®¤å€¼
        
        # ç¡®ä¿ link_lengths é•¿åº¦åŒ¹é…
        while len(link_lengths) < num_joints:
            link_lengths.append(0.1)  # é»˜è®¤é•¿åº¦
        
        if num_joints == 2:
            # MuJoCo Reacher-v5 æ ¼å¼
            joint_cos_sin = obs[:, :2]  # [batch_size, 2]
            joint_velocities = obs[:, 2:4]  # [batch_size, 2]
            global_features = obs[:, 4:]  # [batch_size, 6]
            
            # æ„é€ å…³èŠ‚ç‰¹å¾ + link é•¿åº¦
            joint_features_list = []
            for i in range(num_joints):
                cos_val = joint_cos_sin[:, i:i+1]  # [batch_size, 1]
                sin_val = torch.zeros_like(cos_val)  # ç®€åŒ–ï¼šsin è®¾ä¸º 0
                vel_val = joint_velocities[:, i:i+1]  # [batch_size, 1]
                
                # æ·»åŠ  link é•¿åº¦ä¿¡æ¯
                link_val = torch.full_like(cos_val, link_lengths[i])  # [batch_size, 1]
                
                joint_feature = torch.cat([cos_val, sin_val, vel_val, link_val], dim=1)  # [batch_size, 4]
                joint_features_list.append(joint_feature)
            
            joint_features = torch.stack(joint_features_list, dim=1)  # [batch_size, num_joints, 4]
            
        else:
            # é€šç”¨æ ¼å¼ï¼šå‡è®¾å‰ num_joints*3 æ˜¯å…³èŠ‚ç‰¹å¾ï¼Œå‰©ä½™æ˜¯å…¨å±€ç‰¹å¾
            joint_dim = num_joints * 3
            joint_obs = obs[:, :joint_dim]  # [batch_size, num_joints*3]
            global_features = obs[:, joint_dim:]  # [batch_size, remaining]
            
            # é‡å¡‘å…³èŠ‚ç‰¹å¾å¹¶æ·»åŠ  link é•¿åº¦
            joint_features_3d = joint_obs.view(batch_size, num_joints, 3)  # [batch_size, num_joints, 3]
            
            # ä¸ºæ¯ä¸ªå…³èŠ‚æ·»åŠ  link é•¿åº¦
            joint_features_list = []
            for i in range(num_joints):
                joint_motion = joint_features_3d[:, i]  # [batch_size, 3]
                link_val = torch.full((batch_size, 1), link_lengths[i], device=device)  # [batch_size, 1]
                joint_feature = torch.cat([joint_motion, link_val], dim=1)  # [batch_size, 4]
                joint_features_list.append(joint_feature)
            
            joint_features = torch.stack(joint_features_list, dim=1)  # [batch_size, num_joints, 4]
        
        # Padding åˆ° max_joints
        joint_features_padded = torch.zeros(batch_size, max_joints, 4, device=device)
        joint_features_padded[:, :num_joints] = joint_features
        
        return joint_features_padded, global_features

# ============================================================================
# ğŸ§© æ‰©å±• 3: Link-Aware é€šç”¨ç‰¹å¾æå–å™¨
# ============================================================================

class LinkAwareUniversalExtractor(BaseFeaturesExtractor):
    """
    Link-Aware é€šç”¨ç‰¹å¾æå–å™¨
    åœ¨æˆåŠŸæ¶æ„åŸºç¡€ä¸Šèåˆ link é•¿åº¦ä¿¡æ¯
    """
    def __init__(self, observation_space: gym.Space, features_dim: int = 128, 
                 num_joints: int = 2, max_joints: int = 10, 
                 link_lengths: Optional[List[float]] = None):
        super(LinkAwareUniversalExtractor, self).__init__(observation_space, features_dim)
        
        self.obs_dim = observation_space.shape[0]
        self.num_joints = num_joints
        self.max_joints = max_joints
        self.joint_input_dim = 4  # [cos, sin, vel, link_length]
        self.link_lengths = link_lengths
        
        print(f"ğŸŒŸ LinkAwareUniversalExtractor åˆå§‹åŒ–:")
        print(f"   è§‚å¯Ÿç©ºé—´ç»´åº¦: {self.obs_dim}")
        print(f"   å½“å‰å…³èŠ‚æ•°: {num_joints}")
        print(f"   æœ€å¤§å…³èŠ‚æ•°: {max_joints}")
        print(f"   å…³èŠ‚è¾“å…¥ç»´åº¦: {self.joint_input_dim} [cos, sin, vel, link_length]")
        print(f"   Link é•¿åº¦: {link_lengths}")
        print(f"   è¾“å‡ºç‰¹å¾ç»´åº¦: {features_dim}")
        
        # æ¨¡å— 1: Link-Aware å…³èŠ‚ç¼–ç å™¨
        self.joint_encoder = LinkAwareJointEncoder(
            joint_input_dim=self.joint_input_dim,
            joint_feature_dim=64
        )
        
        # æ¨¡å— 2: è‡ªæ³¨æ„åŠ› (å¤ç”¨ä¿®å¤ç‰ˆ)
        self.self_attention = FixedSelfAttention(
            feature_dim=64,
            num_heads=4,
            dropout=0.0
        )
        
        # æ¨¡å— 3: æ³¨æ„åŠ›æ± åŒ– (å¤ç”¨ä¿®å¤ç‰ˆ)
        self.attention_pooling = FixedAttentionPooling(
            input_dim=64,
            output_dim=features_dim // 2
        )
        
        # å…¨å±€ç‰¹å¾å¤„ç† (å¤ç”¨ç°æœ‰é€»è¾‘)
        if num_joints == 2:
            global_dim = 6  # MuJoCo Reacher-v5 çš„å…¨å±€ç‰¹å¾ç»´åº¦
        else:
            global_dim = max(0, self.obs_dim - (num_joints * 3))  # æ³¨æ„ï¼šè§‚å¯Ÿç©ºé—´ä»æ˜¯ 3 ç»´/å…³èŠ‚
        
        if global_dim > 0:
            self.global_processor = nn.Sequential(
                nn.Linear(global_dim, features_dim // 2),
                nn.ReLU(),
                nn.LayerNorm(features_dim // 2)
            )
            fusion_dim = features_dim
        else:
            self.global_processor = None
            fusion_dim = features_dim // 2
        
        # æœ€ç»ˆèåˆ (å¤ç”¨ç°æœ‰é€»è¾‘)
        self.final_fusion = nn.Sequential(
            nn.Linear(fusion_dim, features_dim),
            nn.ReLU(),
            nn.LayerNorm(features_dim)
        )
        
        # Link-Aware Mask ç³»ç»Ÿ
        self.mask_system = LinkAwareMaskSystem()
        
        print(f"âœ… LinkAwareUniversalExtractor æ„å»ºå®Œæˆ")
        print(f"   ğŸ”— èåˆ link é•¿åº¦ä¿¡æ¯")
        print(f"   âœ… ä¿æŒç°æœ‰æ¶æ„çš„æ‰€æœ‰ä¼˜åŠ¿")
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        Link-Aware å‰å‘ä¼ æ’­
        """
        batch_size = observations.size(0)
        device = observations.device
        
        # æ­¥éª¤ 1: è§£æè§‚å¯Ÿç©ºé—´å¹¶èåˆ link é•¿åº¦
        joint_features_with_links, global_features = self.mask_system.parse_observation_with_links(
            observations, self.num_joints, self.max_joints, self.link_lengths
        )
        
        # æ­¥éª¤ 2: åˆ›å»ºå…³èŠ‚æ©ç 
        joint_mask = self.mask_system.create_joint_mask(
            batch_size, self.num_joints, self.max_joints, device
        )
        
        # æ­¥éª¤ 3: Link-Aware å…³èŠ‚ç¼–ç  (å‡ ä½•+è¿åŠ¨èåˆ)
        encoded_joints = self.joint_encoder(joint_features_with_links)  # [batch_size, max_joints, 64]
        
        # æ­¥éª¤ 4: è‡ªæ³¨æ„åŠ›å»ºæ¨¡å…³èŠ‚é—´äº¤äº’ (ç°åœ¨åŒ…å«å‡ ä½•ä¿¡æ¯)
        attended_joints = self.self_attention(
            encoded_joints, 
            mask=joint_mask
        )  # [batch_size, max_joints, 64]
        
        # æ­¥éª¤ 5: æ³¨æ„åŠ›æ± åŒ–
        pooled_joint_features = self.attention_pooling(
            attended_joints,
            mask=joint_mask
        )  # [batch_size, features_dim//2]
        
        # æ­¥éª¤ 6: å¤„ç†å…¨å±€ç‰¹å¾
        if self.global_processor is not None and global_features.size(1) > 0:
            processed_global = self.global_processor(global_features)  # [batch_size, features_dim//2]
            fused_features = torch.cat([pooled_joint_features, processed_global], dim=1)
        else:
            fused_features = pooled_joint_features
        
        # æ­¥éª¤ 7: æœ€ç»ˆèåˆ
        final_features = self.final_fusion(fused_features)  # [batch_size, features_dim]
        
        return final_features

# ============================================================================
# ğŸ§© æ‰©å±• 4: Link-Aware è®­ç»ƒå‡½æ•° (å–æ¶ˆæ¸²æŸ“)
# ============================================================================

def train_link_aware_sac(num_joints: int = 2, max_joints: int = 10, 
                        link_lengths: Optional[List[float]] = None,
                        total_timesteps: int = 50000):
    """
    è®­ç»ƒ Link-Aware SAC (å–æ¶ˆæ¸²æŸ“ï¼Œæé«˜æ•ˆç‡)
    """
    print("ğŸŒŸ Link-Aware SAC è®­ç»ƒ (æ–¹æ¡ˆ A)")
    print(f"ğŸ”— å½“å‰å…³èŠ‚æ•°: {num_joints}")
    print(f"ğŸ”— æœ€å¤§æ”¯æŒå…³èŠ‚æ•°: {max_joints}")
    print(f"ğŸ”— Link é•¿åº¦: {link_lengths}")
    print(f"ğŸ’¡ æ¶æ„: å…³èŠ‚ç‰¹å¾æ‰©å±• [cos, sin, vel, link_length]")
    print(f"ğŸ¯ ç›®æ ‡: å‡ ä½•æ„ŸçŸ¥ + é«˜æ•ˆè®­ç»ƒ")
    print("=" * 70)
    
    # åˆ›å»ºç¯å¢ƒ - å–æ¶ˆè®­ç»ƒæ¸²æŸ“
    print(f"ğŸ­ åˆ›å»ºç¯å¢ƒ...")
    if num_joints == 2:
        env = gym.make('Reacher-v5')  # è®­ç»ƒæ—¶ä¸æ¸²æŸ“
        eval_env = gym.make('Reacher-v5')  # è¯„ä¼°æ—¶ä¸æ¸²æŸ“
    else:
        print(f"âš ï¸ æš‚ä¸æ”¯æŒ {num_joints} å…³èŠ‚ç¯å¢ƒï¼Œä½¿ç”¨ 2 å…³èŠ‚è¿›è¡ŒéªŒè¯")
        env = gym.make('Reacher-v5')
        eval_env = gym.make('Reacher-v5')
    
    env = Monitor(env)
    eval_env = Monitor(eval_env)
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ (æ— æ¸²æŸ“ï¼Œé«˜æ•ˆè®­ç»ƒ)")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"ğŸ‘ï¸ è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    
    print("=" * 70)
    
    # åˆ›å»º Link-Aware æ¨¡å‹
    print("ğŸ¤– åˆ›å»º Link-Aware SAC æ¨¡å‹...")
    
    policy_kwargs = {
        "features_extractor_class": LinkAwareUniversalExtractor,
        "features_extractor_kwargs": {
            "features_dim": 128,
            "num_joints": num_joints,
            "max_joints": max_joints,
            "link_lengths": link_lengths
        },
        "net_arch": [256, 256],
        "activation_fn": torch.nn.ReLU,
    }
    
    model = SAC(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=1000000,
        learning_starts=100,
        batch_size=256,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        ent_coef='auto',
        target_update_interval=1,
        use_sde=False,
        policy_kwargs=policy_kwargs,
        verbose=1,
        device='cpu'
    )
    
    print("âœ… Link-Aware SAC æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š æ‰©å±•ç‰¹ç‚¹:")
    print(f"   ğŸ”— å…³èŠ‚ç‰¹å¾æ‰©å±•: [cos, sin, vel] â†’ [cos, sin, vel, link_length]")
    print(f"   ğŸ§  å‡ ä½•-è¿åŠ¨åˆ†ç¦»å¤„ç†")
    print(f"   ğŸ¯ æ›´ç²¾ç¡®çš„ç©ºé—´æ„ŸçŸ¥")
    print(f"   âœ… ä¿æŒæ‰€æœ‰ç°æœ‰ä¼˜åŠ¿")
    print(f"   âš¡ æ— æ¸²æŸ“ï¼Œé«˜æ•ˆè®­ç»ƒ")
    
    print("=" * 70)
    
    # è¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f'./link_aware_{num_joints}joints_best/',
        log_path=f'./link_aware_{num_joints}joints_logs/',
        eval_freq=5000,
        n_eval_episodes=10,
        deterministic=True,
        render=False
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹ Link-Aware è®­ç»ƒ...")
    print("ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   æ€»æ­¥æ•°: {total_timesteps:,}")
    print("   è¯„ä¼°é¢‘ç‡: æ¯ 5,000 æ­¥")
    print("   æ¸²æŸ“: å…³é—­ (æé«˜æ•ˆç‡)")
    print("   é¢„æœŸ: å‡ ä½•æ„ŸçŸ¥èƒ½åŠ›æå‡")
    print("=" * 70)
    
    start_time = time.time()
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=eval_callback,
        log_interval=10,
        progress_bar=True
    )
    
    training_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("ğŸ† Link-Aware è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
    print("=" * 70)
    
    # ä¿å­˜æ¨¡å‹
    model_name = f"link_aware_{num_joints}joints_final"
    model.save(model_name)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º: {model_name}.zip")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ” æœ€ç»ˆè¯„ä¼° (20 episodes)...")
    mean_reward, std_reward = evaluate_policy(
        model, 
        eval_env, 
        n_eval_episodes=20,
        deterministic=True,
        render=False
    )
    
    print(f"ğŸ“Š Link-Aware æ¨¡å‹è¯„ä¼°ç»“æœ:")
    print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    # ä¸ä¹‹å‰ç‰ˆæœ¬å¯¹æ¯”
    baseline_reward = -4.86
    simplified_fixed_reward = -3.76  # ä¹‹å‰çš„æœ€ä½³ç»“æœ
    
    print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
    print(f"   Baseline SAC: {baseline_reward:.2f}")
    print(f"   ç®€åŒ–ç‰ˆä¿®å¤: {simplified_fixed_reward:.2f}")
    print(f"   Link-Aware: {mean_reward:.2f}")
    
    improvement_vs_baseline = mean_reward - baseline_reward
    improvement_vs_simplified = mean_reward - simplified_fixed_reward
    
    print(f"\nğŸ“Š æ”¹è¿›å¹…åº¦:")
    print(f"   vs Baseline: {improvement_vs_baseline:+.2f}")
    print(f"   vs ç®€åŒ–ç‰ˆä¿®å¤: {improvement_vs_simplified:+.2f}")
    
    if improvement_vs_simplified > 0.5:
        print("   ğŸ‰ Link å‡ ä½•ä¿¡æ¯èåˆå¤§æˆåŠŸ!")
    elif improvement_vs_simplified > 0.0:
        print("   ğŸ‘ Link å‡ ä½•ä¿¡æ¯èåˆæœ‰æ•ˆ!")
    elif improvement_vs_simplified > -0.5:
        print("   ğŸ“ˆ Link å‡ ä½•ä¿¡æ¯æ•ˆæœä¸­æ€§ï¼Œéœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print("   âš ï¸ Link å‡ ä½•ä¿¡æ¯å¯èƒ½å¼•å…¥å™ªå£°ï¼Œéœ€è¦è°ƒæ•´")
    
    # æ¼”ç¤º - åªåœ¨æ¼”ç¤ºæ—¶æ¸²æŸ“
    print("\nğŸ® æ¼”ç¤º Link-Aware æ¨¡å‹ (10 episodes)...")
    demo_env = gym.make('Reacher-v5', render_mode='human')
    
    episode_rewards = []
    success_count = 0
    
    for episode in range(10):
        obs, info = demo_env.reset()
        episode_reward = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = demo_env.step(action)
            episode_reward += reward
            
            if terminated or truncated:
                break
        
        episode_rewards.append(episode_reward)
        
        if episode_reward > -5:
            success_count += 1
            print(f"ğŸ¯ Episode {episode+1}: æˆåŠŸ! å¥–åŠ±={episode_reward:.2f}")
        else:
            print(f"ğŸ“Š Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}")
    
    demo_env.close()
    
    demo_success_rate = success_count / 10
    demo_avg_reward = np.mean(episode_rewards)
    
    print("\n" + "=" * 70)
    print("ğŸ“Š Link-Aware æ¼”ç¤ºç»Ÿè®¡:")
    print(f"   æˆåŠŸç‡: {demo_success_rate:.1%} ({success_count}/10)")
    print(f"   å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    print(f"   å¥–åŠ±æ ‡å‡†å·®: {np.std(episode_rewards):.2f}")
    
    # ä¸ä¹‹å‰æœ€ä½³ç»“æœå¯¹æ¯”
    simplified_demo_success = 0.6
    simplified_demo_reward = -4.27
    
    print(f"\nğŸ“ˆ ä¸ç®€åŒ–ç‰ˆä¿®å¤å¯¹æ¯”:")
    print(f"   ç®€åŒ–ç‰ˆæˆåŠŸç‡: {simplified_demo_success:.1%}")
    print(f"   Link-Aware æˆåŠŸç‡: {demo_success_rate:.1%}")
    print(f"   æˆåŠŸç‡å˜åŒ–: {demo_success_rate - simplified_demo_success:+.1%}")
    print(f"   ")
    print(f"   ç®€åŒ–ç‰ˆå¹³å‡å¥–åŠ±: {simplified_demo_reward:.2f}")
    print(f"   Link-Aware å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    print(f"   å¥–åŠ±å˜åŒ–: {demo_avg_reward - simplified_demo_reward:+.2f}")
    
    if demo_success_rate >= 0.7:
        print("   ğŸ‰ Link-Aware å¤§æˆåŠŸ!")
    elif demo_success_rate >= 0.6:
        print("   ğŸ‘ Link-Aware æˆåŠŸ!")
    elif demo_success_rate >= 0.5:
        print("   ğŸ“ˆ Link-Aware è‰¯å¥½!")
    else:
        print("   ğŸ“ˆ ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    print(f"\nğŸŒŸ Link-Aware æ¶æ„ä¼˜åŠ¿ (æ–¹æ¡ˆ A):")
    print(f"   ğŸ”— å…³èŠ‚ç‰¹å¾æ‰©å±•: 4ç»´è¾“å…¥ [cos, sin, vel, link_length]")
    print(f"   ğŸ§  å‡ ä½•-è¿åŠ¨åˆ†ç¦»å¤„ç†: 8ç»´å‡ ä½• + 24ç»´è¿åŠ¨")
    print(f"   ğŸ¯ ç²¾ç¡®ç©ºé—´æ„ŸçŸ¥: link é•¿åº¦ç›´æ¥å½±å“å…³èŠ‚è¡¨ç¤º")
    print(f"   âœ… ä¿æŒç°æœ‰ä¼˜åŠ¿: æ‰€æœ‰ GPT-5 ä¿®å¤éƒ½ä¿ç•™")
    print(f"   ğŸŒ é€šç”¨æ‰©å±•æ€§: æ”¯æŒä»»æ„ link é•¿åº¦é…ç½®")
    print(f"   âš¡ é«˜æ•ˆè®­ç»ƒ: æ— æ¸²æŸ“ï¼Œå¿«é€Ÿæ”¶æ•›")
    
    # æ¸…ç†
    env.close()
    eval_env.close()
    
    return {
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'training_time': training_time,
        'demo_success_rate': demo_success_rate,
        'demo_avg_reward': demo_avg_reward,
        'improvement_vs_baseline': improvement_vs_baseline,
        'improvement_vs_simplified': improvement_vs_simplified,
        'num_joints': num_joints,
        'max_joints': max_joints,
        'link_lengths': link_lengths
    }

if __name__ == "__main__":
    print("ğŸŒŸ Link-Aware SAC è®­ç»ƒç³»ç»Ÿ (æ–¹æ¡ˆ A)")
    print("ğŸ’¡ å…³èŠ‚ç‰¹å¾æ‰©å±•: [cos, sin, vel] â†’ [cos, sin, vel, link_length]")
    print("ğŸ¯ ç›®æ ‡: å‡ ä½•æ„ŸçŸ¥ + é«˜æ•ˆè®­ç»ƒ")
    print()
    
    # é»˜è®¤é…ç½®æµ‹è¯•
    print("ğŸ”— æµ‹è¯•é»˜è®¤ Link é•¿åº¦é…ç½®...")
    
    try:
        result = train_link_aware_sac(
            num_joints=2, 
            max_joints=10, 
            link_lengths=None,  # ä½¿ç”¨é»˜è®¤ [0.1, 0.1]
            total_timesteps=50000
        )
        
        print(f"\nğŸŠ Link-Aware è®­ç»ƒç»“æœæ€»ç»“:")
        print(f"   æœ€ç»ˆè¯„ä¼°å¥–åŠ±: {result['mean_reward']:.2f} Â± {result['std_reward']:.2f}")
        print(f"   è®­ç»ƒæ—¶é—´: {result['training_time']/60:.1f} åˆ†é’Ÿ")
        print(f"   æ¼”ç¤ºæˆåŠŸç‡: {result['demo_success_rate']:.1%}")
        print(f"   æ¼”ç¤ºå¹³å‡å¥–åŠ±: {result['demo_avg_reward']:.2f}")
        print(f"   vs Baseline: {result['improvement_vs_baseline']:+.2f}")
        print(f"   vs ç®€åŒ–ç‰ˆä¿®å¤: {result['improvement_vs_simplified']:+.2f}")
        
        if result['improvement_vs_simplified'] > 0.5:
            print(f"\nğŸ† Link-Aware å‡ ä½•æ„ŸçŸ¥å¤§æˆåŠŸ!")
            print("   æ–¹æ¡ˆ A çš„å…³èŠ‚ç‰¹å¾æ‰©å±•ç­–ç•¥éªŒè¯æˆåŠŸ!")
        elif result['improvement_vs_simplified'] > 0.0:
            print(f"\nğŸ‘ Link-Aware å‡ ä½•æ„ŸçŸ¥æœ‰æ•ˆ!")
            print("   æ–¹æ¡ˆ A å¸¦æ¥äº†æ€§èƒ½æå‡!")
        else:
            print(f"\nğŸ“ˆ Link-Aware éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            print("   å¯èƒ½éœ€è¦è°ƒæ•´å‡ ä½•-è¿åŠ¨ç‰¹å¾çš„èåˆæ–¹å¼")
        
        print(f"\nâœ… Link-Aware æ¶æ„ (æ–¹æ¡ˆ A) éªŒè¯å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
