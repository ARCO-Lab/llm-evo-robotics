#!/usr/bin/env python3
"""
ç®€å•çš„MADDPGå®ç°ï¼Œå¸¦æœ‰æ‰‹åŠ¨lossè®¡ç®—å’Œæ˜¾ç¤ºï¼š
1. æ¯ä¸ªå…³èŠ‚ç”±ä¸€ä¸ªDDPGæ™ºèƒ½ä½“æ§åˆ¶
2. æ‰‹åŠ¨è®¡ç®—å’Œæ˜¾ç¤ºActor/CriticæŸå¤±
3. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
4. è¯¦ç»†çš„è®­ç»ƒç»Ÿè®¡
"""

import os
import numpy as np
import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import random
import time

# è®¾ç½®æ¸²æŸ“ç¯å¢ƒå˜é‡
os.environ['MUJOCO_GL'] = 'glfw'
os.environ['MUJOCO_RENDERER'] = 'glfw'

# å¯¼å…¥ç¯å¢ƒ
from maddpg_complete_sequential_training import (
    create_env, 
    SUCCESS_THRESHOLD_RATIO, 
    SUCCESS_THRESHOLD_2JOINT
)

class Actor(nn.Module):
    """Actorç½‘ç»œ"""
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(Actor, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()
        )
    
    def forward(self, state):
        return self.net(state)

class Critic(nn.Module):
    """Criticç½‘ç»œ"""
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(Critic, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state, action):
        x = torch.cat([state, action], dim=-1)
        return self.net(x)

class DDPGAgent:
    """å•ä¸ªDDPGæ™ºèƒ½ä½“"""
    def __init__(self, agent_id, state_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99, tau=0.005):
        self.agent_id = agent_id
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.tau = tau
        
        # ç½‘ç»œ
        self.actor = Actor(state_dim, action_dim)
        self.actor_target = Actor(state_dim, action_dim)
        self.critic = Critic(state_dim, action_dim)
        self.critic_target = Critic(state_dim, action_dim)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # Lossè®°å½•
        self.actor_losses = []
        self.critic_losses = []
        
        print(f"   ğŸ¤– DDPG Agent {agent_id}: çŠ¶æ€ç»´åº¦={state_dim}, åŠ¨ä½œç»´åº¦={action_dim}")
    
    def act(self, state, noise_scale=0.1):
        """é€‰æ‹©åŠ¨ä½œ"""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action = self.actor(state_tensor).cpu().numpy()[0]
            # æ·»åŠ å™ªå£°
            noise = np.random.normal(0, noise_scale, size=action.shape)
            action = np.clip(action + noise, -1, 1)
            return action
    
    def update(self, states, actions, rewards, next_states, dones):
        """æ›´æ–°ç½‘ç»œå¹¶è¿”å›loss"""
        states = torch.FloatTensor(states)
        actions = torch.FloatTensor(actions)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)
        
        # æ›´æ–°Critic
        with torch.no_grad():
            next_actions = self.actor_target(next_states)
            target_q = self.critic_target(next_states, next_actions)
            target_q = rewards + (1 - dones) * self.gamma * target_q
        
        current_q = self.critic(states, actions)
        critic_loss = F.mse_loss(current_q, target_q)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # æ›´æ–°Actor
        predicted_actions = self.actor(states)
        actor_loss = -self.critic(states, predicted_actions).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.soft_update()
        
        # è®°å½•loss
        actor_loss_value = actor_loss.item()
        critic_loss_value = critic_loss.item()
        self.actor_losses.append(actor_loss_value)
        self.critic_losses.append(critic_loss_value)
        
        return actor_loss_value, critic_loss_value
    
    def soft_update(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
        
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (np.array(states), np.array(actions), np.array(rewards), 
                np.array(next_states), np.array(dones))
    
    def __len__(self):
        return len(self.buffer)

class SimpleMADDPG:
    """ç®€å•çš„MADDPGå®ç°"""
    def __init__(self, num_joints=3):
        self.num_joints = num_joints
        
        print(f"ğŸŒŸ åˆ›å»ºç®€å•MADDPGç³»ç»Ÿ: {num_joints}ä¸ªæ™ºèƒ½ä½“åä½œ")
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = create_env(num_joints, render_mode='human', show_position_info=True)
        
        # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
        state_dim = self.env.observation_space.shape[0]
        action_dim = 1  # æ¯ä¸ªæ™ºèƒ½ä½“æ§åˆ¶ä¸€ä¸ªå…³èŠ‚
        
        print(f"ğŸ”§ ç¯å¢ƒé…ç½®: çŠ¶æ€ç»´åº¦={state_dim}, æ¯ä¸ªæ™ºèƒ½ä½“åŠ¨ä½œç»´åº¦={action_dim}")
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agents = []
        for i in range(num_joints):
            agent = DDPGAgent(i, state_dim, action_dim)
            self.agents.append(agent)
        
        # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = ReplayBuffer(capacity=100000)
        
        print(f"ğŸŒŸ ç®€å•MADDPGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def train(self, total_timesteps=5000, batch_size=64):
        """è®­ç»ƒMADDPG"""
        print(f"\nğŸ¯ å¼€å§‹ç®€å•MADDPGè®­ç»ƒ {total_timesteps} æ­¥...")
        print(f"   æ¯ä¸ªæ™ºèƒ½ä½“æ§åˆ¶ä¸€ä¸ªå…³èŠ‚ï¼Œæ˜¾ç¤ºå®æ—¶lossä¿¡æ¯")
        
        obs, _ = self.env.reset()
        episode_count = 0
        episode_reward = 0
        step_count = 0
        
        # è®­ç»ƒç»Ÿè®¡
        training_stats = {
            'episode_rewards': [],
            'episode_successes': [],
            'actor_losses': [[] for _ in range(self.num_joints)],
            'critic_losses': [[] for _ in range(self.num_joints)]
        }
        
        start_time = time.time()
        
        try:
            while step_count < total_timesteps:
                # æ‰€æœ‰æ™ºèƒ½ä½“åŒæ—¶å†³ç­–
                actions = []
                for i, agent in enumerate(self.agents):
                    action = agent.act(obs, noise_scale=0.1)
                    actions.append(action[0])  # å–å‡ºæ ‡é‡
                
                # æ‰§è¡Œè”åˆåŠ¨ä½œ
                joint_action = np.array(actions)
                next_obs, reward, terminated, truncated, info = self.env.step(joint_action)
                episode_reward += reward
                step_count += 1
                
                # å­˜å‚¨ç»éªŒ
                self.replay_buffer.push(obs, actions, reward, next_obs, terminated or truncated)
                
                # è®­ç»ƒæ™ºèƒ½ä½“
                if len(self.replay_buffer) > batch_size and step_count % 10 == 0:
                    self._train_agents(batch_size, training_stats, step_count)
                
                obs = next_obs
                
                # é‡ç½®ç¯å¢ƒ
                if terminated or truncated:
                    obs, _ = self.env.reset()
                    episode_count += 1
                    
                    # è®°å½•ç»Ÿè®¡
                    training_stats['episode_rewards'].append(episode_reward)
                    success = info.get('is_success', False) if info else False
                    training_stats['episode_successes'].append(success)
                    
                    if episode_count % 5 == 0:
                        distance = info.get('distance_to_target', 0) if info else 0
                        recent_success_rate = np.mean(training_stats['episode_successes'][-10:]) if len(training_stats['episode_successes']) >= 10 else 0
                        print(f"   ğŸ® Episode {episode_count}: å¥–åŠ±={episode_reward:.2f}, è·ç¦»={distance:.4f}, æˆåŠŸ={'âœ…' if success else 'âŒ'}, è¿‘æœŸæˆåŠŸç‡={recent_success_rate:.1%}")
                    
                    episode_reward = 0
                
                # å®šæœŸè¾“å‡ºè¿›åº¦ï¼ˆæ¯5000æ­¥ï¼‰
                if step_count % 5000 == 0:
                    elapsed_time = time.time() - start_time
                    fps = step_count / elapsed_time
                    print(f"\n   ğŸš€ æ­¥æ•° {step_count}/{total_timesteps}, FPS: {fps:.1f}, ç”¨æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ")
                    self._print_loss_summary(training_stats)
        
        except KeyboardInterrupt:
            print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        training_time = time.time() - start_time
        print(f"\nâœ… ç®€å•MADDPGè®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ¯ æ€»episodes: {episode_count}")
        print(f"ğŸ“Š å¹³å‡FPS: {step_count/training_time:.1f}")
        
        # æœ€ç»ˆç»Ÿè®¡
        self._print_final_summary(training_stats)
        
        return episode_count, training_stats
    
    def _train_agents(self, batch_size, training_stats, step_count):
        """è®­ç»ƒæ‰€æœ‰æ™ºèƒ½ä½“"""
        # é‡‡æ ·ç»éªŒ
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å‡†å¤‡æ•°æ®å¹¶è®­ç»ƒ
        for i, agent in enumerate(self.agents):
            # æå–è¯¥æ™ºèƒ½ä½“çš„åŠ¨ä½œ
            agent_actions = actions[:, i:i+1]  # ä¿æŒäºŒç»´
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            actor_loss, critic_loss = agent.update(states, agent_actions, rewards, next_states, dones)
            
            # è®°å½•loss
            training_stats['actor_losses'][i].append(actor_loss)
            training_stats['critic_losses'][i].append(critic_loss)
            
            # æ¯500æ­¥æ˜¾ç¤ºä¸€æ¬¡lossï¼ˆå‡å°‘è¾“å‡ºé¢‘ç‡ï¼‰
            if step_count % 500 == 0:
                print(f"     ğŸ¤– Agent {i}: Actor_Loss={actor_loss:.4f}, Critic_Loss={critic_loss:.4f}")
    
    def _print_loss_summary(self, training_stats):
        """æ‰“å°lossç»Ÿè®¡æ‘˜è¦"""
        print(f"     ğŸ“Š Lossç»Ÿè®¡æ‘˜è¦ (æœ€è¿‘10æ¬¡è®­ç»ƒ):")
        for i in range(self.num_joints):
            if training_stats['actor_losses'][i]:
                recent_actor = training_stats['actor_losses'][i][-10:]
                recent_critic = training_stats['critic_losses'][i][-10:]
                avg_actor = np.mean(recent_actor)
                avg_critic = np.mean(recent_critic)
                print(f"       Agent {i}: å¹³å‡Actor_Loss={avg_actor:.4f}, å¹³å‡Critic_Loss={avg_critic:.4f}")
    
    def _print_final_summary(self, training_stats):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        print(f"\nğŸ“Š æœ€ç»ˆè®­ç»ƒç»Ÿè®¡:")
        print("-" * 60)
        
        # Lossç»Ÿè®¡
        for i in range(self.num_joints):
            if training_stats['actor_losses'][i]:
                actor_losses = training_stats['actor_losses'][i]
                critic_losses = training_stats['critic_losses'][i]
                
                print(f"ğŸ¤– Agent {i}:")
                print(f"   Actor Loss: å¹³å‡={np.mean(actor_losses):.4f}, æœ€å°={np.min(actor_losses):.4f}, æœ€å¤§={np.max(actor_losses):.4f}")
                print(f"   Critic Loss: å¹³å‡={np.mean(critic_losses):.4f}, æœ€å°={np.min(critic_losses):.4f}, æœ€å¤§={np.max(critic_losses):.4f}")
        
        # Episodeç»Ÿè®¡
        if training_stats['episode_rewards']:
            print(f"\nğŸ¯ Episodeç»Ÿè®¡:")
            print(f"   æ€»Episodes: {len(training_stats['episode_rewards'])}")
            print(f"   å¹³å‡å¥–åŠ±: {np.mean(training_stats['episode_rewards']):.2f}")
            print(f"   æˆåŠŸç‡: {np.mean(training_stats['episode_successes']):.1%}")
    
    def test(self, n_episodes=5):
        """æµ‹è¯•è®­ç»ƒå¥½çš„MADDPG"""
        print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•ç®€å•MADDPG {n_episodes} episodes...")
        
        success_count = 0
        episode_rewards = []
        episode_distances = []
        
        for episode in range(n_episodes):
            obs, info = self.env.reset()
            episode_reward = 0
            episode_success = False
            min_distance = float('inf')
            
            print(f"\n   ğŸ® Episode {episode+1} å¼€å§‹...")
            
            for step in range(100):
                # æ‰€æœ‰æ™ºèƒ½ä½“åä½œå†³ç­–ï¼ˆç¡®å®šæ€§ï¼‰
                actions = []
                for agent in self.agents:
                    action = agent.act(obs, noise_scale=0.0)  # æµ‹è¯•æ—¶ä¸åŠ å™ªå£°
                    actions.append(action[0])
                
                # æ‰§è¡Œè”åˆåŠ¨ä½œ
                joint_action = np.array(actions)
                obs, reward, terminated, truncated, info = self.env.step(joint_action)
                episode_reward += reward
                
                # æ£€æŸ¥æˆåŠŸå’Œè·ç¦»
                distance = info.get('distance_to_target', float('inf'))
                min_distance = min(min_distance, distance)
                
                if info.get('is_success', False):
                    episode_success = True
                
                if terminated or truncated:
                    break
            
            if episode_success:
                success_count += 1
            
            episode_rewards.append(episode_reward)
            episode_distances.append(min_distance)
            
            print(f"   ğŸ“Š Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, æœ€å°è·ç¦»={min_distance:.4f}, æˆåŠŸ={'âœ…' if episode_success else 'âŒ'}")
        
        success_rate = success_count / n_episodes
        avg_reward = np.mean(episode_rewards)
        avg_distance = np.mean(episode_distances)
        
        print(f"\nğŸ¯ ç®€å•MADDPGæµ‹è¯•ç»“æœ:")
        print(f"   ğŸ† æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{n_episodes})")
        print(f"   ğŸ’° å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"   ğŸ“ å¹³å‡æœ€å°è·ç¦»: {avg_distance:.4f}")
        
        return {
            'success_rate': success_rate,
            'avg_reward': avg_reward,
            'avg_distance': avg_distance
        }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ ç®€å•MADDPGè®­ç»ƒç³»ç»Ÿ (æ‰‹åŠ¨Lossè®¡ç®—)")
    print("ğŸ¤– ç­–ç•¥: æ¯ä¸ªå…³èŠ‚ç”±ç‹¬ç«‹DDPGæ™ºèƒ½ä½“æ§åˆ¶ï¼Œå¤šæ™ºèƒ½ä½“åä½œå­¦ä¹ ")
    print("ğŸ¯ ç›®æ ‡: 3å…³èŠ‚Reacherä»»åŠ¡")
    print("ğŸ‘€ ç‰¹ç‚¹: å®æ—¶å¯è§†åŒ– + è¯¦ç»†Lossæ˜¾ç¤º")
    print("ğŸ“Š ç›‘æ§: æ‰‹åŠ¨è®¡ç®—Actor Loss, Critic Loss, Episodeå¥–åŠ±, æˆåŠŸç‡")
    print()
    
    # åˆ›å»ºMADDPGç³»ç»Ÿ
    maddpg = SimpleMADDPG(num_joints=3)
    
    # è®­ç»ƒ
    print("\n" + "="*60)
    episode_count, training_stats = maddpg.train(total_timesteps=30000)
    
    # æµ‹è¯•
    print("\n" + "="*60)
    result = maddpg.test(n_episodes=3)
    
    print(f"\nğŸ‰ ç®€å•MADDPGè®­ç»ƒå®Œæˆ!")
    print(f"   ğŸ“ˆ è®­ç»ƒepisodes: {episode_count}")
    print(f"   ğŸ† æœ€ç»ˆæˆåŠŸç‡: {result['success_rate']:.1%}")
    print(f"   ğŸ’° å¹³å‡å¥–åŠ±: {result['avg_reward']:.2f}")
    print(f"   ğŸ“ å¹³å‡è·ç¦»: {result['avg_distance']:.4f}")
    
    print(f"\nğŸ” ç®€å•MADDPGç‰¹ç‚¹:")
    print(f"   âœ… æ‰‹åŠ¨å®ç°DDPGï¼Œå®Œå…¨æ§åˆ¶è®­ç»ƒè¿‡ç¨‹")
    print(f"   âœ… å®æ—¶æ˜¾ç¤ºæ¯ä¸ªæ™ºèƒ½ä½“çš„Actorå’ŒCriticæŸå¤±")
    print(f"   âœ… å¤šæ™ºèƒ½ä½“åä½œï¼šæ¯ä¸ªå…³èŠ‚ç‹¬ç«‹å­¦ä¹ ä½†å…±äº«ç¯å¢ƒ")
    print(f"   âœ… å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œç›´è§‚è§‚å¯Ÿå­¦ä¹ æ•ˆæœ")

if __name__ == "__main__":
    main()
