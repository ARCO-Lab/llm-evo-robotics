#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆMADDPG_SB3è®­ç»ƒè„šæœ¬ï¼š
1. ä½¿ç”¨å¤šä¸ªç‹¬ç«‹çš„DDPGæ™ºèƒ½ä½“
2. æ¯ä¸ªæ™ºèƒ½ä½“æ§åˆ¶ä¸€ä¸ªå…³èŠ‚
3. åœ¨åŒä¸€ä¸ªç¯å¢ƒä¸­åè°ƒè®­ç»ƒ
4. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
"""

import os
import numpy as np
import gymnasium as gym
from stable_baselines3 import DDPG
from stable_baselines3.common.noise import NormalActionNoise
from stable_baselines3.common.monitor import Monitor
import time

# è®¾ç½®æ¸²æŸ“ç¯å¢ƒå˜é‡
os.environ['MUJOCO_GL'] = 'glfw'
os.environ['MUJOCO_RENDERER'] = 'glfw'

# å¯¼å…¥ç¯å¢ƒ
from maddpg_complete_sequential_training import (
    create_env, 
    SUCCESS_THRESHOLD_RATIO, 
    SUCCESS_THRESHOLD_2JOINT
)

class SimpleMADDPG_SB3:
    """ç®€åŒ–ç‰ˆMADDPG - ä½¿ç”¨ç‹¬ç«‹çš„DDPGæ™ºèƒ½ä½“"""
    
    def __init__(self, num_joints=3):
        self.num_joints = num_joints
        self.agents = []
        
        print(f"ğŸŒŸ åˆ›å»ºç®€åŒ–ç‰ˆMADDPG_SB3: {num_joints}ä¸ªå…³èŠ‚")
        
        # åˆ›å»ºä¸»ç¯å¢ƒç”¨äºåè°ƒè®­ç»ƒ
        self.main_env = create_env(num_joints, render_mode='human', show_position_info=True)
        
        # ä¸ºæ¯ä¸ªå…³èŠ‚åˆ›å»ºç‹¬ç«‹çš„DDPGæ™ºèƒ½ä½“
        for i in range(num_joints):
            # åˆ›å»ºå•å…³èŠ‚ç¯å¢ƒï¼ˆç”¨äºæ™ºèƒ½ä½“è®­ç»ƒï¼‰
            single_env = create_env(num_joints, render_mode=None, show_position_info=False)
            
            # åŒ…è£…ä¸ºå•å…³èŠ‚åŠ¨ä½œç©ºé—´
            wrapped_env = SingleJointWrapper(single_env, joint_id=i, num_joints=num_joints)
            
            # æ·»åŠ åŠ¨ä½œå™ªå£°
            action_noise = NormalActionNoise(
                mean=np.zeros(1), 
                sigma=0.1 * np.ones(1)
            )
            
            # åˆ›å»ºDDPGæ™ºèƒ½ä½“
            agent = DDPG(
                "MlpPolicy",
                wrapped_env,
                learning_rate=1e-3,
                gamma=0.99,
                tau=0.005,
                action_noise=action_noise,
                verbose=0,
                device='cpu'
            )
            
            self.agents.append(agent)
            print(f"ğŸ¤– DDPGæ™ºèƒ½ä½“ {i}: å·²åˆ›å»ºï¼Œæ§åˆ¶å…³èŠ‚{i}")
        
        print(f"âœ… ç®€åŒ–ç‰ˆMADDPG_SB3åˆå§‹åŒ–å®Œæˆ")
    
    def train_coordinated(self, total_timesteps=10000):
        """åè°ƒè®­ç»ƒæ‰€æœ‰æ™ºèƒ½ä½“"""
        print(f"\nğŸ¯ å¼€å§‹åè°ƒè®­ç»ƒ {total_timesteps} æ­¥...")
        
        obs, _ = self.main_env.reset()
        episode_count = 0
        episode_reward = 0
        step_count = 0
        
        start_time = time.time()
        
        try:
            while step_count < total_timesteps:
                # è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ
                actions = []
                for i, agent in enumerate(self.agents):
                    action, _ = agent.predict(obs, deterministic=False)
                    actions.append(action[0])
                
                # æ‰§è¡Œè”åˆåŠ¨ä½œ
                next_obs, reward, terminated, truncated, info = self.main_env.step(np.array(actions))
                episode_reward += reward
                step_count += 1
                
                # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å­˜å‚¨ç»éªŒå¹¶è®­ç»ƒ
                for i, agent in enumerate(self.agents):
                    # ç®€åŒ–ï¼šæ¯ä¸ªæ™ºèƒ½ä½“çœ‹åˆ°ç›¸åŒçš„å…¨å±€çŠ¶æ€å’Œå¥–åŠ±
                    if hasattr(agent, 'replay_buffer') and len(agent.replay_buffer) > 0:
                        # åªæœ‰å½“bufferæœ‰æ•°æ®æ—¶æ‰è®­ç»ƒ
                        if step_count % 10 == 0:  # æ¯10æ­¥è®­ç»ƒä¸€æ¬¡
                            agent.train(gradient_steps=1)
                
                obs = next_obs
                
                # é‡ç½®ç¯å¢ƒ
                if terminated or truncated:
                    obs, _ = self.main_env.reset()
                    episode_count += 1
                    
                    if episode_count % 5 == 0:
                        print(f"   Episode {episode_count}: å¥–åŠ±={episode_reward:.2f}, æ­¥æ•°={step_count}")
                    
                    episode_reward = 0
                
                # å®šæœŸè¾“å‡ºè¿›åº¦
                if step_count % 1000 == 0:
                    elapsed_time = time.time() - start_time
                    print(f"   æ­¥æ•° {step_count}/{total_timesteps}, ç”¨æ—¶ {elapsed_time:.1f}s")
        
        except KeyboardInterrupt:
            print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        training_time = time.time() - start_time
        print(f"\nâœ… åè°ƒè®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ¯ æ€»episodes: {episode_count}")
        
        return episode_count
    
    def test(self, n_episodes=5):
        """æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“"""
        print(f"\nğŸ§ª å¼€å§‹æµ‹è¯• {n_episodes} episodes...")
        
        success_count = 0
        
        for episode in range(n_episodes):
            obs, info = self.main_env.reset()
            episode_reward = 0
            episode_success = False
            min_distance = float('inf')
            
            for step in range(100):  # æ¯ä¸ªepisodeæœ€å¤š100æ­¥
                # è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œï¼ˆç¡®å®šæ€§ï¼‰
                actions = []
                for agent in self.agents:
                    action, _ = agent.predict(obs, deterministic=True)
                    actions.append(action[0])
                
                obs, reward, terminated, truncated, info = self.main_env.step(np.array(actions))
                episode_reward += reward
                
                # æ£€æŸ¥æˆåŠŸ
                distance = info.get('distance_to_target', float('inf'))
                min_distance = min(min_distance, distance)
                
                if info.get('is_success', False):
                    episode_success = True
                
                if terminated or truncated:
                    break
            
            if episode_success:
                success_count += 1
            
            print(f"   Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, æœ€å°è·ç¦»={min_distance:.4f}, æˆåŠŸ={'âœ…' if episode_success else 'âŒ'}")
        
        success_rate = success_count / n_episodes
        print(f"\nğŸ¯ æµ‹è¯•ç»“æœ: æˆåŠŸç‡ {success_rate:.1%} ({success_count}/{n_episodes})")
        
        return success_rate

class SingleJointWrapper(gym.Wrapper):
    """å•å…³èŠ‚åŒ…è£…å™¨ - ç®€åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, env, joint_id, num_joints):
        super().__init__(env)
        self.joint_id = joint_id
        self.num_joints = num_joints
        
        # é‡æ–°å®šä¹‰åŠ¨ä½œç©ºé—´ä¸ºå•å…³èŠ‚
        from gymnasium.spaces import Box
        self.action_space = Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)
        
        # è§‚å¯Ÿç©ºé—´ä¿æŒä¸å˜
        self.observation_space = env.observation_space
    
    def step(self, action):
        # æ„å»ºå®Œæ•´åŠ¨ä½œï¼ˆå…¶ä»–å…³èŠ‚è®¾ä¸º0ï¼‰
        full_action = np.zeros(self.num_joints)
        full_action[self.joint_id] = action[0]
        
        return self.env.step(full_action)
    
    def reset(self, **kwargs):
        return self.env.reset(**kwargs)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ ç®€åŒ–ç‰ˆMADDPG_SB3è®­ç»ƒç³»ç»Ÿ")
    print("ğŸ¤– ç­–ç•¥: å¤šä¸ªç‹¬ç«‹DDPGæ™ºèƒ½ä½“åè°ƒæ§åˆ¶")
    print("ğŸ¯ ç›®æ ‡: 3å…³èŠ‚Reacherä»»åŠ¡")
    print()
    
    # åˆ›å»ºMADDPGç³»ç»Ÿ
    maddpg = SimpleMADDPG_SB3(num_joints=3)
    
    # åè°ƒè®­ç»ƒ
    episode_count = maddpg.train_coordinated(total_timesteps=5000)  # è¾ƒçŸ­çš„è®­ç»ƒç”¨äºæ¼”ç¤º
    
    # æµ‹è¯•
    success_rate = maddpg.test(n_episodes=5)
    
    print(f"\nğŸ‰ ç®€åŒ–ç‰ˆMADDPG_SB3å®Œæˆ!")
    print(f"   è®­ç»ƒepisodes: {episode_count}")
    print(f"   æœ€ç»ˆæˆåŠŸç‡: {success_rate:.1%}")

if __name__ == "__main__":
    main()
