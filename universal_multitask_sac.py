#!/usr/bin/env python3
"""
GPT-5å»ºè®®çš„çœŸæ­£é€šç”¨å¤šä»»åŠ¡SACæ¶æ„
æ”¯æŒ2-5å…³èŠ‚Reacherçš„åŒæ—¶è®­ç»ƒï¼Œä¸€å¥—æ¨¡å‹æ§åˆ¶æ‰€æœ‰å…³èŠ‚æ•°
"""

import os
import numpy as np
import gymnasium as gym
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.distributions import SquashedDiagGaussianDistribution
from stable_baselines3.common.type_aliases import Schedule
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.sac.policies import SACPolicy
import time
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
import tempfile
import xml.etree.ElementTree as ET

# è®¾ç½®æ¸²æŸ“ç¯å¢ƒå˜é‡
os.environ['MUJOCO_GL'] = 'glfw'
os.environ['MUJOCO_RENDERER'] = 'glfw'

# å…¨å±€é…ç½®
J_MAX = 5  # æ”¯æŒçš„æœ€å¤§å…³èŠ‚æ•°
SUPPORTED_JOINTS = [2, 3, 4, 5]  # æ”¯æŒçš„å…³èŠ‚æ•°åˆ—è¡¨

class JointTokenEncoder(nn.Module):
    """
    å…³èŠ‚Tokenç¼–ç å™¨
    è¾“å…¥: [cos Î¸_i, sin Î¸_i, vel_i, link_len_i, joint_id_onehot/J_max, parent_id_onehot/J_max]
    """
    
    def __init__(self, joint_token_dim: int = 64):
        super().__init__()
        # è¾“å…¥ç»´åº¦: 3 (cos, sin, vel) + 1 (link_len) + 1 (joint_id/J_max) + 1 (parent_id/J_max) = 6
        input_dim = 6
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, joint_token_dim),
            nn.ReLU(),
            nn.LayerNorm(joint_token_dim),
            nn.Linear(joint_token_dim, joint_token_dim),
            nn.ReLU(),
            nn.LayerNorm(joint_token_dim)
        )
        
        self.joint_token_dim = joint_token_dim
        
    def forward(self, joint_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            joint_features: [batch_size, J_max, 6]
        Returns:
            joint_tokens: [batch_size, J_max, joint_token_dim]
        """
        batch_size, num_joints, feature_dim = joint_features.shape
        
        # å±•å¹³å¤„ç†
        joint_features_flat = joint_features.view(-1, feature_dim)
        joint_tokens_flat = self.encoder(joint_features_flat)
        
        # æ¢å¤å½¢çŠ¶
        joint_tokens = joint_tokens_flat.view(batch_size, num_joints, self.joint_token_dim)
        
        return joint_tokens

class TaskTokenEncoder(nn.Module):
    """
    ä»»åŠ¡Tokenç¼–ç å™¨
    è¾“å…¥: [N/J_max, onehot_N (5ç»´), link_len_1..N, å…¶ä½™è¡¥0]
    """
    
    def __init__(self, task_token_dim: int = 64):
        super().__init__()
        # è¾“å…¥ç»´åº¦: 1 (N/J_max) + 5 (onehot_N) + J_max (link_lengths) = 1 + 5 + 5 = 11
        input_dim = 1 + len(SUPPORTED_JOINTS) + J_MAX
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, task_token_dim),
            nn.ReLU(),
            nn.LayerNorm(task_token_dim),
            nn.Linear(task_token_dim, task_token_dim),
            nn.ReLU(),
            nn.LayerNorm(task_token_dim)
        )
        
        self.task_token_dim = task_token_dim
        
    def forward(self, task_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            task_features: [batch_size, input_dim]
        Returns:
            task_tokens: [batch_size, task_token_dim]
        """
        return self.encoder(task_features)

class SharedBackbone(nn.Module):
    """
    å…±äº«éª¨å¹²ç½‘ç»œ
    å…³èŠ‚MLPç¼–ç  â†’ Multi-Head Self-Attention â†’ æ³¨æ„åŠ›æ± åŒ–
    """
    
    def __init__(self, joint_token_dim: int = 64, task_token_dim: int = 64, 
                 num_heads: int = 4, backbone_dim: int = 128):
        super().__init__()
        
        self.joint_encoder = JointTokenEncoder(joint_token_dim)
        self.task_encoder = TaskTokenEncoder(task_token_dim)
        
        # Multi-Head Self-Attention
        self.self_attention = nn.MultiheadAttention(
            embed_dim=joint_token_dim,
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )
        
        # æ³¨æ„åŠ›æ± åŒ–
        self.attention_pooling = nn.MultiheadAttention(
            embed_dim=joint_token_dim,
            num_heads=1,
            dropout=0.0,
            batch_first=True
        )
        
        # å…¨å±€ç‰¹å¾èåˆ
        self.global_fusion = nn.Sequential(
            nn.Linear(joint_token_dim + task_token_dim, backbone_dim),
            nn.ReLU(),
            nn.LayerNorm(backbone_dim),
            nn.Linear(backbone_dim, backbone_dim),
            nn.ReLU(),
            nn.LayerNorm(backbone_dim)
        )
        
        self.backbone_dim = backbone_dim
        
    def forward(self, joint_features: torch.Tensor, task_features: torch.Tensor, 
                joint_mask: torch.Tensor) -> torch.Tensor:
        """
        Args:
            joint_features: [batch_size, J_max, 6]
            task_features: [batch_size, input_dim]
            joint_mask: [batch_size, J_max] - Trueè¡¨ç¤ºæœ‰æ•ˆå…³èŠ‚ï¼ŒFalseè¡¨ç¤ºpadding
        Returns:
            backbone_features: [batch_size, backbone_dim]
        """
        batch_size = joint_features.shape[0]
        
        # ç¼–ç å…³èŠ‚å’Œä»»åŠ¡
        joint_tokens = self.joint_encoder(joint_features)  # [B, J_max, joint_token_dim]
        task_token = self.task_encoder(task_features)      # [B, task_token_dim]
        
        # Self-Attention with mask
        # key_padding_mask: Trueè¡¨ç¤ºéœ€è¦maskçš„ä½ç½®ï¼ˆpaddingï¼‰
        key_padding_mask = ~joint_mask  # åè½¬maskï¼Œå› ä¸ºMultiheadAttentionçš„maskè¯­ä¹‰ç›¸å
        
        attended_joints, _ = self.self_attention(
            query=joint_tokens,
            key=joint_tokens,
            value=joint_tokens,
            key_padding_mask=key_padding_mask
        )  # [B, J_max, joint_token_dim]
        
        # æ³¨æ„åŠ›æ± åŒ–ï¼šä½¿ç”¨task_tokenä½œä¸ºquery
        task_query = task_token.unsqueeze(1)  # [B, 1, task_token_dim]
        
        # éœ€è¦è°ƒæ•´ç»´åº¦åŒ¹é…
        if task_query.shape[-1] != attended_joints.shape[-1]:
            task_query = F.linear(task_query, 
                                torch.randn(attended_joints.shape[-1], task_query.shape[-1]).to(task_query.device))
        
        pooled_joints, _ = self.attention_pooling(
            query=task_query,
            key=attended_joints,
            value=attended_joints,
            key_padding_mask=key_padding_mask
        )  # [B, 1, joint_token_dim]
        
        pooled_joints = pooled_joints.squeeze(1)  # [B, joint_token_dim]
        
        # èåˆå…¨å±€ç‰¹å¾
        global_features = torch.cat([pooled_joints, task_token], dim=-1)  # [B, joint_token_dim + task_token_dim]
        backbone_features = self.global_fusion(global_features)  # [B, backbone_dim]
        
        return backbone_features

class VariableActorHead(nn.Module):
    """
    å¯å˜ç»´åº¦Actorå¤´
    è¾“å‡ºJ_maxä¸ªå…³èŠ‚çš„(Î¼, logÏƒ)ï¼Œæ¨ç†/è®­ç»ƒæ—¶åªå–å‰Nç»´
    """
    
    def __init__(self, backbone_dim: int = 128, action_dim: int = J_MAX):
        super().__init__()
        
        self.mean_head = nn.Sequential(
            nn.Linear(backbone_dim, backbone_dim),
            nn.ReLU(),
            nn.Linear(backbone_dim, action_dim)
        )
        
        self.log_std_head = nn.Sequential(
            nn.Linear(backbone_dim, backbone_dim),
            nn.ReLU(),
            nn.Linear(backbone_dim, action_dim)
        )
        
        self.action_dim = action_dim
        
    def forward(self, backbone_features: torch.Tensor, num_joints: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            backbone_features: [batch_size, backbone_dim]
            num_joints: å®é™…å…³èŠ‚æ•°
        Returns:
            mean: [batch_size, num_joints]
            log_std: [batch_size, num_joints]
        """
        full_mean = self.mean_head(backbone_features)      # [B, J_max]
        full_log_std = self.log_std_head(backbone_features) # [B, J_max]
        
        # åªå–å‰num_jointsç»´
        mean = full_mean[:, :num_joints]
        log_std = full_log_std[:, :num_joints]
        
        # é™åˆ¶log_stdèŒƒå›´
        log_std = torch.clamp(log_std, -20, 2)
        
        return mean, log_std

class MultiHeadCritic(nn.Module):
    """
    å¤šå¤´Criticï¼šæ¯ä¸ªNä¸€ä¸ªQ1/Q2å¤´
    """
    
    def __init__(self, backbone_dim: int = 128, supported_joints: List[int] = SUPPORTED_JOINTS):
        super().__init__()
        
        self.supported_joints = supported_joints
        
        # ä¸ºæ¯ä¸ªå…³èŠ‚æ•°åˆ›å»ºç‹¬ç«‹çš„Q1/Q2å¤´
        self.q1_heads = nn.ModuleDict()
        self.q2_heads = nn.ModuleDict()
        
        for num_joints in supported_joints:
            self.q1_heads[str(num_joints)] = nn.Sequential(
                nn.Linear(backbone_dim + num_joints, backbone_dim),
                nn.ReLU(),
                nn.Linear(backbone_dim, 1)
            )
            
            self.q2_heads[str(num_joints)] = nn.Sequential(
                nn.Linear(backbone_dim + num_joints, backbone_dim),
                nn.ReLU(),
                nn.Linear(backbone_dim, 1)
            )
    
    def forward(self, backbone_features: torch.Tensor, actions: torch.Tensor, 
                num_joints: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            backbone_features: [batch_size, backbone_dim]
            actions: [batch_size, num_joints]
            num_joints: å…³èŠ‚æ•°
        Returns:
            q1_values: [batch_size, 1]
            q2_values: [batch_size, 1]
        """
        if num_joints not in self.supported_joints:
            raise ValueError(f"Unsupported num_joints: {num_joints}")
        
        # æ‹¼æ¥ç‰¹å¾å’ŒåŠ¨ä½œ
        q_input = torch.cat([backbone_features, actions], dim=-1)
        
        # ä½¿ç”¨å¯¹åº”çš„å¤´
        key = str(num_joints)
        q1_values = self.q1_heads[key](q_input)
        q2_values = self.q2_heads[key](q_input)
        
        return q1_values, q2_values

class PerTaskEntropy(nn.Module):
    """
    æ¯ä»»åŠ¡ç†µæ¸©åº¦Î±
    """
    
    def __init__(self, supported_joints: List[int] = SUPPORTED_JOINTS):
        super().__init__()
        
        self.supported_joints = supported_joints
        
        # ä¸ºæ¯ä¸ªå…³èŠ‚æ•°åˆ›å»ºç‹¬ç«‹çš„log_alpha
        self.log_alphas = nn.ParameterDict()
        for num_joints in supported_joints:
            self.log_alphas[str(num_joints)] = nn.Parameter(torch.zeros(1))
    
    def get_alpha(self, num_joints: int) -> torch.Tensor:
        """è·å–æŒ‡å®šå…³èŠ‚æ•°çš„ç†µæ¸©åº¦"""
        if num_joints not in self.supported_joints:
            raise ValueError(f"Unsupported num_joints: {num_joints}")
        
        key = str(num_joints)
        return torch.exp(self.log_alphas[key])
    
    def get_log_alpha(self, num_joints: int) -> torch.Tensor:
        """è·å–æŒ‡å®šå…³èŠ‚æ•°çš„logç†µæ¸©åº¦"""
        if num_joints not in self.supported_joints:
            raise ValueError(f"Unsupported num_joints: {num_joints}")
        
        key = str(num_joints)
        return self.log_alphas[key]

class UniversalMultiTaskExtractor(BaseFeaturesExtractor):
    """
    é€šç”¨å¤šä»»åŠ¡ç‰¹å¾æå–å™¨
    æ•´åˆæ‰€æœ‰ç»„ä»¶
    """
    
    def __init__(self, observation_space: gym.Space, features_dim: int = 128):
        super().__init__(observation_space, features_dim)
        
        self.backbone = SharedBackbone(
            joint_token_dim=64,
            task_token_dim=64,
            num_heads=4,
            backbone_dim=features_dim
        )
        
        print(f"ğŸ”§ UniversalMultiTaskExtractor: æ”¯æŒ{SUPPORTED_JOINTS}å…³èŠ‚")
        print(f"   J_max = {J_MAX}")
        print(f"   ç‰¹å¾ç»´åº¦: {features_dim}")
        
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        Args:
            observations: [batch_size, obs_dim]
        Returns:
            features: [batch_size, features_dim]
        """
        batch_size = observations.shape[0]
        
        # è§£æè§‚å¯Ÿï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´ï¼‰
        joint_features, task_features, joint_mask = self._parse_observations(observations)
        
        # é€šè¿‡å…±äº«éª¨å¹²
        features = self.backbone(joint_features, task_features, joint_mask)
        
        return features
    
    def _parse_observations(self, observations: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è§£æè§‚å¯Ÿä¸ºå…³èŠ‚ç‰¹å¾ã€ä»»åŠ¡ç‰¹å¾å’Œmask
        è¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦æ ¹æ®ç¯å¢ƒè°ƒæ•´
        """
        batch_size = observations.shape[0]
        
        # å‡è®¾è§‚å¯Ÿæ ¼å¼ï¼š[joint_features..., task_info...]
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç¯å¢ƒwrapperçš„è¾“å‡ºæ ¼å¼è°ƒæ•´
        
        # åˆ›å»ºdummyæ•°æ®ä½œä¸ºç¤ºä¾‹
        joint_features = torch.zeros(batch_size, J_MAX, 6).to(observations.device)
        task_features = torch.zeros(batch_size, 1 + len(SUPPORTED_JOINTS) + J_MAX).to(observations.device)
        joint_mask = torch.ones(batch_size, J_MAX, dtype=torch.bool).to(observations.device)
        
        # å‡è®¾å‰2ä¸ªå…³èŠ‚æœ‰æ•ˆ
        joint_mask[:, 2:] = False
        
        return joint_features, task_features, joint_mask

def generate_multi_joint_reacher_xml(num_joints: int, link_lengths: List[float], 
                                   link_masses: List[float]) -> str:
    """ç”ŸæˆNå…³èŠ‚Reacherçš„MuJoCo XML"""
    
    if len(link_lengths) != num_joints or len(link_masses) != num_joints:
        raise ValueError("link_lengthså’Œlink_massesçš„é•¿åº¦å¿…é¡»ç­‰äºnum_joints")
    
    xml_template = f'''
    <mujoco model="reacher_{num_joints}joint">
      <compiler angle="radian" inertiafromgeom="true"/>
      <default>
        <joint armature="1" damping="1" limited="true"/>
        <geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
      </default>
      <option gravity="0 0 -9.81" integrator="RK4" timestep="0.01"/>
      
      <worldbody>
        <geom conaffinity="0" contype="0" name="ground" pos="0 0 0" rgba="0.9 0.9 0.9 1" size="1 1 10" type="plane"/>
        <geom conaffinity="1" contype="1" name="sideS" pos="0 -1 0" rgba="0.9 0.4 0.6 1" size="1 0.02 1" type="box"/>
        <geom conaffinity="1" contype="1" name="sideE" pos="1 0 0" rgba="0.9 0.4 0.6 1" size="0.02 1 1" type="box"/>
        <geom conaffinity="1" contype="1" name="sideN" pos="0 1 0" rgba="0.9 0.4 0.6 1" size="1 0.02 1" type="box"/>
        <geom conaffinity="1" contype="1" name="sideW" pos="-1 0 0" rgba="0.9 0.4 0.6 1" size="0.02 1 1" type="box"/>
        
        <body name="body0" pos="0 0 0">
          <joint axis="0 0 1" limited="true" name="joint0" pos="0 0 0" range="-3.14159 3.14159" type="hinge"/>
          <geom fromto="0 0 0 {link_lengths[0]} 0 0" name="link0" size="0.02" type="capsule"/>
    '''
    
    # æ·»åŠ åç»­å…³èŠ‚å’Œé“¾æ¥
    for i in range(1, num_joints):
        prev_length = sum(link_lengths[:i])
        xml_template += f'''
          <body name="body{i}" pos="{link_lengths[i-1]} 0 0">
            <joint axis="0 0 1" limited="true" name="joint{i}" pos="0 0 0" range="-3.14159 3.14159" type="hinge"/>
            <geom fromto="0 0 0 {link_lengths[i]} 0 0" name="link{i}" size="0.02" type="capsule"/>
        '''
    
    # æ·»åŠ æœ«ç«¯æ‰§è¡Œå™¨
    total_length = sum(link_lengths)
    xml_template += f'''
            <body name="fingertip" pos="{link_lengths[-1]} 0 0">
              <geom name="fingertip" pos="0 0 0" rgba="0.0 0.8 0.0 1" size="0.01" type="sphere"/>
            </body>
    '''
    
    # å…³é—­æ‰€æœ‰bodyæ ‡ç­¾
    for i in range(num_joints):
        xml_template += '          </body>\n'
    
    # æ·»åŠ ç›®æ ‡
    xml_template += '''
        <body name="target" pos="0.4 0.4 0">
          <joint armature="0" axis="1 0 0" damping="0" limited="true" name="target_x" pos="0 0 0" range="-0.45 0.45" ref="0.4" stiffness="0" type="slide"/>
          <joint armature="0" axis="0 1 0" damping="0" limited="true" name="target_y" pos="0 0 0" range="-0.45 0.45" ref="0.4" stiffness="0" type="slide"/>
          <geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size="0.02" type="sphere"/>
        </body>
      </worldbody>
      
      <actuator>
    '''
    
    # æ·»åŠ æ‰§è¡Œå™¨
    for i in range(num_joints):
        xml_template += f'    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="joint{i}" gear="200"/>\n'
    
    xml_template += '''
      </actuator>
    </mujoco>
    '''
    
    return xml_template

from gymnasium.envs.mujoco import MujocoEnv

class RealMultiJointReacherEnv(MujocoEnv):
    """çœŸå®çš„Nå…³èŠ‚Reacherç¯å¢ƒ"""
    
    def __init__(self, num_joints: int, link_lengths: List[float], 
                 link_masses: List[float], render_mode: Optional[str] = None, **kwargs):
        
        self.num_joints = num_joints
        self.link_lengths = link_lengths
        self.link_masses = link_masses
        
        # ç”ŸæˆXML
        xml_string = generate_multi_joint_reacher_xml(num_joints, link_lengths, link_masses)
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        self.temp_xml_file = tempfile.NamedTemporaryFile(mode='w', suffix='.xml', delete=False)
        self.temp_xml_file.write(xml_string)
        self.temp_xml_file.close()
        
        # å®šä¹‰è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´
        obs_dim = 2 * num_joints + 2 * num_joints + 4  # cos, sin, vel, ee_pos, target_pos
        observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float64)
        action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(num_joints,), dtype=np.float32)
        
        # åˆå§‹åŒ–MuJoCoç¯å¢ƒ
        super().__init__(
            model_path=self.temp_xml_file.name,
            frame_skip=2,
            observation_space=observation_space,
            render_mode=render_mode,
            **kwargs
        )
        
        self.action_space = action_space
        self.step_count = 0
        self.max_episode_steps = 50
        
        print(f"âœ… RealMultiJointReacherEnv ({num_joints}å…³èŠ‚) åˆ›å»ºå®Œæˆ")
        print(f"   è§‚å¯Ÿç©ºé—´: {self.observation_space}")
        print(f"   åŠ¨ä½œç©ºé—´: {self.action_space}")
        print(f"   æœ€å¤§episodeæ­¥æ•°: {self.max_episode_steps}")
    
    def step(self, action):
        # æ˜¾å¼æ¸²æŸ“ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.render_mode == 'human':
            self.render()
        
        self.do_simulation(action, self.frame_skip)
        
        obs = self._get_obs()
        reward = self._get_reward()
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        ee_pos = self.get_body_com("fingertip")[:2]
        target_pos = self.get_body_com("target")[:2]
        distance_to_target = np.linalg.norm(ee_pos - target_pos)
        
        terminated = distance_to_target < 0.02
        
        self.step_count += 1
        truncated = self.step_count >= self.max_episode_steps
        
        info = {
            'distance_to_target': distance_to_target,
            'is_success': terminated
        }
        
        return obs, reward, terminated, truncated, info
    
    def _get_obs(self):
        # å…³èŠ‚è§’åº¦çš„coså’Œsin
        joint_cos = np.cos(self.data.qpos[:self.num_joints])
        joint_sin = np.sin(self.data.qpos[:self.num_joints])
        
        # å…³èŠ‚é€Ÿåº¦
        joint_vel = self.data.qvel[:self.num_joints]
        
        # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®
        ee_pos = self.get_body_com("fingertip")[:2]
        
        # ç›®æ ‡ä½ç½®
        target_pos = self.get_body_com("target")[:2]
        
        return np.concatenate([joint_cos, joint_sin, joint_vel, ee_pos, target_pos])
    
    def _get_reward(self):
        ee_pos = self.get_body_com("fingertip")[:2]
        target_pos = self.get_body_com("target")[:2]
        distance = np.linalg.norm(ee_pos - target_pos)
        
        # è·ç¦»å¥–åŠ±
        reward = -distance
        
        # åˆ°è¾¾å¥–åŠ±
        if distance < 0.02:
            reward += 10.0
        
        return reward
    
    def reset_model(self):
        # éšæœºåˆå§‹åŒ–å…³èŠ‚è§’åº¦
        qpos = self.init_qpos + self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq)
        qvel = self.init_qvel + self.np_random.standard_normal(self.model.nv) * 0.1
        
        # éšæœºç›®æ ‡ä½ç½®
        qpos[-2:] = self.np_random.uniform(low=-0.4, high=0.4, size=2)
        
        self.set_state(qpos, qvel)
        self.step_count = 0
        
        return self._get_obs()
    
    def viewer_setup(self):
        assert self.viewer is not None
        self.viewer.cam.trackbodyid = 0

class UniversalMultiTaskWrapper(gym.Wrapper):
    """
    é€šç”¨å¤šä»»åŠ¡åŒ…è£…å™¨
    å°†ä¸åŒå…³èŠ‚æ•°çš„ç¯å¢ƒç»Ÿä¸€ä¸ºç›¸åŒçš„è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´
    """
    
    def __init__(self, env, num_joints: int, link_lengths: List[float]):
        super().__init__(env)
        
        self.num_joints = num_joints
        self.link_lengths = link_lengths
        
        # ç»Ÿä¸€è§‚å¯Ÿç©ºé—´ï¼šå…³èŠ‚ç‰¹å¾ + ä»»åŠ¡ç‰¹å¾
        # å…³èŠ‚ç‰¹å¾: J_max * 6 (cos, sin, vel, link_len, joint_id/J_max, parent_id/J_max)
        # ä»»åŠ¡ç‰¹å¾: 1 + len(SUPPORTED_JOINTS) + J_MAX
        joint_feature_dim = J_MAX * 6
        task_feature_dim = 1 + len(SUPPORTED_JOINTS) + J_MAX
        total_obs_dim = joint_feature_dim + task_feature_dim
        
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(total_obs_dim,), dtype=np.float32
        )
        
        # ç»Ÿä¸€åŠ¨ä½œç©ºé—´ä¸ºJ_maxç»´
        self.action_space = gym.spaces.Box(
            low=-1.0, high=1.0, shape=(J_MAX,), dtype=np.float32
        )
        
        print(f"ğŸ”§ UniversalMultiTaskWrapper ({num_joints}å…³èŠ‚)")
        print(f"   ç»Ÿä¸€è§‚å¯Ÿç©ºé—´: {self.observation_space.shape}")
        print(f"   ç»Ÿä¸€åŠ¨ä½œç©ºé—´: {self.action_space.shape}")
    
    def step(self, action):
        # åªä½¿ç”¨å‰num_jointsç»´åŠ¨ä½œ
        real_action = action[:self.num_joints]
        
        obs, reward, terminated, truncated, info = self.env.step(real_action)
        
        # è½¬æ¢è§‚å¯Ÿ
        unified_obs = self._transform_observation(obs)
        
        return unified_obs, reward, terminated, truncated, info
    
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        unified_obs = self._transform_observation(obs)
        return unified_obs, info
    
    def _transform_observation(self, obs: np.ndarray) -> np.ndarray:
        """å°†åŸå§‹è§‚å¯Ÿè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼"""
        
        # è§£æåŸå§‹è§‚å¯Ÿ
        joint_cos = obs[:self.num_joints]
        joint_sin = obs[self.num_joints:2*self.num_joints]
        joint_vel = obs[2*self.num_joints:3*self.num_joints]
        ee_pos = obs[3*self.num_joints:3*self.num_joints+2]
        target_pos = obs[3*self.num_joints+2:3*self.num_joints+4]
        
        # æ„å»ºå…³èŠ‚ç‰¹å¾ [J_max, 6]
        joint_features = np.zeros((J_MAX, 6))
        
        for i in range(self.num_joints):
            joint_features[i, 0] = joint_cos[i]  # cos
            joint_features[i, 1] = joint_sin[i]  # sin
            joint_features[i, 2] = joint_vel[i]  # vel
            joint_features[i, 3] = self.link_lengths[i] if i < len(self.link_lengths) else 0.0  # link_len
            joint_features[i, 4] = (i + 1) / J_MAX  # joint_id/J_max
            joint_features[i, 5] = i / J_MAX if i > 0 else 0.0  # parent_id/J_max
        
        # æ„å»ºä»»åŠ¡ç‰¹å¾
        task_features = np.zeros(1 + len(SUPPORTED_JOINTS) + J_MAX)
        task_features[0] = self.num_joints / J_MAX  # N/J_max
        
        # onehot_N
        if self.num_joints in SUPPORTED_JOINTS:
            idx = SUPPORTED_JOINTS.index(self.num_joints)
            task_features[1 + idx] = 1.0
        
        # link_lengths (å‰Nä¸ªæœ‰æ•ˆï¼Œå…¶ä½™ä¸º0)
        for i in range(min(self.num_joints, len(self.link_lengths))):
            task_features[1 + len(SUPPORTED_JOINTS) + i] = self.link_lengths[i]
        
        # å±•å¹³å¹¶æ‹¼æ¥
        joint_features_flat = joint_features.flatten()
        unified_obs = np.concatenate([joint_features_flat, task_features])
        
        return unified_obs.astype(np.float32)

def create_multi_joint_env(num_joints: int, render_mode: Optional[str] = None):
    """åˆ›å»ºæŒ‡å®šå…³èŠ‚æ•°çš„ç¯å¢ƒ"""
    
    # é»˜è®¤é“¾æ¥é•¿åº¦å’Œè´¨é‡
    if num_joints == 2:
        link_lengths = [0.1, 0.11]
        link_masses = [1.0, 1.0]
    elif num_joints == 3:
        link_lengths = [0.1, 0.1, 0.1]
        link_masses = [1.0, 1.0, 1.0]
    elif num_joints == 4:
        link_lengths = [0.08, 0.08, 0.08, 0.08]
        link_masses = [1.0, 1.0, 1.0, 1.0]
    elif num_joints == 5:
        link_lengths = [0.06, 0.06, 0.06, 0.06, 0.06]
        link_masses = [1.0, 1.0, 1.0, 1.0, 1.0]
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å…³èŠ‚æ•°: {num_joints}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = RealMultiJointReacherEnv(
        num_joints=num_joints,
        link_lengths=link_lengths,
        link_masses=link_masses,
        render_mode=render_mode
    )
    
    # åŒ…è£…
    env = UniversalMultiTaskWrapper(env, num_joints, link_lengths)
    env = Monitor(env)
    
    return env

def make_env(num_joints: int, render_mode: Optional[str] = None):
    """åˆ›å»ºç¯å¢ƒçš„å·¥å‚å‡½æ•°"""
    def _init():
        return create_multi_joint_env(num_joints, render_mode)
    return _init

def train_universal_multitask_sac(total_timesteps: int = 50000):
    """
    è®­ç»ƒé€šç”¨å¤šä»»åŠ¡SAC
    åŒæ—¶è®­ç»ƒ2-5å…³èŠ‚Reacher
    """
    print("ğŸš€ é€šç”¨å¤šä»»åŠ¡SACè®­ç»ƒ")
    print(f"ğŸ¯ åŒæ—¶è®­ç»ƒ{SUPPORTED_JOINTS}å…³èŠ‚Reacher")
    print(f"ğŸ’¡ ä¸€å¥—æ¨¡å‹æ§åˆ¶æ‰€æœ‰å…³èŠ‚æ•°")
    print("="*60)
    
    # åˆ›å»ºå¤šä»»åŠ¡å¹¶è¡Œç¯å¢ƒ
    print("ğŸŒ åˆ›å»ºå¤šä»»åŠ¡å¹¶è¡Œç¯å¢ƒ...")
    
    env_fns = []
    for num_joints in SUPPORTED_JOINTS:
        # æ¯ä¸ªå…³èŠ‚æ•°åˆ›å»ºä¸€ä¸ªç¯å¢ƒå®ä¾‹
        env_fns.append(make_env(num_joints, render_mode='human'))
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
    train_env = SubprocVecEnv(env_fns)
    
    print(f"âœ… å¤šä»»åŠ¡å¹¶è¡Œç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"   ç¯å¢ƒæ•°é‡: {len(env_fns)}")
    print(f"   æ”¯æŒå…³èŠ‚æ•°: {SUPPORTED_JOINTS}")
    print(f"   è§‚å¯Ÿç©ºé—´: {train_env.observation_space}")
    print(f"   åŠ¨ä½œç©ºé—´: {train_env.action_space}")
    
    # åˆ›å»ºSACæ¨¡å‹
    print("\nğŸ¤– åˆ›å»ºé€šç”¨å¤šä»»åŠ¡SACæ¨¡å‹...")
    
    policy_kwargs = {
        'features_extractor_class': UniversalMultiTaskExtractor,
        'features_extractor_kwargs': {'features_dim': 128},
    }
    
    model = SAC(
        'MlpPolicy',
        train_env,
        policy_kwargs=policy_kwargs,
        verbose=2,
        learning_starts=1000,
        device='cpu',
        tensorboard_log="./tensorboard_logs/universal_multitask/",
        batch_size=256,
        buffer_size=100000,
        learning_rate=3e-4,
        gamma=0.99,
        tau=0.005,
        ent_coef='auto',
    )
    
    print("âœ… é€šç”¨å¤šä»»åŠ¡SACæ¨¡å‹åˆ›å»ºå®Œæˆ")
    print("   âœ… ä½¿ç”¨é€šç”¨å¤šä»»åŠ¡ç‰¹å¾æå–å™¨")
    print("   âœ… æ”¯æŒ2-5å…³èŠ‚åŒæ—¶è®­ç»ƒ")
    print("   âœ… å¹¶è¡Œè®­ç»ƒåŠ é€Ÿ")
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹é€šç”¨å¤šä»»åŠ¡è®­ç»ƒ ({total_timesteps}æ­¥)...")
    print("ğŸ’¡ æ‚¨å°†çœ‹åˆ°å¤šä¸ªå…³èŠ‚æ•°çš„ReacheråŒæ—¶å­¦ä¹ ")
    print("ğŸ’¡ è§‚å¯Ÿä¸åŒå…³èŠ‚æ•°çš„å­¦ä¹ è¿›å±•")
    
    try:
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            log_interval=4,
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        
        print(f"\nâœ… é€šç”¨å¤šä»»åŠ¡è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“Š å¹³å‡FPS: {total_timesteps/training_time:.1f}")
        
        # ä¿å­˜æ¨¡å‹
        model.save("models/universal_multitask_sac")
        print("ğŸ’¾ é€šç”¨å¤šä»»åŠ¡æ¨¡å‹å·²ä¿å­˜: models/universal_multitask_sac")
        
        return model
        
    except KeyboardInterrupt:
        training_time = time.time() - start_time
        print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print(f"â±ï¸ å·²è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        
        model.save("models/universal_multitask_sac_interrupted")
        print("ğŸ’¾ ä¸­æ–­æ¨¡å‹å·²ä¿å­˜")
        return model
    
    finally:
        train_env.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ GPT-5é€šç”¨å¤šä»»åŠ¡SACç³»ç»Ÿ")
    print("ğŸ¯ ä¸€å¥—æ¨¡å‹åŒæ—¶è®­ç»ƒ2-5å…³èŠ‚Reacher")
    print("ğŸ’¡ çœŸæ­£çš„é€šç”¨å¤šä»»åŠ¡æ¶æ„")
    print()
    
    try:
        # è®­ç»ƒé˜¶æ®µ
        print("ğŸš€ å¼€å§‹è®­ç»ƒé˜¶æ®µ...")
        model = train_universal_multitask_sac(total_timesteps=50000)
        
        print(f"\nğŸ‰ é€šç”¨å¤šä»»åŠ¡è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ’¡ ä¸€å¥—æ¨¡å‹ç°åœ¨å¯ä»¥æ§åˆ¶2-5å…³èŠ‚çš„Reacher")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è¢«ç”¨æˆ·ä¸­æ–­")
        print("ğŸ’¡ æ¨¡å‹å·²ä¿å­˜ï¼Œå¯ä»¥ç¨åç»§ç»­è®­ç»ƒæˆ–æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
