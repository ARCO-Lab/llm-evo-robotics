#!/usr/bin/env python3
"""
è½»é‡çº§é€šç”¨æ³¨æ„åŠ› SAC
ä¿æŒåŸå§‹æ¶æ„çš„ç®€æ´æ€§ï¼Œæœ€å°åŒ–ä¿®æ”¹å®ç°é€šç”¨æ€§
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import time
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.evaluation import evaluate_policy
from typing import Dict, List, Tuple, Type, Union
import math

class LightweightAttentionLayer(nn.Module):
    """
    è½»é‡çº§æ³¨æ„åŠ›å±‚ - åŸºäºåŸå§‹ AttentionLayerï¼Œæœ€å°åŒ–ä¿®æ”¹
    """
    def __init__(self, input_dim: int, hidden_dim: int = 64, num_heads: int = 4):
        super(LightweightAttentionLayer, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        assert hidden_dim % num_heads == 0, "hidden_dim must be divisible by num_heads"
        
        # ä¿æŒä¸åŸå§‹ç›¸åŒçš„ç»“æ„
        self.query = nn.Linear(input_dim, hidden_dim)
        self.key = nn.Linear(input_dim, hidden_dim)
        self.value = nn.Linear(input_dim, hidden_dim)
        
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(0.1)
        
        # è¾“å…¥æŠ•å½± (å¦‚æœéœ€è¦)
        if input_dim != hidden_dim:
            self.input_proj = nn.Linear(input_dim, hidden_dim)
        else:
            self.input_proj = None
        
        print(f"ğŸ§  LightweightAttentionLayer: {input_dim} â†’ {hidden_dim}, {num_heads} heads")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ - ä¸åŸå§‹ AttentionLayer ä¿æŒä¸€è‡´
        x: [batch_size, input_dim] æˆ– [batch_size, seq_len, input_dim]
        """
        batch_size = x.size(0)
        
        # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œæ‰©å±•ä¸º3D
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [batch_size, 1, input_dim]
            squeeze_output = True
        else:
            squeeze_output = False
        
        seq_len = x.size(1)
        
        # è®¡ç®— Q, K, V
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)
        
        # å¤šå¤´æ³¨æ„åŠ›
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        attended = torch.matmul(attention_weights, V)
        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_proj(attended)
        
        # æ®‹å·®è¿æ¥
        if self.input_proj is not None:
            x_proj = self.input_proj(x)
        else:
            x_proj = x
        
        output = self.layer_norm(output + x_proj)
        
        # å‹ç¼©è¾“å‡º
        if squeeze_output:
            output = output.squeeze(1)
        
        return output

class LightweightUniversalExtractor(BaseFeaturesExtractor):
    """
    è½»é‡çº§é€šç”¨ç‰¹å¾æå–å™¨
    æœ€å°åŒ–ä¿®æ”¹åŸå§‹æ¶æ„ï¼Œä¿æŒæ€§èƒ½
    """
    def __init__(self, observation_space: gym.Space, features_dim: int = 128, num_joints: int = 2):
        super(LightweightUniversalExtractor, self).__init__(observation_space, features_dim)
        
        self.obs_dim = observation_space.shape[0]
        self.num_joints = num_joints
        
        print(f"ğŸŒŸ LightweightUniversalExtractor åˆå§‹åŒ–:")
        print(f"   è§‚å¯Ÿç©ºé—´ç»´åº¦: {self.obs_dim}")
        print(f"   å…³èŠ‚æ•°é‡: {num_joints}")
        print(f"   è¾“å‡ºç‰¹å¾ç»´åº¦: {features_dim}")
        print(f"   è®¾è®¡ç†å¿µ: æœ€å°åŒ–ä¿®æ”¹ï¼Œä¿æŒæ€§èƒ½")
        
        # å…³é”®æ”¹è¿›ï¼šæ™ºèƒ½è§‚å¯Ÿç©ºé—´å¤„ç†
        if num_joints == 2 and self.obs_dim == 10:
            # MuJoCo Reacher-v5 - ä½¿ç”¨åŸå§‹æ¶æ„çš„é¢„å¤„ç†
            self.input_layer = nn.Sequential(
                nn.Linear(self.obs_dim, 64),  # ä¿æŒåŸå§‹è®¾è®¡
                nn.ReLU(),
                nn.LayerNorm(64)
            )
            self.use_joint_separation = False
            
        else:
            # é€šç”¨æƒ…å†µ - åˆ†ç¦»å…³èŠ‚å’Œå…¨å±€ç‰¹å¾
            joint_dim = num_joints * 2  # æ¯ä¸ªå…³èŠ‚ï¼šè§’åº¦ + é€Ÿåº¦
            global_dim = max(0, self.obs_dim - joint_dim)
            
            # å…³èŠ‚ç‰¹å¾å¤„ç†
            self.joint_processor = nn.Sequential(
                nn.Linear(joint_dim, 32),
                nn.ReLU(),
                nn.LayerNorm(32)
            )
            
            # å…¨å±€ç‰¹å¾å¤„ç†
            if global_dim > 0:
                self.global_processor = nn.Sequential(
                    nn.Linear(global_dim, 32),
                    nn.ReLU(),
                    nn.LayerNorm(32)
                )
                fusion_dim = 64  # 32 + 32
            else:
                self.global_processor = None
                fusion_dim = 32
            
            # èåˆå±‚
            self.input_layer = nn.Sequential(
                nn.Linear(fusion_dim, 64),
                nn.ReLU(),
                nn.LayerNorm(64)
            )
            self.use_joint_separation = True
        
        # æ³¨æ„åŠ›å±‚ - ä¿æŒä¸åŸå§‹ç›¸åŒ
        self.attention = LightweightAttentionLayer(
            input_dim=64,
            hidden_dim=features_dim,
            num_heads=4
        )
        
        # è¾“å‡ºå±‚ - ä¿æŒä¸åŸå§‹ç›¸åŒ
        self.output_layer = nn.Sequential(
            nn.Linear(features_dim, features_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        print(f"âœ… LightweightUniversalExtractor æ„å»ºå®Œæˆ")
        print(f"   ä½¿ç”¨å…³èŠ‚åˆ†ç¦»: {self.use_joint_separation}")
        print(f"   æ¶æ„å¤æ‚åº¦: æœ€å°åŒ–")
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        è½»é‡çº§å‰å‘ä¼ æ’­
        """
        if not self.use_joint_separation:
            # MuJoCo Reacher-v5 - ä½¿ç”¨åŸå§‹æµç¨‹
            x = self.input_layer(observations)
        else:
            # é€šç”¨æƒ…å†µ - åˆ†ç¦»å¤„ç†
            joint_dim = self.num_joints * 2
            joint_features = observations[:, :joint_dim]
            
            # å¤„ç†å…³èŠ‚ç‰¹å¾
            joint_processed = self.joint_processor(joint_features)
            
            # å¤„ç†å…¨å±€ç‰¹å¾
            if self.global_processor is not None:
                global_features = observations[:, joint_dim:]
                global_processed = self.global_processor(global_features)
                fused = torch.cat([joint_processed, global_processed], dim=1)
            else:
                fused = joint_processed
            
            # èåˆå¤„ç†
            x = self.input_layer(fused)
        
        # æ³¨æ„åŠ›å¤„ç† - ä¸åŸå§‹ç›¸åŒ
        x = self.attention(x)
        
        # è¾“å‡ºå¤„ç† - ä¸åŸå§‹ç›¸åŒ
        features = self.output_layer(x)
        
        return features

def train_lightweight_universal_sac(num_joints: int = 2, total_timesteps: int = 50000):
    """
    è®­ç»ƒè½»é‡çº§é€šç”¨æ³¨æ„åŠ› SAC
    """
    print("ğŸŒŸ è½»é‡çº§é€šç”¨æ³¨æ„åŠ› SAC è®­ç»ƒ")
    print(f"ğŸ”— å…³èŠ‚æ•°é‡: {num_joints}")
    print(f"ğŸ’¡ è®¾è®¡ç†å¿µ: æœ€å°åŒ–ä¿®æ”¹ï¼Œä¿æŒæ€§èƒ½")
    print(f"ğŸ¯ ç›®æ ‡: æ¥è¿‘åŸå§‹ 70% æˆåŠŸç‡")
    print("=" * 70)
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"ğŸ­ åˆ›å»ºç¯å¢ƒ...")
    if num_joints == 2:
        env = gym.make('Reacher-v5', render_mode='human')
        eval_env = gym.make('Reacher-v5')
    else:
        print(f"âš ï¸ æš‚ä¸æ”¯æŒ {num_joints} å…³èŠ‚ç¯å¢ƒï¼Œä½¿ç”¨ 2 å…³èŠ‚è¿›è¡ŒéªŒè¯")
        env = gym.make('Reacher-v5', render_mode='human')
        eval_env = gym.make('Reacher-v5')
    
    env = Monitor(env)
    eval_env = Monitor(eval_env)
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"ğŸ‘ï¸ è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    
    print("=" * 70)
    
    # åˆ›å»ºè½»é‡çº§æ¨¡å‹
    print("ğŸ¤– åˆ›å»ºè½»é‡çº§é€šç”¨ SAC æ¨¡å‹...")
    
    policy_kwargs = {
        "features_extractor_class": LightweightUniversalExtractor,
        "features_extractor_kwargs": {
            "features_dim": 128,
            "num_joints": num_joints
        },
        "net_arch": [256, 256],  # ä¿æŒä¸åŸå§‹ç›¸åŒ
        "activation_fn": torch.nn.ReLU,
    }
    
    model = SAC(
        "MlpPolicy",
        env,
        learning_rate=3e-4,          # ä¸åŸå§‹ç›¸åŒ
        buffer_size=1000000,         # ä¸åŸå§‹ç›¸åŒ
        learning_starts=100,         # ä¸åŸå§‹ç›¸åŒ
        batch_size=256,              # ä¸åŸå§‹ç›¸åŒ
        tau=0.005,                   # ä¸åŸå§‹ç›¸åŒ
        gamma=0.99,                  # ä¸åŸå§‹ç›¸åŒ
        train_freq=1,                # ä¸åŸå§‹ç›¸åŒ
        gradient_steps=1,            # ä¸åŸå§‹ç›¸åŒ
        ent_coef='auto',             # ä¸åŸå§‹ç›¸åŒ
        target_update_interval=1,    # ä¸åŸå§‹ç›¸åŒ
        use_sde=False,               # ä¸åŸå§‹ç›¸åŒ
        policy_kwargs=policy_kwargs,
        verbose=1,
        device='cpu'
    )
    
    print("âœ… è½»é‡çº§é€šç”¨ SAC æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š æ¨¡å‹ç‰¹ç‚¹:")
    print(f"   âœ¨ æœ€å°åŒ–ä¿®æ”¹åŸå§‹æ¶æ„")
    print(f"   ğŸ¯ ä¿æŒåŸå§‹æ€§èƒ½")
    print(f"   ğŸ”§ æ™ºèƒ½è§‚å¯Ÿç©ºé—´å¤„ç†")
    print(f"   ğŸŒ æ”¯æŒé€šç”¨æ‰©å±•")
    
    print("=" * 70)
    
    # è¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f'./lightweight_universal_{num_joints}joints_best/',
        log_path=f'./lightweight_universal_{num_joints}joints_logs/',
        eval_freq=5000,
        n_eval_episodes=10,
        deterministic=True,
        render=False
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹è½»é‡çº§è®­ç»ƒ...")
    print("ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   æ€»æ­¥æ•°: {total_timesteps:,}")
    print("   è¯„ä¼°é¢‘ç‡: æ¯ 5,000 æ­¥")
    print("   é¢„æœŸ: æ¥è¿‘åŸå§‹ 70% æˆåŠŸç‡")
    print("=" * 70)
    
    start_time = time.time()
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=eval_callback,
        log_interval=10,
        progress_bar=True
    )
    
    training_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("ğŸ† è½»é‡çº§è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
    print("=" * 70)
    
    # ä¿å­˜æ¨¡å‹
    model_name = f"lightweight_universal_{num_joints}joints_final"
    model.save(model_name)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º: {model_name}.zip")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ” æœ€ç»ˆè¯„ä¼° (20 episodes)...")
    mean_reward, std_reward = evaluate_policy(
        model, 
        eval_env, 
        n_eval_episodes=20,
        deterministic=True,
        render=False
    )
    
    print(f"ğŸ“Š è½»é‡çº§é€šç”¨æ¨¡å‹è¯„ä¼°ç»“æœ:")
    print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    # ä¸åŸå§‹å’Œå¤æ‚é€šç”¨ç‰ˆæœ¬å¯¹æ¯”
    original_reward = -5.70
    complex_universal_reward = -10.21
    
    improvement_vs_original = mean_reward - original_reward
    improvement_vs_complex = mean_reward - complex_universal_reward
    
    print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
    print(f"   åŸå§‹æ³¨æ„åŠ›: {original_reward:.2f}")
    print(f"   å¤æ‚é€šç”¨ç‰ˆ: {complex_universal_reward:.2f}")
    print(f"   è½»é‡çº§é€šç”¨: {mean_reward:.2f}")
    print(f"   vs åŸå§‹: {improvement_vs_original:+.2f}")
    print(f"   vs å¤æ‚é€šç”¨: {improvement_vs_complex:+.2f}")
    
    if improvement_vs_original > -1.0:
        print("   ğŸ‰ è½»é‡çº§é€šç”¨åŒ–æˆåŠŸ!")
    elif improvement_vs_complex > 2.0:
        print("   ğŸ‘ æ˜¾è‘—ä¼˜äºå¤æ‚ç‰ˆæœ¬!")
    else:
        print("   ğŸ“ˆ ä»æœ‰æ”¹è¿›ç©ºé—´")
    
    # æ¼”ç¤º
    print("\nğŸ® æ¼”ç¤ºè½»é‡çº§é€šç”¨æ¨¡å‹ (10 episodes)...")
    demo_env = gym.make('Reacher-v5', render_mode='human')
    
    episode_rewards = []
    success_count = 0
    
    for episode in range(10):
        obs, info = demo_env.reset()
        episode_reward = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = demo_env.step(action)
            episode_reward += reward
            
            if terminated or truncated:
                break
        
        episode_rewards.append(episode_reward)
        
        if episode_reward > -5:
            success_count += 1
            print(f"ğŸ¯ Episode {episode+1}: æˆåŠŸ! å¥–åŠ±={episode_reward:.2f}")
        else:
            print(f"ğŸ“Š Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}")
    
    demo_env.close()
    
    demo_success_rate = success_count / 10
    demo_avg_reward = np.mean(episode_rewards)
    
    print("\n" + "=" * 70)
    print("ğŸ“Š è½»é‡çº§é€šç”¨æ¼”ç¤ºç»Ÿè®¡:")
    print(f"   æˆåŠŸç‡: {demo_success_rate:.1%} ({success_count}/10)")
    print(f"   å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    print(f"   å¥–åŠ±æ ‡å‡†å·®: {np.std(episode_rewards):.2f}")
    
    # ä¸åŸå§‹å¯¹æ¯”
    original_demo_success = 0.7
    original_demo_reward = -4.61
    
    print(f"\nğŸ“ˆ ä¸åŸå§‹æ³¨æ„åŠ›å¯¹æ¯”:")
    print(f"   åŸå§‹æˆåŠŸç‡: {original_demo_success:.1%}")
    print(f"   è½»é‡çº§æˆåŠŸç‡: {demo_success_rate:.1%}")
    print(f"   æˆåŠŸç‡å˜åŒ–: {demo_success_rate - original_demo_success:+.1%}")
    print(f"   ")
    print(f"   åŸå§‹å¹³å‡å¥–åŠ±: {original_demo_reward:.2f}")
    print(f"   è½»é‡çº§å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    print(f"   å¥–åŠ±å˜åŒ–: {demo_avg_reward - original_demo_reward:+.2f}")
    
    if demo_success_rate >= 0.6:
        print("   ğŸ‰ è½»é‡çº§é€šç”¨åŒ–æˆåŠŸ!")
    elif demo_success_rate >= 0.4:
        print("   ğŸ‘ è½»é‡çº§é€šç”¨åŒ–è‰¯å¥½!")
    else:
        print("   ğŸ“ˆ ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    print(f"\nğŸŒŸ è½»é‡çº§é€šç”¨æ¶æ„ä¼˜åŠ¿:")
    print(f"   âœ… æœ€å°åŒ–ä¿®æ”¹åŸå§‹æ¶æ„")
    print(f"   âœ… ä¿æŒåŸå§‹æ€§èƒ½ç‰¹å¾")
    print(f"   âœ… æ™ºèƒ½è§‚å¯Ÿç©ºé—´å¤„ç†")
    print(f"   âœ… é¿å…è¿‡åº¦å¤æ‚åŒ–")
    print(f"   âœ… æ”¯æŒé€šç”¨æ‰©å±•")
    print(f"   âœ… è®­ç»ƒç¨³å®šæ€§å¥½")
    
    # æ¸…ç†
    env.close()
    eval_env.close()
    
    return {
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'training_time': training_time,
        'demo_success_rate': demo_success_rate,
        'demo_avg_reward': demo_avg_reward,
        'improvement_vs_original': improvement_vs_original,
        'improvement_vs_complex': improvement_vs_complex,
        'num_joints': num_joints
    }

if __name__ == "__main__":
    print("ğŸŒŸ è½»é‡çº§é€šç”¨æ³¨æ„åŠ› SAC è®­ç»ƒç³»ç»Ÿ")
    print("ğŸ’¡ è®¾è®¡ç†å¿µ: æœ€å°åŒ–ä¿®æ”¹ï¼Œæœ€å¤§åŒ–æ€§èƒ½ä¿æŒ")
    print("ğŸ¯ ç›®æ ‡: åœ¨ä¿æŒé€šç”¨æ€§çš„åŒæ—¶æ¥è¿‘åŸå§‹æ€§èƒ½")
    print()
    
    try:
        result = train_lightweight_universal_sac(num_joints=2, total_timesteps=50000)
        
        print(f"\nğŸŠ è½»é‡çº§é€šç”¨è®­ç»ƒç»“æœæ€»ç»“:")
        print(f"   æœ€ç»ˆè¯„ä¼°å¥–åŠ±: {result['mean_reward']:.2f} Â± {result['std_reward']:.2f}")
        print(f"   è®­ç»ƒæ—¶é—´: {result['training_time']/60:.1f} åˆ†é’Ÿ")
        print(f"   æ¼”ç¤ºæˆåŠŸç‡: {result['demo_success_rate']:.1%}")
        print(f"   æ¼”ç¤ºå¹³å‡å¥–åŠ±: {result['demo_avg_reward']:.2f}")
        print(f"   vs åŸå§‹æ³¨æ„åŠ›: {result['improvement_vs_original']:+.2f}")
        print(f"   vs å¤æ‚é€šç”¨ç‰ˆ: {result['improvement_vs_complex']:+.2f}")
        
        if result['improvement_vs_original'] > -1.0:
            print(f"\nğŸ† è½»é‡çº§é€šç”¨åŒ–æˆåŠŸ!")
            print("   åœ¨ä¿æŒé€šç”¨æ€§çš„åŒæ—¶æœ€å¤§åŒ–ä¿æŒäº†åŸå§‹æ€§èƒ½")
        elif result['improvement_vs_complex'] > 2.0:
            print(f"\nğŸ‘ æ˜¾è‘—ä¼˜äºå¤æ‚ç‰ˆæœ¬!")
            print("   è¯æ˜äº†ç®€æ´è®¾è®¡çš„ä¼˜åŠ¿")
        else:
            print(f"\nğŸ“ˆ æœ‰æ”¹è¿›ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        print(f"\nâœ… è½»é‡çº§é€šç”¨æ¶æ„éªŒè¯å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
