#!/usr/bin/env python3
"""
çœŸå®å¤šå…³èŠ‚ Reacher SAC è®­ç»ƒè„šæœ¬
åŸºäº GPT-5 å»ºè®®ï¼šä½¿ç”¨çœŸå®çš„ N å…³èŠ‚ MuJoCo ç¯å¢ƒè¿›è¡Œè®­ç»ƒ
"""

import os
import time
import numpy as np
import torch
import torch.nn as nn
from typing import List, Tuple, Dict, Any
import gymnasium as gym
from gymnasium.spaces import Box
from stable_baselines3 import SAC
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import EvalCallback

# å¯¼å…¥çœŸå®å¤šå…³èŠ‚ç¯å¢ƒ
from real_multi_joint_reacher import RealMultiJointWrapper

# ============================================================================
# ğŸ§© Link-Aware é€šç”¨ç‰¹å¾æå–å™¨ (é€‚é…çœŸå®å¤šå…³èŠ‚ç¯å¢ƒ)
# ============================================================================

class RealMultiJointMaskSystem:
    """çœŸå®å¤šå…³èŠ‚ç¯å¢ƒçš„æ©ç ç³»ç»Ÿ"""
    
    @staticmethod
    def parse_observation_for_real_multi_joint(obs: np.ndarray, 
                                             num_joints: int, 
                                             link_lengths: List[float]) -> Tuple[np.ndarray, np.ndarray]:
        """
        è§£æçœŸå®å¤šå…³èŠ‚ç¯å¢ƒçš„è§‚å¯Ÿç©ºé—´
        
        obsæ ¼å¼: [joint_featuresÃ—N, global_featuresÃ—6]
        joint_features: [cos, sin, vel, link_length] Ã— num_joints (4*N ç»´)
        global_features: [ee_x, ee_y, target_x, target_y, target_vec_x, target_vec_y] (6ç»´)
        
        Returns:
            joint_features: [num_joints, 4] (cos, sin, vel, link_length)
            global_features: [6,] (ee_pos, target_pos, target_vec)
        """
        # å…³èŠ‚ç‰¹å¾ï¼šå‰ 4*num_joints ç»´
        joint_features_flat = obs[:4 * num_joints]
        joint_features = joint_features_flat.reshape(num_joints, 4)
        
        # å…¨å±€ç‰¹å¾ï¼šå 6 ç»´
        global_features = obs[4 * num_joints:]
        
        return joint_features, global_features

class RealLinkAwareJointEncoder(nn.Module):
    """çœŸå®å¤šå…³èŠ‚ç¯å¢ƒçš„å…³èŠ‚ç¼–ç å™¨"""
    
    def __init__(self, joint_input_dim: int = 4, joint_hidden_dim: int = 32):
        super().__init__()
        self.joint_input_dim = joint_input_dim
        self.joint_hidden_dim = joint_hidden_dim
        
        # å‡ ä½•ç‰¹å¾å¤„ç† (cos, sin, link_length)
        self.geometric_processor = nn.Sequential(
            nn.Linear(3, 16),  # cos, sin, link_length
            nn.ReLU(),
            nn.Linear(16, 16)
        )
        
        # è¿åŠ¨ç‰¹å¾å¤„ç† (vel)
        self.kinematic_processor = nn.Sequential(
            nn.Linear(1, 8),   # vel
            nn.ReLU(),
            nn.Linear(8, 8)
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Linear(16 + 8, joint_hidden_dim),
            nn.ReLU(),
            nn.Linear(joint_hidden_dim, joint_hidden_dim)
        )
    
    def forward(self, joint_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            joint_features: [batch_size, num_joints, 4] (cos, sin, vel, link_length)
        Returns:
            encoded_features: [batch_size, num_joints, joint_hidden_dim]
        """
        batch_size, num_joints, _ = joint_features.shape
        
        # åˆ†ç¦»å‡ ä½•å’Œè¿åŠ¨ç‰¹å¾
        geometric_features = joint_features[:, :, [0, 1, 3]]  # cos, sin, link_length
        kinematic_features = joint_features[:, :, [2]]        # vel
        
        # å¤„ç†å‡ ä½•ç‰¹å¾
        geometric_encoded = self.geometric_processor(
            geometric_features.reshape(-1, 3)
        ).reshape(batch_size, num_joints, 16)
        
        # å¤„ç†è¿åŠ¨ç‰¹å¾
        kinematic_encoded = self.kinematic_processor(
            kinematic_features.reshape(-1, 1)
        ).reshape(batch_size, num_joints, 8)
        
        # èåˆç‰¹å¾
        combined_features = torch.cat([geometric_encoded, kinematic_encoded], dim=-1)
        encoded_features = self.fusion(
            combined_features.reshape(-1, 24)
        ).reshape(batch_size, num_joints, self.joint_hidden_dim)
        
        return encoded_features

class RealFixedSelfAttention(nn.Module):
    """çœŸå®å¤šå…³èŠ‚ç¯å¢ƒçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, feature_dim: int = 32, num_heads: int = 4):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_heads = num_heads
        
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=num_heads,
            batch_first=True
        )
        
        self.layer_norm = nn.LayerNorm(feature_dim)
    
    def forward(self, joint_features: torch.Tensor, 
                joint_mask: torch.Tensor = None) -> torch.Tensor:
        """
        Args:
            joint_features: [batch_size, num_joints, feature_dim]
            joint_mask: [batch_size, num_joints] (True for valid joints)
        Returns:
            attended_features: [batch_size, num_joints, feature_dim]
        """
        # å‡†å¤‡ key_padding_mask (True for padded positions)
        key_padding_mask = None
        if joint_mask is not None:
            key_padding_mask = ~joint_mask  # åè½¬æ©ç 
        
        # è‡ªæ³¨æ„åŠ›
        attended_features, _ = self.multihead_attn(
            query=joint_features,
            key=joint_features,
            value=joint_features,
            key_padding_mask=key_padding_mask
        )
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        output = self.layer_norm(joint_features + attended_features)
        
        return output

class RealFixedAttentionPooling(nn.Module):
    """çœŸå®å¤šå…³èŠ‚ç¯å¢ƒçš„æ³¨æ„åŠ›æ± åŒ–"""
    
    def __init__(self, feature_dim: int = 32, pooled_dim: int = 64):
        super().__init__()
        self.feature_dim = feature_dim
        self.pooled_dim = pooled_dim
        
        self.attention_weights = nn.Linear(feature_dim, 1)
        self.output_projection = nn.Linear(feature_dim, pooled_dim)
    
    def forward(self, joint_features: torch.Tensor, 
                joint_mask: torch.Tensor = None) -> torch.Tensor:
        """
        Args:
            joint_features: [batch_size, num_joints, feature_dim]
            joint_mask: [batch_size, num_joints] (True for valid joints)
        Returns:
            pooled_features: [batch_size, pooled_dim]
        """
        batch_size, num_joints, feature_dim = joint_features.shape
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_scores = self.attention_weights(joint_features)  # [batch_size, num_joints, 1]
        attention_scores = attention_scores.squeeze(-1)  # [batch_size, num_joints]
        
        # åº”ç”¨æ©ç 
        if joint_mask is not None:
            attention_scores = attention_scores.masked_fill(~joint_mask, float('-inf'))
        
        # Softmax
        attention_weights = torch.softmax(attention_scores, dim=-1)  # [batch_size, num_joints]
        
        # åŠ æƒæ± åŒ–
        pooled_features = torch.sum(
            joint_features * attention_weights.unsqueeze(-1), 
            dim=1
        )  # [batch_size, feature_dim]
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_projection(pooled_features)  # [batch_size, pooled_dim]
        
        return output

class RealMultiJointUniversalExtractor(BaseFeaturesExtractor):
    """çœŸå®å¤šå…³èŠ‚ç¯å¢ƒçš„é€šç”¨ç‰¹å¾æå–å™¨"""
    
    def __init__(self, observation_space: gym.Space, 
                 num_joints: int = 3,
                 joint_hidden_dim: int = 32,
                 pooled_dim: int = 64,
                 global_hidden_dim: int = 32,
                 features_dim: int = 128):
        
        super(RealMultiJointUniversalExtractor, self).__init__(observation_space, features_dim)
        
        self.num_joints = num_joints
        self.joint_hidden_dim = joint_hidden_dim
        self.pooled_dim = pooled_dim
        self.global_hidden_dim = global_hidden_dim
        
        print(f"ğŸ”§ RealMultiJointUniversalExtractor åˆå§‹åŒ–:")
        print(f"   å…³èŠ‚æ•°: {num_joints}")
        print(f"   è§‚å¯Ÿç©ºé—´: {observation_space}")
        print(f"   ç‰¹å¾ç»´åº¦: {features_dim}")
        
        # å…³èŠ‚ç¼–ç å™¨
        self.joint_encoder = RealLinkAwareJointEncoder(
            joint_input_dim=4,
            joint_hidden_dim=joint_hidden_dim
        )
        
        # è‡ªæ³¨æ„åŠ›
        self.self_attention = RealFixedSelfAttention(
            feature_dim=joint_hidden_dim,
            num_heads=4
        )
        
        # æ³¨æ„åŠ›æ± åŒ–
        self.attention_pooling = RealFixedAttentionPooling(
            feature_dim=joint_hidden_dim,
            pooled_dim=pooled_dim
        )
        
        # å…¨å±€ç‰¹å¾å¤„ç†å™¨
        self.global_processor = nn.Sequential(
            nn.Linear(6, global_hidden_dim),  # ee_pos, target_pos, target_vec
            nn.ReLU(),
            nn.Linear(global_hidden_dim, global_hidden_dim)
        )
        
        # æœ€ç»ˆèåˆ
        self.final_fusion = nn.Sequential(
            nn.Linear(pooled_dim + global_hidden_dim, features_dim),
            nn.ReLU(),
            nn.Linear(features_dim, features_dim)
        )
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        Args:
            observations: [batch_size, obs_dim]
        Returns:
            features: [batch_size, features_dim]
        """
        batch_size = observations.shape[0]
        
        # ç›´æ¥åœ¨ tensor ä¸Šè§£æè§‚å¯Ÿ (é¿å… CPU-GPU è½¬æ¢)
        # è§‚å¯Ÿæ ¼å¼: [joint_featuresÃ—N, global_featuresÃ—6]
        # joint_features: [cos, sin, vel, link_length] Ã— num_joints (4*N ç»´)
        # global_features: [ee_x, ee_y, target_x, target_y, target_vec_x, target_vec_y] (6ç»´)
        
        joint_features_flat = observations[:, :4 * self.num_joints]  # [batch_size, 4*num_joints]
        joint_features_tensor = joint_features_flat.reshape(batch_size, self.num_joints, 4)  # [batch_size, num_joints, 4]
        
        global_features_tensor = observations[:, 4 * self.num_joints:]  # [batch_size, 6]
        
        # å…³èŠ‚ç‰¹å¾ç¼–ç 
        encoded_joint_features = self.joint_encoder(joint_features_tensor)
        
        # è‡ªæ³¨æ„åŠ› (çœŸå®å¤šå…³èŠ‚ç¯å¢ƒä¸éœ€è¦æ©ç ï¼Œæ‰€æœ‰å…³èŠ‚éƒ½æ˜¯çœŸå®çš„)
        attended_joint_features = self.self_attention(encoded_joint_features)
        
        # æ³¨æ„åŠ›æ± åŒ–
        pooled_joint_features = self.attention_pooling(attended_joint_features)
        
        # å…¨å±€ç‰¹å¾å¤„ç†
        processed_global_features = self.global_processor(global_features_tensor)
        
        # æœ€ç»ˆèåˆ
        combined_features = torch.cat([pooled_joint_features, processed_global_features], dim=-1)
        final_features = self.final_fusion(combined_features)
        
        return final_features

# ============================================================================
# ğŸ§© è®­ç»ƒå‡½æ•°
# ============================================================================

def train_real_multi_joint_sac(num_joints: int = 3,
                              link_lengths: List[float] = None,
                              total_timesteps: int = 50000,
                              render_mode: str = None) -> Dict[str, Any]:
    """
    è®­ç»ƒçœŸå®å¤šå…³èŠ‚ Reacher SAC æ¨¡å‹
    
    Args:
        num_joints: å…³èŠ‚æ•°é‡
        link_lengths: æ¯ä¸ªå…³èŠ‚çš„ link é•¿åº¦
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
        render_mode: æ¸²æŸ“æ¨¡å¼ ('human' æˆ– None)
    
    Returns:
        è®­ç»ƒç»“æœå­—å…¸
    """
    if link_lengths is None:
        link_lengths = [0.1] * num_joints
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒçœŸå® {num_joints} å…³èŠ‚ Reacher SAC")
    print(f"{'='*60}")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   å…³èŠ‚æ•°: {num_joints}")
    print(f"   Link é•¿åº¦: {link_lengths}")
    print(f"   æ€»æ­¥æ•°: {total_timesteps}")
    print(f"   æ¸²æŸ“æ¨¡å¼: {render_mode}")
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"\nğŸŒ åˆ›å»ºçœŸå®å¤šå…³èŠ‚ç¯å¢ƒ...")
    env = RealMultiJointWrapper(
        num_joints=num_joints,
        link_lengths=link_lengths,
        render_mode=render_mode
    )
    
    # åŒ…è£…ç›‘æ§
    env = Monitor(env)
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = RealMultiJointWrapper(
        num_joints=num_joints,
        link_lengths=link_lengths,
        render_mode=None  # è¯„ä¼°æ—¶ä¸æ¸²æŸ“
    )
    eval_env = Monitor(eval_env)
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"   è®­ç»ƒç¯å¢ƒè§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"   è®­ç»ƒç¯å¢ƒåŠ¨ä½œç©ºé—´: {env.action_space}")
    
    # åˆ›å»º SAC æ¨¡å‹
    print(f"\nğŸ¤– åˆ›å»º SAC æ¨¡å‹...")
    
    policy_kwargs = {
        'features_extractor_class': RealMultiJointUniversalExtractor,
        'features_extractor_kwargs': {
            'num_joints': num_joints,
            'joint_hidden_dim': 32,
            'pooled_dim': 64,
            'global_hidden_dim': 32,
            'features_dim': 128
        },
        'net_arch': [256, 256]
    }
    
    model = SAC(
        'MlpPolicy',
        env,
        policy_kwargs=policy_kwargs,
        learning_rate=3e-4,
        buffer_size=100000,
        learning_starts=100,         # æ›´æ—©å¼€å§‹å­¦ä¹ ï¼Œæ›´å¿«çœ‹åˆ°è®­ç»ƒæ—¥å¿—
        batch_size=256,
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        ent_coef='auto',
        target_update_interval=1,
        verbose=2,                   # å¢åŠ åˆ° verbose=2 è·å¾—æ›´è¯¦ç»†çš„æ—¥å¿—
        device='auto',
        tensorboard_log="./tensorboard_logs/"  # æ·»åŠ  tensorboard æ—¥å¿—
    )
    
    print(f"âœ… SAC æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"   ç­–ç•¥ç½‘ç»œ: MlpPolicy + RealMultiJointUniversalExtractor")
    print(f"   å­¦ä¹ ç‡: 3e-4")
    print(f"   ç¼“å†²åŒºå¤§å°: 100000")
    
    # åˆ›å»ºè¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f'./logs/real_{num_joints}joint_sac/',
        log_path=f'./logs/real_{num_joints}joint_sac/',
        eval_freq=5000,
        deterministic=True,
        render=False
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        print(f"âœ… è®­ç»ƒå®Œæˆï¼ç”¨æ—¶: {training_time:.1f} ç§’")
        
    except KeyboardInterrupt:
        training_time = time.time() - start_time
        print(f"âš ï¸ è®­ç»ƒè¢«ä¸­æ–­ï¼å·²è®­ç»ƒæ—¶é—´: {training_time:.1f} ç§’")
    
    # è¯„ä¼°æ¨¡å‹
    print(f"\nğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    try:
        mean_reward, std_reward = evaluate_policy(
            model, eval_env, n_eval_episodes=10, deterministic=True
        )
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.3f} Â± {std_reward:.3f}")
        
        # è®¡ç®—æˆåŠŸç‡ (å‡è®¾å¥–åŠ± > -1 ä¸ºæˆåŠŸ)
        episode_rewards = []
        for _ in range(20):
            obs, _ = eval_env.reset()
            episode_reward = 0
            done = False
            while not done:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                done = terminated or truncated
            episode_rewards.append(episode_reward)
        
        success_rate = sum(1 for r in episode_rewards if r > -1) / len(episode_rewards)
        
        print(f"   æˆåŠŸç‡: {success_rate:.1%}")
        print(f"   è®­ç»ƒæ—¶é—´: {training_time:.1f} ç§’")
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"real_{num_joints}joint_sac_model"
        model.save(model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        results = {
            'num_joints': num_joints,
            'link_lengths': link_lengths,
            'mean_reward': mean_reward,
            'std_reward': std_reward,
            'success_rate': success_rate,
            'training_time': training_time,
            'total_timesteps': total_timesteps,
            'model_path': model_path,
            'is_real_multi_joint': True
        }
        
        return results
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        return {
            'num_joints': num_joints,
            'training_time': training_time,
            'error': str(e),
            'is_real_multi_joint': True
        }
    
    finally:
        env.close()
        eval_env.close()

# ============================================================================
# ğŸ§© ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°ï¼šæµ‹è¯•ä¸åŒå…³èŠ‚æ•°çš„è®­ç»ƒæ•ˆæœ"""
    
    print("ğŸŒŸ çœŸå®å¤šå…³èŠ‚ Reacher SAC è®­ç»ƒæµ‹è¯•")
    print("ğŸ’¡ åŸºäº GPT-5 å»ºè®®ï¼šçœŸå®çš„ N å…³èŠ‚ MuJoCo åŠ¨åŠ›å­¦")
    print()
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs('logs', exist_ok=True)
    
    # æµ‹è¯•é…ç½® (å‡å°‘è®­ç»ƒæ­¥æ•°ä»¥ä¾¿å¿«é€Ÿçœ‹åˆ°æ•ˆæœ)
    test_configs = [
        {'num_joints': 2, 'link_lengths': [0.1, 0.1], 'timesteps': 10000},
        {'num_joints': 3, 'link_lengths': [0.1, 0.1, 0.1], 'timesteps': 15000},
        # {'num_joints': 4, 'link_lengths': [0.1, 0.1, 0.1, 0.1], 'timesteps': 20000},
    ]
    
    results = []
    
    for config in test_configs:
        print(f"\n{'='*80}")
        print(f"ğŸ§ª æµ‹è¯•é…ç½®: {config['num_joints']} å…³èŠ‚")
        print(f"{'='*80}")
        
        try:
            result = train_real_multi_joint_sac(
                num_joints=config['num_joints'],
                link_lengths=config['link_lengths'],
                total_timesteps=config['timesteps'],
                render_mode=None  # è®­ç»ƒæ—¶ä¸æ¸²æŸ“
            )
            results.append(result)
            
        except Exception as e:
            print(f"âŒ {config['num_joints']} å…³èŠ‚è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            results.append({
                'num_joints': config['num_joints'],
                'error': str(e),
                'is_real_multi_joint': True
            })
    
    # æ€»ç»“ç»“æœ
    print(f"\n{'='*80}")
    print(f"ğŸ“Š è®­ç»ƒç»“æœæ€»ç»“")
    print(f"{'='*80}")
    
    for result in results:
        if 'error' in result:
            print(f"âŒ {result['num_joints']} å…³èŠ‚: è®­ç»ƒå¤±è´¥ - {result['error']}")
        else:
            print(f"âœ… {result['num_joints']} å…³èŠ‚:")
            print(f"   å¹³å‡å¥–åŠ±: {result.get('mean_reward', 'N/A'):.3f}")
            print(f"   æˆåŠŸç‡: {result.get('success_rate', 0):.1%}")
            print(f"   è®­ç»ƒæ—¶é—´: {result.get('training_time', 0):.1f} ç§’")
            print(f"   çœŸå®å¤šå…³èŠ‚: {result.get('is_real_multi_joint', False)}")

if __name__ == "__main__":
    main()
