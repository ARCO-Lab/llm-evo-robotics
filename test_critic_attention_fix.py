#!/usr/bin/env python3
"""
ç®€å•æµ‹è¯•è„šæœ¬ï¼šéªŒè¯ä¿®å¤åçš„Critic attentionç½‘ç»œæ˜¯å¦è¢«æ­£ç¡®ä½¿ç”¨
"""

import sys
import os
import torch
import torch.nn as nn

# æ·»åŠ è·¯å¾„
sys.path.append('.')
sys.path.append('./sac')

def test_critic_attention_usage():
    """æµ‹è¯•Criticæ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†ä¸»è¦attentionç½‘ç»œ"""
    print("ğŸ” æµ‹è¯•Critic attentionç½‘ç»œä½¿ç”¨æƒ…å†µ...")
    
    try:
        from sac.universal_ppo_model import UniversalAttnModel, UniversalPPOCritic
        
        # åˆ›å»ºæ¨¡å‹
        attn_model = UniversalAttnModel(128, 130, 130, 4)
        critic = UniversalPPOCritic(attn_model, device='cpu')
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        num_joints = 3
        joint_q = torch.randn(batch_size, num_joints, 130)
        vertex_k = torch.randn(batch_size, num_joints, 128)
        vertex_v = torch.randn(batch_size, num_joints, 130)
        
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®å½¢çŠ¶:")
        print(f"   joint_q: {joint_q.shape}")
        print(f"   vertex_k: {vertex_k.shape}")
        print(f"   vertex_v: {vertex_v.shape}")
        
        # å‰å‘ä¼ æ’­
        print("\nğŸš€ æ‰§è¡Œå‰å‘ä¼ æ’­...")
        with torch.no_grad():
            output = critic(joint_q, vertex_k, vertex_v)
            print(f"âœ… Criticè¾“å‡º: {output.shape} = {output}")
        
        # æ£€æŸ¥æ¢¯åº¦è®¡ç®—
        print("\nğŸ” æµ‹è¯•æ¢¯åº¦è®¡ç®—...")
        output = critic(joint_q, vertex_k, vertex_v)
        loss = output.sum()
        loss.backward()
        
        # æ£€æŸ¥ä¸»è¦attentionç½‘ç»œçš„æ¢¯åº¦
        main_attn_grad_count = 0
        main_attn_grad_norm = 0.0
        for name, param in critic.attn_model.named_parameters():
            if param.grad is not None:
                main_attn_grad_count += 1
                main_attn_grad_norm += param.grad.data.norm(2).item() ** 2
        
        # æ£€æŸ¥value attentionçš„æ¢¯åº¦
        value_attn_grad_count = 0
        value_attn_grad_norm = 0.0
        for name, param in critic.value_attention.named_parameters():
            if param.grad is not None:
                value_attn_grad_count += 1
                value_attn_grad_norm += param.grad.data.norm(2).item() ** 2
        
        main_attn_grad_norm = main_attn_grad_norm ** 0.5 if main_attn_grad_count > 0 else 0.0
        value_attn_grad_norm = value_attn_grad_norm ** 0.5 if value_attn_grad_count > 0 else 0.0
        
        print(f"\nğŸ“Š æ¢¯åº¦æ£€æŸ¥ç»“æœ:")
        print(f"   ä¸»è¦attentionç½‘ç»œ:")
        print(f"     - æœ‰æ¢¯åº¦çš„å‚æ•°æ•°é‡: {main_attn_grad_count}")
        print(f"     - æ¢¯åº¦èŒƒæ•°: {main_attn_grad_norm:.6f}")
        print(f"   å†…éƒ¨value attention:")
        print(f"     - æœ‰æ¢¯åº¦çš„å‚æ•°æ•°é‡: {value_attn_grad_count}")
        print(f"     - æ¢¯åº¦èŒƒæ•°: {value_attn_grad_norm:.6f}")
        
        # åˆ¤æ–­ä¿®å¤æ˜¯å¦æˆåŠŸ
        if main_attn_grad_count > 0 and main_attn_grad_norm > 0:
            print(f"\nğŸ‰ ä¿®å¤æˆåŠŸï¼ä¸»è¦attentionç½‘ç»œç°åœ¨æœ‰æ¢¯åº¦äº†ï¼")
            print(f"   ä¸»è¦attentionæ¢¯åº¦èŒƒæ•°: {main_attn_grad_norm:.6f}")
            return True
        else:
            print(f"\nâŒ ä¿®å¤å¤±è´¥ï¼ä¸»è¦attentionç½‘ç»œä»ç„¶æ²¡æœ‰æ¢¯åº¦ã€‚")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_critic_attention_usage()
    if success:
        print("\nâœ… æµ‹è¯•é€šè¿‡ï¼šCritic attentionç½‘ç»œä¿®å¤æˆåŠŸï¼")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
