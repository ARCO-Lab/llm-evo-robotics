#!/usr/bin/env python3
"""
å¯è§†åŒ–SACè®­ç»ƒè„šæœ¬ï¼š
1. ä½¿ç”¨å•ä¸ªSACæ™ºèƒ½ä½“æ§åˆ¶3å…³èŠ‚Reacher
2. å¼€å¯æ¸²æŸ“ä»¥è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹
3. ä¸MADDPGè¿›è¡Œå¯¹æ¯”
"""

import os
import numpy as np
import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
import time

# è®¾ç½®æ¸²æŸ“ç¯å¢ƒå˜é‡
os.environ['MUJOCO_GL'] = 'glfw'
os.environ['MUJOCO_RENDERER'] = 'glfw'

# å¯¼å…¥ç¯å¢ƒ
from maddpg_complete_sequential_training import (
    create_env, 
    SUCCESS_THRESHOLD_RATIO, 
    SUCCESS_THRESHOLD_2JOINT
)

def train_visual_sac(num_joints=3, total_timesteps=10000):
    """è®­ç»ƒå¯è§†åŒ–SACæ¨¡å‹"""
    print(f"ğŸŒŸ å¯è§†åŒ–SACè®­ç»ƒç³»ç»Ÿ")
    print(f"ğŸ¤– ç­–ç•¥: å•ä¸ªSACæ™ºèƒ½ä½“æ§åˆ¶{num_joints}å…³èŠ‚Reacher")
    print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {total_timesteps}")
    print(f"ğŸ¯ æˆåŠŸé˜ˆå€¼: {SUCCESS_THRESHOLD_RATIO:.1%} * R")
    print("="*60)
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼ˆå¼€å¯æ¸²æŸ“ï¼‰
    train_env = create_env(num_joints, render_mode='human', show_position_info=True)
    
    print(f"ğŸ”§ ç¯å¢ƒé…ç½®:")
    print(f"   è§‚å¯Ÿç©ºé—´: {train_env.observation_space}")
    print(f"   åŠ¨ä½œç©ºé—´: {train_env.action_space}")
    
    # åˆ›å»ºSACæ¨¡å‹
    model = SAC(
        'MlpPolicy',
        train_env,
        verbose=1,
        learning_starts=1000,
        device='cpu',
        learning_rate=3e-4,
        gamma=0.99,
        tau=0.005,
        batch_size=256,
        buffer_size=100000
    )
    
    print(f"âœ… SACæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹SACè®­ç»ƒ...")
    
    try:
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            log_interval=10,
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        
        print(f"\nâœ… SACè®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“Š å¹³å‡FPS: {total_timesteps/training_time:.1f}")
        
        return model, training_time
        
    except KeyboardInterrupt:
        training_time = time.time() - start_time
        print(f"\nâš ï¸ SACè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print(f"â±ï¸ å·²è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        return model, training_time
    
    finally:
        train_env.close()

def test_visual_sac(model, num_joints=3, n_episodes=5):
    """æµ‹è¯•SACæ¨¡å‹"""
    print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•SACæ¨¡å‹ {n_episodes} episodes...")
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒï¼ˆå¼€å¯æ¸²æŸ“ï¼‰
    test_env = create_env(num_joints, render_mode='human', show_position_info=True)
    
    success_count = 0
    episode_rewards = []
    episode_distances = []
    
    try:
        for episode in range(n_episodes):
            obs, info = test_env.reset()
            episode_reward = 0
            episode_success = False
            min_distance = float('inf')
            
            for step in range(100):  # æ¯ä¸ªepisodeæœ€å¤š100æ­¥
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = test_env.step(action)
                episode_reward += reward
                
                # æ£€æŸ¥æˆåŠŸ
                distance = info.get('distance_to_target', float('inf'))
                min_distance = min(min_distance, distance)
                
                if info.get('is_success', False):
                    episode_success = True
                
                if terminated or truncated:
                    break
            
            if episode_success:
                success_count += 1
            
            episode_rewards.append(episode_reward)
            episode_distances.append(min_distance)
            
            print(f"   Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, æœ€å°è·ç¦»={min_distance:.4f}, æˆåŠŸ={'âœ…' if episode_success else 'âŒ'}")
        
        success_rate = success_count / n_episodes
        avg_reward = np.mean(episode_rewards)
        avg_distance = np.mean(episode_distances)
        
        print(f"\nğŸ¯ SACæµ‹è¯•ç»“æœ:")
        print(f"   æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{n_episodes})")
        print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"   å¹³å‡æœ€å°è·ç¦»: {avg_distance:.4f}")
        
        return {
            'success_rate': success_rate,
            'avg_reward': avg_reward,
            'avg_distance': avg_distance,
            'episode_rewards': episode_rewards,
            'episode_distances': episode_distances
        }
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ SACæµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return None
    
    finally:
        test_env.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ å¯è§†åŒ–SAC vs MADDPGå¯¹æ¯”è®­ç»ƒç³»ç»Ÿ")
    print("ğŸ¯ ç›®æ ‡: è§‚å¯Ÿä¸åŒç®—æ³•åœ¨3å…³èŠ‚Reacherä¸Šçš„è¡¨ç°")
    print()
    
    # è®­ç»ƒSAC
    model, training_time = train_visual_sac(num_joints=3, total_timesteps=5000)
    
    # æµ‹è¯•SAC
    result = test_visual_sac(model, num_joints=3, n_episodes=5)
    
    if result:
        print(f"\nğŸ‰ å¯è§†åŒ–SACè®­ç»ƒå®Œæˆ!")
        print(f"   è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        print(f"   æœ€ç»ˆæˆåŠŸç‡: {result['success_rate']:.1%}")
        print(f"   å¹³å‡å¥–åŠ±: {result['avg_reward']:.2f}")
        
        # ä¿å­˜æ¨¡å‹
        model_path = "models/visual_sac_3joint_reacher"
        model.save(model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

if __name__ == "__main__":
    main()
