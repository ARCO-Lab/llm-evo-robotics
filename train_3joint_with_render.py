#!/usr/bin/env python3
"""
3å…³èŠ‚ Reacher è®­ç»ƒå¹¶æ˜¾ç¤ºæ¸²æŸ“
"""

import os
import time
import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List
import gymnasium as gym
from gymnasium.spaces import Box
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import BaseCallback

# å¯¼å…¥çœŸå®å¤šå…³èŠ‚ç¯å¢ƒ
from real_multi_joint_reacher import RealMultiJointWrapper

# ============================================================================
# ğŸ§© 3å…³èŠ‚ç‰¹å¾æå–å™¨
# ============================================================================

class ThreeJointExtractor(BaseFeaturesExtractor):
    """3å…³èŠ‚ç‰¹å¾æå–å™¨"""
    
    def __init__(self, observation_space: gym.Space, features_dim: int = 128):
        super(ThreeJointExtractor, self).__init__(observation_space, features_dim)
        
        obs_dim = observation_space.shape[0]
        print(f"ğŸ”§ ThreeJointExtractor: {obs_dim} -> {features_dim}")
        
        # é’ˆå¯¹3å…³èŠ‚ä¼˜åŒ–çš„ç½‘ç»œç»“æ„
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, features_dim),
            nn.ReLU()
        )
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        return self.net(observations)

# ============================================================================
# ğŸ§© æ¸²æŸ“å›è°ƒ
# ============================================================================

class RenderCallback(BaseCallback):
    """æ¸²æŸ“å›è°ƒï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾ç¤ºè¿›åº¦"""
    
    def __init__(self, render_freq: int = 100, verbose: int = 0):
        super(RenderCallback, self).__init__(verbose)
        self.render_freq = render_freq
        self.step_count = 0
        
    def _on_step(self) -> bool:
        self.step_count += 1
        
        # æ¯éš”ä¸€å®šæ­¥æ•°æ˜¾ç¤ºä¿¡æ¯
        if self.step_count % self.render_freq == 0:
            if hasattr(self.locals, 'infos') and len(self.locals['infos']) > 0:
                info = self.locals['infos'][0]
                distance = info.get('distance_to_target', 'N/A')
                reward = self.locals.get('rewards', [0])[0]
                print(f"ğŸ¯ Step {self.step_count}: è·ç¦»={distance:.3f}, å¥–åŠ±={reward:.3f}")
        
        return True

# ============================================================================
# ğŸ§© è®­ç»ƒå‡½æ•°
# ============================================================================

def train_3joint_with_render(total_timesteps: int = 15000, 
                           model_name: str = "3joint_render_sac"):
    """è®­ç»ƒ3å…³èŠ‚æ¨¡å‹å¹¶æ˜¾ç¤ºæ¸²æŸ“"""
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ 3å…³èŠ‚ Reacher è®­ç»ƒ (å¸¦æ¸²æŸ“)")
    print(f"{'='*70}")
    
    # å¼ºåˆ¶å¯ç”¨æ¸²æŸ“
    os.environ['MUJOCO_GL'] = 'glfw'
    
    # åˆ›å»º3å…³èŠ‚ç¯å¢ƒ (å¸¦æ¸²æŸ“)
    print(f"ğŸŒ åˆ›å»º3å…³èŠ‚è®­ç»ƒç¯å¢ƒ (å¸¦æ¸²æŸ“)...")
    env = RealMultiJointWrapper(
        num_joints=3,
        link_lengths=[0.1, 0.1, 0.1],
        render_mode='human'  # è®­ç»ƒæ—¶ä¹Ÿæ¸²æŸ“
    )
    env = Monitor(env)
    
    print(f"âœ… 3å…³èŠ‚ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"   è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
    
    # åˆ›å»ºSACæ¨¡å‹ (é’ˆå¯¹3å…³èŠ‚ä¼˜åŒ–)
    print(f"ğŸ¤– åˆ›å»º3å…³èŠ‚ä¼˜åŒ–SACæ¨¡å‹...")
    
    policy_kwargs = {
        'features_extractor_class': ThreeJointExtractor,
        'features_extractor_kwargs': {
            'features_dim': 128
        },
        'net_arch': [256, 256, 128]  # æ›´æ·±çš„ç½‘ç»œ
    }
    
    model = SAC(
        'MlpPolicy',
        env,
        policy_kwargs=policy_kwargs,
        learning_rate=1e-4,          # è¾ƒå°çš„å­¦ä¹ ç‡
        buffer_size=100000,          # æ›´å¤§çš„ç¼“å†²åŒº
        learning_starts=1000,        # æ›´å¤šçš„é¢„çƒ­æ­¥æ•°
        batch_size=256,              # æ›´å¤§çš„æ‰¹æ¬¡
        tau=0.005,                   # è½¯æ›´æ–°ç³»æ•°
        gamma=0.99,                  # æŠ˜æ‰£å› å­
        ent_coef='auto',             # è‡ªåŠ¨ç†µç³»æ•°
        verbose=2,                   # è¯¦ç»†è¾“å‡º
        device='cpu'
    )
    
    print(f"âœ… SACæ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š æ¨¡å‹é…ç½®:")
    print(f"   å­¦ä¹ ç‡: 1e-4")
    print(f"   ç¼“å†²åŒº: 100,000")
    print(f"   æ‰¹æ¬¡å¤§å°: 256")
    print(f"   ç½‘ç»œç»“æ„: [256, 256, 128]")
    
    # åˆ›å»ºæ¸²æŸ“å›è°ƒ
    render_callback = RenderCallback(render_freq=200, verbose=1)
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹3å…³èŠ‚è®­ç»ƒ ({total_timesteps} æ­¥)...")
    print(f"ğŸ’¡ è¯·è§‚å¯ŸMuJoCoçª—å£ä¸­çš„3å…³èŠ‚æœºå™¨äººè®­ç»ƒè¿‡ç¨‹")
    print(f"ğŸ® çª—å£æ ‡é¢˜: MuJoCo")
    print("-" * 70)
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=render_callback,
            log_interval=2,              # æ›´é¢‘ç¹çš„æ—¥å¿—
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        print(f"\nâœ… 3å…³èŠ‚è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        
        # ä¿å­˜æ¨¡å‹
        model.save(model_name)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_name}")
        
        # ç«‹å³æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
        print(f"\nğŸ® ç«‹å³æµ‹è¯•è®­ç»ƒå¥½çš„3å…³èŠ‚æ¨¡å‹...")
        test_trained_3joint_model(model, env)
        
        return model, training_time
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        training_time = time.time() - start_time
        print(f"â±ï¸ å·²è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        
        # ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹
        model.save(f"{model_name}_interrupted")
        print(f"ğŸ’¾ ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {model_name}_interrupted")
        
        return model, training_time
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        return None, 0
    
    finally:
        env.close()

# ============================================================================
# ğŸ§© æµ‹è¯•å‡½æ•°
# ============================================================================

def test_trained_3joint_model(model, env, num_episodes: int = 3):
    """æµ‹è¯•è®­ç»ƒå¥½çš„3å…³èŠ‚æ¨¡å‹"""
    
    print(f"\n{'='*50}")
    print(f"ğŸ® æµ‹è¯•è®­ç»ƒå¥½çš„3å…³èŠ‚æ¨¡å‹")
    print(f"{'='*50}")
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(num_episodes):
        obs, info = env.reset()
        episode_reward = 0
        episode_length = 0
        min_distance = float('inf')
        
        print(f"\nğŸ“ Episode {episode + 1}/{num_episodes}")
        
        while True:
            # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            episode_length += 1
            
            # è®°å½•æœ€å°è·ç¦»
            distance = info.get('distance_to_target', float('inf'))
            min_distance = min(min_distance, distance)
            
            # æ¯10æ­¥æ˜¾ç¤ºä¸€æ¬¡ä¿¡æ¯
            if episode_length % 10 == 0:
                print(f"   Step {episode_length}: è·ç¦»={distance:.3f}, å¥–åŠ±={reward:.3f}")
            
            # ç¨å¾®æ…¢ä¸€ç‚¹è®©æ‚¨è§‚å¯Ÿ
            time.sleep(0.1)
            
            if terminated or truncated:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # åˆ¤æ–­æˆåŠŸ
        success = min_distance < 0.05 or episode_reward > -5
        if success:
            success_count += 1
            print(f"   âœ… æˆåŠŸ! å¥–åŠ±={episode_reward:.3f}, æœ€å°è·ç¦»={min_distance:.3f}")
        else:
            print(f"   âŒ å¤±è´¥. å¥–åŠ±={episode_reward:.3f}, æœ€å°è·ç¦»={min_distance:.3f}")
    
    # ç»Ÿè®¡ç»“æœ
    avg_reward = np.mean(episode_rewards)
    success_rate = success_count / num_episodes
    
    print(f"\nğŸ“Š 3å…³èŠ‚æµ‹è¯•ç»“æœ:")
    print(f"   æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{num_episodes})")
    print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.3f} Â± {np.std(episode_rewards):.3f}")
    print(f"   å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.1f}")
    
    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward,
        'episode_rewards': episode_rewards
    }

# ============================================================================
# ğŸ§© ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ 3å…³èŠ‚ Reacher è®­ç»ƒæ¸²æŸ“æ¼”ç¤º")
    print("ğŸ’¡ è®­ç»ƒè¿‡ç¨‹ä¸­å°†æ˜¾ç¤ºMuJoCoæ¸²æŸ“çª—å£")
    print("ğŸ¯ 3å…³èŠ‚æœºå™¨äººæ¯”2å…³èŠ‚æ›´å¤æ‚ï¼Œéœ€è¦æ›´é•¿æ—¶é—´è®­ç»ƒ")
    print()
    
    try:
        # è®­ç»ƒ3å…³èŠ‚æ¨¡å‹
        model, training_time = train_3joint_with_render(
            total_timesteps=15000,  # å¢åŠ è®­ç»ƒæ­¥æ•°
            model_name="models/3joint_render_sac"
        )
        
        if model is not None:
            print(f"\nğŸ‰ 3å…³èŠ‚è®­ç»ƒæ¼”ç¤ºå®Œæˆï¼")
            print(f"â±ï¸ æ€»ç”¨æ—¶: {training_time/60:.1f} åˆ†é’Ÿ")
        else:
            print(f"\nâŒ 3å…³èŠ‚è®­ç»ƒå¤±è´¥")
            
    except KeyboardInterrupt:
        print(f"\nâš ï¸ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()


