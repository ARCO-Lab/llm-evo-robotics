#!/usr/bin/env python3
"""
MuJoCo Reacher é€‚é…å™¨
å°† OpenAI MuJoCo Reacher ç¯å¢ƒé€‚é…ä¸ºä¸å½“å‰ Reacher2DEnv å…¼å®¹çš„æ¥å£
"""

import numpy as np
import gymnasium as gym
from gymnasium.spaces import Box
from gymnasium import Env
import math
import yaml
import os
from typing import Optional, Tuple, Dict, Any

class MuJoCoReacherAdapter(Env):
    """
    MuJoCo Reacher ç¯å¢ƒé€‚é…å™¨
    æä¾›ä¸ Reacher2DEnv ç›¸åŒçš„æ¥å£ï¼Œä½†ä½¿ç”¨ MuJoCo ç‰©ç†å¼•æ“
    """
    
    def __init__(self, num_links=2, link_lengths=None, render_mode=None, config_path=None, curriculum_stage=1, debug_level='SILENT'):
        """åˆå§‹åŒ–é€‚é…å™¨"""
        
        # åˆ›å»º MuJoCo Reacher ç¯å¢ƒ
        self.mujoco_env = gym.make('Reacher-v5', render_mode=render_mode)
        
        # å…¼å®¹æ€§è®¾ç½®
        self.gym_api_version = "old"  # ä½¿ç”¨æ—§ API ä»¥å…¼å®¹å‘é‡åŒ–ç¯å¢ƒ
        self.render_mode = render_mode
        
        # è®¾ç½® Gymnasium æ ‡å‡†å±æ€§
        super().__init__()
        
        # åŠ è½½é…ç½®ï¼ˆä¿æŒä¸åŸç¯å¢ƒç›¸åŒçš„é…ç½®ç³»ç»Ÿï¼‰
        self.config = self._load_config(config_path)
        
        # ç¯å¢ƒå‚æ•°ï¼ˆä¿æŒä¸åŸç¯å¢ƒå…¼å®¹ï¼‰
        self.num_links = num_links  # æ”¯æŒè‡ªå®šä¹‰å…³èŠ‚æ•°ï¼Œä½† MuJoCo Reacher å›ºå®šä¸º 2
        if self.num_links != 2:
            print(f"âš ï¸ MuJoCo Reacher åªæ”¯æŒ 2 å…³èŠ‚ï¼Œå°†ä½¿ç”¨ 2 å…³èŠ‚è€Œä¸æ˜¯ {num_links}")
            self.num_links = 2
        
        # Link é•¿åº¦è®¾ç½®
        if link_lengths is None:
            self.link_lengths = [0.1, 0.1]  # MuJoCo ä¸­çš„é»˜è®¤ link é•¿åº¦ï¼ˆç±³ï¼‰
        else:
            self.link_lengths = link_lengths[:2]  # åªå–å‰ä¸¤ä¸ªå€¼
        self.max_torque = 0.05  # å¢åŠ æ‰­çŸ©ï¼Œç¡®ä¿èƒ½äº§ç”Ÿæœ‰æ•ˆè¿åŠ¨
        
        # åæ ‡ç³»è½¬æ¢å‚æ•°
        self.scale_factor = 600  # å°† MuJoCo åæ ‡ç¼©æ”¾åˆ°åƒç´ åæ ‡
        self.anchor_point = self.config.get("start", {}).get("position", [480, 620])
        self.goal_pos = np.array(self.config.get("goal", {}).get("position", [600, 550]))
        self.initial_goal_pos = self.goal_pos.copy()  # ä¿å­˜åˆå§‹ç›®æ ‡ä½ç½®
        
        # å®šä¹‰é€‚é…åçš„ç©ºé—´ï¼ˆä¿æŒä¸åŸç¯å¢ƒç›¸åŒï¼‰
        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(12,), dtype=np.float32)
        self.action_space = Box(low=-self.max_torque, high=self.max_torque, shape=(2,), dtype=np.float32)
        
        # æ·»åŠ  spec å±æ€§ä»¥å…¼å®¹å‘é‡åŒ–ç¯å¢ƒ
        self.spec = None
        if hasattr(self.mujoco_env, 'spec'):
            self.spec = self.mujoco_env.spec
        
        # çŠ¶æ€å˜é‡
        self.step_count = 0
        self.collision_count = 0
        self.base_collision_count = 0
        self.self_collision_count = 0
        
        # å¥–åŠ±ç»„ä»¶
        self.current_reward = 0.0
        self.reward_components = {
            'distance_reward': 0.0,
            'reach_reward': 0.0,
            'progress_reward': 0.0,
            'collision_penalty': 0.0,
            'control_penalty': 0.0
        }
        
        # ç»´æŒç³»ç»Ÿï¼ˆä¿æŒä¸åŸç¯å¢ƒå…¼å®¹ï¼‰
        self.maintain_threshold = 150.0
        self.maintain_target_steps = 200
        self.maintain_counter = 0
        self.maintain_bonus_given = False
        self.in_maintain_zone = False
        self.maintain_history = []
        self.max_maintain_streak = 0
        
        # Episode ç®¡ç†
        self.current_episode = 1
        self.episode_ended = False
        
        print(f"ğŸ¯ MuJoCo Reacher é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è§‚å¯Ÿç©ºé—´: {self.observation_space}")
        print(f"   åŠ¨ä½œç©ºé—´: {self.action_space}")
        print(f"   æ¸²æŸ“æ¨¡å¼: {render_mode}")
    
    def _load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä¸åŸç¯å¢ƒç›¸åŒï¼‰"""
        if config_path is None:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            return {
                "start": {"position": [480, 620]},
                "goal": {"position": [600, 550]},
                "obstacles": []
            }
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}: {e}")
            return {
                "start": {"position": [480, 620]},
                "goal": {"position": [600, 550]},
                "obstacles": []
            }
    
    def _mujoco_to_custom_obs(self, mujoco_obs):
        """å°† MuJoCo è§‚å¯Ÿè½¬æ¢ä¸ºè‡ªå®šä¹‰æ ¼å¼"""
        # MuJoCo è§‚å¯Ÿæ ¼å¼: [cos(Î¸1), cos(Î¸2), sin(Î¸1), sin(Î¸2), ç›®æ ‡x, ç›®æ ‡y, Ï‰1, Ï‰2, å‘é‡x, å‘é‡y]
        
        # æå–è§’åº¦ï¼ˆä» cos/sin é‡æ„ï¼‰
        cos_theta1, cos_theta2 = mujoco_obs[0], mujoco_obs[1]
        sin_theta1, sin_theta2 = mujoco_obs[2], mujoco_obs[3]
        
        theta1 = math.atan2(sin_theta1, cos_theta1)
        theta2 = math.atan2(sin_theta2, cos_theta2)
        
        # æå–è§’é€Ÿåº¦
        omega1, omega2 = mujoco_obs[6], mujoco_obs[7]
        
        # è®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ï¼ˆåŸºäº MuJoCo çš„è¿åŠ¨å­¦ï¼‰
        # MuJoCo ä¸­çš„åæ ‡éœ€è¦è½¬æ¢åˆ°æˆ‘ä»¬çš„åƒç´ åæ ‡ç³»
        end_effector_pos = self._calculate_end_effector_from_mujoco(theta1, theta2)
        
        # ç›®æ ‡ä½ç½®ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„ç›®æ ‡ï¼‰
        goal_pos = self.goal_pos
        
        # è®¡ç®—è·ç¦»
        distance = np.linalg.norm(end_effector_pos - goal_pos)
        
        # æ„å»º 12 ç»´è§‚å¯Ÿç©ºé—´
        custom_obs = np.array([
            theta1,                    # å…³èŠ‚è§’åº¦1
            theta2,                    # å…³èŠ‚è§’åº¦2
            omega1,                    # å…³èŠ‚é€Ÿåº¦1
            omega2,                    # å…³èŠ‚é€Ÿåº¦2
            end_effector_pos[0],       # æœ«ç«¯ä½ç½®x
            end_effector_pos[1],       # æœ«ç«¯ä½ç½®y
            goal_pos[0],               # ç›®æ ‡ä½ç½®x
            goal_pos[1],               # ç›®æ ‡ä½ç½®y
            distance,                  # è·ç¦»
            0.0,                       # ç¢°æ’çŠ¶æ€ï¼ˆMuJoCo ç¯å¢ƒä¸­æš‚æ—¶è®¾ä¸º0ï¼‰
            float(self.collision_count),      # ç¢°æ’è®¡æ•°
            float(self.base_collision_count)  # åŸºåº§ç¢°æ’è®¡æ•°
        ], dtype=np.float32)
        
        return custom_obs
    
    def _init_rotation_monitor(self):
        """åˆå§‹åŒ–ç–¯ç‹‚æ—‹è½¬æ£€æµ‹ç³»ç»Ÿ"""
        self.rotation_monitor = {
            'joint_velocities': [],  # å…³èŠ‚é€Ÿåº¦å†å²
            'action_magnitudes': [],  # åŠ¨ä½œå¹…åº¦å†å²
            'position_changes': [],  # ä½ç½®å˜åŒ–å†å²
            'crazy_rotation_count': 0,  # ç–¯ç‹‚æ—‹è½¬è®¡æ•°
            'last_joint_positions': None,  # ä¸Šä¸€æ­¥çš„å…³èŠ‚ä½ç½®
            'high_velocity_steps': 0,  # è¿ç»­é«˜é€Ÿåº¦æ­¥æ•°
            'detection_window': 20,  # æ£€æµ‹çª—å£å¤§å°
            'velocity_threshold': 1.0,  # é€Ÿåº¦é˜ˆå€¼ï¼ˆrad/sï¼‰- æä½é˜ˆå€¼
            'action_threshold': 1.0,  # åŠ¨ä½œå¹…åº¦é˜ˆå€¼ - é™ä½
            'crazy_threshold': 10,  # è¿ç»­å¼‚å¸¸æ­¥æ•°é˜ˆå€¼
        }
    
    def _detect_crazy_rotation(self, mujoco_obs, custom_action):
        """æ£€æµ‹ç–¯ç‹‚æ—‹è½¬è¡Œä¸º"""
        monitor = self.rotation_monitor
        
        # æå–å…³èŠ‚è§’åº¦å’Œé€Ÿåº¦
        joint_positions = mujoco_obs[:2]  # å…³èŠ‚ä½ç½®
        joint_velocities = mujoco_obs[2:4]  # å…³èŠ‚é€Ÿåº¦
        
        # è®¡ç®—åŠ¨ä½œå¹…åº¦
        action_magnitude = np.linalg.norm(custom_action)
        
        # è®¡ç®—å…³èŠ‚é€Ÿåº¦å¹…åº¦
        velocity_magnitude = np.linalg.norm(joint_velocities)
        
        # æ›´æ–°å†å²è®°å½•
        monitor['joint_velocities'].append(velocity_magnitude)
        monitor['action_magnitudes'].append(action_magnitude)
        
        # ä¿æŒçª—å£å¤§å°
        if len(monitor['joint_velocities']) > monitor['detection_window']:
            monitor['joint_velocities'].pop(0)
            monitor['action_magnitudes'].pop(0)
        
        # æ£€æµ‹å¼‚å¸¸æ¡ä»¶
        is_crazy = False
        crazy_reasons = []
        
        # æ¡ä»¶1: å…³èŠ‚é€Ÿåº¦è¿‡é«˜
        if velocity_magnitude > monitor['velocity_threshold']:
            monitor['high_velocity_steps'] += 1
            crazy_reasons.append(f"é«˜é€Ÿåº¦: {velocity_magnitude:.2f} rad/s")
        else:
            monitor['high_velocity_steps'] = 0
        
        # æ¡ä»¶2: åŠ¨ä½œå¹…åº¦è¿‡å¤§
        if action_magnitude > monitor['action_threshold']:
            crazy_reasons.append(f"å¤§åŠ¨ä½œ: {action_magnitude:.2f}")
        
        # æ¡ä»¶3: è¿ç»­é«˜é€Ÿåº¦
        if monitor['high_velocity_steps'] > 5:
            crazy_reasons.append(f"è¿ç»­é«˜é€Ÿ: {monitor['high_velocity_steps']}æ­¥")
            is_crazy = True
        
        # æ¡ä»¶4: å¹³å‡é€Ÿåº¦è¿‡é«˜
        if len(monitor['joint_velocities']) >= 10:
            avg_velocity = np.mean(monitor['joint_velocities'][-10:])
            if avg_velocity > monitor['velocity_threshold'] * 0.7:
                crazy_reasons.append(f"å¹³å‡é«˜é€Ÿ: {avg_velocity:.2f}")
                is_crazy = True
        
        # æ›´æ–°ç–¯ç‹‚æ—‹è½¬è®¡æ•°
        if is_crazy:
            monitor['crazy_rotation_count'] += 1
            if monitor['crazy_rotation_count'] % 20 == 1:  # æ¯20æ­¥æŠ¥å‘Šä¸€æ¬¡
                print(f"ğŸŒªï¸ æ£€æµ‹åˆ°ç–¯ç‹‚æ—‹è½¬! åŸå› : {', '.join(crazy_reasons)}")
                print(f"   å…³èŠ‚é€Ÿåº¦: {joint_velocities}")
                print(f"   åŠ¨ä½œè¾“å…¥: {custom_action}")
                print(f"   ç´¯è®¡å¼‚å¸¸: {monitor['crazy_rotation_count']}æ­¥")
        else:
            # é‡ç½®è®¡æ•°å™¨ï¼ˆå…è®¸å¶å°”çš„å¼‚å¸¸ï¼‰
            if monitor['crazy_rotation_count'] > 0:
                monitor['crazy_rotation_count'] = max(0, monitor['crazy_rotation_count'] - 1)
        
        return is_crazy, crazy_reasons
    
    def _calculate_end_effector_from_mujoco(self, theta1, theta2):
        """æ ¹æ®å…³èŠ‚è§’åº¦è®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®"""
        # å°† MuJoCo çš„è¿åŠ¨å­¦è½¬æ¢ä¸ºæˆ‘ä»¬çš„åæ ‡ç³»
        # MuJoCo ä¸­ link é•¿åº¦é€šå¸¸æ˜¯ 0.1 ç±³
        link1_length = self.link_lengths[0] * self.scale_factor  # è½¬æ¢ä¸ºåƒç´ 
        link2_length = self.link_lengths[1] * self.scale_factor
        
        # è®¡ç®—ç¬¬ä¸€ä¸ª link çš„æœ«ç«¯ä½ç½®
        x1 = self.anchor_point[0] + link1_length * np.cos(theta1)
        y1 = self.anchor_point[1] + link1_length * np.sin(theta1)
        
        # è®¡ç®—ç¬¬äºŒä¸ª link çš„æœ«ç«¯ä½ç½®ï¼ˆæœ«ç«¯æ‰§è¡Œå™¨ï¼‰
        x2 = x1 + link2_length * np.cos(theta1 + theta2)
        y2 = y1 + link2_length * np.sin(theta1 + theta2)
        
        return np.array([x2, y2])
    
    def _custom_to_mujoco_action(self, custom_action):
        """å°†è‡ªå®šä¹‰åŠ¨ä½œè½¬æ¢ä¸º MuJoCo åŠ¨ä½œ"""
        # è‡ªå®šä¹‰åŠ¨ä½œèŒƒå›´: [-max_torque, max_torque]
        # MuJoCo åŠ¨ä½œèŒƒå›´: [-1, 1]
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆæ¯100æ­¥æ‰“å°ä¸€æ¬¡ï¼‰
        if not hasattr(self, '_action_debug_counter'):
            self._action_debug_counter = 0
        self._action_debug_counter += 1
        
        if self._action_debug_counter % 100 == 0:
            print(f"ğŸ® [Step {self._action_debug_counter}] SACåŠ¨ä½œ: {custom_action}, èŒƒå›´: [{np.min(custom_action):.3f}, {np.max(custom_action):.3f}]")
        
        # ä¿®æ­£çš„çº¿æ€§æ˜ å°„ - ç›´æ¥ä½¿ç”¨ custom_actionï¼Œä¸å†é™¤ä»¥ max_torque
        # å› ä¸º custom_action å·²ç»åœ¨ [-max_torque, max_torque] èŒƒå›´å†…
        # æˆ‘ä»¬éœ€è¦å°†å…¶æ˜ å°„åˆ° [-1, 1] èŒƒå›´ï¼Œä½†ä¿æŒæ¯”ä¾‹å…³ç³»
        mujoco_action = np.clip(custom_action, -self.max_torque, self.max_torque)
        
        if self._action_debug_counter % 100 == 0:
            print(f"ğŸ® [Step {self._action_debug_counter}] MuJoCoåŠ¨ä½œ: {mujoco_action}, ç¼©æ”¾æ¯”ä¾‹: 1/{self.max_torque}")
        
        return mujoco_action.astype(np.float32)
    
    def _compute_custom_reward(self, mujoco_obs, mujoco_reward, custom_obs):
        """ğŸ¯ æ”¹è¿›çš„å¥–åŠ±å‡½æ•°ï¼Œæä¾›æ›´å¥½çš„å­¦ä¹ ä¿¡å·"""
        end_pos = custom_obs[4:6]  # æœ«ç«¯ä½ç½®
        goal_pos = custom_obs[6:8]  # ç›®æ ‡ä½ç½®
        distance = custom_obs[8]   # è·ç¦»ï¼ˆåƒç´ ï¼‰
        
        # å°†è·ç¦»è½¬æ¢ä¸ºæ ‡å‡†åŒ–å•ä½ (0-1)
        normalized_distance = min(distance / 200.0, 1.0)  # 200px ä¸ºæœ€å¤§æœ‰æ„ä¹‰è·ç¦»
        
        # ğŸ¯ 1. è·ç¦»å¥–åŠ± - æŒ‡æ•°è¡°å‡ï¼Œè¶Šè¿‘å¥–åŠ±è¶Šå¤§
        distance_reward = np.exp(-normalized_distance * 3.0) - 1.0  # èŒƒå›´ [-1, 0]
        
        # ğŸ¯ 2. åˆ°è¾¾å¥–åŠ± - å¤§å¹…å¥–åŠ±æˆåŠŸ
        reach_reward = 0.0
        if distance < 10.0:  # 10px å†…ç®—çœŸæ­£æˆåŠŸ
            reach_reward = 20.0  # è¶…å¤§å¥–åŠ±
        elif distance < 25.0:  # 25px å†…ç»™äºˆè¾ƒå¤§å¥–åŠ±
            reach_reward = 10.0 * (25.0 - distance) / 15.0
        elif distance < 50.0:  # 50px å†…ç»™äºˆå°å¥–åŠ±
            reach_reward = 2.0 * (50.0 - distance) / 25.0
        
        # ğŸ¯ 3. è¿›æ­¥å¥–åŠ± - é¼“åŠ±æœç›®æ ‡ç§»åŠ¨
        progress_reward = 0.0
        if hasattr(self, 'prev_distance'):
            progress = self.prev_distance - distance
            if progress > 0:  # è·ç¦»å‡å°
                progress_reward = progress * 0.02  # æ¯åƒç´ è¿›æ­¥ç»™ 0.02 å¥–åŠ±
            else:  # è·ç¦»å¢å¤§
                progress_reward = progress * 0.01  # è½»å¾®æƒ©ç½šé€€æ­¥
        self.prev_distance = distance
        
        # ğŸ¯ 4. é€Ÿåº¦å¥–åŠ± - é¼“åŠ±åˆç†çš„ç§»åŠ¨é€Ÿåº¦
        joint_velocities = mujoco_obs[2:4]  # å…³èŠ‚é€Ÿåº¦
        velocity_magnitude = np.linalg.norm(joint_velocities)
        
        # é€‚ä¸­çš„é€Ÿåº¦æœ€å¥½ï¼Œå¤ªå¿«æˆ–å¤ªæ…¢éƒ½ä¸å¥½
        if velocity_magnitude < 0.5:
            velocity_reward = -0.1  # å¤ªæ…¢æƒ©ç½š
        elif velocity_magnitude > 10.0:
            velocity_reward = -0.5  # å¤ªå¿«æƒ©ç½šï¼ˆç–¯ç‹‚æ—‹è½¬ï¼‰
        else:
            velocity_reward = 0.1  # åˆç†é€Ÿåº¦å¥–åŠ±
        
        # ğŸ¯ 5. æ§åˆ¶å¹³æ»‘åº¦å¥–åŠ±
        if hasattr(self, 'prev_action'):
            action_change = np.linalg.norm(self.current_action - self.prev_action)
            if action_change > 2.0:
                smoothness_reward = -0.2  # åŠ¨ä½œå˜åŒ–å¤ªå¤§æƒ©ç½š
            else:
                smoothness_reward = 0.05  # å¹³æ»‘æ§åˆ¶å¥–åŠ±
        else:
            smoothness_reward = 0.0
        
        # ğŸ¯ 6. æ§åˆ¶æƒ©ç½š - é¼“åŠ±å°çš„æ§åˆ¶è¾“å…¥
        control_penalty = -0.01 * np.sum(np.square(self.current_action))
        
        # è®¡ç®—æ€»å¥–åŠ±
        total_reward = (distance_reward + reach_reward + progress_reward + 
                       velocity_reward + smoothness_reward + control_penalty)
        
        # ğŸ¯ è°ƒè¯•ä¿¡æ¯ï¼ˆæ¯100æ­¥æ‰“å°ä¸€æ¬¡ï¼‰
        if not hasattr(self, '_reward_debug_counter'):
            self._reward_debug_counter = 0
        self._reward_debug_counter += 1
        
        if self._reward_debug_counter % 100 == 0:
            print(f"ğŸ [Step {self._reward_debug_counter}] å¥–åŠ±åˆ†è§£:")
            print(f"   è·ç¦»å¥–åŠ±: {distance_reward:.3f} (è·ç¦»: {distance:.1f}px)")
            print(f"   åˆ°è¾¾å¥–åŠ±: {reach_reward:.3f}")
            print(f"   è¿›æ­¥å¥–åŠ±: {progress_reward:.3f}")
            print(f"   é€Ÿåº¦å¥–åŠ±: {velocity_reward:.3f} (é€Ÿåº¦: {velocity_magnitude:.2f})")
            print(f"   å¹³æ»‘å¥–åŠ±: {smoothness_reward:.3f}")
            print(f"   æ§åˆ¶æƒ©ç½š: {control_penalty:.3f}")
            print(f"   æ€»å¥–åŠ±: {total_reward:.3f}")
        
        # å­˜å‚¨å½“å‰åŠ¨ä½œç”¨äºä¸‹æ¬¡è®¡ç®—å¹³æ»‘åº¦
        if hasattr(self, 'current_action'):
            self.prev_action = self.current_action.copy()
        
        # å­˜å‚¨å¥–åŠ±ç»„æˆéƒ¨åˆ†
        self.reward_components = {
            'distance_reward': distance_reward,
            'reach_reward': reach_reward,
            'progress_reward': progress_reward,
            'velocity_reward': velocity_reward,
            'smoothness_reward': smoothness_reward,
            'control_penalty': control_penalty,
            'total_reward': total_reward
        }
        
        self.current_reward = total_reward
        return total_reward
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        # é‡ç½® MuJoCo ç¯å¢ƒ
        mujoco_obs, mujoco_info = self.mujoco_env.reset(seed=seed)
        
        # é‡ç½®é€‚é…å™¨çŠ¶æ€
        self.step_count = 0
        self.collision_count = 0
        self.base_collision_count = 0
        self.self_collision_count = 0
        
        # é‡ç½®ç»´æŒçŠ¶æ€
        self.maintain_counter = 0
        self.maintain_bonus_given = False
        self.in_maintain_zone = False
        
        # Episode ç®¡ç†
        if not hasattr(self, 'current_episode'):
            self.current_episode = 1
            self.episode_ended = False
        elif hasattr(self, 'episode_ended') and self.episode_ended:
            self.current_episode += 1
            self.episode_ended = False
        
        # åˆå§‹åŒ–ç–¯ç‹‚æ—‹è½¬æ£€æµ‹ç³»ç»Ÿ
        self._init_rotation_monitor()
        
        # è½¬æ¢è§‚å¯Ÿ
        custom_obs = self._mujoco_to_custom_obs(mujoco_obs)
        # ç¡®ä¿è§‚å¯Ÿå€¼æ˜¯ float32 ç±»å‹
        custom_obs = custom_obs.astype(np.float32)
        
        print(f"ğŸ”„ MuJoCo é€‚é…å™¨é‡ç½®å®Œæˆ - Episode {self.current_episode}")
        
        if self.gym_api_version == "new":
            info = self._get_info()
            return custom_obs, info
        else:
            return custom_obs
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥"""
        # å­˜å‚¨å½“å‰åŠ¨ä½œç”¨äºå¥–åŠ±è®¡ç®—
        self.current_action = np.array(action)
        
        # è½¬æ¢åŠ¨ä½œ
        mujoco_action = self._custom_to_mujoco_action(action)
        
        # åœ¨ MuJoCo ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ
        mujoco_obs, mujoco_reward, mujoco_terminated, mujoco_truncated, mujoco_info = self.mujoco_env.step(mujoco_action)
        
        # æ£€æµ‹ç–¯ç‹‚æ—‹è½¬
        is_crazy, crazy_reasons = self._detect_crazy_rotation(mujoco_obs, action)
        
        # ç–¯ç‹‚æ—‹è½¬æ£€æµ‹ï¼ˆä»…è®°å½•ï¼Œä¸å¼ºåˆ¶é‡ç½®ï¼‰
        if is_crazy and self.rotation_monitor['crazy_rotation_count'] % 20 == 0:
            print(f"âš ï¸ æ£€æµ‹åˆ°é«˜é€Ÿæ—‹è½¬ï¼Œä½†ç»§ç»­è®­ç»ƒ... (è®¡æ•°: {self.rotation_monitor['crazy_rotation_count']})")
        
        # è½¬æ¢è§‚å¯Ÿ
        custom_obs = self._mujoco_to_custom_obs(mujoco_obs)
        # ç¡®ä¿è§‚å¯Ÿå€¼æ˜¯ float32 ç±»å‹
        custom_obs = custom_obs.astype(np.float32)
        
        # è®¡ç®—è‡ªå®šä¹‰å¥–åŠ±
        custom_reward = self._compute_custom_reward(mujoco_obs, mujoco_reward, custom_obs)
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ï¼ˆä½¿ç”¨è‡ªå®šä¹‰é€»è¾‘ï¼‰
        custom_done = self._is_done(custom_obs)
        
        self.step_count += 1
        
        # è·å–ä¿¡æ¯
        info = self._get_info()
        
        if self.gym_api_version == "new":
            terminated = custom_done
            truncated = self.step_count >= 120000
            return custom_obs, custom_reward, terminated, truncated, info
        else:
            return custom_obs, custom_reward, custom_done, info
    
    def _is_done(self, obs):
        """æ£€æŸ¥æ˜¯å¦å®Œæˆï¼ˆä½¿ç”¨ä¸åŸç¯å¢ƒç›¸åŒçš„é€»è¾‘ï¼‰"""
        distance = obs[8]  # è·ç¦»åœ¨è§‚å¯Ÿçš„ç¬¬9ä¸ªä½ç½®
        
        # ğŸ¯ 1. åˆ°è¾¾ç›®æ ‡å°±è¿›å…¥ä¸‹ä¸€ä¸ªepisode
        if distance < 10.0:  # 10pxå†…ç®—çœŸæ­£åˆ°è¾¾ç›®æ ‡ï¼ˆæ›´ä¸¥æ ¼çš„æ ‡å‡†ï¼‰
            print(f"ğŸ¯ çœŸæ­£åˆ°è¾¾ç›®æ ‡! è·ç¦»: {distance:.1f}pxï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªepisode")
            self.episode_ended = True
            return True
        
        # ğŸ¯ 2. æ­¥æ•°é™åˆ¶ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
        if self.step_count >= 200:  # æ¯ä¸ªepisodeæœ€å¤š200æ­¥ï¼ŒåŠ å¿«å­¦ä¹ 
            print(f"â° Episodeæ­¥æ•°é™åˆ¶: {self.step_count}æ­¥ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªepisode")
            self.episode_ended = True
            return True
        
        return False
    
    def _get_info(self):
        """è·å–é¢å¤–ä¿¡æ¯ï¼ˆä¿æŒä¸åŸç¯å¢ƒç›¸åŒçš„æ ¼å¼ï¼‰"""
        # ä»å½“å‰è§‚å¯Ÿä¸­æå–ä¿¡æ¯
        if hasattr(self, '_last_obs') and self._last_obs is not None:
            end_pos = self._last_obs[4:6]
            goal_pos = self._last_obs[6:8]
            distance = self._last_obs[8]
        else:
            # å¦‚æœæ²¡æœ‰è§‚å¯Ÿï¼Œä½¿ç”¨é»˜è®¤å€¼
            end_pos = np.array([0.0, 0.0])
            goal_pos = self.goal_pos
            distance = np.linalg.norm(end_pos - goal_pos)
        
        return {
            'end_effector_pos': end_pos,
            'goal_pos': goal_pos,
            'distance': float(distance),
            'collision_count': self.collision_count,
            'base_collision_count': self.base_collision_count,
            'step_count': self.step_count,
            'goal': {
                'distance_to_goal': float(distance),
                'goal_reached': distance < 20.0,
                'end_effector_position': end_pos,
                'goal_position': goal_pos,
            },
            'maintain': {
                'in_maintain_zone': self.in_maintain_zone,
                'maintain_counter': self.maintain_counter,
                'maintain_target': self.maintain_target_steps,
                'maintain_progress': self.maintain_counter / self.maintain_target_steps if self.maintain_target_steps > 0 else 0.0,
                'max_maintain_streak': self.max_maintain_streak,
                'maintain_completed': self.maintain_counter >= self.maintain_target_steps
            }
        }
    
    def render(self, mode='human'):
        """æ¸²æŸ“ç¯å¢ƒï¼ˆå§”æ‰˜ç»™ MuJoCo ç¯å¢ƒï¼‰"""
        return self.mujoco_env.render()
    
    def seed(self, seed=None):
        """è®¾ç½®éšæœºç§å­"""
        if hasattr(self.mujoco_env, 'seed'):
            return self.mujoco_env.seed(seed)
        elif hasattr(self.mujoco_env, 'reset'):
            # Gymnasium ç¯å¢ƒä½¿ç”¨ reset(seed=seed)
            return [seed]
        return [seed]
    
    def set_goal_position(self, goal_pos):
        """åŠ¨æ€è®¾ç½®ç›®æ ‡ä½ç½®"""
        self.goal_pos = np.array(goal_pos)
        print(f"ğŸ¯ ç›®æ ‡ä½ç½®æ›´æ–°ä¸º: [{self.goal_pos[0]:.1f}, {self.goal_pos[1]:.1f}]")
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if hasattr(self, 'mujoco_env'):
            self.mujoco_env.close()

# ä¸ºäº†å…¼å®¹æ€§ï¼Œæä¾›ä¸€ä¸ªåˆ«å
MuJoCoReacher2DEnv = MuJoCoReacherAdapter
