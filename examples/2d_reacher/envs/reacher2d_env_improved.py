#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆ2D Reacherç¯å¢ƒ - å®ç°è¯¾ç¨‹å­¦ä¹ å’Œæ”¹è¿›å¥–åŠ±æœºåˆ¶
è§£å†³SACç»´æŒä»»åŠ¡çš„æ¢ç´¢vsåˆ©ç”¨å†²çª
"""

import numpy as np
import gym
from gym import spaces
import math
import random
import time
from collections import deque

class ImprovedReacher2DEnv(gym.Env):
    """
    æ”¹è¿›ç‰ˆ2D Reacherç¯å¢ƒ - ä¸“é—¨ä¸ºSACç»´æŒä»»åŠ¡ä¼˜åŒ–
    """
    
    def __init__(self, num_links=3, link_lengths=None, render_mode=None, 
                 curriculum_stage=0, debug_level='INFO'):
        super(ImprovedReacher2DEnv, self).__init__()
        
        self.num_links = num_links
        self.link_lengths = link_lengths if link_lengths else [90.0] * num_links
        self.render_mode = render_mode
        self.debug_level = debug_level
        
        # ğŸ†• è¯¾ç¨‹å­¦ä¹ å‚æ•°
        self.curriculum_stage = curriculum_stage
        self.setup_curriculum_parameters()
        
        # åŠ¨ä½œå’Œè§‚å¯Ÿç©ºé—´
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(num_links,), dtype=np.float32
        )
        
        # è§‚å¯Ÿç©ºé—´ï¼šå…³èŠ‚è§’åº¦+é€Ÿåº¦+æœ«ç«¯ä½ç½®+ç›®æ ‡ä½ç½®+è·ç¦»+ç»´æŒçŠ¶æ€
        obs_dim = num_links * 2 + 2 + 2 + 1 + 3  # +3 for curriculum info
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32
        )
        
        # ç¯å¢ƒçŠ¶æ€
        self.joint_angles = np.zeros(num_links)
        self.joint_velocities = np.zeros(num_links)
        self.goal_pos = np.array([200.0, 100.0])
        
        # ğŸ†• æ”¹è¿›çš„ç»´æŒç³»ç»Ÿ
        self.in_maintain_zone = False
        self.maintain_counter = 0
        self.maintain_history = deque(maxlen=100)
        self.max_maintain_streak = 0
        self.consecutive_success_count = 0
        
        # è®­ç»ƒç»Ÿè®¡
        self.step_count = 0
        self.episode_count = 0
        self.total_success_episodes = 0
        
        # ğŸ†• å¥–åŠ±ç³»ç»Ÿæ”¹è¿›
        self.milestone_rewards_given = set()  # é˜²æ­¢é‡å¤ç»™å¥–åŠ±
        
        self.reset()
        
    def setup_curriculum_parameters(self):
        """ğŸ†• è®¾ç½®è¯¾ç¨‹å­¦ä¹ å‚æ•°"""
        curriculum_configs = [
            # é˜¶æ®µ0: å®¹æ˜“ - å»ºç«‹ä¿¡å¿ƒ
            {
                'maintain_threshold': 40.0,
                'maintain_target_steps': 50,
                'leave_penalty': -2.0,
                'milestone_rewards': [5.0, 10.0, 20.0],
                'milestone_steps': [10, 25, 50],
                'description': 'å…¥é—¨é˜¶æ®µ - å®½æ¾è¦æ±‚'
            },
            # é˜¶æ®µ1: ä¸­ç­‰ - é€æ­¥æé«˜
            {
                'maintain_threshold': 30.0,
                'maintain_target_steps': 150,
                'leave_penalty': -5.0,
                'milestone_rewards': [5.0, 10.0, 20.0, 30.0],
                'milestone_steps': [25, 50, 100, 150],
                'description': 'è¿›é˜¶é˜¶æ®µ - ä¸­ç­‰è¦æ±‚'
            },
            # é˜¶æ®µ2: å›°éš¾ - æœ€ç»ˆç›®æ ‡
            {
                'maintain_threshold': 20.0,
                'maintain_target_steps': 300,
                'leave_penalty': -10.0,
                'milestone_rewards': [5.0, 10.0, 20.0, 30.0, 50.0],
                'milestone_steps': [50, 100, 150, 200, 300],
                'description': 'ä¸“å®¶é˜¶æ®µ - ä¸¥æ ¼è¦æ±‚'
            }
        ]
        
        # é€‰æ‹©å½“å‰è¯¾ç¨‹é…ç½®
        config = curriculum_configs[min(self.curriculum_stage, len(curriculum_configs) - 1)]
        
        self.maintain_threshold = config['maintain_threshold']
        self.maintain_target_steps = config['maintain_target_steps']
        self.leave_penalty = config['leave_penalty']
        self.milestone_rewards = config['milestone_rewards']
        self.milestone_steps = config['milestone_steps']
        self.curriculum_description = config['description']
        
        if self.debug_level == 'INFO':
            print(f"ğŸ“ è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ {self.curriculum_stage}: {self.curriculum_description}")
            print(f"   ç»´æŒé˜ˆå€¼: {self.maintain_threshold}px, ç›®æ ‡æ­¥æ•°: {self.maintain_target_steps}")
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        super().reset(seed=seed)
        
        # éšæœºåˆå§‹åŒ–å…³èŠ‚è§’åº¦
        self.joint_angles = np.random.uniform(-0.5, 0.5, self.num_links)
        self.joint_velocities = np.zeros(self.num_links)
        
        # éšæœºç›®æ ‡ä½ç½®
        angle = np.random.uniform(0, 2 * np.pi)
        distance = np.random.uniform(100, 250)
        self.goal_pos = np.array([
            distance * np.cos(angle),
            distance * np.sin(angle)
        ])
        
        # é‡ç½®ç»´æŒçŠ¶æ€
        self.in_maintain_zone = False
        self.maintain_counter = 0
        self.milestone_rewards_given.clear()
        
        self.step_count = 0
        self.episode_count += 1
        
        return self._get_observation(), {}
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        self.step_count += 1
        
        # é™åˆ¶åŠ¨ä½œèŒƒå›´å¹¶åº”ç”¨
        action = np.clip(action, -1.0, 1.0)
        self.joint_velocities = action * 0.1  # é™ä½åŠ¨ä½œå¹…åº¦ï¼Œæé«˜ç¨³å®šæ€§
        self.joint_angles += self.joint_velocities
        
        # é™åˆ¶å…³èŠ‚è§’åº¦
        self.joint_angles = np.clip(self.joint_angles, -np.pi, np.pi)
        
        # è®¡ç®—å¥–åŠ±
        reward = self._compute_improved_reward()
        
        # æ£€æŸ¥episodeç»“æŸæ¡ä»¶
        terminated = self._is_terminated()
        truncated = self.step_count >= 1000  # å¢åŠ æœ€å¤§æ­¥æ•°é™åˆ¶
        
        # æ„å»ºinfo
        info = self._build_info()
        
        return self._get_observation(), reward, terminated, truncated, info
    
    def _compute_improved_reward(self):
        """ğŸ†• æ”¹è¿›çš„å¥–åŠ±å‡½æ•° - è§£å†³ç»´æŒä»»åŠ¡é—®é¢˜"""
        end_pos = self._get_end_effector_position()
        distance = np.linalg.norm(end_pos - self.goal_pos)
        
        total_reward = 0.0
        
        # 1. åŸºç¡€è·ç¦»å¥–åŠ± - æ¸©å’Œçš„å¼•å¯¼ä¿¡å·
        max_distance = 400.0
        distance_reward = -distance / max_distance * 1.0  # é™ä½æƒé‡
        total_reward += distance_reward
        
        # 2. åˆ°è¾¾å¥–åŠ±
        if distance < 50.0:
            reach_reward = 2.0
            total_reward += reach_reward
        
        # 3. ğŸ†• æ”¹è¿›çš„ç»´æŒå¥–åŠ±ç³»ç»Ÿ
        maintain_reward = 0.0
        if distance < self.maintain_threshold:
            if not self.in_maintain_zone:
                # åˆšè¿›å…¥ç»´æŒåŒºåŸŸ
                self.in_maintain_zone = True
                self.maintain_counter = 1
                maintain_reward = 3.0  # è¿›å…¥å¥–åŠ±
                if self.debug_level == 'INFO':
                    print(f"ğŸ¯ è¿›å…¥ç»´æŒåŒºåŸŸ! è·ç¦»: {distance:.1f}px (é˜ˆå€¼: {self.maintain_threshold}px)")
            else:
                # ç»§ç»­åœ¨ç»´æŒåŒºåŸŸ
                self.maintain_counter += 1
                
                # åŸºç¡€ç»´æŒå¥–åŠ± - æ¯æ­¥éƒ½ç»™
                maintain_reward = 1.0
                
                # ğŸ†• é‡Œç¨‹ç¢‘å¥–åŠ± - é˜²æ­¢ç¨€ç–å¥–åŠ±é—®é¢˜
                for i, milestone_step in enumerate(self.milestone_steps):
                    if (self.maintain_counter == milestone_step and 
                        milestone_step not in self.milestone_rewards_given):
                        milestone_bonus = self.milestone_rewards[i]
                        maintain_reward += milestone_bonus
                        self.milestone_rewards_given.add(milestone_step)
                        if self.debug_level == 'INFO':
                            print(f"ğŸ† ç»´æŒé‡Œç¨‹ç¢‘! {milestone_step}æ­¥ (+{milestone_bonus:.1f})")
                
                # ğŸ†• ç¨³å®šæ€§å¥–åŠ± - å¥–åŠ±å°å¹…ç§»åŠ¨
                if hasattr(self, 'prev_distance'):
                    movement = abs(distance - self.prev_distance)
                    if movement < 1.0:  # å¾ˆç¨³å®š
                        maintain_reward += 2.0
                    elif movement < 3.0:  # è¾ƒç¨³å®š
                        maintain_reward += 1.0
                
                # è¿›åº¦æ˜¾ç¤º
                if self.maintain_counter % 25 == 0 and self.debug_level == 'INFO':
                    progress = (self.maintain_counter / self.maintain_target_steps) * 100
                    print(f"ğŸ† ç»´æŒè¿›åº¦: {self.maintain_counter}/{self.maintain_target_steps} "
                          f"({progress:.1f}%) ç»´æŒå¥–åŠ±: +{maintain_reward:.2f}")
        else:
            # ç¦»å¼€ç»´æŒåŒºåŸŸ
            if self.in_maintain_zone:
                # ğŸ†• æ¸©å’Œçš„ç¦»å¼€æƒ©ç½š - ä¸è¦å¤ªä¸¥å‰
                maintain_reward = self.leave_penalty  # ä»-20æ”¹ä¸ºå¯é…ç½®çš„æ¸©å’Œæƒ©ç½š
                
                # è®°å½•è¿™æ¬¡ç»´æŒå°è¯•
                self.maintain_history.append(self.maintain_counter)
                self.max_maintain_streak = max(self.max_maintain_streak, self.maintain_counter)
                
                if self.maintain_counter >= 25 and self.debug_level == 'INFO':
                    print(f"âš ï¸ ç¦»å¼€ç»´æŒåŒºåŸŸ! æœ¬æ¬¡ç»´æŒ: {self.maintain_counter}æ­¥ "
                          f"(æœ€ä½³: {self.max_maintain_streak}æ­¥) æƒ©ç½š: {self.leave_penalty}")
                
                # é‡ç½®ç»´æŒçŠ¶æ€
                self.in_maintain_zone = False
                self.maintain_counter = 0
                self.milestone_rewards_given.clear()
        
        total_reward += maintain_reward
        
        # 4. ğŸ†• æ§åˆ¶å¹³æ»‘æ€§å¥–åŠ± - é¼“åŠ±å°å¹…è°ƒæ•´
        control_penalty = -0.1 * np.sum(np.square(self.joint_velocities))
        total_reward += control_penalty
        
        # ä¿å­˜å½“å‰è·ç¦»ç”¨äºä¸‹ä¸€æ­¥è®¡ç®—
        self.prev_distance = distance
        
        return total_reward
    
    def _is_terminated(self):
        """æ£€æŸ¥episodeæ˜¯å¦ç»“æŸ"""
        # ğŸ†• åªæœ‰è¾¾åˆ°ç»´æŒç›®æ ‡æ‰ç»“æŸepisode
        if self.maintain_counter >= self.maintain_target_steps:
            self.total_success_episodes += 1
            self.consecutive_success_count += 1
            if self.debug_level == 'INFO':
                print(f"ğŸ‰ ç»´æŒä»»åŠ¡å®Œæˆ! è¿ç»­æˆåŠŸ: {self.consecutive_success_count}")
            return True
        
        # è¿ç»­å¤±è´¥å¤ªå¤šæ¬¡ï¼Œæå‰ç»“æŸ
        if len(self.maintain_history) >= 5:
            recent_attempts = list(self.maintain_history)[-5:]
            if all(attempt < self.maintain_target_steps * 0.1 for attempt in recent_attempts):
                if self.debug_level == 'INFO':
                    print(f"âš ï¸ è¿ç»­å¤±è´¥ï¼Œæå‰ç»“æŸepisode")
                self.consecutive_success_count = 0
                return True
        
        return False
    
    def _get_end_effector_position(self):
        """è®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®"""
        x, y = 0.0, 0.0
        angle_sum = 0.0
        
        for i in range(self.num_links):
            angle_sum += self.joint_angles[i]
            x += self.link_lengths[i] * np.cos(angle_sum)
            y += self.link_lengths[i] * np.sin(angle_sum)
        
        return np.array([x, y])
    
    def _get_observation(self):
        """è·å–è§‚å¯Ÿ"""
        obs = []
        
        # å…³èŠ‚è§’åº¦å’Œé€Ÿåº¦
        obs.extend(self.joint_angles)
        obs.extend(self.joint_velocities)
        
        # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®
        end_pos = self._get_end_effector_position()
        obs.extend(end_pos)
        
        # ç›®æ ‡ä½ç½®
        obs.extend(self.goal_pos)
        
        # è·ç¦»
        distance = np.linalg.norm(end_pos - self.goal_pos)
        obs.append(distance)
        
        # ğŸ†• ç»´æŒç›¸å…³çŠ¶æ€ä¿¡æ¯ - å¸®åŠ©SACç†è§£ä»»åŠ¡çŠ¶æ€
        obs.append(float(self.in_maintain_zone))  # æ˜¯å¦åœ¨ç»´æŒåŒºåŸŸ
        obs.append(self.maintain_counter / self.maintain_target_steps)  # ç»´æŒè¿›åº¦
        obs.append(self.max_maintain_streak / self.maintain_target_steps)  # å†å²æœ€ä½³
        
        return np.array(obs, dtype=np.float32)
    
    def _build_info(self):
        """æ„å»ºinfoå­—å…¸"""
        end_pos = self._get_end_effector_position()
        distance = np.linalg.norm(end_pos - self.goal_pos)
        
        return {
            'distance_to_goal': float(distance),
            'goal_reached': distance <= self.maintain_threshold,
            'maintain_completed': self.maintain_counter >= self.maintain_target_steps,
            'maintain_progress': self.maintain_counter / self.maintain_target_steps,
            'curriculum_stage': self.curriculum_stage,
            'success_rate': self.total_success_episodes / max(self.episode_count, 1),
            'consecutive_successes': self.consecutive_success_count
        }
    
    def set_curriculum_stage(self, stage):
        """ğŸ†• è®¾ç½®è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ"""
        if stage != self.curriculum_stage:
            self.curriculum_stage = stage
            self.setup_curriculum_parameters()
            print(f"ğŸ“ è¯¾ç¨‹å‡çº§åˆ°é˜¶æ®µ {stage}: {self.curriculum_description}")
    
    def get_curriculum_progress(self):
        """ğŸ†• è·å–è¯¾ç¨‹å­¦ä¹ è¿›åº¦"""
        return {
            'stage': self.curriculum_stage,
            'success_rate': self.total_success_episodes / max(self.episode_count, 1),
            'consecutive_successes': self.consecutive_success_count,
            'max_maintain_streak': self.max_maintain_streak,
            'ready_for_next_stage': self.consecutive_success_count >= 5
        }

# æ³¨å†Œç¯å¢ƒ
if __name__ == "__main__":
    # æµ‹è¯•ç¯å¢ƒ
    env = ImprovedReacher2DEnv(num_links=5, curriculum_stage=0, debug_level='INFO')
    
    print("ğŸ§ª æµ‹è¯•æ”¹è¿›ç‰ˆReacherç¯å¢ƒ...")
    obs, info = env.reset()
    print(f"è§‚å¯Ÿç©ºé—´ç»´åº¦: {obs.shape}")
    print(f"åŠ¨ä½œç©ºé—´ç»´åº¦: {env.action_space.shape}")
    
    # è¿è¡Œå‡ æ­¥æµ‹è¯•
    for step in range(10):
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        print(f"Step {step}: reward={reward:.3f}, distance={info['distance_to_goal']:.1f}")
        
        if terminated or truncated:
            break
    
    print("âœ… ç¯å¢ƒæµ‹è¯•å®Œæˆ!")
