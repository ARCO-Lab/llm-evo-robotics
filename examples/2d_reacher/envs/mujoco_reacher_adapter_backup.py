#!/usr/bin/env python3
"""
MuJoCo Reacher é€‚é…å™¨
å°† OpenAI MuJoCo Reacher ç¯å¢ƒé€‚é…ä¸ºä¸å½“å‰ Reacher2DEnv å…¼å®¹çš„æ¥å£
"""

import numpy as np
import gymnasium as gym
from gymnasium.spaces import Box
import math
import yaml
import os
from typing import Optional, Tuple, Dict, Any

class MuJoCoReacherAdapter:
    """
    MuJoCo Reacher ç¯å¢ƒé€‚é…å™¨
    æä¾›ä¸ Reacher2DEnv ç›¸åŒçš„æ¥å£ï¼Œä½†ä½¿ç”¨ MuJoCo ç‰©ç†å¼•æ“
    """
    
    def __init__(self, num_links=2, link_lengths=None, render_mode=None, config_path=None, curriculum_stage=1, debug_level='SILENT'):
        """åˆå§‹åŒ–é€‚é…å™¨"""
        
        # åˆ›å»º MuJoCo Reacher ç¯å¢ƒ
        self.mujoco_env = gym.make('Reacher-v5', render_mode=render_mode)
        
        # å…¼å®¹æ€§è®¾ç½®
        self.gym_api_version = "old"  # ä¿æŒä¸åŸç¯å¢ƒå…¼å®¹
        self.render_mode = render_mode
        
        # åŠ è½½é…ç½®ï¼ˆä¿æŒä¸åŸç¯å¢ƒç›¸åŒçš„é…ç½®ç³»ç»Ÿï¼‰
        self.config = self._load_config(config_path)
        
        # ç¯å¢ƒå‚æ•°ï¼ˆä¿æŒä¸åŸç¯å¢ƒå…¼å®¹ï¼‰
        self.num_links = num_links  # æ”¯æŒè‡ªå®šä¹‰å…³èŠ‚æ•°ï¼Œä½† MuJoCo Reacher å›ºå®šä¸º 2
        if self.num_links != 2:
            print(f"âš ï¸ MuJoCo Reacher åªæ”¯æŒ 2 å…³èŠ‚ï¼Œå°†ä½¿ç”¨ 2 å…³èŠ‚è€Œä¸æ˜¯ {num_links}")
            self.num_links = 2
        
        # Link é•¿åº¦è®¾ç½®
        if link_lengths is None:
            self.link_lengths = [0.1, 0.1]  # MuJoCo ä¸­çš„é»˜è®¤ link é•¿åº¦ï¼ˆç±³ï¼‰
        else:
            self.link_lengths = link_lengths[:2]  # åªå–å‰ä¸¤ä¸ªå€¼
        self.max_torque = 100  # ä¿æŒä¸åŸç¯å¢ƒç›¸åŒçš„æ‰­çŸ©èŒƒå›´
        
        # åæ ‡ç³»è½¬æ¢å‚æ•°
        self.scale_factor = 600  # å°† MuJoCo åæ ‡ç¼©æ”¾åˆ°åƒç´ åæ ‡
        self.anchor_point = self.config.get("start", {}).get("position", [480, 620])
        self.goal_pos = np.array(self.config.get("goal", {}).get("position", [600, 550]))
        
        # å®šä¹‰é€‚é…åçš„ç©ºé—´ï¼ˆä¿æŒä¸åŸç¯å¢ƒç›¸åŒï¼‰
        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(12,), dtype=np.float32)
        self.action_space = Box(low=-self.max_torque, high=self.max_torque, shape=(2,), dtype=np.float32)
        
        # æ·»åŠ  spec å±æ€§ä»¥å…¼å®¹å‘é‡åŒ–ç¯å¢ƒ
        self.spec = None
        if hasattr(self.mujoco_env, 'spec'):
            self.spec = self.mujoco_env.spec
        
        # çŠ¶æ€å˜é‡
        self.step_count = 0
        self.collision_count = 0
        self.base_collision_count = 0
        self.self_collision_count = 0
        
        # å¥–åŠ±ç»„ä»¶
        self.current_reward = 0.0
        self.reward_components = {
            'distance_reward': 0.0,
            'reach_reward': 0.0,
            'progress_reward': 0.0,
            'collision_penalty': 0.0,
            'control_penalty': 0.0
        }
        
        # ç»´æŒç³»ç»Ÿï¼ˆä¿æŒä¸åŸç¯å¢ƒå…¼å®¹ï¼‰
        self.maintain_threshold = 150.0
        self.maintain_target_steps = 200
        self.maintain_counter = 0
        self.maintain_bonus_given = False
        self.in_maintain_zone = False
        self.maintain_history = []
        self.max_maintain_streak = 0
        
        # Episode ç®¡ç†
        self.current_episode = 1
        self.episode_ended = False
        
        print(f"ğŸ¯ MuJoCo Reacher é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è§‚å¯Ÿç©ºé—´: {self.observation_space}")
        print(f"   åŠ¨ä½œç©ºé—´: {self.action_space}")
        print(f"   æ¸²æŸ“æ¨¡å¼: {render_mode}")
    
    def _load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä¸åŸç¯å¢ƒç›¸åŒï¼‰"""
        if config_path is None:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            return {
                "start": {"position": [480, 620]},
                "goal": {"position": [600, 550]},
                "obstacles": []
            }
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}: {e}")
            return {
                "start": {"position": [480, 620]},
                "goal": {"position": [600, 550]},
                "obstacles": []
            }
    
    def _mujoco_to_custom_obs(self, mujoco_obs):
        """å°† MuJoCo è§‚å¯Ÿè½¬æ¢ä¸ºè‡ªå®šä¹‰æ ¼å¼"""
        # MuJoCo è§‚å¯Ÿæ ¼å¼: [cos(Î¸1), cos(Î¸2), sin(Î¸1), sin(Î¸2), ç›®æ ‡x, ç›®æ ‡y, Ï‰1, Ï‰2, å‘é‡x, å‘é‡y]
        
        # æå–è§’åº¦ï¼ˆä» cos/sin é‡æ„ï¼‰
        cos_theta1, cos_theta2 = mujoco_obs[0], mujoco_obs[1]
        sin_theta1, sin_theta2 = mujoco_obs[2], mujoco_obs[3]
        
        theta1 = math.atan2(sin_theta1, cos_theta1)
        theta2 = math.atan2(sin_theta2, cos_theta2)
        
        # æå–è§’é€Ÿåº¦
        omega1, omega2 = mujoco_obs[6], mujoco_obs[7]
        
        # è®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ï¼ˆåŸºäº MuJoCo çš„è¿åŠ¨å­¦ï¼‰
        # MuJoCo ä¸­çš„åæ ‡éœ€è¦è½¬æ¢åˆ°æˆ‘ä»¬çš„åƒç´ åæ ‡ç³»
        end_effector_pos = self._calculate_end_effector_from_mujoco(theta1, theta2)
        
        # ç›®æ ‡ä½ç½®ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„ç›®æ ‡ï¼‰
        goal_pos = self.goal_pos
        
        # è®¡ç®—è·ç¦»
        distance = np.linalg.norm(end_effector_pos - goal_pos)
        
        # æ„å»º 12 ç»´è§‚å¯Ÿç©ºé—´
        custom_obs = np.array([
            theta1,                    # å…³èŠ‚è§’åº¦1
            theta2,                    # å…³èŠ‚è§’åº¦2
            omega1,                    # å…³èŠ‚é€Ÿåº¦1
            omega2,                    # å…³èŠ‚é€Ÿåº¦2
            end_effector_pos[0],       # æœ«ç«¯ä½ç½®x
            end_effector_pos[1],       # æœ«ç«¯ä½ç½®y
            goal_pos[0],               # ç›®æ ‡ä½ç½®x
            goal_pos[1],               # ç›®æ ‡ä½ç½®y
            distance,                  # è·ç¦»
            0.0,                       # ç¢°æ’çŠ¶æ€ï¼ˆMuJoCo ç¯å¢ƒä¸­æš‚æ—¶è®¾ä¸º0ï¼‰
            float(self.collision_count),      # ç¢°æ’è®¡æ•°
            float(self.base_collision_count)  # åŸºåº§ç¢°æ’è®¡æ•°
        ], dtype=np.float32)
        
        return custom_obs
    
    def _calculate_end_effector_from_mujoco(self, theta1, theta2):
        """æ ¹æ®å…³èŠ‚è§’åº¦è®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®"""
        # å°† MuJoCo çš„è¿åŠ¨å­¦è½¬æ¢ä¸ºæˆ‘ä»¬çš„åæ ‡ç³»
        # MuJoCo ä¸­ link é•¿åº¦é€šå¸¸æ˜¯ 0.1 ç±³
        link1_length = self.link_lengths[0] * self.scale_factor  # è½¬æ¢ä¸ºåƒç´ 
        link2_length = self.link_lengths[1] * self.scale_factor
        
        # è®¡ç®—ç¬¬ä¸€ä¸ª link çš„æœ«ç«¯ä½ç½®
        x1 = self.anchor_point[0] + link1_length * np.cos(theta1)
        y1 = self.anchor_point[1] + link1_length * np.sin(theta1)
        
        # è®¡ç®—ç¬¬äºŒä¸ª link çš„æœ«ç«¯ä½ç½®ï¼ˆæœ«ç«¯æ‰§è¡Œå™¨ï¼‰
        x2 = x1 + link2_length * np.cos(theta1 + theta2)
        y2 = y1 + link2_length * np.sin(theta1 + theta2)
        
        return np.array([x2, y2])
    
    def _custom_to_mujoco_action(self, custom_action):
        """å°†è‡ªå®šä¹‰åŠ¨ä½œè½¬æ¢ä¸º MuJoCo åŠ¨ä½œ"""
        # è‡ªå®šä¹‰åŠ¨ä½œèŒƒå›´: [-100, 100]
        # MuJoCo åŠ¨ä½œèŒƒå›´: [-1, 1]
        
        # ç®€å•çš„çº¿æ€§æ˜ å°„
        mujoco_action = np.clip(custom_action / self.max_torque, -1.0, 1.0)
        return mujoco_action.astype(np.float32)
    
    def _compute_custom_reward(self, mujoco_obs, mujoco_reward, custom_obs):
        """è®¡ç®—è‡ªå®šä¹‰å¥–åŠ±ï¼ˆä¿æŒä¸åŸç¯å¢ƒç›¸åŒçš„å¥–åŠ±ç»“æ„ï¼‰"""
        end_pos = custom_obs[4:6]  # æœ«ç«¯ä½ç½®
        goal_pos = custom_obs[6:8]  # ç›®æ ‡ä½ç½®
        distance = custom_obs[8]   # è·ç¦»
        
        # ğŸ¯ ä¸»è¦å¥–åŠ±ï¼šè·ç¦»å¥–åŠ±
        distance_reward = -distance / 50.0
        
        # ğŸ¯ åˆ°è¾¾å¥–åŠ±
        reach_reward = 0.0
        if distance < 100.0:  # 100pxå†…ç»™äºˆåˆ°è¾¾å¥–åŠ±
            reach_reward = 50.0 - distance  # è¶Šè¿‘å¥–åŠ±è¶Šå¤§
        
        # ğŸ¯ è¿›æ­¥å¥–åŠ±ï¼ˆé¼“åŠ±æœç›®æ ‡ç§»åŠ¨ï¼‰
        progress_reward = 0.0
        if hasattr(self, 'prev_distance'):
            if distance < self.prev_distance:
                progress_reward = (self.prev_distance - distance) * 0.1
        self.prev_distance = distance
        
        # ğŸ¯ æ§åˆ¶æƒ©ç½šï¼ˆä» MuJoCo å¥–åŠ±ä¸­æå–ï¼‰
        control_penalty = mujoco_reward - (-distance / 50.0)  # ä¼°ç®—æ§åˆ¶æƒ©ç½š
        
        # ğŸ¯ ç¢°æ’æƒ©ç½šï¼ˆæš‚æ—¶ä¸º0ï¼Œå› ä¸ºæ ‡å‡† MuJoCo Reacher æ²¡æœ‰éšœç¢ç‰©ï¼‰
        collision_penalty = 0.0
        
        # è®¡ç®—æ€»å¥–åŠ±
        total_reward = distance_reward + reach_reward + progress_reward + control_penalty + collision_penalty
        
        # å­˜å‚¨å¥–åŠ±ç»„æˆéƒ¨åˆ†
        self.reward_components = {
            'distance_reward': distance_reward,
            'reach_reward': reach_reward,
            'progress_reward': progress_reward,
            'control_penalty': control_penalty,
            'collision_penalty': collision_penalty
        }
        
        self.current_reward = total_reward
        return total_reward
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        # é‡ç½® MuJoCo ç¯å¢ƒ
        mujoco_obs, mujoco_info = self.mujoco_env.reset(seed=seed)
        
        # é‡ç½®é€‚é…å™¨çŠ¶æ€
        self.step_count = 0
        self.collision_count = 0
        self.base_collision_count = 0
        self.self_collision_count = 0
        
        # é‡ç½®ç»´æŒçŠ¶æ€
        self.maintain_counter = 0
        self.maintain_bonus_given = False
        self.in_maintain_zone = False
        
        # Episode ç®¡ç†
        if not hasattr(self, 'current_episode'):
            self.current_episode = 1
            self.episode_ended = False
        elif hasattr(self, 'episode_ended') and self.episode_ended:
            self.current_episode += 1
            self.episode_ended = False
        
        # è½¬æ¢è§‚å¯Ÿ
        custom_obs = self._mujoco_to_custom_obs(mujoco_obs)
        
        print(f"ğŸ”„ MuJoCo é€‚é…å™¨é‡ç½®å®Œæˆ - Episode {self.current_episode}")
        
        if self.gym_api_version == "new":
            info = self._get_info()
            return custom_obs, info
        else:
            return custom_obs
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥"""
        # è½¬æ¢åŠ¨ä½œ
        mujoco_action = self._custom_to_mujoco_action(action)
        
        # åœ¨ MuJoCo ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ
        mujoco_obs, mujoco_reward, mujoco_terminated, mujoco_truncated, mujoco_info = self.mujoco_env.step(mujoco_action)
        
        # è½¬æ¢è§‚å¯Ÿ
        custom_obs = self._mujoco_to_custom_obs(mujoco_obs)
        
        # è®¡ç®—è‡ªå®šä¹‰å¥–åŠ±
        custom_reward = self._compute_custom_reward(mujoco_obs, mujoco_reward, custom_obs)
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ï¼ˆä½¿ç”¨è‡ªå®šä¹‰é€»è¾‘ï¼‰
        custom_done = self._is_done(custom_obs)
        
        self.step_count += 1
        
        # è·å–ä¿¡æ¯
        info = self._get_info()
        
        if self.gym_api_version == "new":
            terminated = custom_done
            truncated = self.step_count >= 120000
            return custom_obs, custom_reward, terminated, truncated, info
        else:
            return custom_obs, custom_reward, custom_done, info
    
    def _is_done(self, obs):
        """æ£€æŸ¥æ˜¯å¦å®Œæˆï¼ˆä½¿ç”¨ä¸åŸç¯å¢ƒç›¸åŒçš„é€»è¾‘ï¼‰"""
        distance = obs[8]  # è·ç¦»åœ¨è§‚å¯Ÿçš„ç¬¬9ä¸ªä½ç½®
        
        # ğŸ¯ 1. åˆ°è¾¾ç›®æ ‡å°±è¿›å…¥ä¸‹ä¸€ä¸ªepisode
        if distance < 50.0:  # 50pxå†…ç®—åˆ°è¾¾ç›®æ ‡
            print(f"ğŸ¯ åˆ°è¾¾ç›®æ ‡! è·ç¦»: {distance:.1f}pxï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªepisode")
            self.episode_ended = True
            return True
        
        # ğŸ¯ 2. æ­¥æ•°é™åˆ¶ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
        if self.step_count >= 2000:  # æ¯ä¸ªepisodeæœ€å¤š2000æ­¥
            print(f"â° Episodeæ­¥æ•°é™åˆ¶: {self.step_count}æ­¥ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªepisode")
            self.episode_ended = True
            return True
        
        return False
    
    def _get_info(self):
        """è·å–é¢å¤–ä¿¡æ¯ï¼ˆä¿æŒä¸åŸç¯å¢ƒç›¸åŒçš„æ ¼å¼ï¼‰"""
        # ä»å½“å‰è§‚å¯Ÿä¸­æå–ä¿¡æ¯
        if hasattr(self, '_last_obs') and self._last_obs is not None:
            end_pos = self._last_obs[4:6]
            goal_pos = self._last_obs[6:8]
            distance = self._last_obs[8]
        else:
            # å¦‚æœæ²¡æœ‰è§‚å¯Ÿï¼Œä½¿ç”¨é»˜è®¤å€¼
            end_pos = np.array([0.0, 0.0])
            goal_pos = self.goal_pos
            distance = np.linalg.norm(end_pos - goal_pos)
        
        return {
            'end_effector_pos': end_pos,
            'goal_pos': goal_pos,
            'distance': float(distance),
            'collision_count': self.collision_count,
            'base_collision_count': self.base_collision_count,
            'step_count': self.step_count,
            'goal': {
                'distance_to_goal': float(distance),
                'goal_reached': distance < 20.0,
                'end_effector_position': end_pos,
                'goal_position': goal_pos,
            },
            'maintain': {
                'in_maintain_zone': self.in_maintain_zone,
                'maintain_counter': self.maintain_counter,
                'maintain_target': self.maintain_target_steps,
                'maintain_progress': self.maintain_counter / self.maintain_target_steps if self.maintain_target_steps > 0 else 0.0,
                'max_maintain_streak': self.max_maintain_streak,
                'maintain_completed': self.maintain_counter >= self.maintain_target_steps
            }
        }
    
    def render(self, mode='human'):
        """æ¸²æŸ“ç¯å¢ƒï¼ˆå§”æ‰˜ç»™ MuJoCo ç¯å¢ƒï¼‰"""
        return self.mujoco_env.render()
    
    def seed(self, seed=None):
        """è®¾ç½®éšæœºç§å­"""
        if hasattr(self.mujoco_env, 'seed'):
            return self.mujoco_env.seed(seed)
        elif hasattr(self.mujoco_env, 'reset'):
            # Gymnasium ç¯å¢ƒä½¿ç”¨ reset(seed=seed)
            return [seed]
        return [seed]
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if hasattr(self, 'mujoco_env'):
            self.mujoco_env.close()

# ä¸ºäº†å…¼å®¹æ€§ï¼Œæä¾›ä¸€ä¸ªåˆ«å
MuJoCoReacher2DEnv = MuJoCoReacherAdapter
