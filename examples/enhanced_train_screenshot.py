#!/usr/bin/env python3
"""
æˆªå›¾ç‰ˆæœ¬çš„enhanced_train.py
ä¸“é—¨ç”¨äºæˆªå–å‰5ä¸ªè®­ç»ƒæ­¥éª¤çš„æˆªå›¾
"""

import sys
import os
import numpy as np
import time
import torch

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..")
sys.path.append(base_dir)
sys.path.insert(0, os.path.join(base_dir, 'examples/2d_reacher/envs'))
sys.path.insert(0, os.path.join(base_dir, 'examples/surrogate_model/env_config'))

from reacher2d_env import Reacher2DEnv
from env_wrapper import make_reacher2d_vec_envs

def enhanced_train_screenshot():
    """æ¨¡æ‹Ÿenhanced_train.pyçš„å‰5æ­¥å¹¶æˆªå›¾"""
    print("=" * 60)
    print("ğŸ–¼ï¸ Enhanced Train å‰5æ­¥æˆªå›¾æ¨¡å¼")
    print("=" * 60)
    
    # ç¯å¢ƒå‚æ•° - ä¸enhanced_train.pyç›¸åŒ
    env_params = {
        'num_links': 4,
        'link_lengths': [80, 80, 80, 60],
        'render_mode': 'human',
        'config_path': "/home/xli149/Documents/repos/test_robo/examples/2d_reacher/configs/reacher_with_zigzag_obstacles.yaml"
    }
    
    print("ğŸš€ åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ...")
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ - å•è¿›ç¨‹ä¾¿äºæ§åˆ¶
    envs = make_reacher2d_vec_envs(
        env_params=env_params,
        seed=42,
        num_processes=1,
        gamma=0.99,
        log_dir=None,
        device=torch.device('cpu'),
        allow_early_resets=False
    )
    
    # åˆ›å»ºåŒæ­¥æ¸²æŸ“ç¯å¢ƒ - å¯ç”¨æˆªå›¾æ¨¡å¼
    sync_env = Reacher2DEnv(**env_params)
    sync_env.screenshot_mode = True
    sync_env.screenshot_dir = 'screenshots/enhanced_train'
    
    print("âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    
    # åˆå§‹é‡ç½®
    print("\nğŸ”„ é‡ç½®ç¯å¢ƒ")
    current_obs = envs.reset()
    sync_env.reset()
    
    print(f"ğŸ“Š åˆå§‹è§‚å¯Ÿå½¢çŠ¶: {current_obs.shape}")
    print(f"ğŸ“ åˆå§‹è§’åº¦ (ä»è§‚å¯Ÿ): {current_obs[0][0]:.4f} å¼§åº¦ = {np.degrees(current_obs[0][0].item()):.2f}Â°")
    
    # æ¸²æŸ“åˆå§‹çŠ¶æ€ï¼ˆstep 0ï¼‰
    print(f"\nğŸ“¸ æ¸²æŸ“åˆå§‹çŠ¶æ€ (Step 0)")
    sync_env.render()
    time.sleep(0.5)
    
    # æ¨¡æ‹Ÿå‰5ä¸ªè®­ç»ƒæ­¥éª¤
    for step in range(1, 6):
        print(f"\nğŸ“¸ è®­ç»ƒæ­¥éª¤ {step}")
        
        # ç”ŸæˆéšæœºåŠ¨ä½œ - æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        action_batch = torch.from_numpy(np.array([envs.action_space.sample() for _ in range(1)]))
        
        print(f"   åŠ¨ä½œ: [{', '.join([f'{a:.3f}' for a in action_batch[0].numpy()])}]")
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_obs, reward, done, infos = envs.step(action_batch)
        
        # åŒæ­¥æ¸²æŸ“ç¯å¢ƒ
        sync_action = action_batch[0].cpu().numpy() if hasattr(action_batch, 'cpu') else action_batch[0]
        sync_obs, sync_reward, sync_done, sync_info = sync_env.step(sync_action)
        
        print(f"   å¥–åŠ±: {reward[0].item():.2f}")
        print(f"   ç»“æŸ: {done[0].item()}")
        
        if 'end_effector_pos' in sync_info:
            end_pos = sync_info['end_effector_pos']
            print(f"   æœ«ç«¯ä½ç½®: [{end_pos[0]:.1f}, {end_pos[1]:.1f}]")
        
        # æ¸²æŸ“å¹¶è‡ªåŠ¨æˆªå›¾
        sync_env.render()
        time.sleep(0.5)
        
        # æ›´æ–°è§‚å¯Ÿ
        current_obs = next_obs.clone()
        
        if done[0].item():
            print(f"   Episodeåœ¨æ­¥éª¤{step}ç»“æŸ")
            break
    
    print(f"\nâœ… æˆªå›¾å®Œæˆï¼Œä¿å­˜åœ¨: {sync_env.screenshot_dir}")
    
    # ä¿æŒçª—å£æ‰“å¼€ä¸€ä¼šå„¿
    print("ğŸ–¼ï¸ ä¿æŒçª—å£æ‰“å¼€3ç§’...")
    time.sleep(3)
    
    # æ¸…ç†
    envs.close()
    sync_env.close()
    print("ğŸ”’ ç¯å¢ƒå·²å…³é—­")

if __name__ == "__main__":
    enhanced_train_screenshot()
