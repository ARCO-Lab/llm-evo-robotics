#!/usr/bin/env python3
"""
é€šç”¨PPOè®­ç»ƒè„šæœ¬ - æ”¯æŒä»»æ„å…³èŠ‚æ•°çš„æœºå™¨äººæ§åˆ¶
å¯ä»¥åœ¨å•æ¬¡è®­ç»ƒä¸­å¤„ç†2-6å…³èŠ‚çš„æœºå™¨äººï¼Œè®­ç»ƒå‡ºé€šç”¨æ¨¡å‹
"""

import torch
import numpy as np
import os
import sys
import argparse
import random
from collections import deque
import time

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(__file__), "../../")
sys.path.append(base_dir)
sys.path.append(os.path.join(base_dir, "examples/2d_reacher/envs"))
sys.path.append(os.path.join(base_dir, "examples/2d_reacher/utils"))

from reacher2d_env import Reacher2DEnv
from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
from sac.universal_ppo_model import UniversalPPOWithBuffer


class UniversalRobotTrainer:
    """é€šç”¨æœºå™¨äººè®­ç»ƒå™¨ - æ”¯æŒå¤šç§å…³èŠ‚æ•°æ··åˆè®­ç»ƒ"""
    
    def __init__(self, joint_configs, device='cpu'):
        """
        Args:
            joint_configs: List[dict] - ä¸åŒå…³èŠ‚æ•°çš„é…ç½®
                ä¾‹: [{'num_joints': 3, 'link_lengths': [60, 60, 60]},
                     {'num_joints': 4, 'link_lengths': [50, 50, 50, 50]}]
        """
        self.joint_configs = joint_configs
        self.device = device
        self.envs = {}
        self.gnn_encoders = {}
        self.gnn_embeds = {}
        
        print(f"ğŸ¤– åˆå§‹åŒ–é€šç”¨æœºå™¨äººè®­ç»ƒå™¨")
        print(f"   æ”¯æŒå…³èŠ‚æ•°é…ç½®: {[cfg['num_joints'] for cfg in joint_configs]}")
        
        # ä¸ºæ¯ç§å…³èŠ‚æ•°é…ç½®åˆ›å»ºç¯å¢ƒå’Œç¼–ç å™¨
        for cfg in joint_configs:
            num_joints = cfg['num_joints']
            link_lengths = cfg['link_lengths']
            
            # åˆ›å»ºç¯å¢ƒ
            env = Reacher2DEnv(
                num_links=num_joints,
                link_lengths=link_lengths,
                render_mode=None,
                config_path="../2d_reacher/configs/reacher_with_zigzag_obstacles.yaml"
            )
            self.envs[num_joints] = env
            
            # åˆ›å»ºGNNç¼–ç å™¨
            encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
            gnn_embed = encoder.get_gnn_embeds(num_links=num_joints, link_lengths=link_lengths)
            self.gnn_encoders[num_joints] = encoder
            self.gnn_embeds[num_joints] = gnn_embed
            
            print(f"   âœ… {num_joints}å…³èŠ‚é…ç½®åˆå§‹åŒ–å®Œæˆï¼ŒGNNåµŒå…¥å½¢çŠ¶: {gnn_embed.shape}")
        
        # åˆ›å»ºé€šç”¨PPOæ¨¡å‹
        self.ppo = UniversalPPOWithBuffer(
            buffer_size=2048,
            batch_size=64,
            lr=1e-4,
            device=device,
            env_type='reacher2d'
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = {num_joints: deque(maxlen=100) for num_joints in self.envs.keys()}
        self.success_rates = {num_joints: deque(maxlen=100) for num_joints in self.envs.keys()}
        
    def sample_robot_config(self):
        """éšæœºé‡‡æ ·ä¸€ä¸ªæœºå™¨äººé…ç½®"""
        return random.choice(self.joint_configs)
    
    def train_episode(self, robot_config, max_steps=200):
        """è®­ç»ƒä¸€ä¸ªå›åˆ"""
        num_joints = robot_config['num_joints']
        env = self.envs[num_joints]
        gnn_embed = self.gnn_embeds[num_joints]
        
        obs, info = env.reset()
        episode_reward = 0
        episode_steps = 0
        success = False
        
        for step in range(max_steps):
            # è·å–åŠ¨ä½œ
            action, log_prob, value = self.ppo.get_action(
                torch.tensor(obs, dtype=torch.float32).to(self.device),
                gnn_embed.to(self.device),
                num_joints,
                deterministic=False
            )
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done, truncated, info = env.step(action.cpu().numpy())
            
            # å­˜å‚¨ç»éªŒ
            self.ppo.store_experience(
                torch.tensor(obs, dtype=torch.float32),
                gnn_embed,
                action,
                reward,
                done or truncated,
                log_prob,
                value,
                num_joints
            )
            
            obs = next_obs
            episode_reward += reward
            episode_steps += 1
            
            if info.get('success', False):
                success = True
            
            if done or truncated:
                break
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.episode_rewards[num_joints].append(episode_reward)
        self.success_rates[num_joints].append(1.0 if success else 0.0)
        
        return episode_reward, episode_steps, success
    
    def train(self, total_episodes=10000, update_frequency=50, save_frequency=1000):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹é€šç”¨PPOè®­ç»ƒ")
        print(f"   æ€»å›åˆæ•°: {total_episodes}")
        print(f"   æ›´æ–°é¢‘ç‡: æ¯{update_frequency}å›åˆ")
        print(f"   ä¿å­˜é¢‘ç‡: æ¯{save_frequency}å›åˆ")
        
        best_avg_reward = -float('inf')
        episode_count = 0
        
        for episode in range(total_episodes):
            # éšæœºé€‰æ‹©æœºå™¨äººé…ç½®
            robot_config = self.sample_robot_config()
            num_joints = robot_config['num_joints']
            
            # è®­ç»ƒä¸€ä¸ªå›åˆ
            episode_reward, episode_steps, success = self.train_episode(robot_config)
            episode_count += 1
            
            # å®šæœŸæ›´æ–°æ¨¡å‹
            if episode_count % update_frequency == 0:
                # ä½¿ç”¨æœ€åä¸€ä¸ªç¯å¢ƒçš„çŠ¶æ€ä½œä¸ºnext_state
                last_config = robot_config
                last_env = self.envs[last_config['num_joints']]
                last_gnn_embed = self.gnn_embeds[last_config['num_joints']]
                
                # è·å–æœ€åçŠ¶æ€
                try:
                    last_obs = last_env.get_observation()
                    metrics = self.ppo.update(
                        torch.tensor(last_obs, dtype=torch.float32),
                        last_gnn_embed,
                        last_config['num_joints'],
                        ppo_epochs=4
                    )
                except:
                    # å¦‚æœè·å–ä¸åˆ°æœ€åçŠ¶æ€ï¼Œä½¿ç”¨None
                    metrics = self.ppo.update(ppo_epochs=4)
                
                if metrics:
                    print(f"\nğŸ“Š Episode {episode+1}/{total_episodes} - æ¨¡å‹æ›´æ–°:")
                    print(f"   Actor Loss: {metrics['actor_loss']:.4f}")
                    print(f"   Critic Loss: {metrics['critic_loss']:.4f}")
                    print(f"   Entropy: {metrics['entropy']:.4f}")
                    print(f"   å¤„ç†æ‰¹æ¬¡: {metrics.get('batches_processed', 'N/A')}")
            
            # å®šæœŸæ‰“å°ç»Ÿè®¡ä¿¡æ¯
            if (episode + 1) % 100 == 0:
                print(f"\nğŸ“ˆ Episode {episode+1} ç»Ÿè®¡:")
                
                for joints in self.envs.keys():
                    if len(self.episode_rewards[joints]) > 0:
                        avg_reward = np.mean(self.episode_rewards[joints])
                        avg_success = np.mean(self.success_rates[joints])
                        print(f"   {joints}å…³èŠ‚: å¹³å‡å¥–åŠ±={avg_reward:.2f}, æˆåŠŸç‡={avg_success:.2%}")
                
                # è®¡ç®—æ€»ä½“å¹³å‡å¥–åŠ±
                all_rewards = []
                for rewards in self.episode_rewards.values():
                    all_rewards.extend(rewards)
                
                if all_rewards:
                    current_avg_reward = np.mean(all_rewards)
                    print(f"   ğŸ¯ æ€»ä½“å¹³å‡å¥–åŠ±: {current_avg_reward:.2f}")
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if current_avg_reward > best_avg_reward:
                        best_avg_reward = current_avg_reward
                        self.save_model(f"universal_ppo_best.pth")
                        print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (å¥–åŠ±: {best_avg_reward:.2f})")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (episode + 1) % save_frequency == 0:
                self.save_model(f"universal_ppo_episode_{episode+1}.pth")
                print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: episode_{episode+1}")
        
        print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³å¹³å‡å¥–åŠ±: {best_avg_reward:.2f}")
        self.save_model("universal_ppo_final.pth")
    
    def save_model(self, filename):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs("trained_models/universal_ppo", exist_ok=True)
        filepath = os.path.join("trained_models/universal_ppo", filename)
        self.ppo.save_model(filepath)
    
    def test_all_configurations(self, num_episodes=10):
        """æµ‹è¯•æ‰€æœ‰é…ç½®çš„æ€§èƒ½"""
        print(f"\nğŸ§ª æµ‹è¯•æ‰€æœ‰é…ç½®çš„æ€§èƒ½ ({num_episodes}å›åˆ/é…ç½®)")
        
        for robot_config in self.joint_configs:
            num_joints = robot_config['num_joints']
            env = self.envs[num_joints]
            gnn_embed = self.gnn_embeds[num_joints]
            
            rewards = []
            successes = []
            
            for episode in range(num_episodes):
                obs, info = env.reset()
                episode_reward = 0
                success = False
                
                for step in range(200):
                    action, _, _ = self.ppo.get_action(
                        torch.tensor(obs, dtype=torch.float32).to(self.device),
                        gnn_embed.to(self.device),
                        num_joints,
                        deterministic=True  # æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
                    )
                    
                    obs, reward, done, truncated, info = env.step(action.cpu().numpy())
                    episode_reward += reward
                    
                    if info.get('success', False):
                        success = True
                    
                    if done or truncated:
                        break
                
                rewards.append(episode_reward)
                successes.append(success)
            
            avg_reward = np.mean(rewards)
            success_rate = np.mean(successes)
            
            print(f"   {num_joints}å…³èŠ‚: å¹³å‡å¥–åŠ±={avg_reward:.2f}, æˆåŠŸç‡={success_rate:.2%}")


def main():
    parser = argparse.ArgumentParser(description='é€šç”¨PPOè®­ç»ƒ')
    parser.add_argument('--total_episodes', type=int, default=10000, help='æ€»è®­ç»ƒå›åˆæ•°')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu', help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--test_only', action='store_true', help='ä»…æµ‹è¯•æ¨¡å¼')
    parser.add_argument('--model_path', type=str, help='æ¨¡å‹è·¯å¾„ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰')
    
    args = parser.parse_args()
    
    # å®šä¹‰ä¸åŒçš„æœºå™¨äººé…ç½®
    joint_configs = [
        {'num_joints': 3, 'link_lengths': [60, 60, 60]},
        {'num_joints': 4, 'link_lengths': [50, 50, 50, 50]}, 
        {'num_joints': 5, 'link_lengths': [40, 40, 40, 40, 40]},
        {'num_joints': 6, 'link_lengths': [35, 35, 35, 35, 35, 35]}
    ]
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = UniversalRobotTrainer(joint_configs, device=args.device)
    
    if args.test_only:
        if args.model_path:
            trainer.ppo.load_model(args.model_path)
            print(f"âœ… åŠ è½½æ¨¡å‹: {args.model_path}")
        trainer.test_all_configurations()
    else:
        # å¼€å§‹è®­ç»ƒ
        trainer.train(total_episodes=args.total_episodes)
        
        # è®­ç»ƒå®Œæˆåæµ‹è¯•
        print(f"\nğŸ§ª è®­ç»ƒå®Œæˆï¼Œæµ‹è¯•æœ€ç»ˆæ¨¡å‹æ€§èƒ½...")
        trainer.test_all_configurations()


if __name__ == "__main__":
    main()
