#!/usr/bin/env python3
"""
æµ‹è¯•æ”¹è¿›ç‰ˆSACç»´æŒä»»åŠ¡æ€§èƒ½
éªŒè¯ç†µæƒé‡è°ƒåº¦ã€è¯¾ç¨‹å­¦ä¹ å’Œæ”¹è¿›å¥–åŠ±çš„æ•ˆæœ
"""

import sys
import os
import numpy as np
import torch
import time
from datetime import datetime

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.')
sys.path.extend([
    base_dir,
    os.path.join(base_dir, 'sac'),
    os.path.join(base_dir, 'attn_model'),
    os.path.join(base_dir, '../2d_reacher/envs')
])

# å¯¼å…¥æ¨¡å—
from sac.sac_model import AttentionSACWithBuffer
from attn_model import AttnModel
from reacher2d_env_improved import ImprovedReacher2DEnv

class ImprovedSACTester:
    """æ”¹è¿›ç‰ˆSACæµ‹è¯•å™¨"""
    
    def __init__(self, num_joints=5, curriculum_stage=0):
        self.num_joints = num_joints
        self.curriculum_stage = curriculum_stage
        
        # åˆ›å»ºæ”¹è¿›çš„ç¯å¢ƒ
        self.env = ImprovedReacher2DEnv(
            num_links=num_joints,
            link_lengths=[90.0] * num_joints,
            curriculum_stage=curriculum_stage,
            debug_level='INFO'
        )
        
        # åˆ›å»ºSACæ¨¡å‹
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºattentionæ¨¡å‹
        attn_model = AttnModel(128, 130, 130, 4)
        
        # åˆ›å»ºSACæ¨¡å‹ - å¯ç”¨ç†µæƒé‡è°ƒåº¦
        self.sac = AttentionSACWithBuffer(
            attn_model=attn_model,
            action_dim=num_joints,
            joint_embed_dim=128,
            buffer_capacity=5000,
            batch_size=64,  # è¾ƒå°çš„batch sizeç”¨äºå¿«é€Ÿæµ‹è¯•
            lr=1e-4,
            gamma=0.99,
            tau=0.005,
            alpha=0.2,  # åˆå§‹alphaå€¼ï¼Œä¼šè‡ªåŠ¨è°ƒåº¦
            device=self.device,
            env_type='reacher2d'
        )
        
        # ç¡®ä¿ç†µæƒé‡è°ƒåº¦å¯ç”¨
        self.sac.entropy_schedule_enabled = True
        print(f"ğŸ”„ ç†µæƒé‡è°ƒåº¦: å¯ç”¨ ({self.sac.alpha_start:.3f} â†’ {self.sac.alpha_end:.3f})")
        
        self.stats = {
            'episodes': 0,
            'total_steps': 0,
            'successful_episodes': 0,
            'maintain_times': [],
            'alpha_history': [],
            'reward_history': [],
            'curriculum_upgrades': 0
        }
    
    def run_test(self, total_steps=2000, max_episodes=50):
        """è¿è¡Œæ”¹è¿›ç‰ˆSACæµ‹è¯•"""
        print(f"\nğŸš€ å¼€å§‹æ”¹è¿›ç‰ˆSACç»´æŒä»»åŠ¡æµ‹è¯•")
        print(f"ğŸ“Š å‚æ•°: æ€»æ­¥æ•°={total_steps}, æœ€å¤§episodes={max_episodes}")
        print(f"ğŸ“ åˆå§‹è¯¾ç¨‹é˜¶æ®µ: {self.curriculum_stage}")
        print("="*60)
        
        obs, info = self.env.reset()
        episode_reward = 0
        episode_steps = 0
        step = 0
        
        start_time = time.time()
        
        while step < total_steps and self.stats['episodes'] < max_episodes:
            # è·å–åŠ¨ä½œ
            if step < 200:  # åˆå§‹éšæœºæ¢ç´¢
                action = np.random.uniform(-0.5, 0.5, self.num_joints)
            else:
                # SACç­–ç•¥ï¼Œæ ¹æ®é˜¶æ®µè°ƒæ•´ç¡®å®šæ€§
                is_stable_phase = step > total_steps * self.sac.exploration_phase_ratio
                obs_tensor = torch.FloatTensor(obs).to(self.device)
                gnn_embeds_tensor = torch.zeros(1, 128).to(self.device)
                action = self.sac.get_action(
                    obs_tensor, gnn_embeds_tensor, self.num_joints, 
                    deterministic=is_stable_phase
                )[0].cpu().numpy()
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, terminated, truncated, info = self.env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            if step >= 200:
                self.sac.store_experience(
                    state=obs,
                    gnn_embeds=np.zeros(128),
                    action=action,
                    reward=reward,
                    next_state=next_obs,
                    next_gnn_embeds=np.zeros(128),
                    done=terminated or truncated,
                    num_joints=self.num_joints
                )
            
            # æ›´æ–°ç½‘ç»œ
            if step >= 500 and step % 4 == 0 and self.sac.memory.can_sample(self.sac.batch_size):
                loss_info = self.sac.update()
                
                # æ›´æ–°ç†µæƒé‡è°ƒåº¦
                if step % 50 == 0:  # æ›´é¢‘ç¹çš„è°ƒåº¦æ›´æ–°
                    self.sac.update_alpha_schedule(step, total_steps)
                    self.stats['alpha_history'].append(self.sac.alpha)
            
            episode_reward += reward
            episode_steps += 1
            step += 1
            
            # Episodeç»“æŸå¤„ç†
            if terminated or truncated or episode_steps >= 500:
                self.stats['episodes'] += 1
                self.stats['total_steps'] += episode_steps
                self.stats['reward_history'].append(episode_reward)
                
                # è®°å½•ç»´æŒæ—¶é—´
                if 'maintain_progress' in info:
                    maintain_time = int(info['maintain_progress'] * self.env.maintain_target_steps)
                    self.stats['maintain_times'].append(maintain_time)
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                if info.get('maintain_completed', False):
                    self.stats['successful_episodes'] += 1
                    print(f"ğŸ‰ Episode {self.stats['episodes']}: ç»´æŒä»»åŠ¡æˆåŠŸ! "
                          f"å¥–åŠ±: {episode_reward:.1f}, æ­¥æ•°: {episode_steps}")
                else:
                    max_maintain = max(self.stats['maintain_times'][-10:]) if self.stats['maintain_times'] else 0
                    print(f"ğŸ“Š Episode {self.stats['episodes']}: æœªå®Œæˆç»´æŒ "
                          f"(æœ€å¤§: {max_maintain}æ­¥/{self.env.maintain_target_steps}æ­¥) "
                          f"å¥–åŠ±: {episode_reward:.1f}")
                
                # ğŸ†• è¯¾ç¨‹å­¦ä¹ å‡çº§æ£€æŸ¥
                if self.stats['episodes'] % 10 == 0:
                    self.check_curriculum_upgrade()
                
                # é‡ç½®episode
                obs, info = self.env.reset()
                episode_reward = 0
                episode_steps = 0
            else:
                obs = next_obs
        
        # æµ‹è¯•ç»“æŸï¼Œè¾“å‡ºç»Ÿè®¡
        self.print_final_stats(time.time() - start_time)
    
    def check_curriculum_upgrade(self):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å‡çº§è¯¾ç¨‹"""
        if self.stats['episodes'] < 10:
            return
            
        # æ£€æŸ¥æœ€è¿‘10ä¸ªepisodeçš„æˆåŠŸç‡
        recent_episodes = min(10, len(self.stats['maintain_times']))
        if recent_episodes < 5:
            return
        
        recent_maintains = self.stats['maintain_times'][-recent_episodes:]
        recent_success_rate = sum(1 for m in recent_maintains 
                                 if m >= self.env.maintain_target_steps) / recent_episodes
        
        # å¦‚æœæˆåŠŸç‡ >= 60% ä¸”å½“å‰ä¸æ˜¯æœ€é«˜é˜¶æ®µï¼Œåˆ™å‡çº§
        if recent_success_rate >= 0.6 and self.curriculum_stage < 2:
            old_stage = self.curriculum_stage
            self.curriculum_stage += 1
            self.env.set_curriculum_stage(self.curriculum_stage)
            self.stats['curriculum_upgrades'] += 1
            
            print(f"\nğŸ“ è¯¾ç¨‹å‡çº§! {old_stage} â†’ {self.curriculum_stage}")
            print(f"   æœ€è¿‘æˆåŠŸç‡: {recent_success_rate:.1%}")
            print(f"   æ–°è¦æ±‚: {self.env.maintain_threshold}px, {self.env.maintain_target_steps}æ­¥\n")
    
    def print_final_stats(self, elapsed_time):
        """è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ† æ”¹è¿›ç‰ˆSACç»´æŒä»»åŠ¡æµ‹è¯•å®Œæˆ!")
        print("="*60)
        
        print(f"â±ï¸  æ€»ç”¨æ—¶: {elapsed_time:.1f}ç§’")
        print(f"ğŸ“Š Episodes: {self.stats['episodes']}")
        print(f"ğŸ¯ æˆåŠŸEpisodes: {self.stats['successful_episodes']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {self.stats['successful_episodes']/max(self.stats['episodes'],1):.1%}")
        
        if self.stats['maintain_times']:
            print(f"â³ å¹³å‡ç»´æŒæ—¶é—´: {np.mean(self.stats['maintain_times']):.1f}æ­¥")
            print(f"ğŸ† æœ€é•¿ç»´æŒæ—¶é—´: {max(self.stats['maintain_times'])}æ­¥")
            print(f"ğŸ¯ ç›®æ ‡ç»´æŒæ—¶é—´: {self.env.maintain_target_steps}æ­¥")
        
        if self.stats['alpha_history']:
            print(f"ğŸ”„ Alphaå˜åŒ–: {self.stats['alpha_history'][0]:.3f} â†’ {self.stats['alpha_history'][-1]:.3f}")
        
        if self.stats['reward_history']:
            recent_rewards = self.stats['reward_history'][-10:]
            print(f"ğŸ’° æœ€è¿‘10æ¬¡å¹³å‡å¥–åŠ±: {np.mean(recent_rewards):.1f}")
        
        print(f"ğŸ“ è¯¾ç¨‹å‡çº§æ¬¡æ•°: {self.stats['curriculum_upgrades']}")
        print(f"ğŸ“š æœ€ç»ˆè¯¾ç¨‹é˜¶æ®µ: {self.curriculum_stage}")
        
        # åˆ†ææ”¹è¿›æ•ˆæœ
        print("\nğŸ“‹ æ”¹è¿›æ•ˆæœåˆ†æ:")
        if self.stats['successful_episodes'] > 0:
            print("âœ… æˆåŠŸå­¦ä¼šç»´æŒä»»åŠ¡!")
            print("âœ… ç†µæƒé‡è°ƒåº¦æœ‰æ•ˆ - ä»æ¢ç´¢è½¬å‘ç¨³å®š")
            if self.stats['curriculum_upgrades'] > 0:
                print("âœ… è¯¾ç¨‹å­¦ä¹ æœ‰æ•ˆ - é€æ­¥æé«˜éš¾åº¦")
        else:
            print("âš ï¸ æœªèƒ½å®Œå…¨æŒæ¡ç»´æŒä»»åŠ¡")
            if self.stats['maintain_times']:
                best_maintain = max(self.stats['maintain_times'])
                target = self.env.maintain_target_steps
                progress = best_maintain / target * 100
                print(f"ğŸ“Š æœ€ä½³è¿›åº¦: {progress:.1f}% ({best_maintain}/{target}æ­¥)")
        
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ”¹è¿›ç‰ˆSACç»´æŒä»»åŠ¡æµ‹è¯•")
    print("å®ç°åŠŸèƒ½:")
    print("  ğŸ”„ ç†µæƒé‡è‡ªé€‚åº”è°ƒåº¦ (0.2 â†’ 0.05)")
    print("  ğŸ“ è¯¾ç¨‹å­¦ä¹  (3ä¸ªéš¾åº¦é˜¶æ®µ)")
    print("  ğŸ† æ”¹è¿›å¥–åŠ±è®¾è®¡ (é‡Œç¨‹ç¢‘+æ¸©å’Œæƒ©ç½š)")
    print()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ImprovedSACTester(
        num_joints=5,
        curriculum_stage=0  # ä»æœ€ç®€å•çš„é˜¶æ®µå¼€å§‹
    )
    
    # è¿è¡Œæµ‹è¯•
    tester.run_test(
        total_steps=2000,  # æ€»è®­ç»ƒæ­¥æ•°
        max_episodes=30    # æœ€å¤§episodeæ•°
    )

if __name__ == "__main__":
    main()
