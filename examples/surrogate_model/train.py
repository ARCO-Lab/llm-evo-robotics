import sys
import os
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../')
sys.path.append(base_dir)

sys.path.insert(0, os.path.join(base_dir, 'examples/2d_reacher/envs'))
sys.path.insert(0, os.path.join(base_dir, 'examples/surrogate_model/gnn_encoder'))
sys.path.insert(0, os.path.join(base_dir, 'examples/rl/train'))  # ä½¿ç”¨insertç¡®ä¿ä¼˜å…ˆçº§
sys.path.insert(0, os.path.join(base_dir, 'examples/rl/common'))
sys.path.insert(0, os.path.join(base_dir, 'examples/rl/environments'))
sys.path.append(os.path.join(base_dir, 'examples/rl'))

import numpy as np
import time
from collections import deque

from gnn_encoder import GNN_Encoder

if not hasattr(np, 'bool'):
    np.bool = bool

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gym

gym.logger.set_level(40)


# ç›´æ¥å¯¼å…¥ï¼Œç°åœ¨environmentsåœ¨è·¯å¾„ä¸­
import environments

# ç›´æ¥å¯¼å…¥æ¨¡å—ï¼Œä¸ä½¿ç”¨rl.å‰ç¼€
from arguments import get_parser
from utils import solve_argv_conflict
from common import *
from evaluation import render

from a2c_ppo_acktr import algo, utils
from a2c_ppo_acktr.envs import make_vec_envs, make_env
from a2c_ppo_acktr.model import Policy
from a2c_ppo_acktr.storage import RolloutStorage

from attn_dataset.sim_data_handler import DataHandler

from attn_model.attn_model import AttnModel
from sac.sac_model import AttentionSACWithBuffer

# ä¿®æ”¹ç¬¬50è¡Œçš„å¯¼å…¥
from env_config.env_wrapper import make_reacher2d_vec_envs, make_smart_reacher2d_vec_envs
from reacher2d_env import Reacher2DEnv
# åœ¨ train.py ç¬¬6è¡Œåæ·»åŠ 
sys.path.insert(0, os.path.join(base_dir, 'examples/2d_reacher/envs'))
from async_renderer import AsyncRenderer, StateExtractor  # ğŸ¨ æ·»åŠ è¿™è¡Œ


def check_goal_reached(env, goal_threshold=50.0):  # è°ƒæ•´é»˜è®¤é˜ˆå€¼ä¸º50.0
    """æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡"""
    try:
        if hasattr(env, '_get_end_effector_position') and hasattr(env, 'goal_pos'):
            end_pos = env._get_end_effector_position()
            goal_pos = env.goal_pos
            distance = np.linalg.norm(np.array(end_pos) - goal_pos)
            return distance <= goal_threshold, distance
    except Exception as e:
        print(f"ç›®æ ‡æ£€æµ‹å¤±è´¥: {e}")
    return False, float('inf')

def save_best_model(sac, model_save_path, success_rate, min_distance, step):
    """ä¿å­˜æœ€ä½³æ¨¡å‹"""
    try:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜SACæ‰€æœ‰ç»„ä»¶
        model_data = {
            'step': step,
            'success_rate': success_rate,
            'min_distance': min_distance,
            'timestamp': timestamp,
            'actor_state_dict': sac.actor.state_dict(),
            'critic1_state_dict': sac.critic1.state_dict(),
            'critic2_state_dict': sac.critic2.state_dict(),
            'target_critic1_state_dict': sac.target_critic1.state_dict(),
            'target_critic2_state_dict': sac.target_critic2.state_dict(),
            'actor_optimizer_state_dict': sac.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': sac.critic_optimizer.state_dict(),
            'alpha_optimizer_state_dict': sac.alpha_optimizer.state_dict(),
            'alpha': sac.alpha.item() if torch.is_tensor(sac.alpha) else sac.alpha,
            'log_alpha': sac.log_alpha.item() if torch.is_tensor(sac.log_alpha) else sac.log_alpha,
        }
        
        # ä¿å­˜æ–‡ä»¶
        model_file = os.path.join(model_save_path, f'best_model_step_{step}_{timestamp}.pth')
        torch.save(model_data, model_file)
        
        # åŒæ—¶ä¿å­˜ä¸€ä¸ª"latest_best"ç‰ˆæœ¬ä¾¿äºåŠ è½½
        latest_file = os.path.join(model_save_path, 'latest_best_model.pth')
        torch.save(model_data, latest_file)
        
        print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹: {model_file}")
        print(f"   æˆåŠŸç‡: {success_rate:.3f}, æœ€å°è·ç¦»: {min_distance:.1f}, æ­¥éª¤: {step}")
        
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
        return False

def load_best_model(sac, model_save_path):
    """åŠ è½½æœ€ä½³æ¨¡å‹"""
    try:
        latest_file = os.path.join(model_save_path, 'latest_best_model.pth')
        if os.path.exists(latest_file):
            model_data = torch.load(latest_file, map_location=sac.device)
            
            sac.actor.load_state_dict(model_data['actor_state_dict'])
            sac.critic1.load_state_dict(model_data['critic1_state_dict'])
            sac.critic2.load_state_dict(model_data['critic2_state_dict'])
            sac.target_critic1.load_state_dict(model_data['target_critic1_state_dict'])
            sac.target_critic2.load_state_dict(model_data['target_critic2_state_dict'])
            
            print(f"âœ… åŠ è½½æœ€ä½³æ¨¡å‹æˆåŠŸ: Step {model_data['step']}, æˆåŠŸç‡: {model_data['success_rate']:.3f}")
            return True
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    return False


def main(args):

    torch.manual_seed(args.seed)
    torch.set_num_threads(1)
    device = torch.device('cpu')

    os.makedirs(args.save_dir, exist_ok = True)

    training_log_path = os.path.join(args.save_dir, 'logs.txt')
    fp_log = open(training_log_path, 'w')
    fp_log.close()

    if args.env_name == 'reacher2d':
        print("use reacher2d env")

        env_params = {
            'num_links': 4,
            'link_lengths': [80, 80, 80, 60],
            'render_mode': 'human',
            'config_path': "/home/xli149/Documents/repos/RoboGrammar/examples/2d_reacher/configs/reacher_with_zigzag_obstacles.yaml"
        }
        print(f"num links: {env_params['num_links']}")
        print(f"link lengths: {env_params['link_lengths']}")

        # ğŸ¨ å¼‚æ­¥æ¸²æŸ“æ¨¡å¼ï¼šå¤šè¿›ç¨‹è®­ç»ƒ + ç‹¬ç«‹æ¸²æŸ“
        async_renderer = None
        sync_env = None
        
        if args.num_processes > 1:
            print("ğŸš€ å¤šè¿›ç¨‹æ¨¡å¼ï¼šå¯ç”¨å¼‚æ­¥æ¸²æŸ“")
            
            # åˆ›å»ºæ— æ¸²æŸ“çš„è®­ç»ƒç¯å¢ƒ
            train_env_params = env_params.copy()
            train_env_params['render_mode'] = None  # è®­ç»ƒç¯å¢ƒä¸æ¸²æŸ“
            
            envs = make_reacher2d_vec_envs(
                env_params=train_env_params,
                seed=args.seed,
                num_processes=args.num_processes,
                gamma=args.gamma,
                log_dir=None,
                device=device,
                allow_early_resets=False,
            )
            
            # åˆ›å»ºå¼‚æ­¥æ¸²æŸ“å™¨
       
            async_renderer = AsyncRenderer(env_params)  # ä½¿ç”¨åŸå§‹å‚æ•°ï¼ˆåŒ…å«æ¸²æŸ“ï¼‰
            async_renderer.start()
            
            # åˆ›å»ºçŠ¶æ€åŒæ­¥ç¯å¢ƒ
            sync_env = Reacher2DEnv(**train_env_params)
            print(f"âœ… å¼‚æ­¥æ¸²æŸ“å™¨å·²å¯åŠ¨ (PID: {async_renderer.render_process.pid})")
            
        else:
            print("ğŸƒ å•è¿›ç¨‹æ¨¡å¼ï¼šç›´æ¥æ¸²æŸ“")
            # å•è¿›ç¨‹ç›´æ¥æ¸²æŸ“
            envs = make_reacher2d_vec_envs(
                env_params=env_params,
                seed=args.seed,
                num_processes=args.num_processes,
                gamma=args.gamma,
                log_dir=None,
                device=device,
                allow_early_resets=False
            )

        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        args.env_type = 'reacher2d'

        
    else:
        print(f"use bullet env: {args.env_name}")

        envs = make_vec_envs(args.env_name, args.seed, args.num_processes, 
                            args.gamma, None, device, False, args = args)

        render_env = gym.make(args.env_name, args = args)
        render_env.seed(args.seed)
        args.env_type = 'bullet'

    num_joints = envs.action_space.shape[0]  # è¿™å°±æ˜¯å…³èŠ‚æ•°é‡ï¼
    print(f"Number of joints: {num_joints}")
    num_updates = 5
    num_step = 50000  # ä»2000å¢åŠ åˆ°5000ï¼Œç»™æ›´å¤šå­¦ä¹ æ—¶é—´
    data_handler = DataHandler(num_joints, args.env_type)



    #TODO: éœ€è¦ä¿®æ”¹ï¼Œç°åœ¨reacher2dæ˜¯ä¸æ”¯æŒrule_sequenceçš„

    # åœ¨ç¬¬115è¡Œ data_handler = DataHandler(num_joints, args.env_type) ä¹‹åæ·»åŠ ï¼š

    if args.env_type == 'reacher2d':
        # ğŸ”¸ ä½¿ç”¨ Reacher2D GNN ç¼–ç å™¨
        sys.path.append(os.path.join(os.path.dirname(__file__), '../2d_reacher/utils'))
        from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
        
        print("ğŸ¤– åˆå§‹åŒ– Reacher2D GNN ç¼–ç å™¨...")
        reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
        single_gnn_embed = reacher2d_encoder.get_gnn_embeds(
            num_links=num_joints, 
            # link_lengths=[80, 80, 80, 60]  # æˆ–è€…ä» env_params è·å–
            link_lengths = env_params['link_lengths']
        )
        print(f"âœ… Reacher2D GNN åµŒå…¥ç”ŸæˆæˆåŠŸï¼Œå½¢çŠ¶: {single_gnn_embed.shape}")
    else:
        # ğŸ”¸ ä½¿ç”¨åŸæœ‰çš„ Bullet GNN ç¼–ç å™¨
        rule_sequence = [int(s.strip(",")) for s in args.rule_sequence]
        gnn_encoder = GNN_Encoder(args.grammar_file, rule_sequence, 70, num_joints)
        gnn_graph = gnn_encoder.get_graph(rule_sequence)
        single_gnn_embed = gnn_encoder.get_gnn_embeds(gnn_graph)

    # ç„¶ååˆ é™¤æˆ–æ³¨é‡Šæ‰åŸæ¥çš„ç¬¬117-121è¡Œï¼š
    # rule_sequence = [int(s.strip(",")) for s in args.rule_sequence]
    # gnn_encoder = GNN_Encoder(args.grammar_file, rule_sequence, 70, num_joints)
    # gnn_graph = gnn_encoder.get_graph(rule_sequence)
    # single_gnn_embed = gnn_encoder.get_gnn_embeds(gnn_graph)
    # rule_sequence = [int(s.strip(",")) for s in args.rule_sequence]
    # gnn_encoder = GNN_Encoder(args.grammar_file, rule_sequence, 70, num_joints)
    
    # gnn_graph = gnn_encoder.get_graph(rule_sequence)
    # single_gnn_embed = gnn_encoder.get_gnn_embeds(gnn_graph)  # [1, N, D]


    action_dim = num_joints  # ä½¿ç”¨å®é™…çš„å…³èŠ‚æ•°ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 12
    attn_model = AttnModel(128, 130, 130, 4)
    sac = AttentionSACWithBuffer(attn_model, action_dim, 
                                buffer_capacity=10000, batch_size=32,  # ä»64å‡å°‘åˆ°32
                                lr=1e-4,  # é™ä½å­¦ä¹ ç‡ä»3e-4åˆ°1e-4
                                env_type=args.env_type)
    
    # ğŸ”§ é‡æ–°ä¼˜åŒ–SACå‚æ•°ä»¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨
    sac.warmup_steps = 1000   # ä»500å¢åŠ åˆ°1000ï¼Œæ›´å……åˆ†çš„æ¢ç´¢
    sac.alpha = 0.2          # ä»0.1å¢åŠ åˆ°0.2ï¼Œå¢åŠ æ¢ç´¢æ€§
    if hasattr(sac, 'target_entropy'):
        sac.target_entropy = -action_dim * 0.8  # ä»0.5å¢åŠ åˆ°0.8ï¼Œé¼“åŠ±æ›´å¤šæ ·åŒ–çš„ç­–ç•¥
    current_obs = envs.reset()
    current_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)  # [B, N, D]
    total_steps =0
    episode_rewards = [0.0] * args.num_processes
    eval_frequency = 200  # å¢åŠ è¯„ä¼°é—´éš”
    
    # ğŸ† æ·»åŠ æœ€ä½³æ¨¡å‹ä¿å­˜ç›¸å…³å˜é‡
    best_success_rate = 0.0
    best_min_distance = float('inf')
    goal_threshold = 35.0  # ä»25.0è°ƒæ•´åˆ°35.0åƒç´ ï¼Œæ›´å®¹æ˜“è¾¾æˆæˆåŠŸ
    consecutive_success_count = 0
    min_consecutive_successes = 3  # è¿ç»­æˆåŠŸæ¬¡æ•°è¦æ±‚
    model_save_path = os.path.join(args.save_dir, 'best_models')
    os.makedirs(model_save_path, exist_ok=True)
    
    # æ·»åŠ ç¼ºå°‘çš„å‚æ•°
    if not hasattr(args, 'update_frequency'):
        args.update_frequency = 2  # ä»4å‡å°‘åˆ°2ï¼Œæ›´é¢‘ç¹æ›´æ–°
    
    print(f"start training, warmup {sac.warmup_steps} steps")
    print(f"Total training steps: {num_step}, Update frequency: {args.update_frequency}")
    print(f"Expected warmup completion at step: {sac.warmup_steps}")

    try:

        for step in range(num_step):
            
            # æ·»åŠ è¿›åº¦ä¿¡æ¯
            if step % 100 == 0:
                if step < sac.warmup_steps:
                    print(f"Step {step}/{num_step}: Warmup phase ({step}/{sac.warmup_steps})")
                else:
                    print(f"Step {step}/{num_step}: Training phase, Buffer size: {len(sac.memory)}")

                if async_renderer:
                    stats = async_renderer.get_stats()
                    print(f"   ğŸ¨ æ¸²æŸ“FPS: {stats.get('fps', 0):.1f}")

            if step < sac.warmup_steps:  # ä½¿ç”¨stepè€Œä¸æ˜¯total_stepsæ¥åˆ¤æ–­é¢„çƒ­æœŸ
                action_batch = torch.from_numpy(np.array([envs.action_space.sample() for _ in range(args.num_processes)]))
            else:
                actions = []
                for proc_id in range(args.num_processes):
                    action = sac.get_action(current_obs[proc_id],
                                            current_gnn_embeds[proc_id],
                                            num_joints = envs.action_space.shape[0],
                                            deterministic = False)
                    actions.append(action)

                action_batch = torch.stack(actions)

            # action_batch = torch.from_numpy(np.random.uniform(-100, 100, (args.num_processes, num_joints)))

            # ğŸ” æ·»åŠ Actionç›‘æ§ - æ¯50æ­¥è¯¦ç»†æ‰“å°actionå€¼
            if step % 50 == 0 or step < 20:  # å‰20æ­¥å’Œæ¯50æ­¥
                print(f"\nğŸ¯ Step {step} Action Analysis:")
                action_numpy = action_batch.cpu().numpy() if hasattr(action_batch, 'cpu') else action_batch.numpy()
                
                for proc_id in range(min(args.num_processes, 2)):  # åªæ‰“å°å‰2ä¸ªè¿›ç¨‹
                    action_values = action_numpy[proc_id]
                    print(f"  Process {proc_id}: Actions = [{action_values[0]:+6.2f}, {action_values[1]:+6.2f}, {action_values[2]:+6.2f}, {action_values[3]:+6.2f}]")
                    print(f"    Max action: {np.max(np.abs(action_values)):6.2f}, Mean abs: {np.mean(np.abs(action_values)):6.2f}")
                
                # æ‰“å°actionç»Ÿè®¡
                all_actions = action_numpy.flatten()
                print(f"  ğŸ“Š All Actions Stats:")
                print(f"    Range: [{np.min(all_actions):+6.2f}, {np.max(all_actions):+6.2f}]")
                print(f"    Mean: {np.mean(all_actions):+6.2f}, Std: {np.std(all_actions):6.2f}")
                print(f"    Action space limit: Â±{envs.action_space.high[0]:.1f}")
                
                # æ£€æŸ¥actionæ˜¯å¦é¥±å’Œ
                saturated = np.sum(np.abs(all_actions) > envs.action_space.high[0] * 0.9)
                print(f"    Actions near saturation (>90% limit): {saturated}/{len(all_actions)}")

            next_obs, reward, done, infos = envs.step(action_batch)
            
            # ğŸ” æ·»åŠ è·ç¦»å’Œä½ç½®ç›‘æ§ - æ¯æ­¥éƒ½æ£€æŸ¥
            if step % 10 == 0 or step < 30:  # å‰30æ­¥å’Œæ¯10æ­¥ç›‘æ§è·ç¦»
                # ğŸ¯ è·å–æœºå™¨äººæœ«ç«¯ä½ç½®å’Œç›®æ ‡è·ç¦»
                if async_renderer and sync_env:
                    # å¤šè¿›ç¨‹æ¨¡å¼ï¼šä½¿ç”¨sync_envè·å–å‡†ç¡®çš„çŠ¶æ€ä¿¡æ¯
                    end_pos = sync_env._get_end_effector_position()
                    goal_pos = sync_env.goal_pos
                    distance = np.linalg.norm(np.array(end_pos) - goal_pos)
                    
                    print(f"  ğŸ¯ Distance Monitoring (Step {step}):")
                    print(f"    End Effector: [{end_pos[0]:7.1f}, {end_pos[1]:7.1f}]")
                    print(f"    Goal Position: [{goal_pos[0]:7.1f}, {goal_pos[1]:7.1f}]")
                    print(f"    Distance to Goal: {distance:7.1f} pixels")
                    
                    # ğŸ† æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
                    goal_reached, current_distance = check_goal_reached(sync_env, goal_threshold)
                    if goal_reached:
                        consecutive_success_count += 1
                        print(f"    ğŸ‰ ç›®æ ‡åˆ°è¾¾! è¿ç»­æˆåŠŸæ¬¡æ•°: {consecutive_success_count}")
                        
                        # æ›´æ–°æœ€ä½³è·ç¦»
                        if current_distance < best_min_distance:
                            best_min_distance = current_distance
                        
                        # è¿ç»­æˆåŠŸè¾¾åˆ°è¦æ±‚æ—¶ä¿å­˜æ¨¡å‹
                        if consecutive_success_count >= min_consecutive_successes:
                            current_success_rate = consecutive_success_count / min_consecutive_successes
                            if current_success_rate > best_success_rate:
                                best_success_rate = current_success_rate
                                save_best_model(sac, model_save_path, best_success_rate, best_min_distance, step)
                    else:
                        consecutive_success_count = 0  # é‡ç½®è¿ç»­æˆåŠŸè®¡æ•°
                        
                elif args.num_processes == 1:
                    # ğŸ”§ å•è¿›ç¨‹æ¨¡å¼ï¼šç›´æ¥ä»envsè·å–çŠ¶æ€
                    # éœ€è¦è®¿é—®åº•å±‚ç¯å¢ƒ
                    if hasattr(envs, 'envs') and len(envs.envs) > 0:
                        base_env = envs.envs[0]
                        # æŸ¥æ‰¾çœŸæ­£çš„ç¯å¢ƒå®ä¾‹
                        while hasattr(base_env, 'env'):
                            base_env = base_env.env
                        
                        if hasattr(base_env, '_get_end_effector_position'):
                            end_pos = base_env._get_end_effector_position()
                            goal_pos = base_env.goal_pos
                            distance = np.linalg.norm(np.array(end_pos) - goal_pos)
                            
                            print(f"  ğŸ¯ Distance Monitoring (Step {step}) [Single Process]:")
                            print(f"    End Effector: [{end_pos[0]:7.1f}, {end_pos[1]:7.1f}]")
                            print(f"    Goal Position: [{goal_pos[0]:7.1f}, {goal_pos[1]:7.1f}]")
                            print(f"    Distance to Goal: {distance:7.1f} pixels")
                            
                            # ğŸ† æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡ï¼ˆå•è¿›ç¨‹ç‰ˆæœ¬ï¼‰
                            goal_reached, current_distance = check_goal_reached(base_env, goal_threshold)
                            if goal_reached:
                                consecutive_success_count += 1
                                print(f"    ğŸ‰ ç›®æ ‡åˆ°è¾¾! è¿ç»­æˆåŠŸæ¬¡æ•°: {consecutive_success_count}")
                                
                                if current_distance < best_min_distance:
                                    best_min_distance = current_distance
                                
                                if consecutive_success_count >= min_consecutive_successes:
                                    current_success_rate = consecutive_success_count / min_consecutive_successes
                                    if current_success_rate > best_success_rate:
                                        best_success_rate = current_success_rate
                                        save_best_model(sac, model_save_path, best_success_rate, best_min_distance, step)
                            else:
                                consecutive_success_count = 0
                    
                # è®°å½•è·ç¦»å˜åŒ–è¶‹åŠ¿ï¼ˆå…¬å…±éƒ¨åˆ†ï¼‰
                if 'end_pos' in locals():
                    if not hasattr(main, 'prev_distances'):
                        main.prev_distances = []
                    main.prev_distances.append(distance)
                    
                    # è®¡ç®—è·ç¦»å˜åŒ–è¶‹åŠ¿ï¼ˆæœ€è¿‘5æ­¥ï¼‰
                    if len(main.prev_distances) >= 5:
                        recent_distances = main.prev_distances[-5:]
                        distance_trend = recent_distances[-1] - recent_distances[0]  # æ­£å€¼=è¿œç¦»ï¼Œè´Ÿå€¼=æ¥è¿‘
                        avg_distance = np.mean(recent_distances)
                        print(f"    Distance Trend (last 5 steps): {distance_trend:+6.1f} ({'ğŸ”´ Moving Away' if distance_trend > 10 else 'ğŸŸ¢ Getting Closer' if distance_trend < -10 else 'ğŸŸ¡ No Clear Trend'})")
                        print(f"    Average Distance (last 5): {avg_distance:7.1f}")
                        
                        # å¦‚æœè·ç¦»æ•°æ®å¤ªå¤šï¼Œä¿ç•™æœ€è¿‘50ä¸ª
                        if len(main.prev_distances) > 50:
                            main.prev_distances = main.prev_distances[-50:]
            
            # ğŸ” æ·»åŠ Rewardç›‘æ§ - æ˜¾ç¤ºrewardå˜åŒ–
            if step % 50 == 0 or step < 20:
                # å®‰å…¨å¤„ç†rewardæ•°æ®ç±»å‹
                if hasattr(reward, 'cpu'):
                    reward_numpy = reward.cpu().numpy()
                elif hasattr(reward, 'numpy'):
                    reward_numpy = reward.numpy()
                else:
                    reward_numpy = reward
                    
                # å®‰å…¨åœ°å¤„ç†rewardå€¼ï¼Œè½¬æ¢ä¸ºæ ‡é‡
                reward_0 = float(reward_numpy[0]) if len(reward_numpy) > 0 else 0.0
                reward_1 = float(reward_numpy[1]) if len(reward_numpy) > 1 else 0.0
                reward_min = float(np.min(reward_numpy))
                reward_max = float(np.max(reward_numpy))
                
                print(f"  ğŸ’° Rewards: [{reward_0:+7.3f}, {reward_1:+7.3f}] (Process 0, 1)")
                print(f"    Reward range: [{reward_min:+7.3f}, {reward_max:+7.3f}]")
                
                # æ£€æŸ¥doneçŠ¶æ€
                if hasattr(done, 'cpu'):
                    done_numpy = done.cpu().numpy()
                elif hasattr(done, 'numpy'):
                    done_numpy = done.numpy()
                else:
                    done_numpy = done
                    
                done_count = np.sum(done_numpy)
                if done_count > 0:
                    print(f"  ğŸ Episodes completed: {done_count}/{len(done_numpy)}")

                    # ğŸ¯ æ–°å¢ï¼šä»infoä¸­è·å–ç¢°æ’ä¿¡æ¯
                if 'infos' in locals() and len(infos) > 0:  
                    first_env_info = infos[0]
                    if 'collisions' in first_env_info:
                        collision_info = first_env_info['collisions']
                        goal_info = first_env_info['goal']
                        
                        print(f"  ğŸ’¥ Collision Monitoring:")
                        print(f"    Total Collisions: {collision_info['total_count']}")
                        print(f"    Episode Collisions: {collision_info['collisions_this_episode']}")
                        print(f"    Collision Rate: {collision_info['collision_rate']:.4f}")
                        print(f"    Collision Penalty: {collision_info['collision_penalty']:.2f}")
                        
                        print(f"  ğŸ¯ Goal Monitoring:")
                        print(f"    Distance: {goal_info['distance_to_goal']:.1f} pixels")
                        print(f"    Goal Reached: {'âœ…' if goal_info['goal_reached'] else 'âŒ'}")
                        
                        # å¦‚æœæœ‰å¥–åŠ±åˆ†è§£ä¿¡æ¯
                        if 'reward_breakdown' in first_env_info:
                            breakdown = first_env_info['reward_breakdown']
                            print(f"  ğŸ’° Reward Breakdown:")
                            for key, value in breakdown.items():
                                print(f"    {key}: {value:.3f}")
            
            # ğŸ¨ å¼‚æ­¥æ¸²æŸ“
            if async_renderer and sync_env:
                sync_action = action_batch[0].cpu().numpy() if hasattr(action_batch, 'cpu') else action_batch[0]
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ¸²æŸ“ç¯å¢ƒä¹Ÿä½¿ç”¨ç¼©æ”¾åçš„actionï¼
                # SACåœ¨get_actionä¸­å·²ç»ç¼©æ”¾äº†actionï¼Œä½†è¿™é‡Œéœ€è¦ç¡®è®¤
                if args.env_type == 'reacher2d':
                    # action_batchå·²ç»æ˜¯ç¼©æ”¾åçš„å€¼ï¼Œç›´æ¥ä½¿ç”¨
                    print(f"  ğŸ¨ æ¸²æŸ“Action: [{sync_action[0]:+6.1f}, {sync_action[1]:+6.1f}, {sync_action[2]:+6.1f}, {sync_action[3]:+6.1f}]")
                
                sync_env.step(sync_action)
                robot_state = StateExtractor.extract_robot_state(sync_env, step)
                async_renderer.render_frame(robot_state)
            #TODO: éœ€è¦ä¿®æ”¹ æˆæ›´çµæ´»çš„gnn_embeds
            next_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)  # [B, N, D]

            for proc_id in range(args.num_processes):
                sac.store_experience(
                        obs = current_obs[proc_id],
                        gnn_embeds = current_gnn_embeds[proc_id],
                        action = action_batch[proc_id],
                        reward = reward[proc_id],
                        next_obs = next_obs[proc_id],
                        next_gnn_embeds = next_gnn_embeds[proc_id],
                        done = done[proc_id],
                        num_joints = num_joints
                )
                episode_rewards[proc_id] += reward[proc_id].item()  # å°†tensorè½¬æ¢ä¸ºæ ‡é‡

            current_obs = next_obs.clone()
            current_gnn_embeds = next_gnn_embeds.clone()


            for proc_id in range(args.num_processes):
                # å®‰å…¨æ£€æŸ¥doneçŠ¶æ€
                is_done = done[proc_id].item() if torch.is_tensor(done[proc_id]) else bool(done[proc_id])
                if is_done:
                    print(f"Episode {step} finished with reward {episode_rewards[proc_id]:.2f}")


                    episode_rewards[proc_id] = 0.0
                    
                    # é‡ç½®ç¯å¢ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if hasattr(envs, 'reset_one'):
                        current_obs[proc_id] = envs.reset_one(proc_id)
                        current_gnn_embeds[proc_id] = single_gnn_embed

            if (step >= sac.warmup_steps and 
                step % args.update_frequency == 0 and 
                sac.memory.can_sample(sac.batch_size)):
                
                metrics = sac.update()
                
                if metrics and step % 100 == 0:
                    print(f"Step {step} (total_steps {total_steps}): "
                        f"Critic Loss: {metrics['critic_loss']:.4f}, "
                        f"Actor Loss: {metrics['actor_loss']:.4f}, "
                        f"Alpha: {metrics['alpha']:.4f}, "
                        f"Buffer Size: {len(sac.memory)}")
                    
                    # ğŸ† æ·»åŠ è®­ç»ƒçŠ¶æ€æŠ¥å‘Š
                    print(f"  ğŸ¯ Training Status:")
                    print(f"    Best Success Rate: {best_success_rate:.3f}")
                    print(f"    Best Min Distance: {best_min_distance:.1f} pixels")
                    print(f"    Consecutive Successes: {consecutive_success_count}")
                    print(f"    Goal Threshold: {goal_threshold:.1f} pixels")
                    
                    # æ·»åŠ è¯¦ç»†çš„lossåˆ†æ
                    if 'entropy_term' in metrics:
                        print(f"  Actor Loss ç»„ä»¶åˆ†æ:")
                        print(f"    Entropy Term (Î±*log_Ï€): {metrics['entropy_term']:.4f}")
                        print(f"    Q Term (Qå€¼): {metrics['q_term']:.4f}")
                        print(f"    Actor Loss = {metrics['entropy_term']:.4f} - {metrics['q_term']:.4f} = {metrics['actor_loss']:.4f}")
                        
                        if metrics['actor_loss'] < 0:
                            print(f"    âœ“ è´Ÿæ•°Actor Loss = é«˜Qå€¼ = å¥½çš„ç­–ç•¥!")
                        else:
                            print(f"    âš  æ­£æ•°Actor Loss = ä½Qå€¼ = ç­–ç•¥éœ€è¦æ”¹è¿›")
            
            # ğŸ† å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹æ¨¡å‹ï¼ˆä¸ç®¡æ˜¯å¦åˆ°è¾¾ç›®æ ‡ï¼‰
            if step % 1000 == 0 and step > 0:
                checkpoint_path = os.path.join(model_save_path, f'checkpoint_step_{step}.pth')
                checkpoint_data = {
                    'step': step,
                    'best_success_rate': best_success_rate,
                    'best_min_distance': best_min_distance,
                    'consecutive_success_count': consecutive_success_count,
                    'actor_state_dict': sac.actor.state_dict(),
                    'critic1_state_dict': sac.critic1.state_dict(),
                    'critic2_state_dict': sac.critic2.state_dict(),
                }
                torch.save(checkpoint_data, checkpoint_path)
                print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹æ¨¡å‹: {checkpoint_path}")
            
            total_steps += args.num_processes  # å¹¶è¡Œç¯å¢ƒæ­¥æ•°ç´¯åŠ 

    except Exception as e:
        print(f"ğŸ”´ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise e

    finally:
        # æ¸…ç†èµ„æº
        if 'async_renderer' in locals() and async_renderer:
            async_renderer.stop()
        if 'sync_env' in locals() and sync_env:
            sync_env.close()
            
        # ğŸ† è®­ç»ƒç»“æŸæ—¶ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print(f"\n{'='*60}")
        print(f"ğŸ è®­ç»ƒå®Œæˆæ€»ç»“:")
        print(f"  æ€»æ­¥æ•°: {step}")
        print(f"  æœ€ä½³æˆåŠŸç‡: {best_success_rate:.3f}")
        print(f"  æœ€ä½³æœ€å°è·ç¦»: {best_min_distance:.1f} pixels")
        print(f"  å½“å‰è¿ç»­æˆåŠŸæ¬¡æ•°: {consecutive_success_count}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(model_save_path, f'final_model_step_{step}.pth')
        final_model_data = {
            'step': step,
            'final_success_rate': best_success_rate,
            'final_min_distance': best_min_distance,
            'final_consecutive_successes': consecutive_success_count,
            'training_completed': True,
            'actor_state_dict': sac.actor.state_dict(),
            'critic1_state_dict': sac.critic1.state_dict(),
            'critic2_state_dict': sac.critic2.state_dict(),
            'target_critic1_state_dict': sac.target_critic1.state_dict(),
            'target_critic2_state_dict': sac.target_critic2.state_dict(),
        }
        torch.save(final_model_data, final_model_path)
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_model_path}")
        print(f"{'='*60}")
        
        # 8. å®šæœŸè¯„ä¼°
        # if step % eval_frequency == 0:
        #     eval_reward = evaluate_sac(envs, sac, single_gnn_embed, args)
        #     print(f"Evaluation at step {total_steps}: Average reward = {eval_reward:.2f}")

# def evaluate_sac(envs, sac, single_gnn_embed, args, num_episodes=5):
#         """è¯„ä¼°SACæ€§èƒ½"""
#         total_rewards = []
        
#         for episode in range(num_episodes):
#             gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)
#             episode_reward = 0
#             done = False
            
#             while not done:
#                 # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
#                 actions = []
#                 for proc_id in range(args.num_processes):
#                     action = sac.get_action(obs[proc_id], gnn_embeds[proc_id], deterministic=True)
#                     actions.append(action)
                
#                 action_batch = torch.stack(actions)
#                 obs, rewards, dones, _ = envs.step(action_batch)
#                 episode_reward += rewards.mean().item()
#                 done = dones.any().item()
            
#             total_rewards.append(episode_reward)
        
#         return np.mean(total_rewards)


# def train(args):
#     torch.manual_seed(args.seed)
#     torch.set_num_threads(1)
#     device = torch.device('cpu')

#     os.makedirs(args.save_dir, exist_ok = True)

#     training_log_path = os.path.join(args.save_dir, 'logs.txt')
#     fp_log = open(training_log_path, 'w')
#     fp_log.close()

#     envs = make_vec_envs(args.env_name, args.seed, args.num_processes, 
#                         args.gamma, None, device, False, args = args)

#     render_env = gym.make(args.env_name, args = args)
#     render_env.seed(args.seed)
#     num_joints = envs.action_space.shape[0]  # è¿™å°±æ˜¯å…³èŠ‚æ•°é‡ï¼
#     print(f"Number of joints: {num_joints}")
#     obs = envs.reset()
#     num_updates = 5
#     num_step = 1000
    
#     data_handler = DataHandler(num_joints)
#     rule_sequence = [int(s.strip(",")) for s in args.rule_sequence]
#     gnn_encoder = GNN_Encoder(args.grammar_file, rule_sequence, 70, num_joints)
    
#     gnn_graph = gnn_encoder.get_graph(rule_sequence)
#     single_gnn_embed = gnn_encoder.get_gnn_embeds(gnn_graph)  # [1, N, D]
#     attn_model = AttnModel()
#     for i in range(num_updates):
#         for step in range(num_step):
#             action = torch.from_numpy(np.array([envs.action_space.sample() for _ in range(args.num_processes)]))
#             obs, reward, done, infos = envs.step(action)
           
#             gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)  # [B, N, D]
#             data_handler.save_data(obs, action, reward, gnn_embeds, done)

#         data_loader = data_handler.get_data_loader()
#     print(data_handler.get_data_length())

#     for batch in data_loader:
#         joint_q = batch["joint_q"]
#         vertex_k = batch["vertex_k"]
#         vertex_v = batch["vertex_v"]
#         vertex_mask = batch["vertex_mask"]
#         attn_output = attn_model(joint_q, vertex_k, vertex_v, vertex_mask)


# if __name__ == "__main__":

#     torch.set_default_dtype(torch.float64)
#     args_list = ['--env-name', 'RobotLocomotion-v0',
#                  '--task', 'FlatTerrainTask',
#                  '--grammar-file', '../../data/designs/grammar_jan21.dot',
#                  '--algo', 'ppo',
#                  '--use-gae',
#                  '--log-interval', '5',
#                  '--num-steps', '1024',
#                  '--num-processes', '8',
#                  '--lr', '3e-4',
#                  '--entropy-coef', '0',
#                  '--value-loss-coef', '0.5',
#                  '--ppo-epoch', '10',
#                  '--num-mini-batch', '32',
#                  '--gamma', '0.995',
#                  '--gae-lambda', '0.95',
#                  '--num-env-steps', '30000000',
#                  '--use-linear-lr-decay',
#                  '--use-proper-time-limits',
#                  '--save-interval', '100',
#                  '--seed', '2',
#                  '--save-dir', './trained_models/RobotLocomotion-v0/test/',
#                  '--render-interval', '80']
#     parser = get_parser()
#     args = parser.parse_args(args_list + sys.argv[1:])

        
#     solve_argv_conflict(args_list)
#     parser = get_parser()
#     args = parser.parse_args(args_list + sys.argv[1:])

#     args.cuda = not args.no_cuda and torch.cuda.is_available()

#     args.save_dir = os.path.join(args.save_dir, get_time_stamp())
#     try:
#         os.makedirs(args.save_dir, exist_ok = True)
#     except OSError:
#         pass

#     fp = open(os.path.join(args.save_dir, 'args.txt'), 'w')
#     fp.write(str(args_list + sys.argv[1:]))
#     fp.close()

#     # train(args)
#     main(args)


 
if __name__ == "__main__":
    torch.set_default_dtype(torch.float64)
    
    # ğŸ¯ æ£€æŸ¥æ˜¯å¦è¦æµ‹è¯• Reacher2D
    if len(sys.argv) > 1 and sys.argv[1] == '--test-reacher2d':
        print("ğŸ¤– å¯åŠ¨ Reacher2D ç¯å¢ƒæµ‹è¯•")
        args_list = ['--env-name', 'reacher2d',
                     '--num-processes', '2',  # æ¢å¤å¤šè¿›ç¨‹ä¾¿äºå¹¶è¡Œè®­ç»ƒ
                     '--lr', '3e-4',
                     '--gamma', '0.99',
                     '--seed', '42',
                     '--save-dir', './trained_models/reacher2d/test/',
                     '--grammar-file', '/home/xli149/Documents/repos/RoboGrammar/data/designs/grammar_jan21.dot',
                     '--rule-sequence', '0']
        # ç§»é™¤æµ‹è¯•æ ‡å¿—ï¼Œé¿å…è§£æé”™è¯¯
        test_args = sys.argv[2:] if len(sys.argv) > 2 else []
    else:
        # åŸæ¥çš„ RobotLocomotion-v0 é…ç½®
        args_list = ['--env-name', 'RobotLocomotion-v0',
                     '--task', 'FlatTerrainTask',
                     '--grammar-file', '../../data/designs/grammar_jan21.dot',
                     '--algo', 'ppo',
                     '--use-gae',
                     '--log-interval', '5',
                     '--num-steps', '1024',
                     '--num-processes', '2',  # æ¢å¤å¤šè¿›ç¨‹ä¾¿äºå¹¶è¡Œè®­ç»ƒ
                     '--lr', '3e-4',
                     '--entropy-coef', '0',
                     '--value-loss-coef', '0.5',
                     '--ppo-epoch', '10',
                     '--num-mini-batch', '32',
                     '--gamma', '0.995',
                     '--gae-lambda', '0.95',
                     '--num-env-steps', '30000000',
                     '--use-linear-lr-decay',
                     '--use-proper-time-limits',
                     '--save-interval', '100',
                     '--seed', '2',
                     '--save-dir', './trained_models/RobotLocomotion-v0/test/',
                     '--render-interval', '80']
        test_args = sys.argv[1:]
    
    parser = get_parser()
    args = parser.parse_args(args_list + test_args)

    solve_argv_conflict(args_list)
    parser = get_parser()
    args = parser.parse_args(args_list + test_args)

    args.cuda = not args.no_cuda and torch.cuda.is_available()

    args.save_dir = os.path.join(args.save_dir, get_time_stamp())
    try:
        os.makedirs(args.save_dir, exist_ok=True)
    except OSError:
        pass

    fp = open(os.path.join(args.save_dir, 'args.txt'), 'w')
    fp.write(str(args_list + test_args))
    fp.close()

    main(args)