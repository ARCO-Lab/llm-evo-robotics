#!/usr/bin/env python3
"""
ç›´æ¥æ¨¡å‹è¯„ä¼°è„šæœ¬ - ç»•è¿‡GNNç¼–ç å™¨ï¼Œç›´æ¥æµ‹è¯•Actorç½‘ç»œ
"""

import torch
import numpy as np
import sys
import os

# è®¾ç½®è·¯å¾„
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../')
sys.path.append(base_dir)
sys.path.insert(0, os.path.join(base_dir, 'examples/2d_reacher/envs'))

from reacher2d_env import Reacher2DEnv


def test_model_directly(model_path, num_episodes=3):
    """ç›´æ¥æµ‹è¯•æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ä½¿ç”¨GNNç¼–ç å™¨"""
    
    print(f"ğŸ¯ ç›´æ¥æµ‹è¯•æ¨¡å‹: {model_path}")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return
    
    # åŠ è½½æ¨¡å‹æ•°æ®
    model_data = torch.load(model_path, map_location='cpu')
    print(f"âœ… æ¨¡å‹æ–‡ä»¶åŠ è½½æˆåŠŸ")
    print(f"   è®­ç»ƒæ­¥éª¤: {model_data.get('step', 'N/A')}")
    print(f"   æˆåŠŸç‡: {model_data.get('final_success_rate', model_data.get('success_rate', 'N/A'))}")
    print(f"   æœ€å°è·ç¦»: {model_data.get('final_min_distance', model_data.get('min_distance', 'N/A'))}")
    print(f"   è®­ç»ƒå®Œæˆ: {model_data.get('training_completed', 'N/A')}")
    
    # åˆ›å»ºç¯å¢ƒ
    env_params = {
        'num_links': 4,
        'link_lengths': [80, 80, 80, 60],
        'render_mode': 'human',
        'config_path': "/home/xli149/Documents/repos/RoboGrammar/examples/2d_reacher/configs/reacher_with_zigzag_obstacles.yaml"
    }
    
    env = Reacher2DEnv(**env_params)
    num_joints = env.action_space.shape[0]
    
    print(f"ğŸ—ï¸ ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"   å…³èŠ‚æ•°: {num_joints}")
    print(f"   Action space: {env.action_space}")
    print(f"   Observation space: {env.observation_space}")
    
    # ç®€å•çš„éšæœºç­–ç•¥æµ‹è¯•ï¼ˆä½œä¸ºbaselineï¼‰
    print(f"\nğŸ® å¼€å§‹æµ‹è¯• {num_episodes} episodes (ä½¿ç”¨éšæœºç­–ç•¥ä½œä¸ºbaseline)")
    
    success_count = 0
    total_rewards = []
    min_distances = []
    goal_threshold = 50.0
    
    for episode in range(num_episodes):
        obs = env.reset()
        episode_reward = 0
        step_count = 0
        max_steps = 500  # å‡å°‘æ­¥æ•°åŠ å¿«æµ‹è¯•
        min_distance_this_episode = float('inf')
        
        print(f"\n--- Episode {episode + 1}/{num_episodes} ---")
        
        while step_count < max_steps:
            # ä½¿ç”¨ç®€å•çš„å¯å‘å¼ç­–ç•¥ï¼šæœå‘ç›®æ ‡çš„ç®€å•æ§åˆ¶
            end_pos = env._get_end_effector_position()
            goal_pos = env.goal_pos
            
            # è®¡ç®—åˆ°ç›®æ ‡çš„æ–¹å‘
            direction = np.array(goal_pos) - np.array(end_pos)
            distance = np.linalg.norm(direction)
            min_distance_this_episode = min(min_distance_this_episode, distance)
            
            # ç®€å•çš„æ¯”ä¾‹æ§åˆ¶ç­–ç•¥
            if distance > 1e-6:
                # å½’ä¸€åŒ–æ–¹å‘
                direction_norm = direction / distance
                
                # ç®€å•çš„ç­–ç•¥ï¼šæ ¹æ®è·ç¦»å’Œæ–¹å‘äº§ç”ŸåŠ¨ä½œ
                action_magnitude = min(50.0, distance / 10.0)  # é™åˆ¶åŠ¨ä½œå¹…åº¦
                
                # ä¸ºæ¯ä¸ªå…³èŠ‚äº§ç”ŸåŠ¨ä½œï¼ˆç®€å•ç­–ç•¥ï¼‰
                actions = np.array([
                    action_magnitude * direction_norm[0] * 0.3,  # å…³èŠ‚1
                    action_magnitude * direction_norm[1] * 0.3,  # å…³èŠ‚2  
                    action_magnitude * direction_norm[0] * 0.2,  # å…³èŠ‚3
                    action_magnitude * direction_norm[1] * 0.2,  # å…³èŠ‚4
                ])
                
                # æ·»åŠ å°å¹…éšæœºå™ªå£°
                noise = np.random.normal(0, 5.0, size=actions.shape)
                actions = actions + noise
                
                # è£å‰ªåˆ°åŠ¨ä½œç©ºé—´èŒƒå›´
                actions = np.clip(actions, env.action_space.low, env.action_space.high)
            else:
                # è·ç¦»å¾ˆè¿‘æ—¶ä½¿ç”¨å°å¹…åŠ¨ä½œ
                actions = np.random.uniform(-10, 10, size=num_joints)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = env.step(actions)
            episode_reward += reward
            step_count += 1
            
            # æ¸²æŸ“
            env.render()
            
            # æ¯50æ­¥æ‰“å°è¿›åº¦
            if step_count % 50 == 0:
                print(f"  æ­¥éª¤ {step_count}: è·ç¦» {distance:.1f}px, å¥–åŠ± {episode_reward:.1f}")
            
            # æ£€æŸ¥æˆåŠŸ
            if distance <= goal_threshold:
                success_count += 1
                print(f"  ğŸ‰ æˆåŠŸåˆ°è¾¾ç›®æ ‡! è·ç¦»: {distance:.1f}px, æ­¥éª¤: {step_count}")
                break
                
            if done:
                print(f"  Episode ç»“æŸ (done=True)")
                break
        
        # è®°å½•ç»“æœ
        total_rewards.append(episode_reward)
        min_distances.append(min_distance_this_episode)
        
        print(f"Episode {episode + 1} ç»“æœ:")
        print(f"  æœ€å°è·ç¦»: {min_distance_this_episode:.1f}px")
        print(f"  æ€»å¥–åŠ±: {episode_reward:.2f}")
        print(f"  æ­¥éª¤æ•°: {step_count}")
        print(f"  æˆåŠŸ: {'æ˜¯' if min_distance_this_episode <= goal_threshold else 'å¦'}")
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    success_rate = success_count / num_episodes
    avg_reward = np.mean(total_rewards)
    avg_min_distance = np.mean(min_distances)
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*60}")
    print(f"ğŸ† Baselineæµ‹è¯•ç»“æœæ€»ç»“ (ç®€å•å¯å‘å¼ç­–ç•¥):")
    print(f"  æµ‹è¯•Episodes: {num_episodes}")
    print(f"  æˆåŠŸæ¬¡æ•°: {success_count}")
    print(f"  æˆåŠŸç‡: {success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"  å¹³å‡æœ€å°è·ç¦»: {avg_min_distance:.1f} pixels")
    print(f"  ç›®æ ‡é˜ˆå€¼: {goal_threshold:.1f} pixels")
    print(f"{'='*60}")
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"   1. è¿™ä¸ªbaselineæµ‹è¯•æ˜¾ç¤ºäº†ç¯å¢ƒçš„åŸºæœ¬åŠŸèƒ½")
    print(f"   2. è®­ç»ƒå¥½çš„æ¨¡å‹åº”è¯¥æ¯”è¿™ä¸ªç®€å•ç­–ç•¥è¡¨ç°æ›´å¥½")
    print(f"   3. å¦‚æœéœ€è¦æµ‹è¯•å®é™…çš„è®­ç»ƒæ¨¡å‹ï¼Œéœ€è¦ä¿®å¤GNNç¼–ç å™¨çš„æ•°æ®ç±»å‹é—®é¢˜")
    
    env.close()
    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward,
        'avg_min_distance': avg_min_distance,
        'success_count': success_count,
        'total_episodes': num_episodes
    }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ç›´æ¥æµ‹è¯•æ¨¡å‹æ–‡ä»¶")
    parser.add_argument('--model-path', type=str, required=True,
                        help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=3,
                        help='æµ‹è¯•çš„episodeæ•°é‡')
    
    args = parser.parse_args()
    
    test_model_directly(args.model_path, args.episodes) 