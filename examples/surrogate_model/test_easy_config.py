#!/usr/bin/env python3
"""
æµ‹è¯•ç®€åŒ–é…ç½®çš„å¯è¾¾æ€§
"""

import numpy as np
import sys
import os

sys.path.insert(0, '/home/xli149/Documents/repos/test_robo/examples/2d_reacher/envs')
from reacher2d_env import Reacher2DEnv

def test_easy_config():
    """æµ‹è¯•ç®€åŒ–é…ç½®"""
    print("ğŸ¯ æµ‹è¯•ç®€åŒ–é…ç½®çš„å¯è¾¾æ€§")
    print("="*50)
    
    # ä½¿ç”¨3å…³èŠ‚ï¼Œæ¯ä¸ª80px = 240pxæ€»reach
    env = Reacher2DEnv(
        num_links=3,
        link_lengths=[80, 80, 80],
        render_mode=None,
        config_path="../2d_reacher/configs/reacher_easy.yaml"
    )
    
    obs = env.reset()
    initial_pos = env._get_end_effector_position()
    goal_pos = env.goal_pos
    initial_distance = np.linalg.norm(np.array(initial_pos) - goal_pos)
    max_reach = sum([80, 80, 80])
    
    print(f"ğŸ“ åˆå§‹ä½ç½®: ({initial_pos[0]:.1f}, {initial_pos[1]:.1f})")
    print(f"ğŸ¯ ç›®æ ‡ä½ç½®: ({goal_pos[0]:.1f}, {goal_pos[1]:.1f})")
    print(f"ğŸ“ åˆå§‹è·ç¦»: {initial_distance:.1f}")
    print(f"ğŸ¦¾ ç†è®ºæœ€å¤§reach: {max_reach}")
    print(f"ğŸ“Š å¯è¾¾æ€§: {'âœ… å¯è¾¾' if initial_distance <= max_reach else 'âŒ ä¸å¯è¾¾'}")
    
    if initial_distance <= max_reach:
        # å°è¯•é€šè¿‡ç®€å•ç­–ç•¥æ¥è¿‘ç›®æ ‡
        print("\nğŸ® æµ‹è¯•ç®€å•ç­–ç•¥èƒ½å¦æ¥è¿‘ç›®æ ‡...")
        
        best_distance = initial_distance
        success_steps = 0
        
        for attempt in range(20):
            env.reset()
            
            for step in range(100):
                current_pos = env._get_end_effector_position()
                direction = np.array(goal_pos) - np.array(current_pos)
                distance = np.linalg.norm(direction)
                
                if distance < 35:  # æˆåŠŸ
                    success_steps += 1
                    print(f"ğŸ‰ æˆåŠŸï¼ç¬¬{attempt+1}æ¬¡å°è¯•ï¼Œç¬¬{step+1}æ­¥è¾¾åˆ°ç›®æ ‡")
                    break
                
                # ç®€å•å¯å‘å¼ï¼šæœç›®æ ‡æ–¹å‘ç§»åŠ¨
                if direction[0] > 0:  # ç›®æ ‡åœ¨å³è¾¹
                    action = np.array([40, 20, 10])
                else:
                    action = np.array([-40, -20, -10])
                
                if direction[1] < 0:  # ç›®æ ‡åœ¨ä¸‹æ–¹ï¼Œå‡å°ç¬¬ä¸€ä¸ªå…³èŠ‚è§’åº¦
                    action[0] *= -1
                
                next_obs, reward, done, info = env.step(action)
                current_pos = env._get_end_effector_position()
                current_distance = np.linalg.norm(np.array(current_pos) - goal_pos)
                best_distance = min(best_distance, current_distance)
                
                if done:
                    break
        
        print(f"ğŸ“ˆ æœ€ä½³æ¥è¿‘è·ç¦»: {best_distance:.1f}")
        print(f"ğŸ“Š è·ç¦»æ”¹å–„: {initial_distance - best_distance:.1f}")
        print(f"ğŸ¯ æˆåŠŸæ¬¡æ•°: {success_steps}/20")
        
        if success_steps > 0:
            print("âœ… ç›®æ ‡ç¡®å®å¯è¾¾ï¼ç­–ç•¥å¯ä»¥å­¦ä¹ ï¼")
            return True
        elif best_distance < initial_distance * 0.6:
            print("âš ï¸  æ¥è¿‘æˆåŠŸï¼Œä½†éœ€è¦æ›´å¥½çš„ç­–ç•¥")
            return True
        else:
            print("âŒ ä»ç„¶æ— æ³•æœ‰æ•ˆæ¥è¿‘")
            return False
    else:
        print("âŒ ç›®æ ‡è¶…å‡ºç†è®ºå¯è¾¾èŒƒå›´")
        return False

def demonstrate_reward_learning():
    """æ¼”ç¤ºå¥–åŠ±å‡½æ•°æ˜¯å¦èƒ½æä¾›å­¦ä¹ ä¿¡å·"""
    print("\nğŸ§  æµ‹è¯•å¥–åŠ±å‡½æ•°å­¦ä¹ ä¿¡å·")
    print("="*50)
    
    env = Reacher2DEnv(
        num_links=3,
        link_lengths=[80, 80, 80],
        render_mode=None,
        config_path="../2d_reacher/configs/reacher_easy.yaml"
    )
    
    # æ”¶é›†ä¸åŒåŠ¨ä½œçš„å¥–åŠ±
    rewards_good = []  # æœç›®æ ‡çš„åŠ¨ä½œ
    rewards_bad = []   # è¿œç¦»ç›®æ ‡çš„åŠ¨ä½œ
    
    for test_round in range(10):
        obs = env.reset()
        current_pos = env._get_end_effector_position()
        goal_pos = env.goal_pos
        direction = np.array(goal_pos) - np.array(current_pos)
        
        # æµ‹è¯•æœå‘ç›®æ ‡çš„åŠ¨ä½œ
        if direction[0] > 0:
            good_action = np.array([30, 15, 5])
        else:
            good_action = np.array([-30, -15, -5])
        
        next_obs, reward_good, done, info = env.step(good_action)
        rewards_good.append(reward_good)
        
        # é‡ç½®å¹¶æµ‹è¯•è¿œç¦»ç›®æ ‡çš„åŠ¨ä½œ
        env.reset()
        bad_action = -good_action  # åæ–¹å‘
        next_obs, reward_bad, done, info = env.step(bad_action)
        rewards_bad.append(reward_bad)
    
    avg_good = np.mean(rewards_good)
    avg_bad = np.mean(rewards_bad)
    
    print(f"ğŸ“Š æœå‘ç›®æ ‡åŠ¨ä½œå¹³å‡å¥–åŠ±: {avg_good:.3f}")
    print(f"ğŸ“Š è¿œç¦»ç›®æ ‡åŠ¨ä½œå¹³å‡å¥–åŠ±: {avg_bad:.3f}")
    print(f"ğŸ“Š å¥–åŠ±å·®å¼‚: {avg_good - avg_bad:.3f}")
    
    if avg_good > avg_bad + 0.1:  # æ˜æ˜¾å·®å¼‚
        print("âœ… å¥–åŠ±å‡½æ•°æä¾›æ˜ç¡®å­¦ä¹ ä¿¡å·")
        return True
    else:
        print("âŒ å¥–åŠ±ä¿¡å·ä¸å¤Ÿæ˜ç¡®")
        return False

if __name__ == "__main__":
    success1 = test_easy_config()
    success2 = demonstrate_reward_learning()
    
    print("\n" + "="*60)
    print("ğŸ“‹ ç®€åŒ–é…ç½®æµ‹è¯•ç»“æœ")
    print("="*60)
    print(f"å¯è¾¾æ€§æµ‹è¯•: {'âœ… é€šè¿‡' if success1 else 'âŒ å¤±è´¥'}")
    print(f"å¥–åŠ±ä¿¡å·æµ‹è¯•: {'âœ… é€šè¿‡' if success2 else 'âŒ å¤±è´¥'}")
    
    if success1 and success2:
        print("ğŸ‰ ç®€åŒ–é…ç½®æœ‰æ•ˆï¼å¯ä»¥å¼€å§‹SACè®­ç»ƒæµ‹è¯•")
        print("ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨è¿™ä¸ªé…ç½®é‡æ–°è®­ç»ƒä½ çš„æ¨¡å‹")
    else:
        print("ğŸš¨ éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´")
