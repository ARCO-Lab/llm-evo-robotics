#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆè®­ç»ƒè„šæœ¬ - é‡æ„ç‰ˆ
ä»£ç ç»“æ„æ¸…æ™°ï¼ŒåŠŸèƒ½æ¨¡å—åŒ–ï¼Œä¿æŒæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
"""

import sys
import os
import time
import numpy as np
import torch
import argparse
import logging
from collections import deque

# === è·¯å¾„è®¾ç½® ===
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../')
sys.path.extend([
    base_dir,
    os.path.join(base_dir, 'examples/2d_reacher/envs'),
    os.path.join(base_dir, 'examples/surrogate_model/gnn_encoder'),
    os.path.join(base_dir, 'examples/rl/train'),
    os.path.join(base_dir, 'examples/rl/common'),
    os.path.join(base_dir, 'examples/rl/environments'),
    os.path.join(base_dir, 'examples/rl')
])

# === æ ¸å¿ƒå¯¼å…¥ ===
import gym
gym.logger.set_level(40)

if not hasattr(np, 'bool'):
    np.bool = bool

from training_logger import TrainingLogger, RealTimeMonitor
from gnn_encoder import GNN_Encoder
from attn_model.attn_model import AttnModel
from sac.sac_model import AttentionSACWithBuffer
from env_config.env_wrapper import make_reacher2d_vec_envs
from reacher2d_env import Reacher2DEnv

# RLç›¸å…³å¯¼å…¥
import environments
from common import *
from attn_dataset.sim_data_handler import DataHandler

# === é…ç½®å¸¸é‡ ===
SILENT_MODE = True
GOAL_THRESHOLD = 20.0
DEFAULT_CONFIG = {
    'num_links': 2,
    'link_lengths': [90,90],
    # 'config_path': "/home/xli149/Documents/repos/test_robo/examples/2d_reacher/configs/reacher_with_zigzag_obstacles.yaml"
    'config_path': None
}

# === è‡ªå®šä¹‰å‚æ•°è§£æå™¨ ===
def create_training_parser():
    """åˆ›å»ºè®­ç»ƒä¸“ç”¨çš„å‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='Enhanced SAC Training for Reacher2D')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--env-name', default='reacher2d', help='ç¯å¢ƒåç§°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--num-processes', type=int, default=1, help='å¹¶è¡Œè¿›ç¨‹æ•°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--gamma', type=float, default=0.99, help='æŠ˜æ‰£å› å­')
    parser.add_argument('--alpha', type=float, default=0.1, help='SACç†µç³»æ•°')
    parser.add_argument('--save-dir', default='./trained_models/reacher2d/enhanced_test/', help='ä¿å­˜ç›®å½•')
    
      # ğŸ†• æ·»åŠ æ¸²æŸ“æ§åˆ¶å‚æ•°
    parser.add_argument('--render', action='store_true', default=False, help='æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–çª—å£')
    parser.add_argument('--no-render', action='store_true', default=False, help='å¼ºåˆ¶ç¦ç”¨å¯è§†åŒ–çª—å£')
    # SACç‰¹å®šå‚æ•°
    parser.add_argument('--tau', type=float, default=0.005, help='è½¯æ›´æ–°å‚æ•°')
    parser.add_argument('--warmup-steps', type=int, default=1000, help='çƒ­èº«æ­¥æ•°')
    parser.add_argument('--target-entropy-factor', type=float, default=0.8, help='ç›®æ ‡ç†µç³»æ•°')
    parser.add_argument('--update-frequency', type=int, default=2, help='ç½‘ç»œæ›´æ–°é¢‘ç‡')
    parser.add_argument('--batch-size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--buffer-capacity', type=int, default=10000, help='ç¼“å†²åŒºå®¹é‡')
    
    # æ¢å¤è®­ç»ƒå‚æ•°
    parser.add_argument('--resume-checkpoint', type=str, default=None, help='æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--resume-lr', type=float, default=None, help='æ¢å¤æ—¶çš„å­¦ä¹ ç‡')
    parser.add_argument('--resume-alpha', type=float, default=None, help='æ¢å¤æ—¶çš„alphaå€¼')
    
    # MAP-Elitesæœºå™¨äººé…ç½®å‚æ•°
    parser.add_argument('--num-joints', type=int, default=3, help='æœºå™¨äººå…³èŠ‚æ•°é‡')
    parser.add_argument('--link-lengths', nargs='+', type=float, default=[90.0, 90.0, 90.0], help='æœºå™¨äººé“¾èŠ‚é•¿åº¦')
    parser.add_argument('--total-steps', type=int, default=10000, help='æ€»è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--individual-id', type=str, default='', help='MAP-Elitesä¸ªä½“ID')
    parser.add_argument('--generation', type=int, default=0, help='å½“å‰è¿›åŒ–ä»£æ•°')
    
    # å…¼å®¹æ€§å‚æ•°ï¼ˆç”¨äºå…¶ä»–ç¯å¢ƒï¼‰
    parser.add_argument('--grammar-file', type=str, default='/home/xli149/Documents/repos/RoboGrammar/data/designs/grammar_jan21.dot', help='è¯­æ³•æ–‡ä»¶')
    parser.add_argument('--rule-sequence', nargs='+', default=['0'], help='è§„åˆ™åºåˆ—')
    parser.add_argument('--no-cuda', action='store_true', default=False, help='ç¦ç”¨CUDA')
    
    return parser

# === å·¥å…·å‡½æ•° ===
def smart_print(*args, **kwargs):
    """æ™ºèƒ½æ‰“å°å‡½æ•°"""
    if not SILENT_MODE:
        print(*args, **kwargs)

def get_time_stamp():
    """è·å–æ—¶é—´æˆ³"""
    return time.strftime('%m-%d-%Y-%H-%M-%S')

# === æ¨¡å‹ç®¡ç†å™¨ ===
class ModelManager:
    """æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç®¡ç†å™¨"""
    
    def __init__(self, save_dir):
        self.save_dir = save_dir
        self.best_models_dir = os.path.join(save_dir, 'best_models')
        os.makedirs(self.best_models_dir, exist_ok=True)
    
    def save_best_model(self, sac, success_rate, min_distance, step):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        try:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            
            model_data = {
                'step': step,
                'success_rate': success_rate,
                'min_distance': min_distance,
                'timestamp': timestamp,
                'actor_state_dict': sac.actor.state_dict(),
                'critic1_state_dict': sac.critic1.state_dict(),
                'critic2_state_dict': sac.critic2.state_dict(),
                'target_critic1_state_dict': sac.target_critic1.state_dict(),
                'target_critic2_state_dict': sac.target_critic2.state_dict(),
                'actor_optimizer_state_dict': sac.actor_optimizer.state_dict(),
                'critic_optimizer_state_dict': sac.critic_optimizer.state_dict(),
                'alpha_optimizer_state_dict': sac.alpha_optimizer.state_dict(),
                'alpha': sac.alpha.item() if torch.is_tensor(sac.alpha) else sac.alpha,
                'log_alpha': sac.log_alpha.item() if torch.is_tensor(sac.log_alpha) else sac.log_alpha,
            }
            
            model_file = os.path.join(self.best_models_dir, f'best_model_step_{step}_{timestamp}.pth')
            torch.save(model_data, model_file)
            
            latest_file = os.path.join(self.best_models_dir, 'latest_best_model.pth')
            torch.save(model_data, latest_file)
            
            print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹: æˆåŠŸç‡ {success_rate:.3f}, è·ç¦» {min_distance:.1f}, æ­¥éª¤ {step}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    # ä¿®å¤ save_checkpoint æ–¹æ³• (ç¬¬145-159è¡Œ)
    # def save_checkpoint(self, sac, step, **kwargs):
    #     """ä¿å­˜æ£€æŸ¥ç‚¹ - å®Œæ•´ç‰ˆ"""
    #     try:
    #         checkpoint_data = {
    #             'step': step,
    #             'actor_state_dict': sac.actor.state_dict(),
    #             'critic1_state_dict': sac.critic1.state_dict(),
    #             'critic2_state_dict': sac.critic2.state_dict(),
    #             'target_critic1_state_dict': sac.target_critic1.state_dict(),
    #             'target_critic2_state_dict': sac.target_critic2.state_dict(),
    #             # ğŸ”§ æ·»åŠ ä¼˜åŒ–å™¨çŠ¶æ€
    #             'actor_optimizer_state_dict': sac.actor_optimizer.state_dict(),
    #             'critic_optimizer_state_dict': sac.critic_optimizer.state_dict(),
    #             'alpha_optimizer_state_dict': sac.alpha_optimizer.state_dict(),
    #             # ğŸ”§ æ·»åŠ alphaå€¼
    #             'alpha': sac.alpha.item() if torch.is_tensor(sac.alpha) else sac.alpha,
    #             'log_alpha': sac.log_alpha.item() if torch.is_tensor(sac.log_alpha) else sac.log_alpha,
    #             # ğŸ”§ æ·»åŠ è®­ç»ƒçŠ¶æ€
    #             'buffer_size': len(sac.memory),
    #             'warmup_steps': sac.warmup_steps,
    #             **kwargs
    #         }
            
    #         checkpoint_path = os.path.join(self.best_models_dir, f'checkpoint_step_{step}.pth')
    #         torch.save(checkpoint_data, checkpoint_path)
    #         print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: step {step}, buffer size: {len(sac.memory)}")
    #         return True
    #     except Exception as e:
    #         print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    #         return False
    def save_checkpoint(self, sac, step, **kwargs):
        """æ™ºèƒ½ä¿å­˜æ£€æŸ¥ç‚¹ - åªä¿å­˜æœ€ä½³æ¨¡å‹"""
        try:
            current_best_distance = kwargs.get('current_best_distance', float('inf'))
            best_min_distance = kwargs.get('best_min_distance', float('inf'))
            
            # åªæœ‰åœ¨æ€§èƒ½æ”¹å–„æ—¶æ‰ä¿å­˜
            if current_best_distance < best_min_distance:
                success_rate = kwargs.get('best_success_rate', 0.0)
                print(f"ğŸ† å‘ç°æ›´å¥½æ€§èƒ½ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹: {current_best_distance:.1f}px")
                return self.save_best_model(sac, success_rate, current_best_distance, step)
            else:
                print(f"â­ï¸  æ€§èƒ½æœªæ”¹å–„ ({current_best_distance:.1f}px >= {best_min_distance:.1f}px)ï¼Œè·³è¿‡ä¿å­˜")
                return False
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False
    # ä¿®å¤ save_final_model æ–¹æ³• (ç¬¬161-176è¡Œ)
    def save_final_model(self, sac, step, **kwargs):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹ - å®Œæ•´ç‰ˆ"""
        try:
            final_model_data = {
                'step': step,
                'training_completed': True,
                'actor_state_dict': sac.actor.state_dict(),
                'critic1_state_dict': sac.critic1.state_dict(),
                'critic2_state_dict': sac.critic2.state_dict(),
                'target_critic1_state_dict': sac.target_critic1.state_dict(),
                'target_critic2_state_dict': sac.target_critic2.state_dict(),
                # ğŸ”§ æ·»åŠ ä¼˜åŒ–å™¨çŠ¶æ€
                'actor_optimizer_state_dict': sac.actor_optimizer.state_dict(),
                'critic_optimizer_state_dict': sac.critic_optimizer.state_dict(),
                'alpha_optimizer_state_dict': sac.alpha_optimizer.state_dict(),
                # ğŸ”§ æ·»åŠ alphaå€¼
                'alpha': sac.alpha.item() if torch.is_tensor(sac.alpha) else sac.alpha,
                'log_alpha': sac.log_alpha.item() if torch.is_tensor(sac.log_alpha) else sac.log_alpha,
                **kwargs
            }
            
            final_path = os.path.join(self.best_models_dir, f'final_model_step_{step}.pth')
            torch.save(final_model_data, final_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_path}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æœ€ç»ˆæ¨¡å‹å¤±è´¥: {e}")
            return False

    # ä¿®å¤ load_checkpoint æ–¹æ³• (ç¬¬178-226è¡Œ)
    def load_checkpoint(self, sac, checkpoint_path, device='cpu'):
        """åŠ è½½æ£€æŸ¥ç‚¹ - å¢å¼ºç‰ˆ"""
        try:
            if not os.path.exists(checkpoint_path):
                print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
                return 0
            
            print(f"ğŸ”„ Loading checkpoint from: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device)
            
            print(f"ğŸ“‹ Checkpoint contains: {list(checkpoint.keys())}")
            
            # ğŸ”§ åŠ è½½ç½‘ç»œçŠ¶æ€ - å¢å¼ºé”™è¯¯å¤„ç†
            networks = ['actor', 'critic1', 'critic2', 'target_critic1', 'target_critic2']
            for network_name in networks:
                state_dict_key = f'{network_name}_state_dict'
                if state_dict_key in checkpoint:
                    try:
                        network = getattr(sac, network_name)
                        state_dict = checkpoint[state_dict_key]
                        
                        # ğŸ”§ å¤„ç†proj_dicté—®é¢˜
                        if 'proj_dict' in str(state_dict.keys()):
                            print(f"âš ï¸ {network_name} åŒ…å«proj_dictï¼Œä½¿ç”¨strict=FalseåŠ è½½")
                            missing_keys, unexpected_keys = network.load_state_dict(state_dict, strict=False)
                            if unexpected_keys:
                                print(f"   å¿½ç•¥çš„é”®: {unexpected_keys[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
                        else:
                            network.load_state_dict(state_dict)
                        
                        print(f"âœ… {network_name} loaded")
                    except Exception as e:
                        print(f"âš ï¸ {network_name} åŠ è½½å¤±è´¥: {e}")
                        print("   å°†è·³è¿‡è¯¥ç½‘ç»œï¼Œä½¿ç”¨åˆå§‹åŒ–æƒé‡")
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ° {state_dict_key}")
            
            # ğŸ”§ åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ - å¯é€‰åŠ è½½
            load_optimizers = True  # å¯ä»¥è®¾ä¸ºFalseå¦‚æœæƒ³ç”¨æ–°çš„å­¦ä¹ ç‡
            if load_optimizers:
                optimizers = ['actor_optimizer', 'critic_optimizer', 'alpha_optimizer']
                for opt_name in optimizers:
                    opt_key = f'{opt_name}_state_dict'
                    if opt_key in checkpoint:
                        try:
                            optimizer = getattr(sac, opt_name)
                            optimizer.load_state_dict(checkpoint[opt_key])
                            print(f"âœ… {opt_name} loaded")
                        except Exception as e:
                            print(f"âš ï¸ {opt_name} åŠ è½½å¤±è´¥: {e}")
                            print("   å°†ä½¿ç”¨å½“å‰ä¼˜åŒ–å™¨çŠ¶æ€")
            
            # ğŸ”§ åŠ è½½alphaå€¼
            if 'alpha' in checkpoint:
                try:
                    alpha_val = checkpoint['alpha']
                    if isinstance(alpha_val, (int, float)):
                        sac.alpha = alpha_val
                    else:
                        sac.alpha = alpha_val.item()
                    print(f"âœ… Alpha loaded: {sac.alpha}")
                except Exception as e:
                    print(f"âš ï¸ Alpha åŠ è½½å¤±è´¥: {e}")
                    
            if 'log_alpha' in checkpoint:
                try:
                    log_alpha_val = checkpoint['log_alpha']
                    if isinstance(log_alpha_val, (int, float)):
                        sac.log_alpha.data.fill_(log_alpha_val)
                    else:
                        sac.log_alpha.data.fill_(log_alpha_val.item())
                    print(f"âœ… Log Alpha loaded: {sac.log_alpha.item()}")
                except Exception as e:
                    print(f"âš ï¸ Log Alpha åŠ è½½å¤±è´¥: {e}")
            
            # ğŸ”§ æ˜¾ç¤ºé¢å¤–ä¿¡æ¯
            start_step = checkpoint.get('step', 0)
            buffer_size = checkpoint.get('buffer_size', 'N/A')
            warmup_steps = checkpoint.get('warmup_steps', 'N/A')
            
            print(f"âœ… Checkpoint loaded successfully!")
            print(f"   Starting step: {start_step}")
            print(f"   Buffer size: {buffer_size}")
            print(f"   Warmup steps: {warmup_steps}")
            
            return start_step
            
        except Exception as e:
            print(f"âŒ Failed to load checkpoint: {e}")
            import traceback
            traceback.print_exc()
            print("Training will start from scratch...")
            return 0

# === ç¯å¢ƒè®¾ç½®ç®¡ç†å™¨ ===
class EnvironmentSetup:
    """ç¯å¢ƒè®¾ç½®ç®¡ç†å™¨"""
    
    @staticmethod
    def create_reacher2d_env(args):
        """åˆ›å»ºReacher2Dç¯å¢ƒ"""
        # è·å–ç¯å¢ƒé…ç½®
        if hasattr(args, 'num_joints') and hasattr(args, 'link_lengths'):
            num_links = args.num_joints
            link_lengths = args.link_lengths
            print(f"ğŸ¤– ä½¿ç”¨MAP-Elitesé…ç½®: {num_links}å…³èŠ‚, é•¿åº¦={link_lengths}")
        else:
            num_links = DEFAULT_CONFIG['num_links']
            link_lengths = DEFAULT_CONFIG['link_lengths']
            print(f"ğŸ¤– ä½¿ç”¨é»˜è®¤é…ç½®: {num_links}å…³èŠ‚, é•¿åº¦={link_lengths}")


        should_render = True  # ğŸ”§ é»˜è®¤å¯ç”¨æ¸²æŸ“
        if hasattr(args, 'render') and args.render:
            should_render = True
        elif hasattr(args, 'no_render') and args.no_render:
            should_render = False
        else:
            should_render = True  # ğŸ”§ é»˜è®¤å¯ç”¨æ¸²æŸ“
            print("ğŸ¨ æ¸²æŸ“è®¾ç½®: é»˜è®¤å¯ç”¨")

        env_params = {
            'num_links': num_links,
            'link_lengths': link_lengths,
            'render_mode': 'human' if should_render and args.num_processes == 1 else None,
            'config_path': DEFAULT_CONFIG['config_path']
        }
        
        print(f"ç¯å¢ƒå‚æ•°: {env_params}")
        print(f"ğŸ¨ æ¸²æŸ“è®¾ç½®: {'å¯ç”¨' if should_render else 'ç¦ç”¨'}")
        # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
        envs = make_reacher2d_vec_envs(
            env_params=env_params,
            seed=args.seed,
            num_processes=args.num_processes,
            gamma=args.gamma,
            log_dir=None,
            device=torch.device('cpu'),
            allow_early_resets=False,
        )
        
        # ğŸ”§ ä¸ä½¿ç”¨sync_envï¼Œè®©è®­ç»ƒç¯å¢ƒç›´æ¥æ¸²æŸ“
        sync_env = None
        if should_render:
            print(f"âœ… è®­ç»ƒç¯å¢ƒå·²åˆ›å»ºï¼ˆè¿›ç¨‹æ•°: {args.num_processes}ï¼Œç›´æ¥æ¸²æŸ“ï¼‰")
            # ç¡®ä¿ç¬¬ä¸€ä¸ªç¯å¢ƒæœ‰æ¸²æŸ“æ¨¡å¼
            if hasattr(envs, 'envs') and len(envs.envs) > 0:
                if not hasattr(envs.envs[0], 'render_mode') or envs.envs[0].render_mode != 'human':
                    envs.envs[0].render_mode = 'human'
                    print(f"ğŸ”§ è®¾ç½®ç¬¬ä¸€ä¸ªè®­ç»ƒç¯å¢ƒä¸ºæ¸²æŸ“æ¨¡å¼")
                # å¼ºåˆ¶åˆå§‹åŒ–æ¸²æŸ“
                if hasattr(envs.envs[0], '_init_rendering'):
                    envs.envs[0]._init_rendering()
                    print(f"ğŸ¨ åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒæ¸²æŸ“")
                # å¼ºåˆ¶ç¬¬ä¸€æ¬¡æ¸²æŸ“ä»¥æ˜¾ç¤ºçª—å£
                try:
                    envs.envs[0].render()
                    print(f"ğŸ–¼ï¸ å¼ºåˆ¶ç¬¬ä¸€æ¬¡æ¸²æŸ“ï¼Œæ˜¾ç¤ºpygameçª—å£")
                except Exception as e:
                    print(f"âš ï¸ æ¸²æŸ“åˆå§‹åŒ–é”™è¯¯: {e}")
        else:
            print(f"âœ… è®­ç»ƒç¯å¢ƒå·²åˆ›å»ºï¼ˆè¿›ç¨‹æ•°: {args.num_processes}ï¼Œæ— æ¸²æŸ“ï¼‰")
            
        return envs, sync_env, env_params

# === è®­ç»ƒç®¡ç†å™¨ ===
class TrainingManager:
    """è®­ç»ƒè¿‡ç¨‹ç®¡ç†å™¨"""
    
    def __init__(self, args, sac, logger, model_manager):
        self.args = args
        self.sac = sac
        self.logger = logger
        self.model_manager = model_manager
        
        # è®­ç»ƒçŠ¶æ€
        self.best_success_rate = 0.0
        self.best_min_distance = float('inf')
        self.consecutive_success_count = 0
        self.min_consecutive_successes = 2

                # ğŸ†• Episodesæ§åˆ¶ - 2ä¸ªepisodes Ã— 120kæ­¥
        self.current_episodes = 0
        self.max_episodes = 2
        self.steps_per_episode = 120000
        self.current_episode_steps = 0
        self.total_training_steps = 0
        self.episode_results = []
        self.current_episode_start_step = 0
        self.current_episode_start_time = time.time()
        
        # ğŸ†• è¿½è¸ªæ¯ä¸ªepisodeçš„æœ€ä½³è¡¨ç°
        self.current_episode_best_distance = float('inf')
        self.current_episode_best_reward = float('-inf')
        self.current_episode_min_distance_step = 0
        
        print(f"ğŸ¯ è®­ç»ƒé…ç½®: 2ä¸ªepisodes Ã— 120,000æ­¥/episode = æ€»è®¡240,000æ­¥")

    def update_episode_tracking(self, episode_step, infos, episode_rewards):
        """åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æ›´æ–°episodeè¿½è¸ª"""
        for proc_id in range(len(infos)):
            if len(infos) > proc_id and isinstance(infos[proc_id], dict):
                info = infos[proc_id]
                
                # æå–å½“å‰è·ç¦»
                current_distance = float('inf')
                if 'goal' in info:
                    current_distance = info['goal'].get('distance_to_goal', float('inf'))
                elif 'distance' in info:
                    current_distance = info['distance']
                
                # æ›´æ–°æœ€ä½³è·ç¦»
                if current_distance < self.current_episode_best_distance:
                    self.current_episode_best_distance = current_distance
                    self.current_episode_best_reward = episode_rewards[proc_id]
                    self.current_episode_min_distance_step = episode_step  # ğŸ”§ ç›´æ¥ä½¿ç”¨episode_step
            
    # def update_episode_tracking(self, step, infos, episode_rewards):
    #     """åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æ›´æ–°episodeè¿½è¸ª"""
    #     for proc_id in range(len(infos)):
    #         if len(infos) > proc_id and isinstance(infos[proc_id], dict):
    #             info = infos[proc_id]
                
    #             # æå–å½“å‰è·ç¦»
    #             current_distance = float('inf')
    #             if 'goal' in info:
    #                 current_distance = info['goal'].get('distance_to_goal', float('inf'))
    #             elif 'distance' in info:
    #                 current_distance = info['distance']
                
    #             # æ›´æ–°æœ€ä½³è·ç¦»
    #             if current_distance < self.current_episode_best_distance:
    #                 self.current_episode_best_distance = current_distance
    #                 self.current_episode_best_reward = episode_rewards[proc_id]
    #                 self.current_episode_min_distance_step = step - self.current_episode_start_step

    def _classify_episode_result(self, goal_reached, distance, steps, reward):
        """åˆ†ç±»episodeç»“æœ"""
        if goal_reached:
            if steps < 200:
                return {
                    'type': 'PERFECT_SUCCESS',
                    'score': 1.0,
                    'description': 'å¿«é€Ÿç²¾ç¡®åˆ°è¾¾'
                }
            elif steps < 400:
                return {
                    'type': 'GOOD_SUCCESS', 
                    'score': 0.9,
                    'description': 'æ­£å¸¸é€Ÿåº¦åˆ°è¾¾'
                }
            else:
                return {
                    'type': 'SLOW_SUCCESS',
                    'score': 0.7,
                    'description': 'ç¼“æ…¢ä½†æˆåŠŸåˆ°è¾¾'
                }
        else:
            if distance < 50:
                return {
                    'type': 'NEAR_SUCCESS',
                    'score': 0.5,
                    'description': 'æ¥è¿‘æˆåŠŸ'
                }
            elif distance < 100:
                return {
                    'type': 'TIMEOUT_CLOSE',
                    'score': 0.3,
                    'description': 'è¶…æ—¶ä½†è¾ƒæ¥è¿‘'
                }
            elif reward > -100:
                return {
                    'type': 'TIMEOUT_MEDIUM',
                    'score': 0.2,
                    'description': 'è¶…æ—¶ä¸­ç­‰è¡¨ç°'
                }
            else:
                return {
                    'type': 'COMPLETE_FAILURE',
                    'score': 0.0,
                    'description': 'å®Œå…¨å¤±è´¥'
                }

    def _check_episode_stopping_conditions(self, step):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ"""
        # å®Œæˆ2ä¸ªepisodeså°±åœæ­¢
        if self.current_episodes >= 2:
            print(f"ğŸ å®Œæˆ{self.current_episodes}ä¸ªepisodesï¼Œè®­ç»ƒç»“æŸ")
            return True
        
        # æ£€æŸ¥å½“å‰episodeæ­¥æ•°é™åˆ¶
        episode_steps = step - self.current_episode_start_step
        if episode_steps >= 120000:
            print(f"â° å½“å‰episodeè¾¾åˆ°120,000æ­¥é™åˆ¶")
            return False  # ä¸æ˜¯æ•´ä½“ç»“æŸï¼Œåªæ˜¯å½“å‰episodeç»“æŸ
        
        return False

    def _generate_final_fitness_report(self):
        """ç”Ÿæˆæœ€ç»ˆfitnessæŠ¥å‘Š"""
        if len(self.episode_results) == 0:
            print("âš ï¸ æ²¡æœ‰episodeç»“æœæ•°æ®")
            return
        
        print("\n" + "="*50)
        print("ğŸ¯ æœ€ç»ˆè®­ç»ƒç»“æœæŠ¥å‘Š")
        print("="*50)
        
        # è®¡ç®—ç»¼åˆæŒ‡æ ‡ï¼ˆåŸºäºæœ€ä½³è·ç¦»ï¼‰
        success_count = sum(1 for ep in self.episode_results if ep['success'])
        success_rate = success_count / len(self.episode_results)
        avg_best_distance = sum(ep['best_distance'] for ep in self.episode_results) / len(self.episode_results)
        avg_end_distance = sum(ep['end_distance'] for ep in self.episode_results) / len(self.episode_results)
        avg_steps = sum(ep['steps'] for ep in self.episode_results) / len(self.episode_results)
        avg_score = sum(ep['score'] for ep in self.episode_results) / len(self.episode_results)
        
        print(f"ğŸ“Š Episodeså®Œæˆ: {len(self.episode_results)}/2")
        print(f"ğŸ¯ æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{len(self.episode_results)})")
        print(f"ğŸ† å¹³å‡æœ€ä½³è·ç¦»: {avg_best_distance:.1f}px")
        print(f"ğŸ“ å¹³å‡ç»“æŸè·ç¦»: {avg_end_distance:.1f}px")
        print(f"â±ï¸  å¹³å‡æ­¥æ•°: {avg_steps:.0f}")
        print(f"â­ å¹³å‡å¾—åˆ†: {avg_score:.2f}")
        
        # è¯¦ç»†episodeä¿¡æ¯
        print("\nğŸ“‹ è¯¦ç»†Episodeç»“æœ:")
        for i, ep in enumerate(self.episode_results, 1):
            status = "âœ…" if ep['success'] else "âŒ"
            print(f"   Episode {i}: {status} {ep['type']} - "
                  f"æœ€ä½³è·ç¦»:{ep['best_distance']:.1f}px@{ep['best_distance_step']}æ­¥, "
                  f"ç»“æŸè·ç¦»:{ep['end_distance']:.1f}px, "
                  f"å¾—åˆ†:{ep['score']:.2f}")
        
        print("="*50)   

    # def handle_episode_end(self, proc_id, step, episode_rewards, infos):
    #     """å¤„ç†episodeç»“æŸé€»è¾‘ - ä½¿ç”¨æœ€ä½³è·ç¦»ç‰ˆæœ¬"""
    #     if len(infos) <= proc_id or not isinstance(infos[proc_id], dict):
    #         episode_rewards[proc_id] = 0.0
    #         return False
        
    #     info = infos[proc_id]
        
    #     # ğŸ¯ è®¡ç®—episodeè¯¦ç»†ä¿¡æ¯
    #     episode_steps = step - self.current_episode_start_step
    #     episode_duration = time.time() - self.current_episode_start_time if hasattr(self, 'current_episode_start_time') else 0
    #     episode_reward = episode_rewards[proc_id]
        
    #     # ğŸ¯ ä½¿ç”¨æœ€ä½³è·ç¦»è€Œä¸æ˜¯ç»“æŸæ—¶è·ç¦»
    #     best_distance = self.current_episode_best_distance
    #     best_reward = self.current_episode_best_reward
    #     goal_reached = best_distance < 20.0
        
    #     # è·å–ç»“æŸæ—¶çš„è·ç¦»ç”¨äºå¯¹æ¯”
    #     end_distance = float('inf')
    #     if 'goal' in info:
    #         end_distance = info['goal'].get('distance_to_goal', float('inf'))
    #     elif 'distance' in info:
    #         end_distance = info['distance']
        
    #     # ğŸ¯ åˆ†ç±»episodeç»“æœï¼ˆåŸºäºæœ€ä½³è·ç¦»ï¼‰
    #     episode_type = self._classify_episode_result(goal_reached, best_distance, episode_steps, best_reward)
        
    #     # å­˜å‚¨episodeç»“æœ
    #     episode_result = {
    #         'episode_num': self.current_episodes + 1,
    #         'type': episode_type['type'],
    #         'success': goal_reached,
    #         'best_distance': best_distance,      # ğŸ¯ æœ€ä½³è·ç¦»
    #         'end_distance': end_distance,        # ğŸ¯ ç»“æŸè·ç¦»
    #         'best_distance_step': self.current_episode_min_distance_step,  # ğŸ¯ è¾¾åˆ°æœ€ä½³è·ç¦»çš„æ­¥æ•°
    #         'steps': episode_steps,
    #         'duration': episode_duration,
    #         'reward': episode_reward,
    #         'best_reward': best_reward,          # ğŸ¯ è¾¾åˆ°æœ€ä½³è·ç¦»æ—¶çš„å¥–åŠ±
    #         'score': episode_type['score'],
    #         'description': episode_type['description']
    #     }
        
    #     self.episode_results.append(episode_result)
    #     self.current_episodes += 1
        
    #     # ğŸ¯ æ‰“å°episodeç»“æœï¼ˆæ˜¾ç¤ºæœ€ä½³è·ç¦»ï¼‰
    #     print(f"ğŸ“Š Episode {self.current_episodes}/2 å®Œæˆ:")
    #     print(f"   ç±»å‹: {episode_type['type']} ({episode_type['description']})")
    #     print(f"   æˆåŠŸ: {'âœ…' if goal_reached else 'âŒ'}")
    #     print(f"   æœ€ä½³è·ç¦»: {best_distance:.1f}px (æ­¥æ•°: {self.current_episode_min_distance_step})")
    #     print(f"   ç»“æŸè·ç¦»: {end_distance:.1f}px")
    #     print(f"   æ€»æ­¥æ•°: {episode_steps}")
    #     print(f"   æœ€ç»ˆå¥–åŠ±: {episode_reward:.2f}")
    #     print(f"   å¾—åˆ†: {episode_type['score']:.2f}")
        
    #     # ğŸ¯ é‡ç½®episodeè¿½è¸ª
    #     self.current_episode_best_distance = float('inf')
    #     self.current_episode_best_reward = float('-inf')
    #     self.current_episode_min_distance_step = 0
    #     self.current_episode_start_step = step
    #     self.current_episode_start_time = time.time()
        
    #     # æ£€æŸ¥åœæ­¢æ¡ä»¶
    #     should_stop = self._check_episode_stopping_conditions(step)
    #     if should_stop:
    #         self._generate_final_fitness_report()
        
    #     episode_rewards[proc_id] = 0.0
    #     return should_stop
   # åœ¨ç¬¬547è¡Œå·¦å³ï¼Œä¿®æ”¹handle_episode_endæ–¹æ³•ï¼š

    def handle_episode_end(self, proc_id, step, episode_rewards, infos):
        """å¤„ç†episodeç»“æŸé€»è¾‘ - æ”¯æŒç»´æŒæ£€æŸ¥"""
        if len(infos) <= proc_id or not isinstance(infos[proc_id], dict):
            episode_rewards[proc_id] = 0.0
            return False
        
        info = infos[proc_id]
        
        # ğŸ¯ è®¡ç®—episodeè¯¦ç»†ä¿¡æ¯
        episode_steps = step - self.current_episode_start_step
        episode_duration = time.time() - self.current_episode_start_time if hasattr(self, 'current_episode_start_time') else 0
        episode_reward = episode_rewards[proc_id]
        
        # ğŸ¯ ä½¿ç”¨æœ€ä½³è·ç¦»è€Œä¸æ˜¯ç»“æŸæ—¶è·ç¦»
        best_distance = self.current_episode_best_distance
        best_reward = self.current_episode_best_reward
        goal_reached = best_distance < 20.0
        
        # ğŸ†• æ£€æŸ¥ç»´æŒå®Œæˆæƒ…å†µ
        maintain_completed = False
        maintain_counter = 0
        maintain_target = 500
        
        # å°è¯•ä»ç¯å¢ƒè·å–ç»´æŒä¿¡æ¯
        try:
            # è¿™é‡Œéœ€è¦è®¿é—®å®é™…çš„ç¯å¢ƒå®ä¾‹æ¥è·å–ç»´æŒä¿¡æ¯
            # ç”±äºæˆ‘ä»¬åœ¨è®­ç»ƒå¾ªç¯ä¸­å·²ç»æ£€æŸ¥äº†ï¼Œè¿™é‡Œä¸»è¦æ˜¯è®°å½•
            maintain_completed = info.get('maintain_completed', False)
            maintain_counter = info.get('maintain_counter', 0)
        except:
            pass
        
        # è·å–ç»“æŸæ—¶çš„è·ç¦»ç”¨äºå¯¹æ¯”
        end_distance = float('inf')
        if 'goal' in info:
            end_distance = info['goal'].get('distance_to_goal', float('inf'))
        elif 'distance' in info:
            end_distance = info['distance']
        
        # ğŸ¯ åˆ†ç±»episodeç»“æœï¼ˆåŸºäºç»´æŒå®Œæˆæƒ…å†µï¼‰
        if maintain_completed:
            episode_type = {
                'type': 'MAINTAIN_SUCCESS',
                'score': 1.0,
                'description': f'æˆåŠŸç»´æŒ{maintain_counter}æ­¥'
            }
            goal_reached = True  # ç»´æŒå®Œæˆç®—ä½œæˆåŠŸ
        else:
            episode_type = self._classify_episode_result(goal_reached, best_distance, episode_steps, best_reward)
        
        # å­˜å‚¨episodeç»“æœ
        episode_result = {
            'episode_num': self.current_episodes + 1,
            'type': episode_type['type'],
            'success': maintain_completed or goal_reached,  # ğŸ†• ç»´æŒå®Œæˆæˆ–åˆ°è¾¾ç›®æ ‡éƒ½ç®—æˆåŠŸ
            'maintain_completed': maintain_completed,  # ğŸ†• ç»´æŒå®Œæˆæ ‡å¿—
            'maintain_counter': maintain_counter,  # ğŸ†• ç»´æŒæ­¥æ•°
            'best_distance': best_distance,
            'end_distance': end_distance,
            'best_distance_step': self.current_episode_min_distance_step,
            'steps': episode_steps,
            'duration': episode_duration,
            'reward': episode_reward,
            'best_reward': best_reward,
            'score': episode_type['score'],
            'description': episode_type['description']
        }
        
        self.episode_results.append(episode_result)
        self.current_episodes += 1
        
        # ğŸ¯ æ‰“å°episodeç»“æœï¼ˆæ˜¾ç¤ºç»´æŒä¿¡æ¯ï¼‰
        print(f"ğŸ“Š Episode {self.current_episodes}/2 å®Œæˆ:")
        print(f"   ç±»å‹: {episode_type['type']} ({episode_type['description']})")
        print(f"   æˆåŠŸ: {'âœ…' if episode_result['success'] else 'âŒ'}")
        if maintain_completed:
            print(f"   ğŸŠ ç»´æŒå®Œæˆ: {maintain_counter}/{maintain_target} æ­¥")
        print(f"   æœ€ä½³è·ç¦»: {best_distance:.1f}px (æ­¥æ•°: {self.current_episode_min_distance_step})")
        print(f"   ç»“æŸè·ç¦»: {end_distance:.1f}px")
        print(f"   æ€»æ­¥æ•°: {episode_steps}")
        print(f"   æœ€ç»ˆå¥–åŠ±: {episode_reward:.2f}")
        print(f"   å¾—åˆ†: {episode_type['score']:.2f}")
        
        # ğŸ¯ é‡ç½®episodeè¿½è¸ª
        self.current_episode_best_distance = float('inf')
        self.current_episode_best_reward = float('-inf')
        self.current_episode_min_distance_step = 0
        self.current_episode_start_step = step
        self.current_episode_start_time = time.time()
        
        # æ£€æŸ¥åœæ­¢æ¡ä»¶
        should_stop = self._check_episode_stopping_conditions(step)
        if should_stop:
            self._generate_final_fitness_report()
        
        episode_rewards[proc_id] = 0.0
        return should_stop
    
    def should_update_model(self, step):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°æ¨¡å‹"""
        return (step >= self.sac.warmup_steps and 
                step % self.args.update_frequency == 0 and 
                self.sac.memory.can_sample(self.sac.batch_size))
    
    def update_and_log(self, step, total_steps):
        """æ›´æ–°æ¨¡å‹å¹¶è®°å½•"""
        metrics = self.sac.update()
        
        if metrics:
            enhanced_metrics = metrics.copy()
            enhanced_metrics.update({
                'step': step,
                'buffer_size': len(self.sac.memory),
                'learning_rate': self.sac.actor_optimizer.param_groups[0]['lr'],
                'warmup_progress': min(1.0, step / max(self.sac.warmup_steps, 1))
            })
            
            self.logger.log_step(step, enhanced_metrics, episode=step//100)
            
            if step % 100 == 0:
                # åŸå§‹æ ¼å¼è¾“å‡ºï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
                print(f"Step {step} (total_steps {total_steps}): "
                      f"Learning Rate: {metrics['lr']:.6f}, "
                      f"Critic Loss: {metrics['critic_loss']:.4f}, "
                      f"Actor Loss: {metrics['actor_loss']:.4f}, "
                      f"Alpha: {metrics['alpha']:.4f}, "
                      f"Buffer Size: {len(self.sac.memory)}")
                
                # ğŸ†• æ›´æ–°ç†µæƒé‡è°ƒåº¦ (æ¯100æ­¥è°ƒç”¨ä¸€æ¬¡)
                if step % 100 == 0:
                    self.sac.update_alpha_schedule(step, total_steps)
                    # åŒæ­¥æ›´æ–°metricsä¸­çš„alphaå€¼
                    metrics['alpha'] = self.sac.alpha
                
                # ğŸ†• æ·»åŠ æ ‡å‡†åŒ–çš„æŸå¤±è¾“å‡ºæ ¼å¼ï¼ˆä¾›æŸå¤±æå–å™¨æ•è·ï¼‰
                print(f"ğŸ”¥ SACç½‘ç»œLossæ›´æ–° [Step {step}]:")
                print(f"ğŸ“Š Actor Loss: {metrics['actor_loss']:.6f}")
                print(f"ğŸ“Š Critic Loss: {metrics['critic_loss']:.6f}")
                print(f"ğŸ“Š Alpha Loss: {metrics.get('alpha_loss', 0.0):.6f}")
                print(f"ğŸ“Š Alpha: {metrics['alpha']:.6f} (è°ƒåº¦å)")
                print(f"ğŸ“Š Q1å‡å€¼: {metrics.get('q1_mean', 0.0):.6f}")
                print(f"ğŸ“Š Q2å‡å€¼: {metrics.get('q2_mean', 0.0):.6f}")
                print(f"ğŸ“Š Q1æ ‡å‡†å·®: {metrics.get('q1_std', 0.0):.6f}")
                print(f"ğŸ“Š Q2æ ‡å‡†å·®: {metrics.get('q2_std', 0.0):.6f}")
                print(f"ğŸ“Š ç†µé¡¹: {metrics.get('entropy_term', 0.0):.6f}")
                print(f"ğŸ“Š Qå€¼é¡¹: {metrics.get('q_term', 0.0):.6f}")
                print(f"ğŸ“ˆ å­¦ä¹ ç‡: {metrics['lr']:.2e}")
                print(f"ğŸ’¾ Bufferå¤§å°: {len(self.sac.memory)}")
                
                # ğŸ†• æ·»åŠ Attentionç½‘ç»œæŸå¤±ä¿¡æ¯
                if any(key.startswith('attention_') for key in metrics.keys()):
                    print(f"\nğŸ”¥ Attentionç½‘ç»œLossæ›´æ–° [Step {step}]:")
                    # ğŸ†• ç‹¬ç«‹æ˜¾ç¤ºä¸‰ä¸ªattentionç½‘ç»œçš„ä¿¡æ¯
                    if 'attention_actor_loss' in metrics:
                        print(f"ğŸ“Š Actor Attention Loss: {metrics['attention_actor_loss']:.6f}")
                    if 'attention_critic_main_loss' in metrics:
                        print(f"ğŸ“Š Critic Main Attention Loss: {metrics['attention_critic_main_loss']:.6f}")
                    if 'attention_critic_value_loss' in metrics:
                        print(f"ğŸ“Š Critic Value Attention Loss: {metrics['attention_critic_value_loss']:.6f}")
                    if 'attention_total_loss' in metrics:
                        print(f"ğŸ“Š Attentionæ€»æŸå¤±: {metrics['attention_total_loss']:.6f}")
                    
                    # ğŸ†• æ˜¾ç¤ºæ¢¯åº¦èŒƒæ•°ï¼ˆæ›´è¯¦ç»†çš„ä¿¡æ¯ï¼‰
                    if 'attention_actor_grad_norm' in metrics:
                        print(f"ğŸ” Actor Attentionæ¢¯åº¦èŒƒæ•°: {metrics['attention_actor_grad_norm']:.6f}")
                    if 'attention_critic_main_grad_norm' in metrics:
                        print(f"ğŸ” Critic Main Attentionæ¢¯åº¦èŒƒæ•°: {metrics['attention_critic_main_grad_norm']:.6f}")
                    if 'attention_critic_value_grad_norm' in metrics:
                        print(f"ğŸ” Critic Value Attentionæ¢¯åº¦èŒƒæ•°: {metrics['attention_critic_value_grad_norm']:.6f}")
                    
                    # ğŸ†• åˆ†åˆ«æ˜¾ç¤ºActorå’ŒCriticå‚æ•°ç»Ÿè®¡
                    if 'attention_actor_param_mean' in metrics:
                        print(f"ğŸ“Š Actor Attentionå‚æ•°: å‡å€¼={metrics['attention_actor_param_mean']:.6f}, æ ‡å‡†å·®={metrics.get('attention_actor_param_std', 0):.6f}")
                    if 'attention_critic_param_mean' in metrics:
                        print(f"ğŸ“Š Critic Attentionå‚æ•°: å‡å€¼={metrics['attention_critic_param_mean']:.6f}, æ ‡å‡†å·®={metrics.get('attention_critic_param_std', 0):.6f}")
                    
                    # ğŸ†• æ˜¾ç¤ºattentionç½‘ç»œçš„æ€»ä½“å‚æ•°ç»Ÿè®¡
                    if 'attention_param_mean' in metrics:
                        print(f"ğŸ“Š Attentionå‚æ•°å‡å€¼: {metrics['attention_param_mean']:.6f}")
                    if 'attention_param_std' in metrics:
                        print(f"ğŸ“Š Attentionå‚æ•°æ ‡å‡†å·®: {metrics['attention_param_std']:.6f}")
                    
                    # ğŸ†• æ˜¾ç¤ºå…³èŠ‚å…³æ³¨åº¦åˆ†æ
                    if 'most_important_joint' in metrics:
                        print(f"ğŸ¯ æœ€é‡è¦å…³èŠ‚: Joint {metrics['most_important_joint']}")
                    if 'max_joint_importance' in metrics:
                        print(f"ğŸ¯ æœ€é‡è¦å…³èŠ‚: Joint {metrics.get('most_important_joint', 'N/A')} (é‡è¦æ€§: {metrics['max_joint_importance']:.6f})")
                    if 'importance_concentration' in metrics:
                        print(f"ğŸ“Š é‡è¦æ€§é›†ä¸­åº¦: {metrics['importance_concentration']:.6f}")
                    if 'importance_entropy' in metrics:
                        print(f"ğŸ“Š é‡è¦æ€§ç†µå€¼: {metrics['importance_entropy']:.6f}")
                    if 'robot_num_joints' in metrics:
                        print(f"ğŸ¤– æœºå™¨äººç»“æ„: {metrics['robot_num_joints']}å…³èŠ‚")
                    if 'robot_structure_info' in metrics:
                        print(f"ğŸ¤– æœºå™¨äººç»“æ„: {metrics['robot_num_joints']}å…³èŠ‚ ({metrics['robot_structure_info']})")
                    
                    # ğŸ†• æ˜¾ç¤ºå…³èŠ‚æ´»è·ƒåº¦å’Œé‡è¦æ€§ï¼ˆåªæ˜¾ç¤ºå­˜åœ¨çš„å…³èŠ‚ï¼‰
                    if 'robot_num_joints' in metrics:
                        num_joints = metrics['robot_num_joints']
                        joint_activities = []
                        joint_importances = []
                        joint_angles = []
                        joint_velocities = []
                        link_lengths = []
                        
                        for i in range(min(num_joints, 20)):
                            activity = metrics.get(f'joint_{i}_activity', -1)
                            importance = metrics.get(f'joint_{i}_importance', -1)
                            angle_mag = metrics.get(f'joint_{i}_angle_magnitude', -1)
                            vel_mag = metrics.get(f'joint_{i}_velocity_magnitude', -1)
                            link_len = metrics.get(f'link_{i}_length', -1)
                            
                            if activity != -1:
                                joint_activities.append(f"J{i}:{activity:.3f}")
                            if importance != -1:
                                joint_importances.append(f"J{i}:{importance:.3f}")
                            if angle_mag != -1:
                                joint_angles.append(f"J{i}:{angle_mag:.3f}")
                            if vel_mag != -1:
                                joint_velocities.append(f"J{i}:{vel_mag:.3f}")
                            if link_len != -1:
                                link_lengths.append(f"L{i}:{link_len:.1f}px")
                        
                        if joint_activities:
                            print(f"ğŸ” å…³èŠ‚æ´»è·ƒåº¦: {', '.join(joint_activities)}")
                        if joint_importances:
                            print(f"ğŸ¯ å…³èŠ‚é‡è¦æ€§: {', '.join(joint_importances)}")
                        if joint_angles:
                            print(f"ğŸ“ å…³èŠ‚è§’åº¦å¹…åº¦: {', '.join(joint_angles)}")
                        if joint_velocities:
                            print(f"âš¡ å…³èŠ‚é€Ÿåº¦å¹…åº¦: {', '.join(joint_velocities)}")
                        if link_lengths:
                            print(f"ğŸ“ Linké•¿åº¦: {', '.join(link_lengths)}")
                
                # ğŸ†• æ·»åŠ æ ‡å‡†åŒ–çš„æˆåŠŸç‡æŠ¥å‘Šï¼ˆæ¯500æ­¥è¾“å‡ºä¸€æ¬¡ï¼‰
                if step % 500 == 0 and len(self.episode_results) > 0:
                    success_count = sum(1 for ep in self.episode_results if ep.get('success', False))
                    current_success_rate = (success_count / len(self.episode_results)) * 100
                    
                    print(f"ğŸ“Š SACè®­ç»ƒè¿›åº¦æŠ¥å‘Š [Step {step}]:")
                    print(f"âœ… å½“å‰æˆåŠŸç‡: {current_success_rate:.1f}%")
                    print(f"ğŸ† å½“å‰æœ€ä½³è·ç¦»: {self.best_min_distance:.1f}px")
                    print(f"ğŸ“Š å½“å‰Episodeæœ€ä½³è·ç¦»: {self.current_episode_best_distance:.1f}px")
                    print(f"ğŸ”„ è¿ç»­æˆåŠŸæ¬¡æ•°: {self.consecutive_success_count}")
                    print(f"ğŸ“‹ å·²å®ŒæˆEpisodes: {len(self.episode_results)}")



def main(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    # è®¾ç½®éšæœºç§å­å’Œè®¾å¤‡
    torch.manual_seed(args.seed)
    torch.set_num_threads(1)
    device = torch.device('cpu')
    os.makedirs(args.save_dir, exist_ok=True)
    
    # åˆ›å»ºè®­ç»ƒæ—¥å¿—
    training_log_path = os.path.join(args.save_dir, 'logs.txt')
    with open(training_log_path, 'w') as f:
        pass
    
    # åˆ›å»ºç¯å¢ƒ
    if args.env_name == 'reacher2d':
        print("ä½¿ç”¨ reacher2d ç¯å¢ƒ")
        env_setup = EnvironmentSetup()
        envs, sync_env, env_params = env_setup.create_reacher2d_env(args)
        args.env_type = 'reacher2d'
    else:
        print(f"ä½¿ç”¨ bullet ç¯å¢ƒ: {args.env_name}")
        from a2c_ppo_acktr.envs import make_vec_envs
        envs = make_vec_envs(args.env_name, args.seed, args.num_processes, 
                            args.gamma, None, device, False, args=args)
        sync_env = None
        args.env_type = 'bullet'

    # è·å–å…³èŠ‚æ•°é‡å’Œåˆ›å»ºæ•°æ®å¤„ç†å™¨
    num_joints = envs.action_space.shape[0]
    print(f"å…³èŠ‚æ•°é‡: {num_joints}")
    data_handler = DataHandler(num_joints, args.env_type)

    # åˆ›å»ºGNNç¼–ç å™¨
    if args.env_type == 'reacher2d':
        sys.path.append(os.path.join(base_dir, 'examples/2d_reacher/utils'))
        from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
        
        print("ğŸ¤– åˆå§‹åŒ– Reacher2D GNN ç¼–ç å™¨...")
        reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
        single_gnn_embed = reacher2d_encoder.get_gnn_embeds(
            num_links=num_joints, 
            link_lengths=env_params['link_lengths']
        )
        print(f"âœ… Reacher2D GNN åµŒå…¥ç”ŸæˆæˆåŠŸï¼Œå½¢çŠ¶: {single_gnn_embed.shape}")
    else:
        rule_sequence = [int(s.strip(",")) for s in args.rule_sequence]
        gnn_encoder = GNN_Encoder(args.grammar_file, rule_sequence, 70, num_joints)
        gnn_graph = gnn_encoder.get_graph(rule_sequence)
        single_gnn_embed = gnn_encoder.get_gnn_embeds(gnn_graph)

    # ğŸ”§ åˆ›å»ºä¼˜åŒ–çš„SACæ¨¡å‹ - ä¸“é—¨ä¸ºCriticç¨³å®šæ€§ä¼˜åŒ–
    print("ğŸ”§ Criticç¨³å®šæ€§ä¼˜åŒ–é…ç½®:")
    optimized_batch_size = max(args.batch_size, 256)  # å¢åŠ åˆ°256ï¼Œæé«˜ç¨³å®šæ€§
    print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size} â†’ {optimized_batch_size}")
    print(f"   å­¦ä¹ ç‡ç­–ç•¥: Actor={args.lr:.2e}, Critic={args.lr*1.5:.2e} (1.5å€ï¼Œæ›´ç¨³å®š)")
    print(f"   Tauå‚æ•°: å°†è‡ªåŠ¨è°ƒæ•´åˆ°è‡³å°‘0.01")
    
    attn_model = AttnModel(128, 130, 130, 4)
    sac = AttentionSACWithBuffer(
        attn_model, num_joints, 
        buffer_capacity=args.buffer_capacity, 
        batch_size=optimized_batch_size,  # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        lr=args.lr, 
        tau=args.tau,  # tauä¼šåœ¨SACå†…éƒ¨è‡ªåŠ¨è°ƒæ•´
        gamma=args.gamma,
        alpha=args.alpha,
        env_type=args.env_type
    )
    
    # æ·»åŠ SACç‰¹å®šå‚æ•°
    sac.warmup_steps = args.warmup_steps
    sac.alpha = torch.tensor(args.alpha)
    sac.min_alpha = 0.05
    print(f"ğŸ”’ Alphaè¡°å‡ä¸‹é™è®¾ç½®ä¸º: {sac.min_alpha}")
    
    # ğŸ†• è®¾ç½®individual_idåˆ°SACæ¨¡å‹
    if hasattr(args, 'individual_id') and args.individual_id:
        sac.individual_id = args.individual_id
        print(f"ğŸ†” è®¾ç½®Individual ID: {args.individual_id}")
        
        # ğŸ†• ç›´æ¥è®¾ç½®ç¯å¢ƒå±æ€§
        generation = getattr(args, 'generation', 0)
        
        # è®¾ç½®åŒæ­¥ç¯å¢ƒ
        if sync_env:
            sync_env.current_generation = generation
            sync_env.individual_id = args.individual_id
            print(f"ğŸ†” è®¾ç½®åŒæ­¥ç¯å¢ƒä¸Šä¸‹æ–‡: ä¸ªä½“={args.individual_id}, ä»£æ•°={generation}")
        
        # è®¾ç½®å‘é‡ç¯å¢ƒ
        if hasattr(envs, 'envs'):
            for i, env_wrapper in enumerate(envs.envs):
                # ğŸ”§ é€’å½’è®¾ç½®æ‰€æœ‰å±‚çº§çš„ç¯å¢ƒå±æ€§
                current_env = env_wrapper
                while hasattr(current_env, 'env'):
                    if hasattr(current_env, 'current_generation'):
                        current_env.current_generation = generation
                        current_env.individual_id = args.individual_id
                    current_env = current_env.env
                
                # æœ€ç»ˆçš„ç¯å¢ƒå¯¹è±¡
                if hasattr(current_env, '__class__'):
                    current_env.current_generation = generation
                    current_env.individual_id = args.individual_id
                    print(f"ğŸ†” è®¾ç½®ç¯å¢ƒ{i}ä¸Šä¸‹æ–‡: ä¸ªä½“={args.individual_id}, ä»£æ•°={generation} (ç±»å‹: {current_env.__class__.__name__})")

    if hasattr(sac, 'target_entropy'):
        sac.target_entropy = -num_joints * args.target_entropy_factor
    
    # åˆ›å»ºè®­ç»ƒç›‘æ§ç³»ç»Ÿ
    experiment_name = f"reacher2d_sac_{time.strftime('%Y%m%d_%H%M%S')}"
    
    # é…ç½®ä¿¡æ¯
    hyperparams = {
        'learning_rate': args.lr,
        'alpha': args.alpha,
        'warmup_steps': args.warmup_steps,
        'target_entropy_factor': args.target_entropy_factor,
        'batch_size': args.batch_size,
        'buffer_capacity': args.buffer_capacity,
        'gamma': args.gamma,
        'seed': args.seed,
        'num_processes': args.num_processes,
        'update_frequency': args.update_frequency,
        'total_steps': 120000,
        'optimizer': 'Adam',
        'network_architecture': {
            'attn_model_dims': '128-130-130-4',
            'action_dim': num_joints,
        }
    }
    
    env_config = {
        'env_name': args.env_name,
        'env_type': args.env_type,
        'goal_threshold': GOAL_THRESHOLD,
        'action_dim': num_joints,
    }
    
    if args.env_type == 'reacher2d':
        env_config.update({
            'num_links': env_params['num_links'],
            'link_lengths': env_params['link_lengths'],
            'config_path': env_params.get('config_path', 'N/A'),
            'render_mode': env_params.get('render_mode', 'human'),
            'reward_function': 'è·ç¦»+è¿›åº¦+æˆåŠŸ+ç¢°æ’+æ–¹å‘å¥–åŠ±',
            'physics_engine': 'PyMunk',
            'obstacle_type': 'zigzag',
            'action_space': 'continuous',
            'observation_space': 'joint_angles_and_positions'
        })
    
    logger = TrainingLogger(
        log_dir=os.path.join(args.save_dir, 'training_logs'),
        experiment_name=experiment_name,
        hyperparams=hyperparams,
        env_config=env_config
    )
    
    # monitor = RealTimeMonitor(logger, alert_thresholds={
    #     'critic_loss': {'max': 50.0, 'nan_check': True},
    #     'actor_loss': {'max': 10.0, 'nan_check': True},
    #     'alpha_loss': {'max': 5.0, 'nan_check': True},
    #     'alpha': {'min': 0.01, 'max': 2.0, 'nan_check': True}
    # })
    
    print(f"ğŸ“Š è®­ç»ƒç›‘æ§ç³»ç»Ÿå·²åˆå§‹åŒ–: {logger.experiment_dir}")
    
    # åˆ›å»ºç®¡ç†å™¨
    model_manager = ModelManager(args.save_dir)
    training_manager = TrainingManager(args, sac, logger, model_manager)
    
    # å¤„ç†checkpointæ¢å¤
    start_step = 0
    if args.resume_checkpoint:
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume_checkpoint}")
        start_step = model_manager.load_checkpoint(sac, args.resume_checkpoint)

        if start_step > 0:
            print(f"æˆåŠŸåŠ è½½checkpoint, ä»step {start_step} å¼€å§‹è®­ç»ƒ")
            sac.warmup_steps = 0

            # æ›´æ–°å­¦ä¹ ç‡å’Œalpha
            if args.resume_lr:
                for param_group in sac.actor_optimizer.param_groups:
                    param_group['lr'] = args.resume_lr
                for param_group in sac.critic_optimizer.param_groups:
                    param_group['lr'] = args.resume_lr
                print(f"æ›´æ–°å­¦ä¹ ç‡ä¸º {args.resume_lr}")

            if args.resume_alpha:
                sac.alpha = args.resume_alpha
                print(f"æ›´æ–°alphaä¸º {args.resume_alpha}")
            
    # è¿è¡Œè®­ç»ƒå¾ªç¯
    run_training_loop(args, envs, sync_env, sac, single_gnn_embed, training_manager, num_joints, start_step)
    training_results = collect_training_results(training_manager)
    # æ¸…ç†èµ„æº
    cleanup_resources(sync_env, logger, model_manager, training_manager)
    return training_results
def collect_training_results(training_manager):
    """æ”¶é›†è®­ç»ƒç»“æœç”¨äºfitnessè®¡ç®—"""
    import numpy as np
    
    if not hasattr(training_manager, 'episode_results') or not training_manager.episode_results:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°episode_resultsï¼Œè¿”å›é»˜è®¤ç»“æœ")
        return {
            'success': False,
            'error': 'No episode results available',
            'episodes_completed': 0,
            'success_rate': 0.0,
            'avg_best_distance': float('inf'),
            'avg_score': 0.0,
            'total_training_time': 0.0,
            'episode_details': [],
            'learning_progress': 0.0,
            'avg_steps_to_best': 120000
        }
    
    episodes = training_manager.episode_results
    print(f"ğŸ“Š æ”¶é›†åˆ° {len(episodes)} ä¸ªepisodeçš„ç»“æœ")
    
    # è®¡ç®—åŸºç¡€ç»Ÿè®¡
    success_count = sum(1 for ep in episodes if ep.get('success', False))
    total_episodes = len(episodes)
    
    # è®¡ç®—å¹³å‡æœ€ä½³è·ç¦»
    distances = [ep.get('best_distance', float('inf')) for ep in episodes]
    avg_best_distance = np.mean([d for d in distances if d != float('inf')]) if distances else float('inf')
    
    # è®¡ç®—å­¦ä¹ è¿›æ­¥
    if len(episodes) >= 2:
        first_score = episodes[0].get('episode_score', 0)
        last_score = episodes[-1].get('episode_score', 0)
        learning_progress = last_score - first_score
    else:
        learning_progress = 0.0
    
    # è®¡ç®—å¹³å‡åˆ°è¾¾æœ€ä½³è·ç¦»çš„æ­¥æ•°
    steps_to_best = [ep.get('steps_to_best', 120000) for ep in episodes]
    avg_steps_to_best = np.mean(steps_to_best)
    
    # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
    durations = [ep.get('duration', 0) for ep in episodes]
    total_training_time = sum(durations)
    
    result = {
        'success': True,
        'episodes_completed': total_episodes,
        'success_rate': success_count / total_episodes if total_episodes > 0 else 0.0,
        'avg_best_distance': avg_best_distance,
        'avg_score': np.mean([ep.get('episode_score', 0) for ep in episodes]),
        'total_training_time': total_training_time,
        'episode_details': episodes,
        'learning_progress': learning_progress,
        'avg_steps_to_best': avg_steps_to_best,
        'episode_results': episodes
    }
    
    print(f"âœ… è®­ç»ƒç»“æœæ”¶é›†å®Œæˆ:")
    print(f"   Episodes: {total_episodes}")
    print(f"   æˆåŠŸç‡: {result['success_rate']:.1%}")
    print(f"   å¹³å‡æœ€ä½³è·ç¦»: {result['avg_best_distance']:.1f}px")
    print(f"   å­¦ä¹ è¿›æ­¥: {result['learning_progress']:+.3f}")
    
    return result
def run_training_loop(args, envs, sync_env, sac, single_gnn_embed, training_manager, num_joints, start_step=0):
    """è¿è¡Œè®­ç»ƒå¾ªç¯ - Episodesç‰ˆæœ¬"""
    current_obs = envs.reset()
    print(f"åˆå§‹è§‚å¯Ÿ: {current_obs.shape}")
    
    # ğŸ”§ ä¸ä½¿ç”¨sync_envï¼Œè®­ç»ƒç¯å¢ƒç›´æ¥æ¸²æŸ“
    print("ğŸ”§ è®­ç»ƒç¯å¢ƒå·²é‡ç½®ï¼ˆç›´æ¥æ¸²æŸ“æ¨¡å¼ï¼‰")
    
    current_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)
    episode_rewards = [0.0] * args.num_processes
    
    # ğŸ†• Episodesæ§åˆ¶å‚æ•°
    max_episodes = 2  # ğŸ”§ ä¿®æ”¹ä¸º2ä¸ªepisodes
    steps_per_episode = 120000
    
    print(f"å¼€å§‹è®­ç»ƒ: warmup {sac.warmup_steps} æ­¥")
    print(f"è®­ç»ƒé…ç½®: {max_episodes}ä¸ªepisodes Ã— {steps_per_episode}æ­¥/episode")
    
    training_completed = False
    early_termination_reason = ""
    global_step = start_step  # å…¨å±€æ­¥æ•°è®¡æ•°å™¨ï¼ˆç”¨äºæ¨¡å‹æ›´æ–°ç­‰ï¼‰

    try:
        # ğŸ†• Episodeså¾ªç¯
        for episode_num in range(max_episodes):
            print(f"\nğŸ¯ å¼€å§‹Episode {episode_num + 1}/{max_episodes}")
            
            print(f"ğŸ”„ é‡ç½®ç¯å¢ƒå¼€å§‹Episode {episode_num + 1}...")
            current_obs = envs.reset()
            if sync_env:
                sync_env.reset()
                print("ğŸ”§ sync_env å·²é‡ç½®")
            current_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)
            episode_rewards = [0.0] * args.num_processes  # é‡ç½®episodeå¥–åŠ±
            # é‡ç½®episodeè¿½è¸ª
            training_manager.current_episode_start_step = global_step
            training_manager.current_episode_start_time = time.time()
            training_manager.current_episode_best_distance = float('inf')
            training_manager.current_episode_best_reward = float('-inf')
            training_manager.current_episode_min_distance_step = 0
            
            episode_step = 0  # ğŸ¯ æ¯ä¸ªepisodeå†…çš„æ­¥æ•°è®¡æ•°
            episode_completed = False
            
            # ğŸ†• å•ä¸ªEpisodeçš„è®­ç»ƒå¾ªç¯
            while episode_step < steps_per_episode and not episode_completed:
                # è¿›åº¦æ˜¾ç¤º
                if episode_step % 100 == 0:
                    if global_step < sac.warmup_steps:
                        smart_print(f"Episode {episode_num+1}, Step {episode_step}/{steps_per_episode}: Warmup phase ({global_step}/{sac.warmup_steps})")
                    else:
                        smart_print(f"Episode {episode_num+1}, Step {episode_step}/{steps_per_episode}: Training phase, Buffer size: {len(sac.memory)}")

                # è·å–åŠ¨ä½œ
                if global_step < sac.warmup_steps:
                    action_batch = torch.from_numpy(np.array([
                        envs.action_space.sample() for _ in range(args.num_processes)
                    ]))
                else:
                    actions = []
                    for proc_id in range(args.num_processes):
                        # ğŸ†• è®¡ç®—è·ç¦»ä»¥å¯ç”¨è·ç¦»è‡ªé€‚åº”æ§åˆ¶
                        current_obs_np = current_obs[proc_id].cpu().numpy()
                        # ä»è§‚å¯Ÿä¸­æå–æœ«ç«¯ä½ç½®å’Œç›®æ ‡ä½ç½®
                        if len(current_obs_np) >= 8:  # reacher2dè§‚å¯Ÿæ ¼å¼
                            end_pos = current_obs_np[-5:-3]  # æœ«ç«¯ä½ç½®
                            goal_pos = current_obs_np[-3:-1]  # ç›®æ ‡ä½ç½®
                            distance_to_goal = np.linalg.norm(end_pos - goal_pos)
                        else:
                            distance_to_goal = None
                        
                        action = sac.get_action(
                            current_obs[proc_id],
                            current_gnn_embeds[proc_id],
                            num_joints=envs.action_space.shape[0],
                            deterministic=False,
                            distance_to_goal=distance_to_goal  # ğŸ†• ä¼ é€’è·ç¦»ä¿¡æ¯
                        )
                        actions.append(action)
                    action_batch = torch.stack(actions)

                # åŠ¨ä½œåˆ†æï¼ˆè°ƒè¯•ç”¨ï¼‰
                if episode_step % 50 == 0 or episode_step < 20:
                    if hasattr(envs, 'envs') and len(envs.envs) > 0:
                        env_goal = getattr(envs.envs[0], 'goal_pos', 'NOT FOUND')
                        # ğŸ”§ æ˜¾ç¤ºç¯å¢ƒå†…éƒ¨çš„çœŸå®episodeè®¡æ•°
                        env_episode = getattr(envs.envs[0], 'current_episode', episode_num+1)
                        print(f"ğŸ¯ [Episode {env_episode}] Step {episode_step} - ç¯å¢ƒgoal_pos: {env_goal}")

                # æ‰§è¡ŒåŠ¨ä½œ
                next_obs, reward, done, infos = envs.step(action_batch)

                # ğŸ”§ ç›´æ¥ä½¿ç”¨è®­ç»ƒç¯å¢ƒè¿›è¡Œæ¸²æŸ“ï¼ˆä¸ä½¿ç”¨sync_envï¼‰
                if hasattr(envs, 'envs') and len(envs.envs) > 0 and hasattr(envs.envs[0], 'render_mode') and envs.envs[0].render_mode == 'human':
                    try:
                        envs.envs[0].render()
                        # æ¯100æ­¥æ˜¾ç¤ºä¸€æ¬¡æ¸²æŸ“çŠ¶æ€
                        if episode_step % 100 == 0:
                            print(f"ğŸ–¼ï¸ [Step {episode_step}] è®­ç»ƒç¯å¢ƒæ¸²æŸ“æ›´æ–°")
                    except Exception as e:
                        if episode_step % 500 == 0:  # å‡å°‘é”™è¯¯æ¶ˆæ¯é¢‘ç‡
                            print(f"âš ï¸ æ¸²æŸ“é”™è¯¯: {e}")

                next_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)

                # å­˜å‚¨ç»éªŒ
                for proc_id in range(args.num_processes):
                    sac.store_experience(
                        obs=current_obs[proc_id],
                        gnn_embeds=current_gnn_embeds[proc_id],
                        action=action_batch[proc_id],
                        reward=reward[proc_id],
                        next_obs=next_obs[proc_id],
                        next_gnn_embeds=next_gnn_embeds[proc_id],
                        done=done[proc_id],
                        num_joints=num_joints
                    )
                    episode_rewards[proc_id] += reward[proc_id].item()

                current_obs = next_obs.clone()
                current_gnn_embeds = next_gnn_embeds.clone()

                # ğŸ†• æ›´æ–°episodeè¿½è¸ª
                training_manager.update_episode_tracking(global_step, infos, episode_rewards)

                # å¤„ç†episodeç»“æŸ
                # æ›¿æ¢ç¬¬1055-1083è¡Œçš„ä»£ç ï¼š

                # ğŸ”§ åˆ é™¤é‡å¤çš„doneæ£€æµ‹å¾ªç¯ï¼ˆç§»åŠ¨åˆ°ä¸‹é¢ç»Ÿä¸€å¤„ç†ï¼‰

                # ğŸ”§ åªåœ¨ç¬¬ä¸€æ¬¡doneæ—¶å¤„ç†episodeç»“æŸï¼Œé¿å…é‡å¤è§¦å‘
                episode_end_handled = False
                for proc_id in range(args.num_processes):
                        is_done = done[proc_id].item() if torch.is_tensor(done[proc_id]) else bool(done[proc_id])
                        if is_done and not episode_end_handled:
                            print(f"ğŸ” [DEBUG] Episodeç»“æŸæ£€æµ‹: proc_id={proc_id}, å½“å‰episodes={training_manager.current_episodes}")
                            
                            should_end = training_manager.handle_episode_end(proc_id, episode_step, episode_rewards, infos)
                            print(f"ğŸ” [DEBUG] handle_episode_endè¿”å›: should_end={should_end}, æ–°çš„current_episodes={training_manager.current_episodes}")
                            episode_end_handled = True  # æ ‡è®°å·²å¤„ç†ï¼Œé¿å…é‡å¤
                            
                            # ğŸ”§ æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
                            goal_reached = infos[proc_id].get('goal', {}).get('distance_to_goal', float('inf')) < 20.0
                            print(f"ğŸ” [DEBUG] ç›®æ ‡æ£€æŸ¥: goal_reached={goal_reached}")
                            
                            if should_end:  # å®Œæˆ2ä¸ªè®­ç»ƒepisodes
                                print(f"ğŸ” [DEBUG] è§¦å‘should_endï¼Œæ•´ä¸ªè®­ç»ƒç»“æŸ")
                                training_completed = True
                                early_termination_reason = f"å®Œæˆ{training_manager.current_episodes}ä¸ªepisodes"
                                episode_completed = True
                                break
                            elif goal_reached:  # åˆ°è¾¾ç›®æ ‡ï¼Œç»“æŸå½“å‰è®­ç»ƒepisode
                                print(f"ğŸ” [DEBUG] è§¦å‘goal_reachedï¼Œå½“å‰è®­ç»ƒepisodeç»“æŸ")
                                print(f"ğŸ‰ è®­ç»ƒEpisode {episode_num+1} æˆåŠŸå®Œæˆï¼å¼€å§‹ä¸‹ä¸€ä¸ªepisode...")
                                episode_completed = True  # ç»“æŸå½“å‰è®­ç»ƒepisodeï¼Œä½†ä¸ç»“æŸæ•´ä¸ªè®­ç»ƒ
                                break
                            
                            # ç¯å¢ƒé‡ç½®ï¼ˆç»§ç»­å½“å‰è®­ç»ƒepisodeï¼‰
                            if hasattr(envs, 'reset_one'):
                                current_obs[proc_id] = envs.reset_one(proc_id)
                                current_gnn_embeds[proc_id] = single_gnn_embed
                
                # æ¨¡å‹æ›´æ–°
                if training_manager.should_update_model(global_step):
                    training_manager.update_and_log(global_step, global_step)
                
                # ğŸ†• å®šæœŸè¾“å‡ºgoalåˆ°è¾¾ç»Ÿè®¡ (æ¯500æ­¥)
                if global_step % 500 == 0 and global_step > 0:
                    # è·å–æ‰€æœ‰ç¯å¢ƒçš„goalåˆ°è¾¾ç»Ÿè®¡
                    total_goal_reaches = 0
                    total_steps = 0
                    for proc_id in range(args.num_processes):
                        if hasattr(envs, 'envs') and len(envs.envs) > proc_id:
                            env = envs.envs[proc_id]
                            if hasattr(env, 'get_goal_reach_stats'):
                                stats = env.get_goal_reach_stats()
                                total_goal_reaches += stats['goal_reach_count']
                                total_steps += stats['total_steps']
                    
                    if total_steps > 0:
                        goal_reach_percentage = (total_goal_reaches / total_steps) * 100
                        print(f"ğŸ“Š Goalåˆ°è¾¾ç»Ÿè®¡ [Step {global_step}]:")
                        print(f"   ğŸ¯ åˆ°è¾¾æ¬¡æ•°: {total_goal_reaches}")
                        print(f"   ğŸ“ˆ æ€»æ­¥æ•°: {total_steps}")
                        print(f"   âœ… åˆ°è¾¾ç‡: {goal_reach_percentage:.2f}%")
                
                # å®šæœŸä¿å­˜å’Œç»˜å›¾
                if global_step % 200 == 0 and global_step > 0:  # ğŸ†• æ”¹ä¸º200æ­¥æ£€æµ‹
                    # ğŸ†• è·å–å½“å‰æœ€ä½³è·ç¦»
                    current_best_distance = training_manager.current_episode_best_distance
                    
                    # ğŸ†• ä¼ é€’å½“å‰è·ç¦»ç”¨äºæ¯”è¾ƒ
                    saved = training_manager.model_manager.save_checkpoint(
                        sac, global_step,
                        best_success_rate=training_manager.best_success_rate,
                        best_min_distance=training_manager.best_min_distance,
                        current_best_distance=current_best_distance,  # ğŸ†• å…³é”®å‚æ•°
                        consecutive_success_count=training_manager.consecutive_success_count,
                        current_episode=episode_num + 1,
                        episode_step=episode_step
                    )
                    
                    # ğŸ†• å¦‚æœä¿å­˜æˆåŠŸï¼Œæ›´æ–°æœ€ä½³è®°å½•
                    if saved:
                        training_manager.best_min_distance = current_best_distance
                        print(f"ğŸ“ˆ æ›´æ–°å…¨å±€æœ€ä½³è·ç¦»: {current_best_distance:.1f}px")

                # ğŸ†• ä½é¢‘æ—¥å¿—è®°å½•
                if global_step % 2000 == 0 and global_step > 0:
                    training_manager.logger.plot_losses(recent_steps=2000, show=False)
                    print(f"ğŸ“Š Step {global_step}: å½“å‰æœ€ä½³è·ç¦» {training_manager.best_min_distance:.1f}px")
                
                episode_step += 1  # ğŸ¯ episodeå†…æ­¥æ•°é€’å¢
                global_step += args.num_processes  # å…¨å±€æ­¥æ•°é€’å¢
                
                if training_completed:
                    break
            
            print(f"ğŸ“Š Episode {episode_num + 1} å®Œæˆ: {episode_step} æ­¥")
            
            # ğŸ†• è¾“å‡ºå½“å‰episodeçš„goalåˆ°è¾¾ç»Ÿè®¡
            if hasattr(envs, 'envs') and len(envs.envs) > 0:
                env = envs.envs[0]  # è·å–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„ç»Ÿè®¡
                if hasattr(env, 'get_goal_reach_stats'):
                    stats = env.get_goal_reach_stats()
                    print(f"   ğŸ¯ Goalåˆ°è¾¾ç»Ÿè®¡: {stats['goal_reach_count']}æ¬¡")
                    print(f"   ğŸ“ˆ åˆ°è¾¾ç‡: {stats['goal_reach_percentage']:.2f}%")
                    if stats['max_maintain_streak'] > 0:
                        print(f"   ğŸ† æœ€é•¿ç»´æŒ: {stats['max_maintain_streak']}æ­¥")
            
            if training_completed:
                print(f"ğŸ è®­ç»ƒæå‰ç»ˆæ­¢: {early_termination_reason}")
                break

    except Exception as e:
        print(f"ğŸ”´ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        training_manager.logger.save_logs()
        training_manager.logger.generate_report()
        raise e
# def run_training_loop(args, envs, sync_env, sac, single_gnn_embed, training_manager, num_joints, start_step=0):
#     """è¿è¡Œè®­ç»ƒå¾ªç¯"""
#     current_obs = envs.reset()
#     print(f"åˆå§‹è§‚å¯Ÿ: {current_obs.shape}")
    
#     # é‡ç½®æ¸²æŸ“ç¯å¢ƒ
#     if sync_env:
#         sync_env.reset()
#         print("ğŸ”§ sync_env å·²é‡ç½®")
    
#     current_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)
#     episode_rewards = [0.0] * args.num_processes
#     num_step = 120000
#     total_steps = 0
    
#     print(f"å¼€å§‹è®­ç»ƒ: warmup {sac.warmup_steps} æ­¥")
#     print(f"æ€»è®­ç»ƒæ­¥æ•°: {num_step}, æ›´æ–°é¢‘ç‡: {args.update_frequency}")
#     if start_step > 0:
#         print(f"ä»æ­¥éª¤ {start_step} æ¢å¤è®­ç»ƒ")
#     else:
#         print(f"é¢„æœŸwarmupå®Œæˆæ­¥éª¤: {sac.warmup_steps}")

#     training_completed = False
#     early_termination_reason = ""

#     try:
#         for step in range(start_step, num_step):
#             # è¿›åº¦æ˜¾ç¤º
#             if step % 100 == 0:
#                 if step < sac.warmup_steps:
#                     smart_print(f"Step {step}/{num_step}: Warmup phase ({step}/{sac.warmup_steps})")
#                 else:
#                     smart_print(f"Step {step}/{num_step}: Training phase, Buffer size: {len(sac.memory)}")

#             # è·å–åŠ¨ä½œ
#             if step < sac.warmup_steps:
#                 action_batch = torch.from_numpy(np.array([
#                     envs.action_space.sample() for _ in range(args.num_processes)
#                 ]))
#             else:
#                 actions = []
#                 for proc_id in range(args.num_processes):
#                     action = sac.get_action(
#                         current_obs[proc_id],
#                                             current_gnn_embeds[proc_id],
#                                             num_joints=envs.action_space.shape[0],
#                         deterministic=False
#                     )
#                     actions.append(action)
#                 action_batch = torch.stack(actions)

#             # åŠ¨ä½œåˆ†æï¼ˆè°ƒè¯•ç”¨ï¼‰
#             if step % 50 == 0 or step < 20:
#                 if hasattr(envs, 'envs') and len(envs.envs) > 0:
#                     env_goal = getattr(envs.envs[0], 'goal_pos', 'NOT FOUND')
#                     print(f"ğŸ¯ [è®­ç»ƒ] Step {step} - ç¯å¢ƒgoal_pos: {env_goal}")
                
#                 smart_print(f"\nğŸ¯ Step {step} Action Analysis:")
#                 action_numpy = action_batch.cpu().numpy() if hasattr(action_batch, 'cpu') else action_batch.numpy()
                
#                 for proc_id in range(min(args.num_processes, 2)):
#                     action_values = action_numpy[proc_id]
#                     action_str = ', '.join([f"{val:+6.2f}" for val in action_values])
#                     smart_print(f"  Process {proc_id}: Actions = [{action_str}]")
#                     smart_print(f"    Max action: {np.max(np.abs(action_values)):6.2f}, Mean abs: {np.mean(np.abs(action_values)):6.2f}")

#             # æ‰§è¡ŒåŠ¨ä½œ
#             next_obs, reward, done, infos = envs.step(action_batch)

#             # æ¸²æŸ“å¤„ç†
#             if sync_env:
#                 sync_action = action_batch[0].cpu().numpy() if hasattr(action_batch, 'cpu') else action_batch[0]
#                 sync_env.step(sync_action)
#                 sync_env.render()

#             next_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)

#             # å­˜å‚¨ç»éªŒ
#             for proc_id in range(args.num_processes):
#                 sac.store_experience(
#                         obs=current_obs[proc_id],
#                         gnn_embeds=current_gnn_embeds[proc_id],
#                         action=action_batch[proc_id],
#                         reward=reward[proc_id],
#                         next_obs=next_obs[proc_id],
#                         next_gnn_embeds=next_gnn_embeds[proc_id],
#                         done=done[proc_id],
#                         num_joints=num_joints
#                 )
#                 episode_rewards[proc_id] += reward[proc_id].item()

#             current_obs = next_obs.clone()
#             current_gnn_embeds = next_gnn_embeds.clone()

#             # å¤„ç†episodeç»“æŸ
#             for proc_id in range(args.num_processes):
#                 is_done = done[proc_id].item() if torch.is_tensor(done[proc_id]) else bool(done[proc_id])
#                 if is_done:
#                     should_end = training_manager.handle_episode_end(proc_id, step, episode_rewards, infos)
#                     if should_end:
#                         training_completed = True
#                         early_termination_reason = f"è¿ç»­æˆåŠŸ{training_manager.consecutive_success_count}æ¬¡ï¼Œè¾¾åˆ°è®­ç»ƒç›®æ ‡"
#                         break
                    
#                     if hasattr(envs, 'reset_one'):
#                         current_obs[proc_id] = envs.reset_one(proc_id)
#                         current_gnn_embeds[proc_id] = single_gnn_embed
            
#             # æ¨¡å‹æ›´æ–°
#             if training_manager.should_update_model(step):
#                 training_manager.update_and_log(step, total_steps)
            
#             # å®šæœŸä¿å­˜å’Œç»˜å›¾
#             if step % 1000 == 0 and step > 0:
#                 training_manager.logger.plot_losses(recent_steps=2000, show=False)
#                 training_manager.model_manager.save_checkpoint(
#                     sac, step,
#                     best_success_rate=training_manager.best_success_rate,
#                     best_min_distance=training_manager.best_min_distance,
#                     consecutive_success_count=training_manager.consecutive_success_count
#                 )
            
#             total_steps += args.num_processes
            
#             if training_completed:
#                 print(f"ğŸ è®­ç»ƒæå‰ç»ˆæ­¢: {early_termination_reason}")
#                 break

#     except Exception as e:
#         print(f"ğŸ”´ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
#         training_manager.logger.save_logs()
#         training_manager.logger.generate_report()
#         raise e

def cleanup_resources(sync_env, logger, model_manager, training_manager):
    """æ¸…ç†èµ„æº"""
    if sync_env:
            sync_env.close()
            
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print(f"\n{'='*60}")
    print(f"ğŸ è®­ç»ƒå®Œæˆæ€»ç»“:")
    print(f"  æœ€ä½³æˆåŠŸç‡: {training_manager.best_success_rate:.3f}")
    print(f"  æœ€ä½³æœ€å°è·ç¦»: {training_manager.best_min_distance:.1f} pixels")
    print(f"  å½“å‰è¿ç»­æˆåŠŸæ¬¡æ•°: {training_manager.consecutive_success_count}")
    
    logger.generate_report()
    logger.plot_losses(show=False)
    print(f"ğŸ“Š å®Œæ•´è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {logger.experiment_dir}")
    print(f"{'='*60}")

# === æµ‹è¯•åŠŸèƒ½ ===
# def test_trained_model(model_path, num_episodes=10, render=True):
#     """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½"""
#     print(f"ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_path}")
    
#     # ç¯å¢ƒé…ç½®
#     env_params = {
#         'num_links': DEFAULT_CONFIG['num_links'],
#         'link_lengths': DEFAULT_CONFIG['link_lengths'],
#         'render_mode': 'human' if render else None,
#         'config_path': DEFAULT_CONFIG['config_path']
#     }
    
#     # åˆ›å»ºç¯å¢ƒ
#     env = Reacher2DEnv(**env_params)
#     num_joints = env.action_space.shape[0]
    
#     # åˆ›å»ºGNNç¼–ç å™¨
#     sys.path.append(os.path.join(base_dir, 'examples/2d_reacher/utils'))
#     from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
    
#     reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
#     gnn_embed = reacher2d_encoder.get_gnn_embeds(
#         num_links=num_joints, 
#         link_lengths=env_params['link_lengths']
#     )
    
#     # åˆ›å»ºSACæ¨¡å‹
#     attn_model = AttnModel(128, 130, 130, 4)
#     sac = AttentionSACWithBuffer(attn_model, num_joints, 
#                                 buffer_capacity=10000, batch_size=64,
#                                 lr=1e-5, env_type='reacher2d')
    
#     # åŠ è½½æ¨¡å‹
#     try:
#         if not os.path.exists(model_path):
#             print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
#             return None
            
#         print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
#         model_data = torch.load(model_path, map_location='cpu')
        
#         if 'actor_state_dict' in model_data:
#             sac.actor.load_state_dict(model_data['actor_state_dict'], strict=False)
#             print("âœ… Actor åŠ è½½æˆåŠŸ")
        
#         print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
#         print(f"   è®­ç»ƒæ­¥æ•°: {model_data.get('step', 'N/A')}")
#         print(f"   æ—¶é—´æˆ³: {model_data.get('timestamp', 'N/A')}")
#         if 'success_rate' in model_data:
#             print(f"   è®­ç»ƒæ—¶æˆåŠŸç‡: {model_data.get('success_rate', 'N/A'):.3f}")
#         if 'min_distance' in model_data:
#             print(f"   è®­ç»ƒæ—¶æœ€å°è·ç¦»: {model_data.get('min_distance', 'N/A'):.1f}")
            
#     except Exception as e:
#         print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
#         return None
    
#     # æµ‹è¯•å¤šä¸ªepisode
#     success_count = 0
#     total_rewards = []
#     min_distances = []
#     episode_lengths = []
    
#     print(f"\nğŸ® å¼€å§‹æµ‹è¯• {num_episodes} ä¸ªepisodes...")
#     print(f"ğŸ¯ ç›®æ ‡é˜ˆå€¼: {GOAL_THRESHOLD} pixels")
    
#     for episode in range(num_episodes):
#         obs = env.reset()
#         episode_reward = 0
#         step_count = 0
#         max_steps = 2500
#         min_distance_this_episode = float('inf')
#         episode_success = False
        
#         print(f"\nğŸ“ Episode {episode + 1}/{num_episodes}")
        
#         while step_count < max_steps:
#             # è·å–åŠ¨ä½œï¼ˆä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼‰
#             action = sac.get_action(
#                 torch.from_numpy(obs).float(),
#                 gnn_embed.squeeze(0),
#                 num_joints=num_joints,
#                 deterministic=True
#             )
            
#             # æ‰§è¡ŒåŠ¨ä½œ
#             obs, reward, done, info = env.step(action.cpu().numpy())
#             episode_reward += reward
#             step_count += 1
            
#             # æ£€æŸ¥è·ç¦»
#             end_pos = env._get_end_effector_position()
#             goal_pos = env.goal_pos
#             distance = np.linalg.norm(np.array(end_pos) - goal_pos)
#             min_distance_this_episode = min(min_distance_this_episode, distance)
            
#             # æ¸²æŸ“
#             if render:
#                 env.render()
#                 time.sleep(0.02)
            
#             # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
#             if done:
#                 if not episode_success:
#                     success_count += 1
#                     episode_success = True
#                     print(f"  ğŸ‰ ç›®æ ‡åˆ°è¾¾! è·ç¦»: {distance:.1f} pixels, æ­¥éª¤: {step_count}")
#                 break
        
#         total_rewards.append(episode_reward)
#         min_distances.append(min_distance_this_episode)
#         episode_lengths.append(step_count)
        
#         print(f"  ğŸ“Š Episode {episode + 1} ç»“æœ:")
#         print(f"    å¥–åŠ±: {episode_reward:.2f}")
#         print(f"    æœ€å°è·ç¦»: {min_distance_this_episode:.1f} pixels")
#         print(f"    æ­¥éª¤æ•°: {step_count}")
#         print(f"    æˆåŠŸ: {'âœ… æ˜¯' if episode_success else 'âŒ å¦'}")
    
#     # æµ‹è¯•æ€»ç»“
#     success_rate = success_count / num_episodes
#     avg_reward = np.mean(total_rewards)
#     avg_min_distance = np.mean(min_distances)
#     avg_episode_length = np.mean(episode_lengths)
    
#     print(f"\n{'='*60}")
#     print(f"ğŸ† æµ‹è¯•ç»“æœæ€»ç»“:")
#     print(f"  æµ‹è¯•Episodes: {num_episodes}")
#     print(f"  æˆåŠŸæ¬¡æ•°: {success_count}")
#     print(f"  æˆåŠŸç‡: {success_rate:.1%}")
#     print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
#     print(f"  å¹³å‡æœ€å°è·ç¦»: {avg_min_distance:.1f} pixels")
#     print(f"  å¹³å‡Episodeé•¿åº¦: {avg_episode_length:.1f} steps")
#     print(f"  ç›®æ ‡é˜ˆå€¼: {GOAL_THRESHOLD:.1f} pixels")
    
#     # æ€§èƒ½è¯„ä»·
#     print(f"\nğŸ“‹ æ€§èƒ½è¯„ä»·:")
#     if success_rate >= 0.8:
#         print(f"  ğŸ† ä¼˜ç§€! æˆåŠŸç‡ >= 80%")
#     elif success_rate >= 0.5:
#         print(f"  ğŸ‘ è‰¯å¥½! æˆåŠŸç‡ >= 50%")
#     elif success_rate >= 0.2:
#         print(f"  âš ï¸  ä¸€èˆ¬! æˆåŠŸç‡ >= 20%")
#     else:
#         print(f"  âŒ éœ€è¦æ”¹è¿›! æˆåŠŸç‡ < 20%")
        
#     if avg_min_distance <= GOAL_THRESHOLD:
#         print(f"  âœ… å¹³å‡æœ€å°è·ç¦»è¾¾åˆ°ç›®æ ‡é˜ˆå€¼")
#     else:
#         print(f"  âš ï¸  å¹³å‡æœ€å°è·ç¦»è¶…å‡ºç›®æ ‡é˜ˆå€¼ {avg_min_distance - GOAL_THRESHOLD:.1f} pixels")
    
#     print(f"{'='*60}")
    
#     env.close()
#     return {
#         'success_rate': success_rate,
#         'avg_reward': avg_reward, 
#         'avg_min_distance': avg_min_distance,
#         'avg_episode_length': avg_episode_length,
#         'success_count': success_count,
#         'total_episodes': num_episodes
#     }
def test_trained_model(model_path, num_episodes=10, render=True):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½"""
    print(f"ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_path}")
    
    # ç¯å¢ƒé…ç½®
    env_params = {
        'num_links': DEFAULT_CONFIG['num_links'],
        'link_lengths': DEFAULT_CONFIG['link_lengths'],
        'render_mode': 'human' if render else None,
        'config_path': DEFAULT_CONFIG['config_path']
    }
    
    # ğŸ”§ æ·»åŠ ç¯å¢ƒé…ç½®è°ƒè¯•
    print(f"ğŸ”§ æµ‹è¯•ç¯å¢ƒé…ç½®: {env_params}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = Reacher2DEnv(**env_params)
    num_joints = env.action_space.shape[0]
    
    # ğŸ”§ æ·»åŠ ç¯å¢ƒéªŒè¯
    print(f"ğŸ” æµ‹è¯•ç¯å¢ƒéªŒè¯:")
    print(f"   èµ·å§‹ä½ç½®: {getattr(env, 'anchor_point', 'N/A')}")
    print(f"   ç›®æ ‡ä½ç½®: {getattr(env, 'goal_pos', 'N/A')}")
    print(f"   å…³èŠ‚æ•°é‡: {num_joints}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
    
    # åˆ›å»ºGNNç¼–ç å™¨
    sys.path.append(os.path.join(base_dir, 'examples/2d_reacher/utils'))
    from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
    
    reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
    gnn_embed = reacher2d_encoder.get_gnn_embeds(
        num_links=num_joints, 
        link_lengths=env_params['link_lengths']
    )
    
    print(f"   GNNåµŒå…¥å½¢çŠ¶: {gnn_embed.shape}")
    
    # åˆ›å»ºSACæ¨¡å‹
    attn_model = AttnModel(128, 130, 130, 4)
    sac = AttentionSACWithBuffer(attn_model, num_joints, 
                                buffer_capacity=10000, batch_size=64,
                                lr=1e-5, env_type='reacher2d')
    
    # åŠ è½½æ¨¡å‹
    try:
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
            
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
        model_data = torch.load(model_path, map_location='cpu')
        
        if 'actor_state_dict' in model_data:
            sac.actor.load_state_dict(model_data['actor_state_dict'], strict=False)
            print("âœ… Actor åŠ è½½æˆåŠŸ")
        
        # ğŸ”§ ä¿®å¤æ¨¡å‹éªŒè¯ä»£ç 
        print(f"ğŸ” æ¨¡å‹éªŒè¯:")
        print(f"   Actorå‚æ•°æ•°é‡: {sum(p.numel() for p in sac.actor.parameters())}")
        
        # ğŸ”§ å®‰å…¨çš„æƒé‡æ£€æŸ¥
        try:
            first_param = next(iter(sac.actor.parameters()))
            if first_param.numel() > 5:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å…ƒç´ 
                print(f"   Actorç¬¬ä¸€å±‚æƒé‡ç¤ºä¾‹: {first_param.flatten()[:5]}")
            else:
                print(f"   Actorç¬¬ä¸€å±‚æƒé‡: {first_param}")
        except Exception as e:
            print(f"   æƒé‡æ£€æŸ¥è·³è¿‡: {e}")
        
        print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"   è®­ç»ƒæ­¥æ•°: {model_data.get('step', 'N/A')}")
        print(f"   æ—¶é—´æˆ³: {model_data.get('timestamp', 'N/A')}")
        if 'success_rate' in model_data:
            print(f"   è®­ç»ƒæ—¶æˆåŠŸç‡: {model_data.get('success_rate', 'N/A'):.3f}")
        if 'min_distance' in model_data:
            print(f"   è®­ç»ƒæ—¶æœ€å°è·ç¦»: {model_data.get('min_distance', 'N/A'):.1f}")
            
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None
    
    # æµ‹è¯•å¤šä¸ªepisode
    success_count = 0
    total_rewards = []
    min_distances = []
    episode_lengths = []
    
    print(f"\nğŸ® å¼€å§‹æµ‹è¯• {num_episodes} ä¸ªepisodes...")
    print(f"ğŸ¯ ç›®æ ‡é˜ˆå€¼: {GOAL_THRESHOLD} pixels")
    
    for episode in range(num_episodes):
        obs = env.reset()
        episode_reward = 0
        step_count = 0
        max_steps = 2500
        min_distance_this_episode = float('inf')
        episode_success = False
        
        print(f"\nğŸ“ Episode {episode + 1}/{num_episodes}")
        print(f"   åˆå§‹è§‚å¯Ÿå½¢çŠ¶: {obs.shape}")
        print(f"   åˆå§‹æœ«ç«¯ä½ç½®: {env._get_end_effector_position()}")
        
        # è®¡ç®—åˆå§‹è·ç¦»
        initial_distance = np.linalg.norm(np.array(env._get_end_effector_position()) - env.goal_pos)
        print(f"   åˆå§‹ç›®æ ‡è·ç¦»: {initial_distance:.1f}px")
        
        while step_count < max_steps:
            # ğŸ”§ æ·»åŠ åŠ¨ä½œè°ƒè¯•
            if step_count % 100 == 0 or step_count < 5:
                # è·å–åŠ¨ä½œï¼ˆä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼‰
                action = sac.get_action(
                    torch.from_numpy(obs).float(),
                    gnn_embed.squeeze(0),
                    num_joints=num_joints,
                    deterministic=True
                )
                print(f"   Step {step_count}: Action = {action.detach().cpu().numpy()}")
                print(f"   Step {step_count}: æœ«ç«¯ä½ç½® = {env._get_end_effector_position()}")
                
                # è®¡ç®—è·ç¦»
                end_pos = env._get_end_effector_position()
                goal_pos = env.goal_pos
                distance = np.linalg.norm(np.array(end_pos) - goal_pos)
                print(f"   Step {step_count}: è·ç¦» = {distance:.1f}px")
                
                # ğŸ”§ å°è¯•éç¡®å®šæ€§åŠ¨ä½œè¿›è¡Œå¯¹æ¯”
                if step_count == 0:  # åªåœ¨ç¬¬ä¸€æ­¥å¯¹æ¯”
                    action_random = sac.get_action(
                        torch.from_numpy(obs).float(),
                        gnn_embed.squeeze(0),
                        num_joints=num_joints,
                        deterministic=False
                    )
                    print(f"   Step {step_count}: éšæœºAction = {action_random.detach().cpu().numpy()}")
            else:
                # è·å–åŠ¨ä½œï¼ˆä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼‰
                action = sac.get_action(
                    torch.from_numpy(obs).float(),
                    gnn_embed.squeeze(0),
                    num_joints=num_joints,
                    deterministic=True  # ğŸ”§ å¯ä»¥æ”¹ä¸ºFalseè¯•è¯•
                )
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = env.step(action.cpu().numpy())
            episode_reward += reward
            step_count += 1
            
            # æ£€æŸ¥è·ç¦»
            end_pos = env._get_end_effector_position()
            goal_pos = env.goal_pos
            distance = np.linalg.norm(np.array(end_pos) - goal_pos)
            min_distance_this_episode = min(min_distance_this_episode, distance)
            
            # æ¸²æŸ“
            if render:
                env.render()
                time.sleep(0.02)
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
            if done:
                if not episode_success:
                    success_count += 1
                    episode_success = True
                    print(f"  ğŸ‰ ç›®æ ‡åˆ°è¾¾! è·ç¦»: {distance:.1f} pixels, æ­¥éª¤: {step_count}")
                break
        
        total_rewards.append(episode_reward)
        min_distances.append(min_distance_this_episode)
        episode_lengths.append(step_count)
        
        print(f"  ğŸ“Š Episode {episode + 1} ç»“æœ:")
        print(f"    å¥–åŠ±: {episode_reward:.2f}")
        print(f"    æœ€å°è·ç¦»: {min_distance_this_episode:.1f} pixels")
        print(f"    æ­¥éª¤æ•°: {step_count}")
        print(f"    æˆåŠŸ: {'âœ… æ˜¯' if episode_success else 'âŒ å¦'}")
        print(f"    è·ç¦»æ”¹å–„: {initial_distance - min_distance_this_episode:.1f}px")
    
    # æµ‹è¯•æ€»ç»“
    success_rate = success_count / num_episodes
    avg_reward = np.mean(total_rewards)
    avg_min_distance = np.mean(min_distances)
    avg_episode_length = np.mean(episode_lengths)
    
    print(f"\n{'='*60}")
    print(f"ğŸ† æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"  æµ‹è¯•Episodes: {num_episodes}")
    print(f"  æˆåŠŸæ¬¡æ•°: {success_count}")
    print(f"  æˆåŠŸç‡: {success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"  å¹³å‡æœ€å°è·ç¦»: {avg_min_distance:.1f} pixels")
    print(f"  å¹³å‡Episodeé•¿åº¦: {avg_episode_length:.1f} steps")
    print(f"  ç›®æ ‡é˜ˆå€¼: {GOAL_THRESHOLD:.1f} pixels")
    
    # æ€§èƒ½è¯„ä»·
    print(f"\nğŸ“‹ æ€§èƒ½è¯„ä»·:")
    if success_rate >= 0.8:
        print(f"  ğŸ† ä¼˜ç§€! æˆåŠŸç‡ >= 80%")
    elif success_rate >= 0.5:
        print(f"  ğŸ‘ è‰¯å¥½! æˆåŠŸç‡ >= 50%")
    elif success_rate >= 0.2:
        print(f"  âš ï¸  ä¸€èˆ¬! æˆåŠŸç‡ >= 20%")
    else:
        print(f"  âŒ éœ€è¦æ”¹è¿›! æˆåŠŸç‡ < 20%")
        
    if avg_min_distance <= GOAL_THRESHOLD:
        print(f"  âœ… å¹³å‡æœ€å°è·ç¦»è¾¾åˆ°ç›®æ ‡é˜ˆå€¼")
    else:
        print(f"  âš ï¸  å¹³å‡æœ€å°è·ç¦»è¶…å‡ºç›®æ ‡é˜ˆå€¼ {avg_min_distance - GOAL_THRESHOLD:.1f} pixels")
    
    print(f"{'='*60}")
    
    env.close()
    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward, 
        'avg_min_distance': avg_min_distance,
        'avg_episode_length': avg_episode_length,
        'success_count': success_count,
        'total_episodes': num_episodes
    }
def find_latest_model():
    """æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶"""
    base_path = "./trained_models/reacher2d/enhanced_test"
    
    if not os.path.exists(base_path):
        print(f"âŒ è®­ç»ƒæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {base_path}")
        return None
    
    model_candidates = []
    
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.pth') and ('final_model' in file or 'checkpoint' in file):
                full_path = os.path.join(root, file)
                mtime = os.path.getmtime(full_path)
                model_candidates.append((full_path, mtime))
    
    if not model_candidates:
        print(f"âŒ åœ¨ {base_path} ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        return None
    
    model_candidates.sort(key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ” æ‰¾åˆ° {len(model_candidates)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
    for i, (path, mtime) in enumerate(model_candidates[:5]):
        import datetime
        time_str = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        print(f"  {i+1}. {os.path.basename(path)} ({time_str})")
    
    latest_model = model_candidates[0][0]
    print(f"âœ… é€‰æ‹©æœ€æ–°æ¨¡å‹: {latest_model}")
    return latest_model

# === ä¸»ç¨‹åºå…¥å£ ===
if __name__ == "__main__":
    torch.set_default_dtype(torch.float64)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == '--test':
        print("ğŸ§ª è¿›å…¥æµ‹è¯•æ¨¡å¼")
        
        model_path = None
        num_episodes = 5
        render = True
        
        for i, arg in enumerate(sys.argv):
            if arg == '--model-path' and i + 1 < len(sys.argv):
                model_path = sys.argv[i + 1]
            elif arg == '--episodes' and i + 1 < len(sys.argv):
                num_episodes = int(sys.argv[i + 1])
            elif arg == '--no-render':
                render = False
            elif arg == '--latest':
                model_path = 'latest'
        
        if model_path is None or model_path == 'latest':
            print("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹...")
            model_path = find_latest_model()
        
        if model_path:
            print(f"ğŸ¯ æµ‹è¯•å‚æ•°: episodes={num_episodes}, render={render}")
            result = test_trained_model(model_path, num_episodes, render)
            
            if result:
                print(f"\nğŸ¯ å¿«é€Ÿç»“è®º:")
                if result['success_rate'] >= 0.8:
                    print(f"  âœ… æ¨¡å‹è¡¨ç°ä¼˜ç§€! ç»§ç»­å½“å‰è®­ç»ƒç­–ç•¥")
                elif result['success_rate'] >= 0.3:
                    print(f"  âš ï¸  æ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´å‚æ•°")
                else:
                    print(f"  âŒ æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦é‡æ–°å®¡è§†å¥–åŠ±å‡½æ•°æˆ–ç½‘ç»œç»“æ„")
        else:
            print("âŒ æœªæ‰¾åˆ°å¯æµ‹è¯•çš„æ¨¡å‹")
        
        exit(0)
    
    # è®­ç»ƒæ¨¡å¼ - å‚æ•°è§£æ
    parser = create_training_parser()
    args = parser.parse_args()

    args.cuda = not args.no_cuda and torch.cuda.is_available()
    args.save_dir = os.path.join(args.save_dir, get_time_stamp())
    os.makedirs(args.save_dir, exist_ok=True)
    
    # ä¿å­˜å‚æ•°
    with open(os.path.join(args.save_dir, 'args.txt'), 'w') as f:
        f.write(str(sys.argv))

    main(args)