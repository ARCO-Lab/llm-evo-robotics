#!/usr/bin/env python3
"""
ç½‘ç»œæŸå¤±è®°å½•å™¨ - ç‹¬ç«‹è¿›ç¨‹ç‰ˆæœ¬
åŠŸèƒ½ï¼š
- ç‹¬ç«‹è¿›ç¨‹è®°å½•attentionã€GNNã€PPOç½‘ç»œçš„æ¯æ­¥æŸå¤±
- å®æ—¶ç”ŸæˆæŸå¤±æ›²çº¿å›¾è¡¨
- æ”¯æŒå¤šç½‘ç»œåŒæ—¶ç›‘æ§
- æä¾›æŸå¤±ç»Ÿè®¡åˆ†æå’Œé¢„è­¦
- ä¸MAP-Elitesè®­ç»ƒç³»ç»Ÿé›†æˆ
"""

import os
import json
import time
import queue
import threading
import multiprocessing as mp
from multiprocessing import Queue, Process, Event, Manager
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from collections import defaultdict, deque
from datetime import datetime
import csv
import pickle
import signal
import sys

# è®¾ç½®matplotlibåç«¯ï¼Œé¿å…GUIé—®é¢˜
plt.switch_backend('Agg')

class NetworkLossCollector:
    """å•ä¸ªç½‘ç»œçš„æŸå¤±æ”¶é›†å™¨"""
    
    def __init__(self, network_name, max_history=50000):
        self.network_name = network_name
        self.max_history = max_history
        
        # æŸå¤±å†å²
        self.loss_history = defaultdict(deque)
        self.step_history = deque(maxlen=max_history)
        self.timestamp_history = deque(maxlen=max_history)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_updates': 0,
            'avg_loss': 0.0,
            'min_loss': float('inf'),
            'max_loss': float('-inf'),
            'recent_trend': 'stable',  # 'increasing', 'decreasing', 'stable'
            'last_update': time.time()
        }
        
    def add_loss(self, step, timestamp, loss_dict):
        """æ·»åŠ æŸå¤±æ•°æ®"""
        self.step_history.append(step)
        self.timestamp_history.append(timestamp)
        
        for loss_name, loss_value in loss_dict.items():
            if isinstance(loss_value, (int, float)) and not np.isnan(loss_value):
                self.loss_history[loss_name].append(loss_value)
                
                # ç»´æŠ¤æœ€å¤§å†å²é•¿åº¦
                if len(self.loss_history[loss_name]) > self.max_history:
                    self.loss_history[loss_name].popleft()
        
        self.stats['total_updates'] += 1
        self.stats['last_update'] = timestamp
        self._update_stats()
    
    def _update_stats(self):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        if not self.loss_history:
            return
            
        # è®¡ç®—ä¸»è¦æŸå¤±çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡è®¾ç¬¬ä¸€ä¸ªæŸå¤±æ˜¯ä¸»è¦çš„ï¼‰
        main_loss_name = list(self.loss_history.keys())[0]
        main_losses = list(self.loss_history[main_loss_name])
        
        if main_losses:
            self.stats['avg_loss'] = np.mean(main_losses)
            self.stats['min_loss'] = min(self.stats['min_loss'], min(main_losses))
            self.stats['max_loss'] = max(self.stats['max_loss'], max(main_losses))
            
            # è®¡ç®—è¶‹åŠ¿ï¼ˆæœ€è¿‘20ä¸ªç‚¹çš„æ–œç‡ï¼‰
            if len(main_losses) >= 20:
                recent_losses = main_losses[-20:]
                x = np.arange(len(recent_losses))
                slope = np.polyfit(x, recent_losses, 1)[0]
                
                if slope > 0.001:
                    self.stats['recent_trend'] = 'increasing'
                elif slope < -0.001:
                    self.stats['recent_trend'] = 'decreasing'
                else:
                    self.stats['recent_trend'] = 'stable'


class NetworkLossLogger:
    """ç½‘ç»œæŸå¤±è®°å½•å™¨ä¸»ç±»"""
    
    def __init__(self, log_dir="network_loss_logs", experiment_name=None, 
                 networks=['attention', 'ppo', 'gnn'], update_interval=10.0):
        """
        åˆå§‹åŒ–ç½‘ç»œæŸå¤±è®°å½•å™¨
        
        Args:
            log_dir: æ—¥å¿—ç›®å½•
            experiment_name: å®éªŒåç§°
            networks: è¦ç›‘æ§çš„ç½‘ç»œåˆ—è¡¨
            update_interval: å›¾è¡¨æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
        """
        self.log_dir = log_dir
        self.experiment_name = experiment_name or f"network_loss_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.experiment_dir = os.path.join(log_dir, self.experiment_name)
        self.networks = networks
        self.update_interval = update_interval
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.experiment_dir, exist_ok=True)
        
        # è¿›ç¨‹é—´é€šä¿¡
        self.loss_queue = Queue(maxsize=50000)
        self.control_queue = Queue()
        self.stop_event = Event()
        
        # è®°å½•è¿›ç¨‹
        self.logger_process = None
        
        # é…ç½®ä¿¡æ¯
        self.config = {
            'experiment_info': {
                'experiment_name': self.experiment_name,
                'start_time': datetime.now().isoformat(),
                'log_dir': self.experiment_dir,
                'monitored_networks': networks,
                'update_interval': update_interval
            }
        }
        
        print(f"ğŸš€ ç½‘ç»œæŸå¤±è®°å½•å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   å®éªŒåç§°: {self.experiment_name}")
        print(f"   ç›‘æ§ç½‘ç»œ: {', '.join(networks)}")
        print(f"   æ—¥å¿—ç›®å½•: {self.experiment_dir}")
        
        # ä¿å­˜é…ç½®
        self._save_config()
        
    def start_logging(self):
        """å¯åŠ¨è®°å½•è¿›ç¨‹"""
        if self.logger_process is not None and self.logger_process.is_alive():
            print("âš ï¸  è®°å½•è¿›ç¨‹å·²åœ¨è¿è¡Œ")
            return
            
        self.stop_event.clear()
        self.logger_process = Process(
            target=self._logging_worker, 
            args=(self.loss_queue, self.control_queue, self.stop_event, 
                  self.experiment_dir, self.networks, self.update_interval)
        )
        self.logger_process.daemon = True
        self.logger_process.start()
        
        print(f"âœ… è®°å½•è¿›ç¨‹å·²å¯åŠ¨ (PID: {self.logger_process.pid})")
        
    def stop_logging(self):
        """åœæ­¢è®°å½•è¿›ç¨‹"""
        if self.logger_process is None:
            print("âš ï¸  è®°å½•è¿›ç¨‹æœªåœ¨è¿è¡Œ")
            return
            
        print("ğŸ›‘ æ­£åœ¨åœæ­¢è®°å½•è¿›ç¨‹...")
        self.stop_event.set()
        
        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        self.logger_process.join(timeout=15)
        if self.logger_process.is_alive():
            print("âš ï¸  å¼ºåˆ¶ç»ˆæ­¢è®°å½•è¿›ç¨‹")
            self.logger_process.terminate()
            self.logger_process.join()
            
        self.logger_process = None
        print("âœ… è®°å½•è¿›ç¨‹å·²åœæ­¢")
        
    def log_loss(self, network_name, step, loss_dict, timestamp=None):
        """è®°å½•æŸå¤±æ•°æ®"""
        if network_name not in self.networks:
            print(f"âš ï¸  æœªçŸ¥ç½‘ç»œ: {network_name}, æ”¯æŒçš„ç½‘ç»œ: {self.networks}")
            return
            
        timestamp = timestamp or time.time()
        
        try:
            # å‘é€åˆ°è®°å½•è¿›ç¨‹
            loss_data = {
                'network': network_name,
                'step': step,
                'timestamp': timestamp,
                'losses': loss_dict
            }
            
            # éé˜»å¡å‘é€ï¼Œå¦‚æœé˜Ÿåˆ—æ»¡äº†å°±è·³è¿‡æ—§æ•°æ®
            if self.loss_queue.full():
                # æ¸…ç†ä¸€äº›æ—§æ•°æ®
                for _ in range(100):
                    try:
                        self.loss_queue.get_nowait()
                    except queue.Empty:
                        break
                        
            self.loss_queue.put_nowait(loss_data)
                
        except Exception as e:
            print(f"âŒ è®°å½•æŸå¤±å¤±è´¥: {e}")
            
    def is_alive(self):
        """æ£€æŸ¥è®°å½•è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ"""
        return self.logger_process is not None and self.logger_process.is_alive()
        
    def _save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        config_path = os.path.join(self.experiment_dir, 'config.json')
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
            
    @staticmethod
    def _logging_worker(loss_queue, control_queue, stop_event, experiment_dir, 
                       networks, update_interval):
        """è®°å½•è¿›ç¨‹å·¥ä½œå‡½æ•°"""
        print(f"ğŸ“Š æŸå¤±è®°å½•è¿›ç¨‹å¯åŠ¨ (PID: {os.getpid()})")
        
        # åˆå§‹åŒ–æ”¶é›†å™¨
        collectors = {network: NetworkLossCollector(network) for network in networks}
        
        # è®¾ç½®ä¿¡å·å¤„ç†
        def signal_handler(signum, frame):
            print(f"ğŸ“Š è®°å½•è¿›ç¨‹æ¥æ”¶åˆ°ä¿¡å· {signum}")
            stop_event.set()
            
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
        
        # ğŸ†• å¯åŠ¨å®æ—¶æŸå¤±æ”¶é›†å™¨
        try:
            from loss_communication import RealTimeLossCollector
            experiment_name = os.path.basename(experiment_dir).replace('_loss_log', '')
            real_time_collector = RealTimeLossCollector(experiment_name, None)
            
            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œå®æ—¶æ”¶é›†å™¨
            collect_thread = threading.Thread(
                target=real_time_collector.start_collecting,
                daemon=True
            )
            collect_thread.start()
            print("ğŸ”„ å®æ—¶æŸå¤±æ”¶é›†å™¨å·²å¯åŠ¨")
            
        except ImportError as e:
            print(f"âš ï¸ å®æ—¶æŸå¤±æ”¶é›†å™¨ä¸å¯ç”¨: {e}")
            real_time_collector = None
        
        # åˆ›å»ºå›¾è¡¨æ›´æ–°çº¿ç¨‹
        plot_thread = threading.Thread(
            target=NetworkLossLogger._plot_worker,
            args=(collectors, experiment_dir, stop_event, update_interval),
            daemon=True
        )
        plot_thread.start()
        
        # ä¸»å¾ªç¯
        last_save_time = time.time()
        save_interval = 30.0  # æ¯30ç§’ä¿å­˜ä¸€æ¬¡æ•°æ®
        
        try:
            while not stop_event.is_set():
                try:
                    # å¤„ç†æŸå¤±æ•°æ®
                    loss_data = loss_queue.get(timeout=1.0)
                    
                    network = loss_data['network']
                    step = loss_data['step']
                    timestamp = loss_data['timestamp']
                    losses = loss_data['losses']
                    
                    # æ·»åŠ åˆ°å¯¹åº”çš„æ”¶é›†å™¨
                    if network in collectors:
                        collectors[network].add_loss(step, timestamp, losses)
                        
                    # å®šæœŸä¿å­˜æ•°æ®
                    current_time = time.time()
                    if current_time - last_save_time > save_interval:
                        NetworkLossLogger._save_data(collectors, experiment_dir)
                        last_save_time = current_time
                        
                except queue.Empty:
                    continue
                except Exception as e:
                    print(f"âŒ è®°å½•è¿›ç¨‹é”™è¯¯: {e}")
                    
        except KeyboardInterrupt:
            print("ğŸ“Š è®°å½•è¿›ç¨‹è¢«ä¸­æ–­")
        finally:
            # åœæ­¢å®æ—¶æ”¶é›†å™¨
            if real_time_collector:
                real_time_collector.stop_collecting()
            
            # æœ€ç»ˆä¿å­˜æ•°æ®
            NetworkLossLogger._save_data(collectors, experiment_dir)
            stop_event.set()  # ç¡®ä¿å›¾è¡¨çº¿ç¨‹ä¹Ÿåœæ­¢
            plot_thread.join(timeout=5)
            print("ğŸ“Š è®°å½•è¿›ç¨‹ç»“æŸ")
            
    @staticmethod
    def _plot_worker(collectors, experiment_dir, stop_event, update_interval):
        """å›¾è¡¨æ›´æ–°å·¥ä½œçº¿ç¨‹"""
        print(f"ğŸ“ˆ å›¾è¡¨æ›´æ–°çº¿ç¨‹å¯åŠ¨")
        
        plt.style.use('default')
        
        while not stop_event.is_set():
            try:
                NetworkLossLogger._generate_plots(collectors, experiment_dir)
                time.sleep(update_interval)
            except Exception as e:
                print(f"âŒ å›¾è¡¨ç”Ÿæˆé”™è¯¯: {e}")
                time.sleep(update_interval)
                
        print("ğŸ“ˆ å›¾è¡¨æ›´æ–°çº¿ç¨‹ç»“æŸ")
        
    @staticmethod
    def _generate_plots(collectors, experiment_dir):
        """ç”ŸæˆæŸå¤±æ›²çº¿å›¾"""
        if not collectors:
            return
            
        # åˆ›å»ºå­å›¾
        n_networks = len(collectors)
        fig, axes = plt.subplots(n_networks, 1, figsize=(15, 5*n_networks))
        if n_networks == 1:
            axes = [axes]
            
        fig.suptitle(f'Network Loss Real-time Monitor - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}', 
                     fontsize=16, fontweight='bold')
        
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']
        
        for idx, (network_name, collector) in enumerate(collectors.items()):
            ax = axes[idx]
            
            if not collector.loss_history:
                ax.text(0.5, 0.5, f'{network_name.upper()}\nNo Data', 
                       ha='center', va='center', transform=ax.transAxes, fontsize=14)
                ax.set_title(f'{network_name.upper()} Network Loss')
                continue
                
            # ç»˜åˆ¶æ¯ç§æŸå¤±
            steps = list(collector.step_history)
            
            for loss_idx, (loss_name, loss_values) in enumerate(collector.loss_history.items()):
                if len(loss_values) == len(steps):
                    color = colors[loss_idx % len(colors)]
                    ax.plot(steps, list(loss_values), label=loss_name, 
                           color=color, linewidth=1.5, alpha=0.8)
                    
                    # æ·»åŠ æœ€è¿‘çš„è¶‹åŠ¿çº¿
                    if len(loss_values) > 20:
                        recent_steps = steps[-20:]
                        recent_losses = list(loss_values)[-20:]
                        z = np.polyfit(range(len(recent_losses)), recent_losses, 1)
                        trend_line = np.poly1d(z)(range(len(recent_losses)))
                        ax.plot(recent_steps, trend_line, '--', color=color, alpha=0.5, linewidth=1)
            
            # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
            ax.set_title(f'{network_name.upper()} Network Loss (Updates: {collector.stats["total_updates"]})')
            ax.set_xlabel('Training Steps')
            ax.set_ylabel('Loss Value')
            ax.legend(loc='upper right', fontsize=8)
            ax.grid(True, alpha=0.3)
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
            last_update_str = datetime.fromtimestamp(collector.stats['last_update']).strftime('%H:%M:%S')
            stats_text = f'Trend: {collector.stats["recent_trend"]}\n'
            stats_text += f'Average: {collector.stats["avg_loss"]:.4f}\n'
            stats_text += f'Range: [{collector.stats["min_loss"]:.4f}, {collector.stats["max_loss"]:.4f}]\n'
            stats_text += f'Last Update: {last_update_str}'
            
            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, 
                   verticalalignment='top', fontsize=8,
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_path = os.path.join(experiment_dir, 'network_loss_curves_realtime.png')
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # ä¿å­˜å¸¦æ—¶é—´æˆ³çš„ç‰ˆæœ¬ï¼ˆæ¯å°æ—¶ä¿å­˜ä¸€æ¬¡ï¼‰
        current_hour = datetime.now().strftime("%Y%m%d_%H")
        timestamp_plot_path = os.path.join(experiment_dir, f'network_loss_curves_{current_hour}00.png')
        if not os.path.exists(timestamp_plot_path):
            plt.savefig(timestamp_plot_path, dpi=150, bbox_inches='tight')
        
    @staticmethod
    def _save_data(collectors, experiment_dir):
        """ä¿å­˜æŸå¤±æ•°æ®"""
        for network_name, collector in collectors.items():
            # ä¿å­˜CSVæ ¼å¼
            csv_path = os.path.join(experiment_dir, f'{network_name}_losses.csv')
            
            if collector.step_history and collector.loss_history:
                with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
                    # è·å–æ‰€æœ‰æŸå¤±ç±»å‹
                    loss_names = list(collector.loss_history.keys())
                    fieldnames = ['step', 'timestamp'] + loss_names
                    
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    
                    steps = list(collector.step_history)
                    timestamps = list(collector.timestamp_history)
                    
                    for i in range(len(steps)):
                        row = {
                            'step': steps[i],
                            'timestamp': timestamps[i] if i < len(timestamps) else ''
                        }
                        
                        # æ·»åŠ æŸå¤±å€¼
                        for loss_name in loss_names:
                            loss_values = list(collector.loss_history[loss_name])
                            if i < len(loss_values):
                                row[loss_name] = loss_values[i]
                            else:
                                row[loss_name] = ''
                                
                        writer.writerow(row)
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_path = os.path.join(experiment_dir, f'{network_name}_stats.json')
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(collector.stats, f, indent=2, ensure_ascii=False)


# å…¨å±€å®ä¾‹
_global_logger = None

def init_network_loss_logger(experiment_name=None, networks=['attention', 'ppo', 'gnn'], 
                            log_dir="network_loss_logs", update_interval=10.0):
    """åˆå§‹åŒ–å…¨å±€ç½‘ç»œæŸå¤±è®°å½•å™¨"""
    global _global_logger
    
    if _global_logger is not None:
        print("âš ï¸  ç½‘ç»œæŸå¤±è®°å½•å™¨å·²åˆå§‹åŒ–")
        return _global_logger
        
    _global_logger = NetworkLossLogger(
        log_dir=log_dir,
        experiment_name=experiment_name,
        networks=networks,
        update_interval=update_interval
    )
    
    _global_logger.start_logging()
    
    # æ³¨å†Œé€€å‡ºæ—¶çš„æ¸…ç†å‡½æ•°
    import atexit
    atexit.register(cleanup_network_loss_logger)
    
    return _global_logger

def get_network_loss_logger():
    """è·å–å…¨å±€ç½‘ç»œæŸå¤±è®°å½•å™¨"""
    return _global_logger

def log_network_loss(network_name, step, loss_dict, timestamp=None):
    """è®°å½•ç½‘ç»œæŸå¤±çš„ä¾¿æ·å‡½æ•°"""
    global _global_logger
    if _global_logger is not None:
        _global_logger.log_loss(network_name, step, loss_dict, timestamp)
    else:
        print("âš ï¸  ç½‘ç»œæŸå¤±è®°å½•å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_network_loss_logger()")

def cleanup_network_loss_logger():
    """æ¸…ç†ç½‘ç»œæŸå¤±è®°å½•å™¨"""
    global _global_logger
    if _global_logger is not None:
        _global_logger.stop_logging()
        _global_logger = None
        print("ğŸ§¹ ç½‘ç»œæŸå¤±è®°å½•å™¨å·²æ¸…ç†")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•ç½‘ç»œæŸå¤±è®°å½•å™¨")
    
    # åˆå§‹åŒ–è®°å½•å™¨
    logger = init_network_loss_logger(
        experiment_name="test_network_loss_logger",
        networks=['attention', 'ppo', 'gnn'],
        update_interval=5.0
    )
    
    try:
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        print("ğŸš€ å¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒ...")
        for step in range(1000):
            # æ¨¡æ‹Ÿattentionç½‘ç»œæŸå¤±
            attention_loss = {
                'attention_loss': max(0.1, 2.0 - step*0.001 + np.random.normal(0, 0.1)),
                'attention_accuracy': min(1.0, 0.5 + step * 0.0005 + np.random.normal(0, 0.02))
            }
            log_network_loss('attention', step, attention_loss)
            
            # æ¨¡æ‹ŸPPOç½‘ç»œæŸå¤±
            ppo_loss = {
                'actor_loss': max(0.01, 1.5 - step*0.0008 + np.random.normal(0, 0.08)),
                'critic_loss': max(0.01, 1.2 - step*0.0006 + np.random.normal(0, 0.06)),
                'entropy': max(0.001, 0.8 - step*0.0003 + np.random.normal(0, 0.02))
            }
            log_network_loss('ppo', step, ppo_loss)
            
            # æ¨¡æ‹ŸGNNç½‘ç»œæŸå¤±
            gnn_loss = {
                'gnn_loss': max(0.1, 3.0 - step*0.0012 + np.random.normal(0, 0.15)),
                'node_accuracy': min(1.0, 0.3 + step * 0.0007 + np.random.normal(0, 0.01))
            }
            log_network_loss('gnn', step, gnn_loss)
            
            # æ‰“å°è¿›åº¦
            if step % 100 == 0:
                print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {step}")
                print(f"   æ—¥å¿—ç›®å½•: {logger.experiment_dir}")
            
            time.sleep(0.01)  # æ¨¡æ‹Ÿè®­ç»ƒé—´éš”
            
    except KeyboardInterrupt:
        print("ğŸ›‘ æµ‹è¯•è¢«ä¸­æ–­")
    finally:
        print("âœ… æµ‹è¯•å®Œæˆ")
