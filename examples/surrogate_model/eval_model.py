#!/usr/bin/env python3
"""
åŸºäºtrain.pyçš„æ¨¡å‹è¯„ä¼°è„šæœ¬
é‡ç”¨è®­ç»ƒè„šæœ¬ä¸­å·²éªŒè¯çš„å¯¼å…¥å’Œé…ç½®
"""

import sys
import os
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../')
sys.path.append(base_dir)

sys.path.insert(0, os.path.join(base_dir, 'examples/2d_reacher/envs'))
sys.path.insert(0, os.path.join(base_dir, 'examples/surrogate_model/gnn_encoder'))
sys.path.insert(0, os.path.join(base_dir, 'examples/rl/train'))
sys.path.insert(0, os.path.join(base_dir, 'examples/rl/common'))
sys.path.insert(0, os.path.join(base_dir, 'examples/rl/environments'))
sys.path.append(os.path.join(base_dir, 'examples/rl'))

import numpy as np
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gym
import argparse

gym.logger.set_level(40)

# ç›´æ¥å¯¼å…¥ï¼Œç°åœ¨environmentsåœ¨è·¯å¾„ä¸­
import environments

from arguments import get_parser
from utils import solve_argv_conflict
from common import *

from attn_dataset.sim_data_handler import DataHandler
from attn_model.attn_model import AttnModel
from sac.sac_model import AttentionSACWithBuffer

# ä¿®æ”¹ç¬¬50è¡Œçš„å¯¼å…¥
from env_config.env_wrapper import make_reacher2d_vec_envs, make_smart_reacher2d_vec_envs
from reacher2d_env import Reacher2DEnv
sys.path.insert(0, os.path.join(base_dir, 'examples/2d_reacher/envs'))


def evaluate_model(model_path, num_episodes=5, render=True):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    
    print(f"ğŸ§ª å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_path}")
    
    # ç¯å¢ƒå‚æ•°ï¼ˆä»train.pyå¤åˆ¶ï¼‰
    env_params = {
        'num_links': 4,
        'link_lengths': [80, 80, 80, 60],
        'render_mode': 'human' if render else None,
        'config_path': "/home/xli149/Documents/repos/RoboGrammar/examples/2d_reacher/configs/reacher_with_zigzag_obstacles.yaml"
    }
    
    print(f"ğŸ—ï¸ ç¯å¢ƒé…ç½®:")
    print(f"   å…³èŠ‚æ•°: {env_params['num_links']}")
    print(f"   è¿æ†é•¿åº¦: {env_params['link_lengths']}")
    print(f"   æ¸²æŸ“: {'æ˜¯' if render else 'å¦'}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = Reacher2DEnv(**env_params)
    num_joints = env.action_space.shape[0]
    
    # åˆ›å»ºGNNç¼–ç å™¨ï¼ˆä»train.pyå¤åˆ¶ï¼‰
    sys.path.append(os.path.join(os.path.dirname(__file__), '../2d_reacher/utils'))
    from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
    
    print("ğŸ¤– åˆå§‹åŒ– Reacher2D GNN ç¼–ç å™¨...")
    reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
    single_gnn_embed = reacher2d_encoder.get_gnn_embeds(
        num_links=num_joints, 
        link_lengths=env_params['link_lengths']
    )
    print(f"âœ… GNN åµŒå…¥ç”ŸæˆæˆåŠŸï¼Œå½¢çŠ¶: {single_gnn_embed.shape}")
    
    # åˆ›å»ºSACæ¨¡å‹ï¼ˆä»train.pyå¤åˆ¶ï¼‰
    action_dim = num_joints
    attn_model = AttnModel(128, 130, 130, 4)
    sac = AttentionSACWithBuffer(attn_model, action_dim, 
                                buffer_capacity=10000, batch_size=32,
                                lr=1e-4,
                                env_type='reacher2d')
    
    # åŠ è½½æ¨¡å‹
    if os.path.exists(model_path):
        model_data = torch.load(model_path, map_location='cpu')
        sac.actor.load_state_dict(model_data['actor_state_dict'])
        if 'critic1_state_dict' in model_data:
            sac.critic1.load_state_dict(model_data['critic1_state_dict'])
        if 'critic2_state_dict' in model_data:
            sac.critic2.load_state_dict(model_data['critic2_state_dict'])
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"   è®­ç»ƒæ­¥éª¤: {model_data.get('step', 'N/A')}")
        print(f"   æˆåŠŸç‡: {model_data.get('final_success_rate', model_data.get('success_rate', 'N/A'))}")
        print(f"   æœ€å°è·ç¦»: {model_data.get('final_min_distance', model_data.get('min_distance', 'N/A'))}")
        print(f"   è®­ç»ƒå®Œæˆ: {model_data.get('training_completed', 'N/A')}")
    else:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return
    
    # æµ‹è¯•æ¨¡å‹
    print(f"\nğŸ® å¼€å§‹æµ‹è¯• {num_episodes} episodes")
    
    success_count = 0
    total_rewards = []
    min_distances = []
    step_counts = []
    goal_threshold = 50.0
    
    for episode in range(num_episodes):
        obs = env.reset()
        episode_reward = 0
        step_count = 0
        max_steps = 1000
        min_distance_this_episode = float('inf')
        
        print(f"\n--- Episode {episode + 1}/{num_episodes} ---")
        
        while step_count < max_steps:
            # è·å–åŠ¨ä½œï¼ˆä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¿›è¡Œè¯„ä¼°ï¼‰
            action = sac.get_action(
                torch.from_numpy(obs).float(),
                single_gnn_embed.squeeze(0),
                num_joints=num_joints,
                deterministic=True  # è¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            )
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = env.step(action.cpu().numpy())
            episode_reward += reward
            step_count += 1
            
            # è®¡ç®—è·ç¦»
            end_pos = env._get_end_effector_position()
            goal_pos = env.goal_pos
            distance = np.linalg.norm(np.array(end_pos) - goal_pos)
            min_distance_this_episode = min(min_distance_this_episode, distance)
            
            # æ¸²æŸ“
            if render:
                env.render()
                time.sleep(0.02)  # æ§åˆ¶é€Ÿåº¦
            
            # æ¯100æ­¥æ‰“å°è¿›åº¦
            if step_count % 100 == 0:
                print(f"  æ­¥éª¤ {step_count}: è·ç¦» {distance:.1f}px, å¥–åŠ± {episode_reward:.1f}")
            
            # æ£€æŸ¥æˆåŠŸ
            if distance <= goal_threshold:
                success_count += 1
                print(f"  ğŸ‰ æˆåŠŸåˆ°è¾¾ç›®æ ‡! è·ç¦»: {distance:.1f}px, æ­¥éª¤: {step_count}")
                break
                
            if done:
                print(f"  Episode ç»“æŸ (done=True)")
                break
        
        # è®°å½•ç»“æœ
        total_rewards.append(episode_reward)
        min_distances.append(min_distance_this_episode)
        step_counts.append(step_count)
        
        print(f"Episode {episode + 1} ç»“æœ:")
        print(f"  æœ€å°è·ç¦»: {min_distance_this_episode:.1f}px")
        print(f"  æ€»å¥–åŠ±: {episode_reward:.2f}")
        print(f"  æ­¥éª¤æ•°: {step_count}")
        print(f"  æˆåŠŸ: {'æ˜¯' if min_distance_this_episode <= goal_threshold else 'å¦'}")
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    success_rate = success_count / num_episodes
    avg_reward = np.mean(total_rewards)
    avg_min_distance = np.mean(min_distances)
    avg_steps = np.mean(step_counts)
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*60}")
    print(f"ğŸ† è¯„ä¼°ç»“æœæ€»ç»“:")
    print(f"  æµ‹è¯•Episodes: {num_episodes}")
    print(f"  æˆåŠŸæ¬¡æ•°: {success_count}")
    print(f"  æˆåŠŸç‡: {success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"  å¹³å‡æœ€å°è·ç¦»: {avg_min_distance:.1f} pixels")
    print(f"  å¹³å‡æ­¥éª¤æ•°: {avg_steps:.1f}")
    print(f"  ç›®æ ‡é˜ˆå€¼: {goal_threshold:.1f} pixels")
    print(f"{'='*60}")
    
    env.close()
    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward,
        'avg_min_distance': avg_min_distance,
        'avg_steps': avg_steps,
        'success_count': success_count,
        'total_episodes': num_episodes
    }


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹")
    parser.add_argument('--model-path', type=str, required=True,
                        help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=5,
                        help='æµ‹è¯•çš„episodeæ•°é‡')
    parser.add_argument('--no-render', action='store_true',
                        help='ä¸æ˜¾ç¤ºæ¸²æŸ“ï¼ˆåŠ å¿«æµ‹è¯•ï¼‰')
    
    args = parser.parse_args()
    
    render = not args.no_render
    evaluate_model(args.model_path, args.episodes, render)


if __name__ == "__main__":
    main() 