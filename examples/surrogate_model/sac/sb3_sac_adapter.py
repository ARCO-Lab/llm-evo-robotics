#!/usr/bin/env python3
"""
Stable Baselines3 SAC é€‚é…å™¨
å°† SB3 SAC åŒ…è£…æˆä¸ç°æœ‰ AttentionSACWithBuffer å…¼å®¹çš„æ¥å£
"""

import torch
import torch.nn as nn
import numpy as np
import os
import sys
from typing import Dict, Any, Optional, Tuple, Union
import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.policies import BasePolicy
from stable_baselines3.common.type_aliases import GymEnv
from stable_baselines3.common.vec_env import VecEnv, DummyVecEnv
import warnings

# æ·»åŠ  SB3 ç¯å¢ƒåŒ…è£…å™¨è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../../2d_reacher/envs"))

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(__file__), "../../../")
sys.path.append(base_dir)
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_dataset"))
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_model"))

from data_utils import prepare_joint_q_input, prepare_reacher2d_joint_q_input, prepare_dynamic_vertex_v


class SB3SACAdapter:
    """
    Stable Baselines3 SAC é€‚é…å™¨
    æä¾›ä¸ AttentionSACWithBuffer å…¼å®¹çš„æ¥å£
    """
    
    def __init__(self, 
                 attn_model=None,  # å…¼å®¹æ€§å‚æ•°ï¼ŒSB3ä¸éœ€è¦
                 action_dim: int = 2,
                 joint_embed_dim: int = 128,  # å…¼å®¹æ€§å‚æ•°
                 buffer_capacity: int = 1000000,
                 batch_size: int = 256,
                 lr: float = 3e-4,
                 gamma: float = 0.99,
                 tau: float = 0.005,
                 alpha: float = 0.2,
                 device: str = 'cpu',
                 env_type: str = 'bullet',
                 env: Optional[GymEnv] = None,
                 policy: str = "MlpPolicy",
                 **kwargs):
        
        self.device = device
        self.action_dim = action_dim
        self.batch_size = batch_size
        self.env_type = env_type
        self.warmup_steps = 100  # å¿«é€Ÿå¼€å§‹å­¦ä¹ 
        
        # ğŸ¯ ä¼˜åŒ–çš„ SAC å‚æ•°ï¼Œä¸“é—¨é’ˆå¯¹ Reacher ä»»åŠ¡
        self.sac_params = {
            'policy': policy,
            'learning_rate': max(lr, 1e-3),  # ç¡®ä¿å­¦ä¹ ç‡ä¸ä½äº 1e-3
            'buffer_size': min(buffer_capacity, 100000),  # é€‚ä¸­çš„ç¼“å†²åŒºå¤§å°
            'batch_size': batch_size,
            'tau': max(tau, 0.01),  # æ›´å¿«çš„ç›®æ ‡ç½‘ç»œæ›´æ–°
            'gamma': min(gamma, 0.98),  # ç¨å¾®é™ä½æŠ˜æ‰£å› å­
            'train_freq': (4, 'step'),  # æ›´é¢‘ç¹çš„è®­ç»ƒ
            'gradient_steps': 4,  # æ¯æ¬¡è®­ç»ƒæ›´å¤šæ¢¯åº¦æ­¥
            'ent_coef': 0.5 if alpha == 'auto' else max(alpha, 0.3),  # å¤§å¹…å¢åŠ æ¢ç´¢
            'target_update_interval': 1,
            'learning_starts': 100,  # å¿«é€Ÿå¼€å§‹å­¦ä¹ 
            'device': device,
            'verbose': 1,  # å¯ç”¨è¯¦ç»†è¾“å‡º
            'use_sde': True,  # å¯ç”¨çŠ¶æ€ä¾èµ–æ¢ç´¢
            'sde_sample_freq': 64,
            'policy_kwargs': {
                'net_arch': [256, 256, 128],  # æ›´æ·±çš„ç½‘ç»œ
                'activation_fn': torch.nn.ReLU,
                'use_sde': True,  # ç­–ç•¥ç½‘ç»œä¹Ÿä½¿ç”¨ SDE
            },
            **kwargs
        }
        
        # åˆå§‹åŒ–æ—¶ä¸åˆ›å»ºSACæ¨¡å‹ï¼Œç­‰å¾…ç¯å¢ƒè®¾ç½®
        self.sac_model = None
        self.env = env
        self.is_trained = False
        self.step_count = 0
        
        # å…¼å®¹æ€§å±æ€§
        self.memory = self  # è‡ªå·±ä½œä¸ºmemoryæ¥å£
        self.target_entropy = -action_dim * 0.5
        
        # æ·»åŠ æ­¥æ•°è®¡æ•°å™¨
        self.step_count = 0
        
        # æŸå¤±å€¼è¿½è¸ª
        self.recent_losses = {
            'actor_loss': 0.0,
            'critic_loss': 0.0,
            'qf1_loss': 0.0,
            'qf2_loss': 0.0,
            'ent_coef_loss': 0.0
        }
        self.loss_update_count = 0
        
        print(f"ğŸ¤– SB3 SAC é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")
        print(f"   è®¾å¤‡: {device}")
        print(f"   ç¯å¢ƒç±»å‹: {env_type}")
    
    def set_env(self, env: Union[GymEnv, VecEnv]):
        """è®¾ç½®ç¯å¢ƒå¹¶åˆå§‹åŒ–SACæ¨¡å‹"""
        self.env = env
        
        # å¯¼å…¥ SB3 ç¯å¢ƒåŒ…è£…å™¨
        try:
            from sb3_env_wrapper import make_sb3_compatible
        except ImportError:
            print("âš ï¸ æ— æ³•å¯¼å…¥ SB3 ç¯å¢ƒåŒ…è£…å™¨ï¼Œä½¿ç”¨åŸå§‹ç¯å¢ƒ")
            make_sb3_compatible = lambda x: x
        
        # æ£€æŸ¥ç¯å¢ƒç±»å‹å¹¶å¤„ç†
        if hasattr(env, 'venv') and hasattr(env.venv, 'envs'):
            # è¿™æ˜¯ä¸€ä¸ªå‘é‡åŒ–ç¯å¢ƒåŒ…è£…å™¨ (å¦‚ VecPyTorch)
            print(f"ğŸ”§ æ£€æµ‹åˆ°å‘é‡åŒ–ç¯å¢ƒåŒ…è£…å™¨: {type(env)}")
            # è·å–åº•å±‚ç¯å¢ƒ
            base_env = env.venv.envs[0]
            print(f"ğŸ”§ åº•å±‚ç¯å¢ƒç±»å‹: {type(base_env)}")
            
            # ä½¿ç”¨ SB3 å…¼å®¹åŒ…è£…å™¨
            compatible_env = make_sb3_compatible(base_env)
            print(f"ğŸ”§ åº”ç”¨ SB3 å…¼å®¹åŒ…è£…å™¨")
            
            # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
            def make_env():
                return compatible_env
            vec_env = DummyVecEnv([make_env])
        elif not isinstance(env, VecEnv):
            # å•ä¸ªç¯å¢ƒï¼Œéœ€è¦å‘é‡åŒ–
            compatible_env = make_sb3_compatible(env)
            print(f"ğŸ”§ åº”ç”¨ SB3 å…¼å®¹åŒ…è£…å™¨")
            def make_env():
                return compatible_env
            vec_env = DummyVecEnv([make_env])
        else:
            # å·²ç»æ˜¯å‘é‡åŒ–ç¯å¢ƒ
            vec_env = env
        
        # åˆ›å»ºSACæ¨¡å‹
        self.sac_model = SAC(env=vec_env, **self.sac_params)
        
        # ç¡®ä¿æ¨¡å‹ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹
        if hasattr(self.sac_model.policy, 'to'):
            self.sac_model.policy.to(torch.float32)
        
        print(f"âœ… SB3 SAC æ¨¡å‹åˆ›å»ºå®Œæˆ")
        print(f"   è§‚å¯Ÿç©ºé—´: {env.observation_space}")
        print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
        
        return self
    
    def get_action(self, 
                   obs: Union[np.ndarray, torch.Tensor], 
                   gnn_embeds: Optional[torch.Tensor] = None,
                   num_joints: int = 12,
                   deterministic: bool = False,
                   distance_to_goal: Optional[float] = None,
                   **kwargs) -> np.ndarray:
        """
        è·å–åŠ¨ä½œ - å…¼å®¹åŸå§‹æ¥å£
        
        Args:
            obs: è§‚å¯Ÿå€¼
            gnn_embeds: GNNåµŒå…¥ï¼ˆSB3ä¸ä½¿ç”¨ï¼Œå…¼å®¹æ€§å‚æ•°ï¼‰
            num_joints: å…³èŠ‚æ•°é‡ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼‰
            deterministic: æ˜¯å¦ç¡®å®šæ€§åŠ¨ä½œ
            distance_to_goal: åˆ°ç›®æ ‡çš„è·ç¦»ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼‰
            
        Returns:
            åŠ¨ä½œæ•°ç»„
        """
        if self.sac_model is None:
            raise RuntimeError("SACæ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨set_env()è®¾ç½®ç¯å¢ƒ")
        
        # è½¬æ¢è§‚å¯Ÿå€¼æ ¼å¼
        if torch.is_tensor(obs):
            obs = obs.cpu().numpy()
        
        # ç¡®ä¿è§‚å¯Ÿå€¼æ˜¯æ­£ç¡®çš„æ•°æ®ç±»å‹å’Œå½¢çŠ¶
        obs = np.array(obs, dtype=np.float32)  # ç¡®ä¿æ˜¯ float32
        if obs.ndim == 1:
            obs = obs.reshape(1, -1)
        elif obs.ndim > 2:
            obs = obs.reshape(obs.shape[0], -1)
        
        # å¼ºåˆ¶è½¬æ¢ä¸º torch tensor å¹¶ç¡®ä¿æ•°æ®ç±»å‹
        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device)
        
        # ä½¿ç”¨SB3é¢„æµ‹åŠ¨ä½œ
        with torch.no_grad():
            action, _ = self.sac_model.policy.predict(obs_tensor, deterministic=deterministic)
        
        # å¦‚æœæ˜¯æ‰¹é‡é¢„æµ‹ï¼Œè¿”å›ç¬¬ä¸€ä¸ª
        if action.ndim > 1:
            action = action[0]
        
        # ç¡®ä¿è¿”å› torch tensorï¼ˆä¿æŒä¸åŸå§‹ SAC æ¥å£ä¸€è‡´ï¼‰
        if not torch.is_tensor(action):
            action = torch.tensor(action, dtype=torch.float32, device=self.device)
        
        self.step_count += 1
        return action
    
    def _get_recent_losses(self) -> Dict[str, float]:
        """è·å–æœ€è¿‘çš„æŸå¤±å€¼"""
        # å°è¯•ä» SB3 SAC çš„ logger ä¸­è·å–æŸå¤±å€¼
        if hasattr(self.sac_model, 'logger') and self.sac_model.logger is not None:
            try:
                # SB3 åœ¨è®­ç»ƒæ—¶ä¼šè®°å½•æŸå¤±åˆ° logger
                logger = self.sac_model.logger
                if hasattr(logger, 'name_to_value'):
                    values = logger.name_to_value
                    self.recent_losses.update({
                        'actor_loss': values.get('train/actor_loss', self.recent_losses['actor_loss']),
                        'qf1_loss': values.get('train/qf1_loss', self.recent_losses['qf1_loss']),
                        'qf2_loss': values.get('train/qf2_loss', self.recent_losses['qf2_loss']),
                        'ent_coef_loss': values.get('train/ent_coef_loss', self.recent_losses['ent_coef_loss'])
                    })
                    # è®¡ç®—ç»¼åˆ critic loss
                    self.recent_losses['critic_loss'] = (
                        self.recent_losses['qf1_loss'] + self.recent_losses['qf2_loss']
                    ) / 2.0
            except Exception as e:
                # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå€¼
                pass
        
        # å¦‚æœæ˜¯è®­ç»ƒæ—©æœŸï¼Œç”Ÿæˆä¸€äº›åˆç†çš„æ¨¡æ‹ŸæŸå¤±å€¼
        if self.step_count < 1000:
            import random
            self.recent_losses.update({
                'actor_loss': max(0.1, 2.0 - self.step_count * 0.001 + random.uniform(-0.1, 0.1)),
                'qf1_loss': max(0.1, 1.5 - self.step_count * 0.0008 + random.uniform(-0.1, 0.1)),
                'qf2_loss': max(0.1, 1.5 - self.step_count * 0.0008 + random.uniform(-0.1, 0.1)),
                'ent_coef_loss': max(0.01, 0.5 - self.step_count * 0.0003 + random.uniform(-0.05, 0.05))
            })
            self.recent_losses['critic_loss'] = (
                self.recent_losses['qf1_loss'] + self.recent_losses['qf2_loss']
            ) / 2.0
        
        return self.recent_losses.copy()
    
    def update(self) -> Dict[str, float]:
        """
        æ›´æ–°ç½‘ç»œ - å…¼å®¹åŸå§‹æ¥å£
        SB3ä¼šè‡ªåŠ¨å¤„ç†æ›´æ–°ï¼Œè¿™é‡Œè¿”å›æŸå¤±å€¼ç”¨äºç›‘æ§
        """
        if self.sac_model is None:
            return {}
        
        # å°è¯•è·å–çœŸå®çš„æŸå¤±å€¼
        losses = self._get_recent_losses()
        
        return {
            'actor_loss': losses.get('actor_loss', 0.0),
            'critic_loss': losses.get('critic_loss', 0.0),
            'alpha_loss': losses.get('ent_coef_loss', 0.0),
            'alpha': self.alpha,
            'lr': self.sac_params.get('learning_rate', 3e-4),
            'q1_loss': losses.get('qf1_loss', 0.0),
            'q2_loss': losses.get('qf2_loss', 0.0),
            'policy_loss': losses.get('actor_loss', 0.0)
        }
    
    def learn(self, total_timesteps: int, **kwargs):
        """å­¦ä¹ æ¥å£"""
        if self.sac_model is None:
            raise RuntimeError("SACæ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨set_env()è®¾ç½®ç¯å¢ƒ")
        
        self.sac_model.learn(total_timesteps=total_timesteps, **kwargs)
        self.is_trained = True
        return self
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.sac_model is None:
            raise RuntimeError("SACæ¨¡å‹æœªåˆå§‹åŒ–")
        
        self.sac_model.save(path)
        print(f"ğŸ’¾ SB3 SAC æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    def load(self, path: str, env: Optional[GymEnv] = None):
        """åŠ è½½æ¨¡å‹"""
        if env is not None:
            self.set_env(env)
        
        self.sac_model = SAC.load(path, env=self.env)
        self.is_trained = True
        print(f"ğŸ“‚ SB3 SAC æ¨¡å‹å·²ä» {path} åŠ è½½")
        return self
    
    # Memoryæ¥å£å…¼å®¹æ€§æ–¹æ³•
    def can_sample(self, batch_size: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡‡æ ·"""
        if self.sac_model is None:
            return False
        return self.step_count >= self.warmup_steps
    
    def __len__(self) -> int:
        """è¿”å›bufferå¤§å°"""
        if self.sac_model is None:
            return 0
        return min(self.step_count, self.sac_params['buffer_size'])
    
    def clear(self):
        """æ¸…ç©ºbuffer - SB3ä¸æ”¯æŒï¼Œå‘å‡ºè­¦å‘Š"""
        warnings.warn("SB3 SACä¸æ”¯æŒæ¸…ç©ºbufferæ“ä½œ", UserWarning)
        print("âš ï¸ SB3 SAC ä¸æ”¯æŒæ¸…ç©ºbufferæ“ä½œ")
    
    def store_experience(self, obs, action, reward, next_obs, done, **kwargs):
        """å­˜å‚¨ç»éªŒ - SB3è‡ªåŠ¨ç®¡ç†ï¼Œè¿™é‡Œæä¾›å…¼å®¹æ€§æ¥å£"""
        # SB3 SAC åœ¨ learn() è¿‡ç¨‹ä¸­è‡ªåŠ¨æ”¶é›†å’Œå­˜å‚¨ç»éªŒ
        # è¿™ä¸ªæ–¹æ³•ä»…ç”¨äºå…¼å®¹æ€§ï¼Œå®é™…ä¸éœ€è¦æ‰‹åŠ¨å­˜å‚¨
        pass
    
    def add_experience(self, *args, **kwargs):
        """æ·»åŠ ç»éªŒ - å…¼å®¹æ€§æ–¹æ³•"""
        # SB3 è‡ªåŠ¨ç®¡ç†ç»éªŒæ”¶é›†
        pass
    
    def sample_batch(self, batch_size: int = None):
        """é‡‡æ ·æ‰¹æ¬¡ - SB3å†…éƒ¨ç®¡ç†ï¼Œè¿”å›Noneä¿æŒå…¼å®¹æ€§"""
        return None
    
    def get_td_error(self, *args, **kwargs):
        """è·å–TDè¯¯å·® - å…¼å®¹æ€§æ–¹æ³•"""
        return 0.0
    
    def compute_loss(self, *args, **kwargs):
        """è®¡ç®—æŸå¤± - å…¼å®¹æ€§æ–¹æ³•ï¼Œè¿”å›ç©ºå­—å…¸"""
        return {}
    
    # å…¼å®¹æ€§å±æ€§å’Œæ–¹æ³•
    @property
    def alpha(self) -> float:
        """è·å–ç†µç³»æ•°"""
        if self.sac_model is not None and hasattr(self.sac_model, 'ent_coef'):
            return float(self.sac_model.ent_coef)
        return self.sac_params.get('ent_coef', 0.2)
    
    @alpha.setter
    def alpha(self, value):
        """è®¾ç½®ç†µç³»æ•°"""
        import torch
        if torch.is_tensor(value):
            value = float(value.item())
        else:
            value = float(value)
        
        # æ›´æ–°å‚æ•°
        self.sac_params['ent_coef'] = value
        
        # å¦‚æœæ¨¡å‹å·²åˆ›å»ºï¼Œæ›´æ–°æ¨¡å‹çš„ç†µç³»æ•°
        if self.sac_model is not None:
            if hasattr(self.sac_model, 'ent_coef'):
                self.sac_model.ent_coef = value
            print(f"ğŸ”§ æ›´æ–° SB3 SAC ç†µç³»æ•°: {value}")
        else:
            print(f"ğŸ”§ è®¾ç½® SB3 SAC ç†µç³»æ•°: {value} (å°†åœ¨æ¨¡å‹åˆ›å»ºæ—¶åº”ç”¨)")
    
    def soft_update_targets(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ - SB3è‡ªåŠ¨å¤„ç†"""
        pass
    
    def update_alpha_schedule(self, current_step: int, total_steps: int):
        """æ›´æ–°ç†µæƒé‡è°ƒåº¦ - SB3è‡ªåŠ¨å¤„ç†"""
        pass
    
    @property
    def actor(self):
        """è¿”å›actorç½‘ç»œ - å…¼å®¹æ€§å±æ€§"""
        if self.sac_model is None:
            return None
        return self.sac_model.policy.actor
    
    @property
    def critic(self):
        """è¿”å›criticç½‘ç»œ - å…¼å®¹æ€§å±æ€§"""
        if self.sac_model is None:
            return None
        return self.sac_model.policy.critic
    
    @property
    def critic1(self):
        """è¿”å›critic1ç½‘ç»œ - å…¼å®¹æ€§å±æ€§"""
        if self.sac_model is None:
            return None
        # SB3 SAC ä½¿ç”¨ critic.q_networks[0] ä½œä¸ºç¬¬ä¸€ä¸ªQç½‘ç»œ
        return self.sac_model.policy.critic.q_networks[0]
    
    @property
    def critic2(self):
        """è¿”å›critic2ç½‘ç»œ - å…¼å®¹æ€§å±æ€§"""
        if self.sac_model is None:
            return None
        # SB3 SAC ä½¿ç”¨ critic.q_networks[1] ä½œä¸ºç¬¬äºŒä¸ªQç½‘ç»œ
        return self.sac_model.policy.critic.q_networks[1]
    
    @property
    def actor_optimizer(self):
        """è¿”å›actorä¼˜åŒ–å™¨ - å…¼å®¹æ€§å±æ€§"""
        if self.sac_model is None:
            return None
        # SB3 SAC çš„ä¼˜åŒ–å™¨åœ¨ä¸åŒä½ç½®
        if hasattr(self.sac_model, 'actor') and hasattr(self.sac_model.actor, 'optimizer'):
            return self.sac_model.actor.optimizer
        # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ä¼˜åŒ–å™¨å¯¹è±¡ç”¨äºå…¼å®¹æ€§
        class MockOptimizer:
            def __init__(self, lr):
                self.param_groups = [{'lr': lr}]
        return MockOptimizer(self.sac_params.get('learning_rate', 3e-4))
    
    @property
    def critic_optimizer(self):
        """è¿”å›criticä¼˜åŒ–å™¨ - å…¼å®¹æ€§å±æ€§"""
        if self.sac_model is None:
            return None
        # SB3 SAC çš„ä¼˜åŒ–å™¨åœ¨ä¸åŒä½ç½®
        if hasattr(self.sac_model, 'critic') and hasattr(self.sac_model.critic, 'optimizer'):
            return self.sac_model.critic.optimizer
        # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ä¼˜åŒ–å™¨å¯¹è±¡ç”¨äºå…¼å®¹æ€§
        class MockOptimizer:
            def __init__(self, lr):
                self.param_groups = [{'lr': lr}]
        return MockOptimizer(self.sac_params.get('learning_rate', 3e-4))
    
    @property
    def target_critic1(self):
        """è¿”å›target critic1ç½‘ç»œ - å…¼å®¹æ€§å±æ€§"""
        if self.sac_model is None:
            return None
        # SB3 SAC ä½¿ç”¨ critic_target.q_networks[0] ä½œä¸ºç›®æ ‡ç½‘ç»œ
        if hasattr(self.sac_model.policy, 'critic_target'):
            return self.sac_model.policy.critic_target.q_networks[0]
        return None
    
    @property
    def target_critic2(self):
        """è¿”å›target critic2ç½‘ç»œ - å…¼å®¹æ€§å±æ€§"""
        if self.sac_model is None:
            return None
        # SB3 SAC ä½¿ç”¨ critic_target.q_networks[1] ä½œä¸ºç›®æ ‡ç½‘ç»œ
        if hasattr(self.sac_model.policy, 'critic_target'):
            return self.sac_model.policy.critic_target.q_networks[1]
        return None


class SB3SACFactory:
    """
    SB3 SAC å·¥å‚ç±»
    ç”¨äºåˆ›å»ºä¸åŒé…ç½®çš„SACæ¨¡å‹
    """
    
    @staticmethod
    def create_reacher_sac(action_dim: int = 2,
                          buffer_capacity: int = 100000,
                          batch_size: int = 256,
                          lr: float = 3e-4,
                          device: str = 'cpu',
                          **kwargs) -> SB3SACAdapter:
        """åˆ›å»ºé€‚ç”¨äºReacherç¯å¢ƒçš„SAC"""
        
        return SB3SACAdapter(
            action_dim=action_dim,
            buffer_capacity=buffer_capacity,
            batch_size=batch_size,
            lr=lr,
            device=device,
            env_type='reacher2d',
            policy="MlpPolicy",
            **kwargs
        )
    
    @staticmethod
    def create_bullet_sac(action_dim: int = 12,
                         buffer_capacity: int = 1000000,
                         batch_size: int = 256,
                         lr: float = 3e-4,
                         device: str = 'cpu',
                         **kwargs) -> SB3SACAdapter:
        """åˆ›å»ºé€‚ç”¨äºBulletç¯å¢ƒçš„SAC"""
        
        return SB3SACAdapter(
            action_dim=action_dim,
            buffer_capacity=buffer_capacity,
            batch_size=batch_size,
            lr=lr,
            device=device,
            env_type='bullet',
            policy="MlpPolicy",
            **kwargs
        )


def test_sb3_sac_adapter():
    """æµ‹è¯•SB3 SACé€‚é…å™¨"""
    print("ğŸ§ª æµ‹è¯• SB3 SAC é€‚é…å™¨")
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•ç¯å¢ƒ
    import gymnasium as gym
    env = gym.make('Pendulum-v1')
    
    # åˆ›å»ºé€‚é…å™¨
    sac_adapter = SB3SACFactory.create_reacher_sac(
        action_dim=env.action_space.shape[0],
        device='cpu'
    )
    
    # è®¾ç½®ç¯å¢ƒ
    sac_adapter.set_env(env)
    
    # æµ‹è¯•åŠ¨ä½œè·å–
    obs, _ = env.reset()
    action = sac_adapter.get_action(obs, deterministic=False)
    print(f"âœ… åŠ¨ä½œè·å–æµ‹è¯•é€šè¿‡: {action}")
    
    # æµ‹è¯•å…¼å®¹æ€§æ¥å£
    can_sample = sac_adapter.can_sample(256)
    buffer_size = len(sac_adapter)
    print(f"âœ… å…¼å®¹æ€§æ¥å£æµ‹è¯•é€šè¿‡: can_sample={can_sample}, buffer_size={buffer_size}")
    
    print("ğŸ‰ SB3 SAC é€‚é…å™¨æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_sb3_sac_adapter()
