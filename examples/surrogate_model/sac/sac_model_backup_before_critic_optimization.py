import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import random
from collections import deque, namedtuple
import os
import sys

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(__file__), "../../../")
sys.path.append(base_dir)
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_dataset"))
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_model"))
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/sac"))
from data_utils import prepare_joint_q_input, prepare_reacher2d_joint_q_input, prepare_dynamic_vertex_v
# å¯¼å…¥ä½ çš„ç»„ä»¶
from attn_actor import AttentionActor
from attn_critic import AttentionCritic
import copy
        
# å®šä¹‰ç»éªŒå…ƒç»„
Experience = namedtuple('Experience', [
    'joint_q', 'vertex_k', 'vertex_v', 'action', 'reward', 
    'next_joint_q', 'next_vertex_k', 'next_vertex_v', 'done', 'vertex_mask'
])

class ReplayBuffer:
    def __init__(self, capacity=100000, device='cpu'):
        self.capacity = capacity
        self.device = device
        self.buffer = deque(maxlen=capacity)
        self.position = 0
    
    def push(self, joint_q, vertex_k, vertex_v, action, reward, 
             next_joint_q, next_vertex_k, next_vertex_v, done, vertex_mask=None):
        """å­˜å‚¨ä¸€ä¸ªç»éªŒ"""
        
        # è½¬æ¢ä¸ºCPU tensorä»¥èŠ‚çœGPUå†…å­˜
        experience = Experience(
            joint_q=joint_q.cpu() if torch.is_tensor(joint_q) else joint_q,
            vertex_k=vertex_k.cpu() if torch.is_tensor(vertex_k) else vertex_k,
            vertex_v=vertex_v.cpu() if torch.is_tensor(vertex_v) else vertex_v,
            action=action.cpu() if torch.is_tensor(action) else action,
            reward=reward.cpu() if torch.is_tensor(reward) else reward,
            next_joint_q=next_joint_q.cpu() if torch.is_tensor(next_joint_q) else next_joint_q,
            next_vertex_k=next_vertex_k.cpu() if torch.is_tensor(next_vertex_k) else next_vertex_k,
            next_vertex_v=next_vertex_v.cpu() if torch.is_tensor(next_vertex_v) else next_vertex_v,
            done=done.cpu() if torch.is_tensor(done) else done,
            vertex_mask=vertex_mask.cpu() if vertex_mask is not None and torch.is_tensor(vertex_mask) else vertex_mask
        )
        
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        """é‡‡æ ·ä¸€ä¸ªbatchçš„ç»éªŒ"""
        if len(self.buffer) < batch_size:
            batch_size = len(self.buffer)
            
        experiences = random.sample(self.buffer, batch_size)
        
        # å°†ç»éªŒè½¬æ¢ä¸ºæ‰¹é‡tensor
        joint_q = torch.stack([e.joint_q for e in experiences]).to(self.device)
        vertex_k = torch.stack([e.vertex_k for e in experiences]).to(self.device)
        vertex_v = torch.stack([e.vertex_v for e in experiences]).to(self.device)
        actions = torch.stack([e.action for e in experiences]).to(self.device)
        rewards = torch.stack([e.reward for e in experiences]).to(self.device)
        next_joint_q = torch.stack([e.next_joint_q for e in experiences]).to(self.device)
        next_vertex_k = torch.stack([e.next_vertex_k for e in experiences]).to(self.device)
        next_vertex_v = torch.stack([e.next_vertex_v for e in experiences]).to(self.device)
        dones = torch.stack([e.done for e in experiences]).to(self.device)
        
        # å¤„ç†vertex_mask (å¯èƒ½ä¸ºNone)
        vertex_mask = None
        if experiences[0].vertex_mask is not None:
            vertex_mask = torch.stack([e.vertex_mask for e in experiences]).to(self.device)
        
        return (joint_q, vertex_k, vertex_v, actions, rewards, 
                next_joint_q, next_vertex_k, next_vertex_v, dones, vertex_mask)
    
    def __len__(self):
        return len(self.buffer)
    
    def can_sample(self, batch_size):
        return len(self.buffer) >= batch_size
    
    def clear(self):
        """æ¸…ç©ºbufferä¸­çš„æ‰€æœ‰ç»éªŒ"""
        self.buffer.clear()
        print(f"ğŸ§¹ Bufferå·²æ¸…ç©º")


class AttentionSACWithBuffer:
    def __init__(self, attn_model, action_dim, joint_embed_dim=128, 
                 buffer_capacity=10000, batch_size=256, lr=3e-4, 
                 gamma=0.99, tau=0.005, alpha=0.2, device='cpu', env_type='bullet'):
        
        self.device = device
        self.action_dim = action_dim
        self.batch_size = batch_size
        self.warmup_steps = 10000
        self.env_type = env_type
 
        # åˆ›å»ºç‹¬ç«‹çš„æ¨¡å‹æ‹·è´ - è§£å†³å…±äº«é—®é¢˜
        actor_model = copy.deepcopy(attn_model)
        critic1_model = copy.deepcopy(attn_model)
        critic2_model = copy.deepcopy(attn_model)
        
        self.actor = AttentionActor(actor_model, action_dim).to(device)
        self.critic1 = AttentionCritic(critic1_model, joint_embed_dim, device=device).to(device)
        self.critic2 = AttentionCritic(critic2_model, joint_embed_dim, device=device).to(device)
        
        # Target networks - ç°åœ¨æ˜¯çœŸæ­£ç‹¬ç«‹çš„
        self.target_critic1 = copy.deepcopy(self.critic1)
        self.target_critic2 = copy.deepcopy(self.critic2)
        
        # å†»ç»“target networks
        for param in self.target_critic1.parameters():
            param.requires_grad = False
        for param in self.target_critic2.parameters():
            param.requires_grad = False
            
        # Memory Buffer
        self.memory = ReplayBuffer(buffer_capacity, device)
        
        # è¶…å‚æ•°
        self.gamma = gamma
        self.tau = tau
        # ğŸ†• Alphaè°ƒåº¦å‚æ•°
        self.alpha_start = alpha      # åˆå§‹alphaå€¼ (0.2)
        self.alpha_end = 0.02        # æœ€ç»ˆalphaå€¼ (ä¸“é—¨ä¸ºç»´æŒä»»åŠ¡ä¼˜åŒ–ï¼Œæ›´ç¡®å®šæ€§)
        self.alpha = alpha
        self.entropy_schedule_enabled = True
        self.exploration_phase_ratio = 0.85  # å‰85%ä¸ºæ¢ç´¢é˜¶æ®µï¼ˆä»70%å¢åŠ åˆ°85%ï¼‰
        # lr = 2e-5  # ğŸ”§ ç§»é™¤ç¡¬ç¼–ç ï¼Œä½¿ç”¨ä¼ å…¥çš„lrå‚æ•°
        # ä¼˜åŒ–å™¨ - ç°åœ¨å®Œå…¨ç‹¬ç«‹
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = torch.optim.Adam(
            list(self.critic1.parameters()) + list(self.critic2.parameters()), lr=lr
        )
        
        # è‡ªåŠ¨è°ƒæ•´alpha
        self.target_entropy = -action_dim * 0.5
        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
        self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=lr)
        
    def store_experience(self, obs, gnn_embeds, action, reward, next_obs, next_gnn_embeds, done, num_joints=12):
        """å­˜å‚¨ç¯å¢ƒäº¤äº’ç»éªŒ"""
        if self.env_type == 'reacher2d':
            joint_q = prepare_reacher2d_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints).squeeze(0)
            next_joint_q = prepare_reacher2d_joint_q_input(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints).squeeze(0)
        else:  # bullet ç¯å¢ƒ
            joint_q = prepare_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints).squeeze(0)
            next_joint_q = prepare_joint_q_input(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints).squeeze(0)
        # å‡†å¤‡å½“å‰çŠ¶æ€
        vertex_k = gnn_embeds
        vertex_v = vertex_v = prepare_dynamic_vertex_v(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints, self.env_type).squeeze(0)  # ğŸ¯ åŠ¨æ€V
        
        # å‡†å¤‡ä¸‹ä¸€ä¸ªçŠ¶æ€
        # next_joint_q = prepare_joint_q_input(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints).squeeze(0)
        next_vertex_k = next_gnn_embeds
        next_vertex_v = prepare_dynamic_vertex_v(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints, self.env_type).squeeze(0)  # ğŸ¯ åŠ¨æ€V
        
        # è½¬æ¢ä¸ºé€‚å½“çš„tensoræ ¼å¼
        if not torch.is_tensor(reward):
            reward = torch.tensor([reward], dtype=torch.float32)
        if not torch.is_tensor(done):
            done = torch.tensor([done], dtype=torch.float32)
            
        self.memory.push(
            joint_q, vertex_k, vertex_v, action, reward,
            next_joint_q, next_vertex_k, next_vertex_v, done
        )
    
    def get_action(self, obs, gnn_embeds, num_joints=12, deterministic=False, distance_to_goal=None):
        """è·å–åŠ¨ä½œ"""
          
        # ğŸ¯ æ ¹æ®ç¯å¢ƒç±»å‹é€‰æ‹©æ•°æ®å¤„ç†å‡½æ•°
        if self.env_type == 'reacher2d':
            joint_q = prepare_reacher2d_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints)
        else:  # bullet ç¯å¢ƒ
            joint_q = prepare_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints)
        vertex_k = gnn_embeds.unsqueeze(0)
        vertex_v = prepare_dynamic_vertex_v(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints, self.env_type)  # ğŸ¯ åŠ¨æ€V
        
        with torch.no_grad():
            # ğŸ†• è·ç¦»è‡ªé€‚åº”ç¡®å®šæ€§æ§åˆ¶ - ä¸“é—¨ä¸ºç»´æŒä»»åŠ¡ä¼˜åŒ–
            maintenance_mode = False
            if distance_to_goal is not None:
                if distance_to_goal < 120.0:
                    # æ¥è¿‘ç›®æ ‡æ—¶ä½¿ç”¨æ›´ç¡®å®šæ€§çš„ç­–ç•¥
                    deterministic = True
                    maintenance_mode = True
                    if distance_to_goal < 100.0:  # è¿›å…¥ç»´æŒåŒºåŸŸ
                        print(f"ğŸ¯ è¿›å…¥ç»´æŒæ¨¡å¼({distance_to_goal:.1f}px)ï¼Œä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥")
            
            if deterministic:
                mean, _ = self.actor.forward(joint_q, vertex_k, vertex_v)
                tanh_action = torch.tanh(mean).squeeze(0)
            else:
                tanh_action, _, _ = self.actor.sample(joint_q, vertex_k, vertex_v)
                tanh_action = tanh_action.squeeze(0)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šAction Scalingï¼
            # SACè¾“å‡º[-1,+1]ï¼Œéœ€è¦ç¼©æ”¾åˆ°ç¯å¢ƒçš„action space
            if self.env_type == 'reacher2d':
                # ğŸ†• ç»´æŒæ¨¡å¼çš„ç‰¹æ®ŠåŠ¨ä½œç¼©æ”¾
                if maintenance_mode:
                    # ç»´æŒæ¨¡å¼ï¼šä½¿ç”¨æ›´å°çš„åŠ¨ä½œå¹…åº¦ï¼Œæé«˜ç¨³å®šæ€§
                    action_scale = 30.0  # ç»´æŒæ—¶ä½¿ç”¨è¾ƒå°åŠ¨ä½œ
                    scaled_action = tanh_action * action_scale
                    if distance_to_goal and distance_to_goal < 100.0:
                        # åœ¨ç»´æŒåŒºåŸŸå†…è¿›ä¸€æ­¥å‡å°‘åŠ¨ä½œå¹…åº¦
                        scaled_action = scaled_action * 0.5  # æ›´ç²¾ç»†çš„æ§åˆ¶
                else:
                    # æ­£å¸¸æ¢ç´¢æ¨¡å¼ï¼šä½¿ç”¨å®Œæ•´åŠ¨ä½œèŒƒå›´
                    action_scale = 100.0  # æ­£å¸¸åŠ¨ä½œèŒƒå›´
                    scaled_action = tanh_action * action_scale
                return scaled_action
            else:
                # Bulletç¯å¢ƒä¿æŒåŸæœ‰é€»è¾‘
                return tanh_action
    
    def soft_update_targets(self):
        """è½¯æ›´æ–°target networks"""
        for target_param, param in zip(self.target_critic1.parameters(), self.critic1.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
        
        for target_param, param in zip(self.target_critic2.parameters(), self.critic2.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
    
    def update(self):
        """ä»memory bufferé‡‡æ ·å¹¶æ›´æ–°ç½‘ç»œ - å¢å¼ºæ•°å€¼ç¨³å®šæ€§"""
        if not self.memory.can_sample(self.batch_size):
            return None
        
        # batch = self.memory.sample(min(32, self.batch_size))  # å°æ‰¹é‡å¿«é€Ÿè¯„ä¼°
        # joint_q, vertex_k, vertex_v, actions, rewards, next_joint_q, next_vertex_k, next_vertex_v, dones, vertex_mask = batch
        
        # with torch.no_grad():
        #     current_q1 = self.critic1(joint_q, vertex_k, vertex_v, vertex_mask=vertex_mask, action=actions)
        #     current_q2 = self.critic2(joint_q, vertex_k, vertex_v, vertex_mask=vertex_mask, action=actions)
        #     quick_loss = (current_q1.std() + current_q2.std()).item()  # ç®€å•è¯„ä¼°
        
        # # åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´
        # if quick_loss < 1.0:  # å¦‚æœç¨³å®š
        #     new_lr = 5e-5  # å¯ä»¥æé«˜å­¦ä¹ ç‡
        # else:  # å¦‚æœä¸ç¨³å®š
        #     new_lr = 2e-5  # ä¿æŒä½å­¦ä¹ ç‡
        
        # # æ›´æ–°æ‰€æœ‰ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        # for param_group in self.critic_optimizer.param_groups:
        #     param_group['lr'] = new_lr
        # for param_group in self.actor_optimizer.param_groups:
        #     param_group['lr'] = new_lr
        # for param_group in self.alpha_optimizer.param_groups:
        #     param_group['lr'] = new_lr
            
        # # ä»bufferé‡‡æ ·
        batch = self.memory.sample(self.batch_size)
        joint_q, vertex_k, vertex_v, actions, rewards, next_joint_q, next_vertex_k, next_vertex_v, dones, vertex_mask = batch

        
        
        # ğŸ›¡ï¸ å¥–åŠ±ç¨³å®šæ€§æ£€æŸ¥
        rewards = torch.clamp(rewards, -10.0, 10.0)  # ä¸¥æ ¼é™åˆ¶å¥–åŠ±èŒƒå›´
        
        # === Critic Update ===
        with torch.no_grad():
            next_actions, next_log_probs, _ = self.actor.sample(next_joint_q, next_vertex_k, next_vertex_v, vertex_mask)
            
            target_q1 = self.target_critic1(next_joint_q, next_vertex_k, next_vertex_v, 
                                        vertex_mask=vertex_mask, action=next_actions)
            target_q2 = self.target_critic2(next_joint_q, next_vertex_k, next_vertex_v, 
                                        vertex_mask=vertex_mask, action=next_actions)
            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs
            target_q = rewards + (1 - dones) * self.gamma * target_q
            
            # ğŸ›¡ï¸ Target Qå€¼ç¨³å®šæ€§æ£€æŸ¥
            target_q = torch.clamp(target_q, -50.0, 50.0)
        
        # å½“å‰Qå€¼
        current_q1 = self.critic1(joint_q, vertex_k, vertex_v, vertex_mask=vertex_mask, action=actions)
        current_q2 = self.critic2(joint_q, vertex_k, vertex_v, vertex_mask=vertex_mask, action=actions)
        
        # ğŸ›¡ï¸ å½“å‰Qå€¼ç¨³å®šæ€§æ£€æŸ¥
        current_q1 = torch.clamp(current_q1, -50.0, 50.0)
        current_q2 = torch.clamp(current_q2, -50.0, 50.0)
        
        # ä½¿ç”¨Huber Lossä»£æ›¿MSE Lossï¼Œæ›´ç¨³å®š
        critic_loss = nn.SmoothL1Loss()(current_q1, target_q) + nn.SmoothL1Loss()(current_q2, target_q)

        current_lr = self.critic_optimizer.param_groups[0]['lr']

        # æ·»åŠ æ¢å¤é€»è¾‘
        if not hasattr(self, 'consecutive_low_loss_count'):
            self.consecutive_low_loss_count = 0

        if critic_loss.item() < 0.5:  # éå¸¸ç¨³å®š
            self.consecutive_low_loss_count += 1
            if self.consecutive_low_loss_count > 50:  # è¿ç»­50æ¬¡ä½loss
                new_lr = min(current_lr * 1.2, 5e-5)  # å¯ä»¥å¤§å¹…æ¢å¤
            else:
                new_lr = min(current_lr * 1.05, 5e-5)  # å°å¹…æé«˜
        elif critic_loss.item() > 2.0:  # ä¸¥é‡ä¸ç¨³å®š
            self.consecutive_low_loss_count = 0
            new_lr = max(current_lr * 0.5, 1e-5)  # å¤§å¹…é™ä½
        elif critic_loss.item() > 1.0:  # è½»å¾®ä¸ç¨³å®š
            self.consecutive_low_loss_count = 0
            new_lr = max(current_lr * 0.9, 1e-5)  # é€‚åº¦é™ä½
        else:
            new_lr = current_lr  # ä¿æŒä¸å˜
                # é™åˆ¶è°ƒæ•´é¢‘ç‡
        if not hasattr(self, 'last_lr_adjust_step'):
            self.last_lr_adjust_step = 0

        # è·å–å½“å‰æ­¥æ•°ï¼ˆå¯ä»¥ä»å¤–éƒ¨ä¼ å…¥æˆ–ä½¿ç”¨è®¡æ•°å™¨ï¼‰
        if not hasattr(self, 'update_counter'):
            self.update_counter = 0
        self.update_counter += 1

        # è‡³å°‘100æ¬¡updateæ‰è°ƒæ•´ä¸€æ¬¡å­¦ä¹ ç‡
        if self.update_counter - self.last_lr_adjust_step > 100:
            # åªåœ¨æœ‰æ˜¾è‘—å˜åŒ–æ—¶æ›´æ–°å­¦ä¹ ç‡
            if abs(current_lr - new_lr) > 1e-7:
                for param_group in self.critic_optimizer.param_groups:
                    param_group['lr'] = new_lr
                for param_group in self.actor_optimizer.param_groups:
                    param_group['lr'] = new_lr
                for param_group in self.alpha_optimizer.param_groups:
                    param_group['lr'] = new_lr
                print(f"ğŸ“ˆ å­¦ä¹ ç‡è°ƒæ•´ (ç¬¬{self.update_counter}æ¬¡æ›´æ–°): {current_lr:.1e} â†’ {new_lr:.1e} (critic_loss: {critic_loss.item():.3f})")
                self.last_lr_adjust_step = self.update_counter

        
        # ğŸ›¡ï¸ Lossç¨³å®šæ€§æ£€æŸ¥
        if critic_loss > 25.0:
            print(f"âš ï¸ å¤§Critic Loss: {critic_loss:.3f}, è·³è¿‡æ­¤æ¬¡æ›´æ–°")
            return None
        
        # æ›´æ–°Critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(
            list(self.critic1.parameters()) + list(self.critic2.parameters()),
            max_norm=0.5  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
        )
        self.critic_optimizer.step()
        
        # === Actor Update ===
        # é‡‡æ ·æ–°åŠ¨ä½œ
        new_actions, log_probs, _ = self.actor.sample(joint_q, vertex_k, vertex_v, vertex_mask)
        
        # è®¡ç®—Qå€¼
        q1 = self.critic1(joint_q, vertex_k, vertex_v, vertex_mask=vertex_mask, action=new_actions)
        q2 = self.critic2(joint_q, vertex_k, vertex_v, vertex_mask=vertex_mask, action=new_actions)
        q = torch.min(q1, q2)
        
        # ActoræŸå¤±
        actor_loss = (self.alpha * log_probs - q).mean()
        
        # åˆ†ælossç»„ä»¶
        entropy_term = (self.alpha * log_probs).mean()
        q_term = q.mean()
        
        # æ›´æ–°Actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(
                self.actor.parameters(),
                max_norm=1.0
            )
        self.actor_optimizer.step()
        
        # === Alpha Update ===
        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
        
        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.alpha_optimizer.step()
        
        self.alpha = self.log_alpha.exp()
        # ğŸ”’ Alphaä¸‹é™çº¦æŸ
        # min_alpha = getattr(self, 'min_alpha', 0.01)
        # if self.alpha < min_alpha:
        #     self.alpha = torch.tensor(min_alpha, device=self.alpha.device)  # ä¿æŒtensorç±»å‹
        #     self.log_alpha.data.fill_(torch.log(torch.tensor(min_alpha)).item())
        #     print(f"âš ï¸ Alphaè¾¾åˆ°ä¸‹é™ {min_alpha}ï¼Œå·²é™åˆ¶")
        # è½¯æ›´æ–°target networks
        self.soft_update_targets()

        def detect_q_divergence(current_q1, current_q2, target_q, step_info=""):
            """æ£€æµ‹Qå€¼æ˜¯å¦å‘æ•£"""
            divergence_signals = []
            
            # 1. ç»å¯¹å€¼æ£€æµ‹
            q1_max = current_q1.max().item()
            q1_min = current_q1.min().item()
            q2_max = current_q2.max().item()
            q2_min = current_q2.min().item()
            target_max = target_q.max().item()
            target_min = target_q.min().item()
            
            # æ£€æŸ¥ç»å¯¹å€¼è¿‡å¤§
            if max(abs(q1_max), abs(q1_min), abs(q2_max), abs(q2_min)) > 100.0:
                divergence_signals.append(f"Qå€¼ç»å¯¹å€¼è¿‡å¤§: Q1[{q1_min:.2f}, {q1_max:.2f}], Q2[{q2_min:.2f}, {q2_max:.2f}]")
            
            # 2. Qå€¼æ–¹å·®æ£€æµ‹
            q1_std = current_q1.std().item()
            q2_std = current_q2.std().item()
            if q1_std > 50.0 or q2_std > 50.0:
                divergence_signals.append(f"Qå€¼æ–¹å·®è¿‡å¤§: Q1_std={q1_std:.2f}, Q2_std={q2_std:.2f}")
            
            # 3. Qå€¼å·®å¼‚æ£€æµ‹
            q_diff = torch.abs(current_q1 - current_q2).mean().item()
            if q_diff > 20.0:
                divergence_signals.append(f"Q1å’ŒQ2å·®å¼‚è¿‡å¤§: mean_diff={q_diff:.2f}")
            
            # 4. Target-Currentå·®å¼‚æ£€æµ‹
            target_diff_1 = torch.abs(current_q1 - target_q).mean().item()
            target_diff_2 = torch.abs(current_q2 - target_q).mean().item()
            if target_diff_1 > 30.0 or target_diff_2 > 30.0:
                divergence_signals.append(f"Qå€¼ä¸ç›®æ ‡å·®å¼‚è¿‡å¤§: diff1={target_diff_1:.2f}, diff2={target_diff_2:.2f}")
            
            return divergence_signals
        
        # åœ¨updateæ–¹æ³•ä¸­è°ƒç”¨æ£€æµ‹
        divergence_signals = detect_q_divergence(current_q1, current_q2, target_q)
        if divergence_signals:
            print(f"\nğŸ”¥ Qå€¼å‘æ•£è­¦æŠ¥:")
            for signal in divergence_signals:
                print(f"   {signal}")
            print(f"   å»ºè®®: é™ä½å­¦ä¹ ç‡ã€å¢åŠ æ­£åˆ™åŒ–æˆ–é‡å¯è®­ç»ƒ")

            
        # ğŸ†• ä¿å­˜batchæ•°æ®ç”¨äºå…³èŠ‚åˆ†æ
        self._last_batch_data = {
            'joint_q': joint_q,
            'vertex_k': vertex_k,
            'vertex_v': vertex_v,
            'actions': actions,
            'vertex_mask': vertex_mask
        }
        
        # ğŸ†• è®¡ç®—attentionç½‘ç»œçš„ç‹¬ç«‹æŸå¤±
        attention_metrics = self._calculate_attention_losses()
        
        result = {
            'lr': new_lr,
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item(),
            'alpha_loss': alpha_loss.item(),
            'alpha': self.alpha.item(),
            'q1_mean': current_q1.mean().item(),
            'q2_mean': current_q2.mean().item(),
            'q1_std': current_q1.std().item(),        # æ–°å¢
            'q2_std': current_q2.std().item(),        # æ–°å¢
            'q1_max': current_q1.max().item(),        # æ–°å¢
            'q1_min': current_q1.min().item(),        # æ–°å¢
            'q2_max': current_q2.max().item(),        # æ–°å¢
            'q2_min': current_q2.min().item(),        # æ–°å¢
            'q_diff_mean': torch.abs(current_q1 - current_q2).mean().item(),  # æ–°å¢
            'target_q_mean': target_q.mean().item(),  # æ–°å¢
            'buffer_size': len(self.memory),
            'entropy_term': entropy_term.item(),
            'q_term': q_term.item(),
            'log_probs_mean': log_probs.mean().item(),
            **attention_metrics  # ğŸ†• æ·»åŠ attentionæŸå¤±
        }
        
        return result
    
    def clear_buffer(self):
        """æ¸…ç©ºç»éªŒå›æ”¾ç¼“å†²åŒº"""
        self.memory.clear()
        print(f"ğŸ§¹ SACæ¨¡å‹bufferå·²æ¸…ç©º (å®¹é‡: {self.memory.capacity})")
    
    def update_alpha_schedule(self, current_step, total_steps):
        """ğŸ†• æ›´æ–°ç†µæƒé‡è°ƒåº¦"""
        if not self.entropy_schedule_enabled:
            return
            
        # è®¡ç®—è®­ç»ƒè¿›åº¦
        progress = min(current_step / (total_steps * self.exploration_phase_ratio), 1.0)
        
        # çº¿æ€§è¡°å‡alpha
        scheduled_alpha = self.alpha_start * (1 - progress) + self.alpha_end * progress
        
        # æ›´æ–°alphaå€¼
        old_alpha = self.alpha
        self.alpha = scheduled_alpha
        
        # åŒæ­¥æ›´æ–°log_alpha (å¦‚æœä½¿ç”¨è‡ªåŠ¨è°ƒæ•´alpha)
        if hasattr(self, 'log_alpha'):
            self.log_alpha.data.fill_(torch.log(torch.tensor(scheduled_alpha)).item())
        
        # æ¯100æ­¥è¾“å‡ºä¸€æ¬¡è°ƒåº¦ä¿¡æ¯
        if current_step % 100 == 0 and abs(float(old_alpha) - float(scheduled_alpha)) > 0.001:
            phase = "æ¢ç´¢é˜¶æ®µ" if progress < 1.0 else "ç¨³å®šé˜¶æ®µ"
            print(f"ğŸ”„ Alphaè°ƒåº¦æ›´æ–° [Step {current_step}]: {float(old_alpha):.4f} â†’ {float(scheduled_alpha):.4f} ({phase})")
    
    def reset_for_new_reward_function(self):
        """ä¸ºæ–°å¥–åŠ±å‡½æ•°é‡ç½®è®­ç»ƒçŠ¶æ€"""
        self.clear_buffer()
        print(f"ğŸ”„ æ¨¡å‹å·²é‡ç½®ä»¥é€‚åº”æ–°å¥–åŠ±å‡½æ•°")
        print(f"   å»ºè®®è¿›è¡Œæ–°çš„warmupæœŸ: {self.warmup_steps}æ­¥")
    
    def _calculate_attention_losses(self):
        """è®¡ç®—attentionç½‘ç»œçš„ç‹¬ç«‹æŸå¤±æŒ‡æ ‡"""
        attention_metrics = {}
        
        try:
            # 1. è®¡ç®—Actorä¸­attentionç½‘ç»œçš„æ¢¯åº¦èŒƒæ•°
            actor_attn_grad_norm = 0.0
            actor_attn_param_count = 0
            
            for name, param in self.actor.attn_model.named_parameters():
                if param.grad is not None:
                    param_norm = param.grad.data.norm(2).item()
                    actor_attn_grad_norm += param_norm ** 2
                    actor_attn_param_count += 1
            
            if actor_attn_param_count > 0:
                actor_attn_grad_norm = (actor_attn_grad_norm ** 0.5) / actor_attn_param_count
                attention_metrics['attention_actor_grad_norm'] = actor_attn_grad_norm
                attention_metrics['attention_actor_loss'] = actor_attn_grad_norm
            
            # 2. è®¡ç®—Critic1ä¸­attentionç½‘ç»œçš„æ¢¯åº¦èŒƒæ•°
            critic1_attn_grad_norm = 0.0
            critic1_attn_param_count = 0
            
            for name, param in self.critic1.attn_model.named_parameters():
                if param.grad is not None:
                    param_norm = param.grad.data.norm(2).item()
                    critic1_attn_grad_norm += param_norm ** 2
                    critic1_attn_param_count += 1
            
            if critic1_attn_param_count > 0:
                critic1_attn_grad_norm = (critic1_attn_grad_norm ** 0.5) / critic1_attn_param_count
                attention_metrics['attention_critic_main_grad_norm'] = critic1_attn_grad_norm
                attention_metrics['attention_critic_main_loss'] = critic1_attn_grad_norm
            
            # 3. è®¡ç®—Critic2ä¸­attentionç½‘ç»œçš„æ¢¯åº¦èŒƒæ•°
            critic2_attn_grad_norm = 0.0
            critic2_attn_param_count = 0
            
            for name, param in self.critic2.attn_model.named_parameters():
                if param.grad is not None:
                    param_norm = param.grad.data.norm(2).item()
                    critic2_attn_grad_norm += param_norm ** 2
                    critic2_attn_param_count += 1
            
            if critic2_attn_param_count > 0:
                critic2_attn_grad_norm = (critic2_attn_grad_norm ** 0.5) / critic2_attn_param_count
                attention_metrics['attention_critic_value_grad_norm'] = critic2_attn_grad_norm
                attention_metrics['attention_critic_value_loss'] = critic2_attn_grad_norm
            
            # 4. è®¡ç®—æ€»çš„attentionæŸå¤±
            total_attn_loss = (attention_metrics.get('attention_actor_loss', 0) + 
                             attention_metrics.get('attention_critic_main_loss', 0) + 
                             attention_metrics.get('attention_critic_value_loss', 0))
            attention_metrics['attention_total_loss'] = total_attn_loss
            
            # 5. è®¡ç®—attentionç½‘ç»œå‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯
            # 5.1 Actor attentionå‚æ•°ç»Ÿè®¡
            actor_attn_params = list(self.actor.attn_model.parameters())
            if actor_attn_params:
                actor_param_values = torch.cat([p.data.flatten() for p in actor_attn_params])
                attention_metrics['attention_actor_param_mean'] = actor_param_values.mean().item()
                attention_metrics['attention_actor_param_std'] = actor_param_values.std().item()
            
            # 5.2 Critic attentionå‚æ•°ç»Ÿè®¡ï¼ˆä½¿ç”¨critic1ä½œä¸ºä»£è¡¨ï¼‰
            critic_attn_params = list(self.critic1.attn_model.parameters())
            if critic_attn_params:
                critic_param_values = torch.cat([p.data.flatten() for p in critic_attn_params])
                attention_metrics['attention_critic_param_mean'] = critic_param_values.mean().item()
                attention_metrics['attention_critic_param_std'] = critic_param_values.std().item()
            
            # 6. ä¿ç•™åŸæœ‰çš„æ€»ä½“ç»Ÿè®¡ï¼ˆå‘åå…¼å®¹ï¼‰
            attention_metrics['attention_param_mean'] = attention_metrics.get('attention_actor_param_mean', 0)
            attention_metrics['attention_param_std'] = attention_metrics.get('attention_actor_param_std', 0)
            
            # 7. ğŸ†• åˆ†æå…³èŠ‚æ³¨æ„åŠ›åˆ†å¸ƒ
            joint_focus_metrics = self._analyze_joint_attention_focus()
            attention_metrics.update(joint_focus_metrics)
            
        except Exception as e:
            attention_metrics['attention_calculation_error'] = str(e)
        
        return attention_metrics
    
    def _analyze_joint_attention_focus(self):
        """åˆ†æattentionç½‘ç»œå…³æ³¨å“ªäº›å…³èŠ‚å’Œå…³èŠ‚å®é™…ä½¿ç”¨æƒ…å†µ"""
        focus_metrics = {}
        
        try:
            # è·å–æœ€è¿‘ä¸€æ¬¡çš„batchæ•°æ®æ¥åˆ†æ
            if hasattr(self, '_last_batch_data'):
                batch = self._last_batch_data
                
                with torch.no_grad():
                    joint_q = batch['joint_q']
                    vertex_k = batch['vertex_k']
                    vertex_v = batch['vertex_v']
                    vertex_mask = batch.get('vertex_mask')
                    
                    batch_size, num_joints, _ = joint_q.shape
                    
                    # ğŸ†• è®°å½•æœºå™¨äººç»“æ„ä¿¡æ¯
                    focus_metrics['robot_num_joints'] = num_joints
                    focus_metrics['robot_structure_info'] = f"{num_joints}_joint_reacher"
                    
                    # ğŸ†• æ–¹æ³•1: åˆ†æå…³èŠ‚çŠ¶æ€çš„å˜åŒ–å¹…åº¦ï¼ˆçœŸå®çš„å…³èŠ‚ä½¿ç”¨æƒ…å†µï¼‰
                    # joint_qåŒ…å« [å…³èŠ‚ä½ç½®, å…³èŠ‚é€Ÿåº¦, GNNåµŒå…¥]
                    joint_positions = joint_q[:, :, 0]  # [B, J] - å…³èŠ‚è§’åº¦
                    joint_velocities = joint_q[:, :, 1]  # [B, J] - å…³èŠ‚è§’é€Ÿåº¦
                    
                    # è®¡ç®—å…³èŠ‚è§’åº¦çš„å˜åŒ–å¹…åº¦ï¼ˆç»å¯¹å€¼ï¼‰
                    joint_angle_magnitude = torch.abs(joint_positions).mean(dim=0)  # [J]
                    
                    # è®¡ç®—å…³èŠ‚é€Ÿåº¦çš„å˜åŒ–å¹…åº¦
                    joint_velocity_magnitude = torch.abs(joint_velocities).mean(dim=0)  # [J]
                    
                    # è®¡ç®—å…³èŠ‚æ´»è·ƒåº¦ï¼ˆè§’åº¦å¹…åº¦ + é€Ÿåº¦å¹…åº¦ï¼‰
                    joint_activity = joint_angle_magnitude + joint_velocity_magnitude  # [J]
                    
                    # æ‰¾å‡ºæœ€æ´»è·ƒçš„å…³èŠ‚
                    most_active_joint = torch.argmax(joint_activity).item()
                    max_activity = joint_activity[most_active_joint].item()
                    
                    # ğŸ†• æ–¹æ³•2: åˆ†æattentionç½‘ç»œå¯¹å„å…³èŠ‚çš„é‡è¦æ€§
                    # é€šè¿‡actorçš„attentionæ¨¡å‹è®¡ç®—attentionæƒé‡
                    attn_output = self.actor.attn_model(joint_q, vertex_k, vertex_v, vertex_mask)  # [B, J]
                    joint_importance = torch.abs(attn_output).mean(dim=0)  # [J] - æ¯ä¸ªå…³èŠ‚çš„é‡è¦æ€§
                    
                    # æ‰¾å‡ºæœ€é‡è¦çš„å…³èŠ‚
                    most_important_joint = torch.argmax(joint_importance).item()
                    max_importance = joint_importance[most_important_joint].item()
                    
                    # è®¡ç®—é‡è¦æ€§ç†µå€¼ï¼ˆè¡¡é‡æ³¨æ„åŠ›åˆ†å¸ƒçš„å‡åŒ€ç¨‹åº¦ï¼‰
                    importance_probs = F.softmax(joint_importance, dim=0)
                    importance_entropy = -(importance_probs * torch.log(importance_probs + 1e-8)).sum().item()
                    
                    # è®¡ç®—é‡è¦æ€§é›†ä¸­åº¦
                    importance_concentration = max_importance / (joint_importance.mean().item() + 1e-8)
                    
                    focus_metrics.update({
                        'most_important_joint': most_important_joint,
                        'max_joint_importance': max_importance,
                        'importance_entropy': importance_entropy,
                        'importance_concentration': importance_concentration,
                    })
                    
                    # ğŸ†• ç»Ÿä¸€è®°å½•20ä¸ªå…³èŠ‚çš„æ•°æ®ï¼ˆä¸å­˜åœ¨çš„å¡«-1ï¼‰
                    MAX_JOINTS = 20  # è®¾ç½®æœ€å¤§å…³èŠ‚æ•°ä¸Šé™
                    
                    for i in range(MAX_JOINTS):
                        if i < num_joints:
                            # å­˜åœ¨çš„å…³èŠ‚ï¼Œè®°å½•çœŸå®æ•°æ®
                            focus_metrics[f'joint_{i}_activity'] = joint_activity[i].item()
                            focus_metrics[f'joint_{i}_importance'] = joint_importance[i].item()
                            focus_metrics[f'joint_{i}_angle_magnitude'] = joint_angle_magnitude[i].item()
                            focus_metrics[f'joint_{i}_velocity_magnitude'] = joint_velocity_magnitude[i].item()
                        else:
                            # ä¸å­˜åœ¨çš„å…³èŠ‚ï¼Œå¡«å…¥-1
                            focus_metrics[f'joint_{i}_activity'] = -1.0
                            focus_metrics[f'joint_{i}_importance'] = -1.0
                            focus_metrics[f'joint_{i}_angle_magnitude'] = -1.0
                            focus_metrics[f'joint_{i}_velocity_magnitude'] = -1.0
                    
                    # ğŸ†• ç»Ÿä¸€è®°å½•20ä¸ªlinkçš„é•¿åº¦ä¿¡æ¯ï¼ˆä¸å­˜åœ¨çš„å¡«-1ï¼‰
                    for i in range(MAX_JOINTS):
                        if hasattr(self, 'robot_link_lengths') and i < len(self.robot_link_lengths):
                            # å­˜åœ¨çš„linkï¼Œè®°å½•çœŸå®é•¿åº¦
                            focus_metrics[f'link_{i}_length'] = self.robot_link_lengths[i]
                        else:
                            # ä¸å­˜åœ¨çš„linkï¼Œå¡«å…¥-1
                            focus_metrics[f'link_{i}_length'] = -1.0
                            
        except Exception as e:
            focus_metrics['joint_analysis_error'] = str(e)
            # å¦‚æœåˆ†æå¤±è´¥ï¼Œå¡«å…¥é»˜è®¤å€¼
            for i in range(20):
                focus_metrics[f'joint_{i}_activity'] = -1.0
                focus_metrics[f'joint_{i}_importance'] = -1.0
                focus_metrics[f'joint_{i}_angle_magnitude'] = -1.0
                focus_metrics[f'joint_{i}_velocity_magnitude'] = -1.0
                focus_metrics[f'link_{i}_length'] = -1.0
        
        return focus_metrics


# è®­ç»ƒå¾ªç¯ç¤ºä¾‹
def train_sac_with_buffer():
    """è®­ç»ƒSACçš„ç¤ºä¾‹"""
    from attn_model import AttnModel
    
    # åˆå§‹åŒ–
    action_dim = 12
    attn_model = AttnModel(128, 128, 130, 4)
    sac = AttentionSACWithBuffer(attn_model, action_dim, device='cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    num_episodes = 1000
    max_steps_per_episode = 200
    update_frequency = 4  # æ¯4æ­¥æ›´æ–°ä¸€æ¬¡
    
    for episode in range(num_episodes):
        # æ¨¡æ‹Ÿç¯å¢ƒé‡ç½®
        obs = torch.randn(40)  # [40] è§‚å¯Ÿç©ºé—´
        gnn_embeds = torch.randn(12, 128)  # [12, 128] GNNåµŒå…¥
        episode_reward = 0
        
        for step in range(max_steps_per_episode):
            # è·å–åŠ¨ä½œ
            action = sac.get_action(obs, gnn_embeds, deterministic=False)
            
            # æ¨¡æ‹Ÿç¯å¢ƒäº¤äº’
            next_obs = torch.randn(40)  # ä¸‹ä¸€ä¸ªè§‚å¯Ÿ
            next_gnn_embeds = torch.randn(12, 128)  # ä¸‹ä¸€ä¸ªGNNåµŒå…¥
            reward = torch.randn(1).item()  # éšæœºå¥–åŠ±
            done = step == max_steps_per_episode - 1  # æœ€åä¸€æ­¥ç»“æŸ
            
            # å­˜å‚¨ç»éªŒ
            sac.store_experience(obs, gnn_embeds, action, reward, next_obs, next_gnn_embeds, done)
            
            # æ›´æ–°ç½‘ç»œ
            if step % update_frequency == 0 and sac.memory.can_sample(sac.batch_size):
                metrics = sac.update()
                if metrics and step % 20 == 0:
                    print(f"Episode {episode}, Step {step}: {metrics}")
            
            # å‡†å¤‡ä¸‹ä¸€æ­¥
            obs = next_obs
            gnn_embeds = next_gnn_embeds
            episode_reward += reward
            
            if done:
                break
        
        if episode % 10 == 0:
            print(f"Episode {episode}, Total Reward: {episode_reward:.2f}, Buffer Size: {len(sac.memory)}")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("Testing SAC with Memory Buffer...")
    
    # åˆ›å»ºSACå®ä¾‹
    from attn_model import AttnModel
    action_dim = 12
    attn_model = AttnModel(128, 128, 130, 4)
    sac = AttentionSACWithBuffer(attn_model, action_dim)
    
    # æ¨¡æ‹Ÿä¸€äº›ç»éªŒå­˜å‚¨
    for i in range(10):
        obs = torch.randn(40)
        gnn_embeds = torch.randn(12, 128)
        action = torch.randn(12)
        reward = torch.randn(1).item()
        next_obs = torch.randn(40)
        next_gnn_embeds = torch.randn(12, 128)
        done = i == 9
        
        sac.store_experience(obs, gnn_embeds, action, reward, next_obs, next_gnn_embeds, done)
    
    print(f"Buffer size: {len(sac.memory)}")
    
    # æµ‹è¯•è·å–åŠ¨ä½œ
    obs = torch.randn(40)
    gnn_embeds = torch.randn(12, 128)
    action = sac.get_action(obs, gnn_embeds)
    print(f"Action shape: {action.shape}")
    
    # æµ‹è¯•æ›´æ–°ï¼ˆå¦‚æœbufferè¶³å¤Ÿå¤§ï¼‰
    if sac.memory.can_sample(sac.batch_size):
        metrics = sac.update()
        print("Update metrics:", metrics)
    else:
        print("Buffer too small for update")
    
    print("Buffer integration successful!")