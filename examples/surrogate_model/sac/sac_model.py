import torch
import torch.nn as nn
import numpy as np
import random
from collections import deque, namedtuple
import os
import sys

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(__file__), "../../../")
sys.path.append(base_dir)
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_dataset"))
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_model"))
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/sac"))
from data_utils import prepare_joint_q_input, prepare_reacher2d_joint_q_input, prepare_dynamic_vertex_v
# å¯¼å…¥ä½ çš„ç»„ä»¶
from attn_actor import AttentionActor
from attn_critic import AttentionCritic
import copy
        
# å®šä¹‰ç»éªŒå…ƒç»„
Experience = namedtuple('Experience', [
    'joint_q', 'vertex_k', 'vertex_v', 'action', 'reward', 
    'next_joint_q', 'next_vertex_k', 'next_vertex_v', 'done', 'vertex_mask'
])

class ReplayBuffer:
    def __init__(self, capacity=100000, device='cpu'):
        self.capacity = capacity
        self.device = device
        self.buffer = deque(maxlen=capacity)
        self.position = 0
    
    def push(self, joint_q, vertex_k, vertex_v, action, reward, 
             next_joint_q, next_vertex_k, next_vertex_v, done, vertex_mask=None):
        """å­˜å‚¨ä¸€ä¸ªç»éªŒ"""
        
        # è½¬æ¢ä¸ºCPU tensorä»¥èŠ‚çœGPUå†…å­˜
        experience = Experience(
            joint_q=joint_q.cpu() if torch.is_tensor(joint_q) else joint_q,
            vertex_k=vertex_k.cpu() if torch.is_tensor(vertex_k) else vertex_k,
            vertex_v=vertex_v.cpu() if torch.is_tensor(vertex_v) else vertex_v,
            action=action.cpu() if torch.is_tensor(action) else action,
            reward=reward.cpu() if torch.is_tensor(reward) else reward,
            next_joint_q=next_joint_q.cpu() if torch.is_tensor(next_joint_q) else next_joint_q,
            next_vertex_k=next_vertex_k.cpu() if torch.is_tensor(next_vertex_k) else next_vertex_k,
            next_vertex_v=next_vertex_v.cpu() if torch.is_tensor(next_vertex_v) else next_vertex_v,
            done=done.cpu() if torch.is_tensor(done) else done,
            vertex_mask=vertex_mask.cpu() if vertex_mask is not None and torch.is_tensor(vertex_mask) else vertex_mask
        )
        
        self.buffer.append(experience)
    
    def sample(self, batch_size):
        """é‡‡æ ·ä¸€ä¸ªbatchçš„ç»éªŒ"""
        if len(self.buffer) < batch_size:
            batch_size = len(self.buffer)
            
        experiences = random.sample(self.buffer, batch_size)
        
        # å°†ç»éªŒè½¬æ¢ä¸ºæ‰¹é‡tensor
        joint_q = torch.stack([e.joint_q for e in experiences]).to(self.device)
        vertex_k = torch.stack([e.vertex_k for e in experiences]).to(self.device)
        vertex_v = torch.stack([e.vertex_v for e in experiences]).to(self.device)
        actions = torch.stack([e.action for e in experiences]).to(self.device)
        rewards = torch.stack([e.reward for e in experiences]).to(self.device)
        next_joint_q = torch.stack([e.next_joint_q for e in experiences]).to(self.device)
        next_vertex_k = torch.stack([e.next_vertex_k for e in experiences]).to(self.device)
        next_vertex_v = torch.stack([e.next_vertex_v for e in experiences]).to(self.device)
        dones = torch.stack([e.done for e in experiences]).to(self.device)
        
        # å¤„ç†vertex_mask (å¯èƒ½ä¸ºNone)
        vertex_mask = None
        if experiences[0].vertex_mask is not None:
            vertex_mask = torch.stack([e.vertex_mask for e in experiences]).to(self.device)
        
        return (joint_q, vertex_k, vertex_v, actions, rewards, 
                next_joint_q, next_vertex_k, next_vertex_v, dones, vertex_mask)
    
    def __len__(self):
        return len(self.buffer)
    
    def can_sample(self, batch_size):
        return len(self.buffer) >= batch_size


class AttentionSACWithBuffer:
    def __init__(self, attn_model, action_dim, joint_embed_dim=128, 
                 buffer_capacity=100000, batch_size=256, lr=3e-4, 
                 gamma=0.99, tau=0.005, alpha=0.2, device='cpu', env_type='bullet'):
        
        self.device = device
        self.action_dim = action_dim
        self.batch_size = batch_size
        self.warmup_steps = 10000
        self.env_type = env_type
 
        # åˆ›å»ºç‹¬ç«‹çš„æ¨¡å‹æ‹·è´ - è§£å†³å…±äº«é—®é¢˜
        actor_model = copy.deepcopy(attn_model)
        critic1_model = copy.deepcopy(attn_model)
        critic2_model = copy.deepcopy(attn_model)
        
        self.actor = AttentionActor(actor_model, action_dim).to(device)
        self.critic1 = AttentionCritic(critic1_model, joint_embed_dim, device=device).to(device)
        self.critic2 = AttentionCritic(critic2_model, joint_embed_dim, device=device).to(device)
        
        # Target networks - ç°åœ¨æ˜¯çœŸæ­£ç‹¬ç«‹çš„
        self.target_critic1 = copy.deepcopy(self.critic1)
        self.target_critic2 = copy.deepcopy(self.critic2)
        
        # å†»ç»“target networks
        for param in self.target_critic1.parameters():
            param.requires_grad = False
        for param in self.target_critic2.parameters():
            param.requires_grad = False
            
        # Memory Buffer
        self.memory = ReplayBuffer(buffer_capacity, device)
        
        # è¶…å‚æ•°
        self.gamma = gamma
        self.tau = tau
        self.alpha = alpha
        
        # ä¼˜åŒ–å™¨ - ç°åœ¨å®Œå…¨ç‹¬ç«‹
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = torch.optim.Adam(
            list(self.critic1.parameters()) + list(self.critic2.parameters()), lr=lr
        )
        
        # è‡ªåŠ¨è°ƒæ•´alpha
        self.target_entropy = -action_dim
        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
        self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=lr)
        
    def store_experience(self, obs, gnn_embeds, action, reward, next_obs, next_gnn_embeds, done, num_joints=12):
        """å­˜å‚¨ç¯å¢ƒäº¤äº’ç»éªŒ"""
        if self.env_type == 'reacher2d':
            joint_q = prepare_reacher2d_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints).squeeze(0)
            next_joint_q = prepare_reacher2d_joint_q_input(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints).squeeze(0)
        else:  # bullet ç¯å¢ƒ
            joint_q = prepare_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints).squeeze(0)
            next_joint_q = prepare_joint_q_input(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints).squeeze(0)
        # å‡†å¤‡å½“å‰çŠ¶æ€
        vertex_k = gnn_embeds
        vertex_v = vertex_v = prepare_dynamic_vertex_v(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints, self.env_type).squeeze(0)  # ğŸ¯ åŠ¨æ€V
        
        # å‡†å¤‡ä¸‹ä¸€ä¸ªçŠ¶æ€
        # next_joint_q = prepare_joint_q_input(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints).squeeze(0)
        next_vertex_k = next_gnn_embeds
        next_vertex_v = prepare_dynamic_vertex_v(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints, self.env_type).squeeze(0)  # ğŸ¯ åŠ¨æ€V
        
        # è½¬æ¢ä¸ºé€‚å½“çš„tensoræ ¼å¼
        if not torch.is_tensor(reward):
            reward = torch.tensor([reward], dtype=torch.float32)
        if not torch.is_tensor(done):
            done = torch.tensor([done], dtype=torch.float32)
            
        self.memory.push(
            joint_q, vertex_k, vertex_v, action, reward,
            next_joint_q, next_vertex_k, next_vertex_v, done
        )
    
    def get_action(self, obs, gnn_embeds, num_joints=12, deterministic=False):
        """è·å–åŠ¨ä½œ"""
          
        # ğŸ¯ æ ¹æ®ç¯å¢ƒç±»å‹é€‰æ‹©æ•°æ®å¤„ç†å‡½æ•°
        if self.env_type == 'reacher2d':
            joint_q = prepare_reacher2d_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints)
        else:  # bullet ç¯å¢ƒ
            joint_q = prepare_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints)
        vertex_k = gnn_embeds.unsqueeze(0)
        vertex_v = prepare_dynamic_vertex_v(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints, self.env_type)  # ğŸ¯ åŠ¨æ€V
        
        with torch.no_grad():
            if deterministic:
                mean, _ = self.actor.forward(joint_q, vertex_k, vertex_v)
                tanh_action = torch.tanh(mean).squeeze(0)
            else:
                tanh_action, _, _ = self.actor.sample(joint_q, vertex_k, vertex_v)
                tanh_action = tanh_action.squeeze(0)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šAction Scalingï¼
            # SACè¾“å‡º[-1,+1]ï¼Œéœ€è¦ç¼©æ”¾åˆ°ç¯å¢ƒçš„action space
            if self.env_type == 'reacher2d':
                # Reacher2Dç¯å¢ƒä½¿ç”¨Â±100çš„action spaceï¼ˆä¿®æ”¹ä¸ºæ›´å°çš„èŒƒå›´ï¼‰
                action_scale = 100.0  # ä»500.0æ”¹ä¸º100.0
                scaled_action = tanh_action * action_scale
                return scaled_action
            else:
                # Bulletç¯å¢ƒä¿æŒåŸæœ‰é€»è¾‘
                return tanh_action
    
    def soft_update_targets(self):
        """è½¯æ›´æ–°target networks"""
        for target_param, param in zip(self.target_critic1.parameters(), self.critic1.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
        
        for target_param, param in zip(self.target_critic2.parameters(), self.critic2.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
    
    def update(self):
        """ä»memory bufferé‡‡æ ·å¹¶æ›´æ–°ç½‘ç»œ"""
        if not self.memory.can_sample(self.batch_size):
            return None
            
        # ä»bufferé‡‡æ ·
        batch = self.memory.sample(self.batch_size)
        joint_q, vertex_k, vertex_v, actions, rewards, next_joint_q, next_vertex_k, next_vertex_v, dones, vertex_mask = batch
        
        # === Critic Update ===
        with torch.no_grad():
            # é‡‡æ ·ä¸‹ä¸€ä¸ªçŠ¶æ€çš„åŠ¨ä½œ
            next_actions, next_log_probs, _ = self.actor.sample(next_joint_q, next_vertex_k, next_vertex_v, vertex_mask)
            
            # è®¡ç®—target Qå€¼
            target_q1 = self.target_critic1(next_joint_q, next_vertex_k, next_vertex_v, 
                                          vertex_mask=vertex_mask, action=next_actions)
            target_q2 = self.target_critic2(next_joint_q, next_vertex_k, next_vertex_v, 
                                          vertex_mask=vertex_mask, action=next_actions)
            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs
            target_q = rewards + (1 - dones) * self.gamma * target_q
        
        # å½“å‰Qå€¼
        current_q1 = self.critic1(joint_q, vertex_k, vertex_v, vertex_mask=vertex_mask, action=actions)
        current_q2 = self.critic2(joint_q, vertex_k, vertex_v, vertex_mask=vertex_mask, action=actions)
        
        # CriticæŸå¤±
        critic_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)
        
        # æ›´æ–°Critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # === Actor Update ===
        # é‡‡æ ·æ–°åŠ¨ä½œ
        new_actions, log_probs, _ = self.actor.sample(joint_q, vertex_k, vertex_v, vertex_mask)
        
        # è®¡ç®—Qå€¼
        q1 = self.critic1(joint_q, vertex_k, vertex_v, vertex_mask=vertex_mask, action=new_actions)
        q2 = self.critic2(joint_q, vertex_k, vertex_v, vertex_mask=vertex_mask, action=new_actions)
        q = torch.min(q1, q2)
        
        # ActoræŸå¤±
        actor_loss = (self.alpha * log_probs - q).mean()
        
        # åˆ†ælossç»„ä»¶
        entropy_term = (self.alpha * log_probs).mean()
        q_term = q.mean()
        
        # æ›´æ–°Actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # === Alpha Update ===
        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
        
        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.alpha_optimizer.step()
        
        self.alpha = self.log_alpha.exp()
        
        # è½¯æ›´æ–°target networks
        self.soft_update_targets()
        
        return {
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item(),
            'alpha_loss': alpha_loss.item(),
            'alpha': self.alpha.item(),
            'q1_mean': current_q1.mean().item(),
            'q2_mean': current_q2.mean().item(),
            'buffer_size': len(self.memory),
            # æ·»åŠ lossç»„ä»¶åˆ†æ
            'entropy_term': entropy_term.item(),
            'q_term': q_term.item(),
            'log_probs_mean': log_probs.mean().item()
        }


# è®­ç»ƒå¾ªç¯ç¤ºä¾‹
def train_sac_with_buffer():
    """è®­ç»ƒSACçš„ç¤ºä¾‹"""
    from attn_model import AttnModel
    
    # åˆå§‹åŒ–
    action_dim = 12
    attn_model = AttnModel(128, 128, 130, 4)
    sac = AttentionSACWithBuffer(attn_model, action_dim, device='cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    num_episodes = 1000
    max_steps_per_episode = 200
    update_frequency = 4  # æ¯4æ­¥æ›´æ–°ä¸€æ¬¡
    
    for episode in range(num_episodes):
        # æ¨¡æ‹Ÿç¯å¢ƒé‡ç½®
        obs = torch.randn(40)  # [40] è§‚å¯Ÿç©ºé—´
        gnn_embeds = torch.randn(12, 128)  # [12, 128] GNNåµŒå…¥
        episode_reward = 0
        
        for step in range(max_steps_per_episode):
            # è·å–åŠ¨ä½œ
            action = sac.get_action(obs, gnn_embeds, deterministic=False)
            
            # æ¨¡æ‹Ÿç¯å¢ƒäº¤äº’
            next_obs = torch.randn(40)  # ä¸‹ä¸€ä¸ªè§‚å¯Ÿ
            next_gnn_embeds = torch.randn(12, 128)  # ä¸‹ä¸€ä¸ªGNNåµŒå…¥
            reward = torch.randn(1).item()  # éšæœºå¥–åŠ±
            done = step == max_steps_per_episode - 1  # æœ€åä¸€æ­¥ç»“æŸ
            
            # å­˜å‚¨ç»éªŒ
            sac.store_experience(obs, gnn_embeds, action, reward, next_obs, next_gnn_embeds, done)
            
            # æ›´æ–°ç½‘ç»œ
            if step % update_frequency == 0 and sac.memory.can_sample(sac.batch_size):
                metrics = sac.update()
                if metrics and step % 20 == 0:
                    print(f"Episode {episode}, Step {step}: {metrics}")
            
            # å‡†å¤‡ä¸‹ä¸€æ­¥
            obs = next_obs
            gnn_embeds = next_gnn_embeds
            episode_reward += reward
            
            if done:
                break
        
        if episode % 10 == 0:
            print(f"Episode {episode}, Total Reward: {episode_reward:.2f}, Buffer Size: {len(sac.memory)}")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("Testing SAC with Memory Buffer...")
    
    # åˆ›å»ºSACå®ä¾‹
    from attn_model import AttnModel
    action_dim = 12
    attn_model = AttnModel(128, 128, 130, 4)
    sac = AttentionSACWithBuffer(attn_model, action_dim)
    
    # æ¨¡æ‹Ÿä¸€äº›ç»éªŒå­˜å‚¨
    for i in range(10):
        obs = torch.randn(40)
        gnn_embeds = torch.randn(12, 128)
        action = torch.randn(12)
        reward = torch.randn(1).item()
        next_obs = torch.randn(40)
        next_gnn_embeds = torch.randn(12, 128)
        done = i == 9
        
        sac.store_experience(obs, gnn_embeds, action, reward, next_obs, next_gnn_embeds, done)
    
    print(f"Buffer size: {len(sac.memory)}")
    
    # æµ‹è¯•è·å–åŠ¨ä½œ
    obs = torch.randn(40)
    gnn_embeds = torch.randn(12, 128)
    action = sac.get_action(obs, gnn_embeds)
    print(f"Action shape: {action.shape}")
    
    # æµ‹è¯•æ›´æ–°ï¼ˆå¦‚æœbufferè¶³å¤Ÿå¤§ï¼‰
    if sac.memory.can_sample(sac.batch_size):
        metrics = sac.update()
        print("Update metrics:", metrics)
    else:
        print("Buffer too small for update")
    
    print("Buffer integration successful!")