#!/usr/bin/env python3
"""
é€šç”¨PPOæ¨¡å‹ - æ”¯æŒä»»æ„å…³èŠ‚æ•°çš„æœºå™¨äººæ§åˆ¶
åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ¨æ€é€‚åº”ä¸åŒçš„å…³èŠ‚æ•°é‡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import sys
from collections import deque
from torch.distributions import Normal
import copy

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(__file__), "../../../")
sys.path.append(base_dir)
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_dataset"))
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_model"))

from data_utils import prepare_joint_q_input, prepare_reacher2d_joint_q_input, prepare_dynamic_vertex_v


class UniversalAttnModel(nn.Module):
    """é€šç”¨æ³¨æ„åŠ›æ¨¡å‹ - æ”¯æŒåŠ¨æ€å…³èŠ‚æ•°è¾“å‡º"""
    def __init__(self, vertex_key_size=128, vertex_value_size=130, joint_query_size=130, num_heads=4):
        super(UniversalAttnModel, self).__init__()
        self.vertex_key_size = vertex_key_size
        self.vertex_value_size = vertex_value_size
        self.joint_query_size = joint_query_size
        self.num_heads = num_heads
        
        # å…³èŠ‚æŸ¥è¯¢ç¼–ç å™¨
        self.joint_q_encoder = nn.Sequential(
            nn.Linear(joint_query_size, 64),
            nn.ReLU(),
            nn.Linear(64, vertex_key_size * num_heads)
        )
        
        # ğŸ¯ å…³é”®æ”¹è¿›ï¼šæ¯ä¸ªå…³èŠ‚ç‹¬ç«‹çš„è¾“å‡ºå¤´
        self.joint_output_layer = nn.Sequential(
            nn.Linear(vertex_value_size * num_heads, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # æ¯ä¸ªå…³èŠ‚è¾“å‡ºä¸€ä¸ªæ ‡é‡
        )

    def forward(self, joint_q, vertex_k, vertex_v, vertex_mask=None):
        """
        Args:
            joint_q: [B, num_joints, 130] - åŠ¨æ€å…³èŠ‚æ•°
            vertex_k: [B, N, 128]
            vertex_v: [B, N, 130]
        Returns:
            output: [B, num_joints] - åŠ¨æ€è¾“å‡ºç»´åº¦
        """
        batch_size, num_joints, _ = joint_q.shape
        
        # ç¼–ç å…³èŠ‚æŸ¥è¯¢
        joint_q_encoded = self.joint_q_encoder(joint_q)  # [B, num_joints, 512]
        joint_q_encoded = joint_q_encoded.view(batch_size, num_joints, self.num_heads, self.vertex_key_size)
        
        # æ³¨æ„åŠ›è®¡ç®— - ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        attn_output = self._compute_attention(vertex_k, vertex_v, joint_q_encoded, vertex_mask)
        
        # å±•å¹³å¤šå¤´è¾“å‡º
        attn_output = attn_output.view(batch_size, num_joints, self.num_heads * self.vertex_value_size)
        
        # ğŸ¯ æ¯ä¸ªå…³èŠ‚ç‹¬ç«‹è¾“å‡º
        output = self.joint_output_layer(attn_output)  # [B, num_joints, 1]
        output = output.squeeze(-1)  # [B, num_joints]
        
        return output
    
    def _compute_attention(self, vertex_k, vertex_v, joint_q, vertex_mask=None):
        """ç®€åŒ–çš„æ³¨æ„åŠ›è®¡ç®— - ä¿®å¤ç»´åº¦é—®é¢˜"""
        batch_size, num_joints, num_heads, key_size = joint_q.shape
        _, num_vertices, vertex_k_dim = vertex_k.shape
        _, _, vertex_v_dim = vertex_v.shape
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†ç»´åº¦åŒ¹é…
        # å°†joint_qé‡å¡‘ä¸º [B*J*H, 1, key_size]
        joint_q_flat = joint_q.view(batch_size * num_joints * num_heads, 1, key_size)
        
        # æ‰©å±•vertex_kå’Œvertex_våˆ°æ‰€æœ‰å…³èŠ‚å’Œå¤´
        vertex_k_expanded = vertex_k.unsqueeze(1).unsqueeze(1).expand(
            batch_size, num_joints, num_heads, num_vertices, vertex_k_dim
        ).contiguous().view(batch_size * num_joints * num_heads, num_vertices, vertex_k_dim)
        
        vertex_v_expanded = vertex_v.unsqueeze(1).unsqueeze(1).expand(
            batch_size, num_joints, num_heads, num_vertices, vertex_v_dim
        ).contiguous().view(batch_size * num_joints * num_heads, num_vertices, vertex_v_dim)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(joint_q_flat, vertex_k_expanded.transpose(-2, -1))  # [B*J*H, 1, N]
        scores = scores / (key_size ** 0.5)
        
        # åº”ç”¨æ©ç 
        if vertex_mask is not None:
            # æ‰©å±•æ©ç 
            mask_expanded = vertex_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1).expand(
                batch_size, num_joints, num_heads, 1, num_vertices
            ).contiguous().view(batch_size * num_joints * num_heads, 1, num_vertices)
            scores.masked_fill_(mask_expanded, -1e9)
        
        # æ³¨æ„åŠ›æƒé‡å’Œè¾“å‡º
        attn_weights = F.softmax(scores, dim=-1)  # [B*J*H, 1, N]
        output_flat = torch.matmul(attn_weights, vertex_v_expanded)  # [B*J*H, 1, vertex_v_dim]
        
        # é‡å¡‘å›åŸå§‹ç»´åº¦
        output = output_flat.view(batch_size, num_joints, num_heads, vertex_v_dim)
        
        return output


class UniversalPPOActor(nn.Module):
    """é€šç”¨PPO Actor - æ”¯æŒä»»æ„å…³èŠ‚æ•°"""
    def __init__(self, attn_model, log_std_init=-1.5, device='cpu'):
        super(UniversalPPOActor, self).__init__()
        self.attn_model = attn_model
        self.device = device
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹å€¼ï¼Œé˜²æ­¢entropyçˆ†ç‚¸
        self.log_std_base = nn.Parameter(torch.tensor(log_std_init))
        
    # def forward(self, joint_q, vertex_k, vertex_v, vertex_mask=None):
    #     """å‰å‘ä¼ æ’­ï¼Œè¾“å‡ºåŠ¨ä½œåˆ†å¸ƒå‚æ•°"""
    #     batch_size, num_joints, _ = joint_q.shape
        
    #     # è·å–åŠ¨ä½œå‡å€¼
    #     mean = self.attn_model(joint_q, vertex_k, vertex_v, vertex_mask)  # [B, num_joints]
        
    #     # åŠ¨æ€ç”Ÿæˆæ ‡å‡†å·®
    #     std = torch.exp(self.log_std_base).expand_as(mean)  # [B, num_joints]
        
    #     return mean, std

    def forward(self, joint_q, vertex_k, vertex_v, vertex_mask=None):
        """å‰å‘ä¼ æ’­ï¼Œè¾“å‡ºåŠ¨ä½œåˆ†å¸ƒå‚æ•°"""
        batch_size, num_joints, _ = joint_q.shape
        
        # è·å–åŠ¨ä½œå‡å€¼
        mean = self.attn_model(joint_q, vertex_k, vertex_v, vertex_mask)  # [B, num_joints]
        
        # ğŸ”§ ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„æ ‡å‡†å·®é™åˆ¶ï¼Œé˜²æ­¢entropyçˆ†ç‚¸
        log_std_clamped = torch.clamp(self.log_std_base, min=-2.3, max=-0.5)
        std = torch.exp(log_std_clamped).expand_as(mean)  # stdèŒƒå›´: [0.1, 0.6]
        
        return mean, std
    
    def get_action(self, joint_q, vertex_k, vertex_v, vertex_mask=None, deterministic=False):
        """è·å–åŠ¨ä½œå’Œlogæ¦‚ç‡"""
        mean, std = self.forward(joint_q, vertex_k, vertex_v, vertex_mask)
        
        if deterministic:
            action = mean
            log_prob = None
        else:
            dist = Normal(mean, std)
            action = dist.sample()
            log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        
        return action, log_prob, mean, std
    
    def evaluate_action(self, joint_q, vertex_k, vertex_v, action, vertex_mask=None):
        """è¯„ä¼°ç»™å®šåŠ¨ä½œçš„logæ¦‚ç‡å’Œç†µ"""
        mean, std = self.forward(joint_q, vertex_k, vertex_v, vertex_mask)
        
        dist = Normal(mean, std)
        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        entropy = dist.entropy().sum(dim=-1, keepdim=True)
        
        return log_prob, entropy


class UniversalPPOCritic(nn.Module):
    """é€šç”¨PPO Critic - æ”¯æŒä»»æ„å…³èŠ‚æ•°"""
    def __init__(self, attn_model, hidden_dim=256, device='cpu'):
        super(UniversalPPOCritic, self).__init__()
        self.attn_model = copy.deepcopy(attn_model)
        self.device = device
        
        # ğŸ¯ ä½¿ç”¨æ³¨æ„åŠ›æ± åŒ–ï¼Œè€Œä¸æ˜¯ç®€å•å¹³å‡
        self.value_attention = nn.MultiheadAttention(128, 4, batch_first=True)  # 128å¯ä»¥è¢«4æ•´é™¤
        
        # è¾“å…¥æŠ•å½±å±‚ï¼Œå°†130ç»´æ˜ å°„åˆ°128ç»´
        self.input_projection = nn.Linear(130, 128)
        
        # ä»·å€¼ç½‘ç»œ
        self.value_head = nn.Sequential(
            nn.Linear(128, hidden_dim),  # ä¿®æ”¹ä¸º128ç»´è¾“å…¥
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.LayerNorm(hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim//2, 1)
        )
        
        self.value_scale = 200.0
        
    def forward(self, joint_q, vertex_k, vertex_v, vertex_mask=None):
        """å‰å‘ä¼ æ’­ï¼Œè¾“å‡ºçŠ¶æ€ä»·å€¼"""
        batch_size, num_joints, feature_dim = joint_q.shape
        
        # æŠ•å½±åˆ°128ç»´
        joint_q_proj = self.input_projection(joint_q)  # [B, num_joints, 128]
        
        # ğŸ¯ ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èšåˆå…³èŠ‚ä¿¡æ¯
        # åˆ›å»ºå…¨å±€æŸ¥è¯¢
        global_query = joint_q_proj.mean(dim=1, keepdim=True)  # [B, 1, 128]
        
        # æ³¨æ„åŠ›èšåˆ
        attn_output, _ = self.value_attention(
            global_query,    # query: [B, 1, 128]
            joint_q_proj,    # key: [B, num_joints, 128] 
            joint_q_proj     # value: [B, num_joints, 128]
        )  # output: [B, 1, 128]
        
        # è®¡ç®—ä»·å€¼
        value = self.value_head(attn_output.squeeze(1))  # [B, 1]
        value = torch.tanh(value) * self.value_scale
        
        return value


class UniversalRolloutBuffer:
    """é€šç”¨ç»éªŒå›æ”¾ç¼“å†²åŒº - æ”¯æŒå˜é•¿æ•°æ®"""
    def __init__(self, buffer_size, device='cpu'):
        self.buffer_size = buffer_size
        self.device = device
        self.clear()
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.experiences = []  # å­˜å‚¨å®Œæ•´çš„ç»éªŒå…ƒç»„
        self.advantages = []
        self.returns = []
        self.ptr = 0
    
    def store(self, joint_q, vertex_k, vertex_v, action, reward, value, log_prob, done, num_joints):
        """å­˜å‚¨ä¸€æ­¥ç»éªŒ - åŒ…å«å…³èŠ‚æ•°ä¿¡æ¯"""
        experience = {
            'joint_q': joint_q.cpu(),
            'vertex_k': vertex_k.cpu(), 
            'vertex_v': vertex_v.cpu(),
            'action': action.cpu(),
            'reward': reward,
            'value': value.cpu().squeeze(),
            'log_prob': log_prob.cpu() if log_prob is not None else torch.tensor(0.0),
            'done': done,
            'num_joints': num_joints
        }
        self.experiences.append(experience)
        self.ptr += 1
    
    def compute_advantages(self, last_value, gamma=0.99, gae_lambda=0.95):
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥"""
        values = [exp['value'] for exp in self.experiences] + [last_value.cpu().squeeze()]
        rewards = [exp['reward'] for exp in self.experiences]
        dones = [exp['done'] for exp in self.experiences]
        
        values = torch.stack(values)
        rewards = torch.tensor(rewards, dtype=torch.float32)
        dones = torch.tensor(dones, dtype=torch.float32)
        
        # å¥–åŠ±å½’ä¸€åŒ–
        if len(rewards) > 1:
            reward_mean = rewards.mean()
            reward_std = rewards.std() + 1e-8
            rewards = (rewards - reward_mean) / reward_std
        
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)
        
        gae = 0
        for step in reversed(range(len(rewards))):
            delta = rewards[step] + gamma * values[step + 1] * (1 - dones[step]) - values[step]
            gae = delta + gamma * gae_lambda * (1 - dones[step]) * gae
            advantages[step] = gae
            returns[step] = advantages[step] + values[step]
        
        # ä¼˜åŠ¿å‡½æ•°å½’ä¸€åŒ–
        if len(advantages) > 1:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # å­˜å‚¨åˆ°ç»éªŒä¸­
        for i, exp in enumerate(self.experiences):
            exp['advantage'] = advantages[i]
            exp['return'] = returns[i]
    
    def get_batch(self, batch_size):
        """è·å–æ‰¹é‡æ•°æ® - æŒ‰å…³èŠ‚æ•°åˆ†ç»„"""
        if len(self.experiences) < batch_size:
            indices = list(range(len(self.experiences)))
        else:
            indices = torch.randperm(len(self.experiences))[:batch_size].tolist()
        
        # æŒ‰å…³èŠ‚æ•°åˆ†ç»„
        joint_groups = {}
        for i in indices:
            exp = self.experiences[i]
            num_joints = exp['num_joints']
            if num_joints not in joint_groups:
                joint_groups[num_joints] = []
            joint_groups[num_joints].append(exp)
        
        return joint_groups


class UniversalPPOWithBuffer:
    """é€šç”¨PPOç®—æ³• - æ”¯æŒä»»æ„å…³èŠ‚æ•°çš„æœºå™¨äºº"""
    def __init__(self, buffer_size=2048, batch_size=64, lr=1e-4, gamma=0.99, 
                 gae_lambda=0.95, clip_epsilon=0.2, entropy_coef=0.01, 
                 value_coef=0.5, max_grad_norm=0.5, device='cpu', env_type='reacher2d'):
        
        self.device = device
        self.env_type = env_type
        self.batch_size = batch_size
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        # ğŸ”§ ä¿®å¤ï¼šé™ä½ç†µç³»æ•°ï¼Œé˜²æ­¢è¿‡åº¦æ¢ç´¢
        self.entropy_coef = min(entropy_coef, 0.005)
        self.value_coef = value_coef
        self.max_grad_norm = max_grad_norm
        
        # åˆ›å»ºé€šç”¨ç½‘ç»œ
        self.universal_attn = UniversalAttnModel(128, 130, 130, 4)
        self.actor = UniversalPPOActor(self.universal_attn, device=device)
        self.critic = UniversalPPOCritic(self.universal_attn, device=device)
        
        # ğŸ”§ ä¿®å¤ï¼šæ›´ä¿å®ˆçš„å­¦ä¹ ç‡å’Œæ›´æ¿€è¿›çš„è¡°å‡
        actor_lr = lr * 0.3  # Actorå­¦ä¹ ç‡é™ä½
        critic_lr = lr * 0.2  # Criticå­¦ä¹ ç‡æ›´ä½
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)
        
        # ğŸ”§ ä¿®å¤ï¼šæ›´æ¿€è¿›çš„å­¦ä¹ ç‡è¡°å‡
        self.actor_scheduler = torch.optim.lr_scheduler.StepLR(self.actor_optimizer, step_size=500, gamma=0.9)
        self.critic_scheduler = torch.optim.lr_scheduler.StepLR(self.critic_optimizer, step_size=500, gamma=0.9)
        
        # ç»éªŒç¼“å†²åŒº
        self.buffer = UniversalRolloutBuffer(buffer_size, device)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.update_count = 0
        self.recent_losses = deque(maxlen=10)
        
        print(f"ğŸ¯ é€šç”¨PPOåˆå§‹åŒ–å®Œæˆ:")
        print(f"   æ”¯æŒä»»æ„å…³èŠ‚æ•°: 2-20")
        print(f"   å­¦ä¹ ç‡: Actor={lr}, Critic={lr*0.5}")
        print(f"   Bufferå¤§å°: {buffer_size}")
        print(f"   ç¯å¢ƒç±»å‹: {env_type}")
    
    def get_action(self, obs, gnn_embeds, num_joints, deterministic=False):
        """è·å–åŠ¨ä½œ - æ”¯æŒä»»æ„å…³èŠ‚æ•°"""
        joint_q, vertex_k, vertex_v = self._prepare_inputs(obs, gnn_embeds, num_joints)
        
        with torch.no_grad():
            action, log_prob, _, _ = self.actor.get_action(
                joint_q, vertex_k, vertex_v, deterministic=deterministic
            )
            value = self.critic(joint_q, vertex_k, vertex_v)
        
        # åŠ¨ä½œç¼©æ”¾
        if self.env_type == 'reacher2d':
            action_scale = 10.0
            scaled_action = torch.tanh(action) * action_scale
            return scaled_action.squeeze(0), log_prob.squeeze(0) if log_prob is not None else None, value.squeeze(0)
        else:
            return action.squeeze(0), log_prob.squeeze(0) if log_prob is not None else None, value.squeeze(0)
    
    def store_experience(self, obs, gnn_embeds, action, reward, done, 
                        log_prob=None, value=None, num_joints=None):
        """å­˜å‚¨ç»éªŒ"""
        joint_q, vertex_k, vertex_v = self._prepare_inputs(obs, gnn_embeds, num_joints)
        
        if value is None:
            with torch.no_grad():
                value = self.critic(joint_q, vertex_k, vertex_v)
        
        self.buffer.store(
            joint_q.squeeze(0), vertex_k.squeeze(0), vertex_v.squeeze(0),
            action, reward, value, log_prob, done, num_joints
        )
    
    def update(self, next_obs=None, next_gnn_embeds=None, num_joints=None, ppo_epochs=4):
        """PPOæ›´æ–° - å¤„ç†æ··åˆå…³èŠ‚æ•°æ•°æ®"""
        if len(self.buffer.experiences) < self.batch_size:
            return None
        
        # ğŸ”§ æ·»åŠ ï¼šè®­ç»ƒå‰æ£€æŸ¥å’Œé‡ç½®
        with torch.no_grad():
            current_log_std = self.actor.log_std_base.item()
            current_std = torch.exp(torch.clamp(torch.tensor(current_log_std), -2.3, -0.5)).item()
            
            # å¦‚æœæ ‡å‡†å·®å¼‚å¸¸ï¼Œå¼ºåˆ¶é‡ç½®
            if current_std > 0.8 or current_log_std > -0.3:
                print(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸æ ‡å‡†å·® {current_std:.4f}ï¼Œé‡ç½®å‚æ•°")
                self.actor.log_std_base.data.fill_(-1.8)  # é‡ç½®åˆ°å®‰å…¨å€¼
        
        # è®¡ç®—æœ€åçŠ¶æ€çš„ä»·å€¼
        if next_obs is not None and next_gnn_embeds is not None:
            joint_q, vertex_k, vertex_v = self._prepare_inputs(next_obs, next_gnn_embeds, num_joints)
            with torch.no_grad():
                last_value = self.critic(joint_q, vertex_k, vertex_v)
        else:
            last_value = torch.zeros(1)
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        self.buffer.compute_advantages(last_value, self.gamma, self.gae_lambda)
        
        # PPOæ›´æ–°
        total_actor_loss = 0
        total_critic_loss = 0
        total_entropy = 0
        total_batches = 0
        
        for epoch in range(ppo_epochs):
            joint_groups = self.buffer.get_batch(min(self.batch_size, len(self.buffer.experiences)))
            
            # å¯¹æ¯ä¸ªå…³èŠ‚æ•°ç»„åˆ†åˆ«å¤„ç†
            for num_joints, experiences in joint_groups.items():
                if len(experiences) == 0:
                    continue
                
                # æ„å»ºæ‰¹æ¬¡æ•°æ®
                batch = self._build_batch_from_experiences(experiences)
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                for key in batch:
                    if isinstance(batch[key], torch.Tensor):
                        batch[key] = batch[key].to(self.device)
                
                # å‰å‘ä¼ æ’­
                new_log_probs, entropy = self.actor.evaluate_action(
                    batch['joint_q'], batch['vertex_k'], batch['vertex_v'], batch['actions']
                )
                new_values = self.critic(batch['joint_q'], batch['vertex_k'], batch['vertex_v']).squeeze(-1)
                
                # PPOæŸå¤±è®¡ç®—
                ratio = torch.exp(new_log_probs.squeeze(-1) - batch['old_log_probs'])
                surr1 = ratio * batch['advantages']
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch['advantages']
                actor_loss = -torch.min(surr1, surr2).mean()
                
                critic_loss = F.smooth_l1_loss(new_values, batch['returns'])
                entropy_loss = -entropy.mean()
                
                # åˆ†åˆ«ä¼˜åŒ–
                self.actor_optimizer.zero_grad()
                actor_total_loss = actor_loss + self.entropy_coef * entropy_loss
                actor_total_loss.backward(retain_graph=True)
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
                self.actor_optimizer.step()
                
                self.critic_optimizer.zero_grad()
                critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
                self.critic_optimizer.step()
                
                total_actor_loss += actor_loss.item()
                total_critic_loss += critic_loss.item()
                total_entropy += entropy.mean().item()
                total_batches += 1
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.actor_scheduler.step()
        self.critic_scheduler.step()
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.buffer.clear()
        self.update_count += 1
        
        if total_batches == 0:
            return None
        
        metrics = {
            'actor_loss': total_actor_loss / total_batches,
            'critic_loss': total_critic_loss / total_batches,
            'total_loss': (total_actor_loss + total_critic_loss) / total_batches,
            'entropy': total_entropy / total_batches,
            'update_count': self.update_count,
            'learning_rate': self.actor_optimizer.param_groups[0]['lr'],
            'batches_processed': total_batches
        }
        
        # ğŸ”§ æ·»åŠ ï¼šæ›´æ–°åæ£€æŸ¥å’Œç´§æ€¥å¤„ç†
        if metrics['entropy'] > 3.0:  # ç†µå€¼è¿‡é«˜
            print(f"ğŸš¨ ç†µå€¼å¼‚å¸¸é«˜ {metrics['entropy']:.2f}ï¼Œé™ä½å­¦ä¹ ç‡")
            for param_group in self.actor_optimizer.param_groups:
                param_group['lr'] *= 0.5
            # å¼ºåˆ¶é‡ç½®æ ‡å‡†å·®
            with torch.no_grad():
                self.actor.log_std_base.data.fill_(-2.0)
        
        if metrics['critic_loss'] > 5.0:  # Critic lossè¿‡é«˜
            print(f"ğŸš¨ Critic losså¼‚å¸¸é«˜ {metrics['critic_loss']:.2f}ï¼Œé™ä½å­¦ä¹ ç‡")
            for param_group in self.critic_optimizer.param_groups:
                param_group['lr'] *= 0.3
        
        return metrics
    
    def _build_batch_from_experiences(self, experiences):
        """ä»ç»éªŒåˆ—è¡¨æ„å»ºæ‰¹æ¬¡æ•°æ®"""
        batch = {
            'joint_q': torch.stack([exp['joint_q'] for exp in experiences]),
            'vertex_k': torch.stack([exp['vertex_k'] for exp in experiences]),
            'vertex_v': torch.stack([exp['vertex_v'] for exp in experiences]),
            'actions': torch.stack([exp['action'] for exp in experiences]),
            'old_log_probs': torch.stack([exp['log_prob'] for exp in experiences]),
            'advantages': torch.stack([exp['advantage'] for exp in experiences]),
            'returns': torch.stack([exp['return'] for exp in experiences]),
            'old_values': torch.stack([exp['value'] for exp in experiences])
        }
        return batch
    
    def _prepare_inputs(self, obs, gnn_embeds, num_joints):
        """å‡†å¤‡è¾“å…¥æ•°æ® - ä¿®å¤ç»´åº¦åŒ¹é…é—®é¢˜"""
        if obs.dim() == 1:
            obs_batch = obs.unsqueeze(0)
        else:
            obs_batch = obs
            
        if gnn_embeds.dim() == 2:
            gnn_embeds_batch = gnn_embeds.unsqueeze(0)
        else:
            gnn_embeds_batch = gnn_embeds
        
        # ğŸ”§ ä¿®å¤ï¼šç®€åŒ–æ•°æ®å‡†å¤‡ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…
        batch_size = obs_batch.size(0)
        
        # å‡†å¤‡joint_qï¼šç›´æ¥ä½¿ç”¨å…³èŠ‚çŠ¶æ€ä¿¡æ¯
        if self.env_type == 'reacher2d':
            # Reacher2D: [angles, angular_vels, end_effector_pos]
            joint_angles = obs_batch[:, :num_joints]                    # [B, num_joints]
            joint_angular_vels = obs_batch[:, num_joints:2*num_joints]  # [B, num_joints]
        else:
            # å…¶ä»–ç¯å¢ƒçš„å¤„ç†
            joint_pos_start = 16
            joint_angles = obs_batch[:, joint_pos_start:joint_pos_start + num_joints]
            joint_angular_vels = obs_batch[:, joint_pos_start + num_joints:joint_pos_start + 2*num_joints]
        
        # è·å–GNNåµŒå…¥çš„å‰num_jointsä¸ªèŠ‚ç‚¹
        gnn_embed_joints = gnn_embeds_batch[:, :num_joints, :]  # [B, num_joints, 128]
        
        # æ„å»ºjoint_q: [position, velocity, gnn_embed] = [1 + 1 + 128] = 130
        joint_q = torch.cat([
            joint_angles.unsqueeze(-1),       # [B, num_joints, 1]
            joint_angular_vels.unsqueeze(-1), # [B, num_joints, 1]
            gnn_embed_joints                  # [B, num_joints, 128]
        ], dim=-1)  # [B, num_joints, 130]
        
        # vertex_kå’Œvertex_vä½¿ç”¨ç›¸åŒçš„æ•°æ®ï¼Œä½†ç»´åº¦å¯¹é½
        vertex_k = gnn_embed_joints  # [B, num_joints, 128]
        
        # vertex_våŒ…å«æ›´å¤šåŠ¨æ€ä¿¡æ¯
        vertex_v = torch.cat([
            gnn_embed_joints,                 # [B, num_joints, 128] 
            joint_angles.unsqueeze(-1),       # [B, num_joints, 1]
            joint_angular_vels.unsqueeze(-1)  # [B, num_joints, 1]
        ], dim=-1)  # [B, num_joints, 130]
        
        # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        joint_q = joint_q.to(self.device)
        vertex_k = vertex_k.to(self.device)
        vertex_v = vertex_v.to(self.device)
        
        return joint_q, vertex_k, vertex_v
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'update_count': self.update_count
        }, filepath)
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.update_count = checkpoint.get('update_count', 0)
