#!/usr/bin/env python3
"""
PPOç‰ˆæœ¬çš„Attentionæ¨¡å‹ - ä¿®å¤ç»´åº¦é”™è¯¯ç‰ˆ
åŸºäºAttnModelçš„PPOå®ç°ï¼Œæ”¯æŒ2,3,4,5,6...ä»»æ„å…³èŠ‚æ•°
ä¿®å¤Critic Lossè¿‡é«˜å’Œç»´åº¦ä¸åŒ¹é…é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import sys
from collections import deque
from torch.distributions import Normal
import copy

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(__file__), "../../../")
sys.path.append(base_dir)
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_dataset"))
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_model"))

from data_utils import prepare_joint_q_input, prepare_reacher2d_joint_q_input, prepare_dynamic_vertex_v
from attn_model.attn_model import AttnModel

class AttentionPPOActor(nn.Module):
    """åŸºäºAttentionçš„PPO Actorç½‘ç»œ"""
    def __init__(self, attn_model, action_dim, log_std_init=-1.0, device='cpu'):
        super(AttentionPPOActor, self).__init__()
        self.attn_model = attn_model
        self.action_dim = action_dim
        
        # ğŸ¯ PPOä½¿ç”¨å›ºå®šæˆ–å­¦ä¹ çš„log_std
        self.log_std = nn.Parameter(torch.ones(action_dim) * log_std_init)
        
    def forward(self, joint_q, vertex_k, vertex_v, vertex_mask=None):
        """å‰å‘ä¼ æ’­ï¼Œè¾“å‡ºåŠ¨ä½œåˆ†å¸ƒå‚æ•°"""
        # AttnModelè¾“å‡ºåŠ¨ä½œå‡å€¼
        mean = self.attn_model(joint_q, vertex_k, vertex_v, vertex_mask)  # [B, action_dim]
        
        # æ ‡å‡†å·®ï¼ˆå¯å­¦ä¹ æˆ–å›ºå®šï¼‰
        std = torch.exp(self.log_std.expand_as(mean))
        
        return mean, std
    
    # def get_action(self, joint_q, vertex_k, vertex_v, vertex_mask=None, deterministic=False):
    #     """è·å–åŠ¨ä½œå’Œlogæ¦‚ç‡"""
    #     mean, std = self.forward(joint_q, vertex_k, vertex_v, vertex_mask)
        
    #     if deterministic:
    #         action = mean
    #         log_prob = None
    #     else:
    #         dist = Normal(mean, std)
    #         action = dist.sample()
    #         log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        
    #     return action, log_prob, mean, std
    def get_action(self, joint_q, vertex_k, vertex_v, vertex_mask=None, deterministic=False):
        """è·å–åŠ¨ä½œå’Œlogæ¦‚ç‡"""
        mean, std = self.forward(joint_q, vertex_k, vertex_v, vertex_mask)
        
        if deterministic:
            action = mean
            log_prob = None
        else:
            dist = Normal(mean, std)
            action = dist.sample()
            log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        
        return action, log_prob, mean, std
    def evaluate_action(self, joint_q, vertex_k, vertex_v, action, vertex_mask=None):
        """è¯„ä¼°ç»™å®šåŠ¨ä½œçš„logæ¦‚ç‡å’Œç†µ"""
        mean, std = self.forward(joint_q, vertex_k, vertex_v, vertex_mask)
        
        dist = Normal(mean, std)
        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        entropy = dist.entropy().sum(dim=-1, keepdim=True)
        
        return log_prob, entropy

class AttentionPPOCritic(nn.Module):
    """ä¿®å¤ç‰ˆPPO Criticç½‘ç»œ - è§£å†³ç»´åº¦é”™è¯¯"""
    def __init__(self, attn_model, hidden_dim=256, device='cpu'):
        super(AttentionPPOCritic, self).__init__()
        self.attn_model = copy.deepcopy(attn_model)  # ç‹¬ç«‹çš„AttnModelå‰¯æœ¬
        self.device = device
        
        # ğŸ”§ ä¿®å¤: ä½¿ç”¨ç®€å•ç¨³å®šçš„ç½‘ç»œç»“æ„
        self.value_head = nn.Sequential(
            nn.Linear(130, hidden_dim),  # ç›´æ¥ä½¿ç”¨å¹³å‡æ± åŒ–ç‰¹å¾
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.LayerNorm(hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim//2, 1)
        )
        
        # ğŸ”§ ä¿®å¤: å€¼å‡½æ•°è¾“å‡ºå½’ä¸€åŒ–
        self.value_scale = 200.0  # å¢å¤§è¾“å‡ºèŒƒå›´ï¼ŒåŒ¹é…ç¯å¢ƒå¥–åŠ±
        
    def forward(self, joint_q, vertex_k, vertex_v, vertex_mask=None):
        """å‰å‘ä¼ æ’­ï¼Œè¾“å‡ºçŠ¶æ€ä»·å€¼ - ç®€åŒ–ç¨³å®šç‰ˆ"""
        batch_size, num_joints, feature_dim = joint_q.shape
        
        # ğŸ”§ ä½¿ç”¨ç®€å•çš„å¹³å‡æ± åŒ–ï¼Œé¿å…å¤æ‚çš„tensoræ“ä½œ
        avg_features = joint_q.mean(dim=1)  # [B, 130]
        
        # ç›´æ¥ä½¿ç”¨å¹³å‡ç‰¹å¾
        value = self.value_head(avg_features)  # [B, 1]
        
        # ğŸ”§ è¾“å‡ºç¼©æ”¾
        value = torch.tanh(value) * self.value_scale
        
        return value

class RolloutBuffer:
    """PPOç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, buffer_size, device='cpu'):
        self.buffer_size = buffer_size
        self.device = device
        self.clear()
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.joint_q = []
        self.vertex_k = []
        self.vertex_v = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
        self.advantages = []
        self.returns = []
        self.ptr = 0
    
    def store(self, joint_q, vertex_k, vertex_v, action, reward, value, log_prob, done):
        """å­˜å‚¨ä¸€æ­¥ç»éªŒ"""
        self.joint_q.append(joint_q.cpu())
        self.vertex_k.append(vertex_k.cpu())
        self.vertex_v.append(vertex_v.cpu())
        self.actions.append(action.cpu())
        self.rewards.append(reward)
        self.values.append(value.cpu().squeeze())  # ç¡®ä¿æ˜¯æ ‡é‡
        self.log_probs.append(log_prob.cpu() if log_prob is not None else torch.tensor(0.0))
        self.dones.append(done)
        self.ptr += 1
    
    def compute_advantages(self, last_value, gamma=0.99, gae_lambda=0.95):
        """è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥"""
        # ğŸ”§ ä¿®å¤: æ”¹è¿›ä¼˜åŠ¿å‡½æ•°è®¡ç®—
        values = torch.stack(self.values + [last_value.cpu().squeeze()])
        rewards = torch.tensor(self.rewards, dtype=torch.float32)
        dones = torch.tensor(self.dones, dtype=torch.float32)
        
        # ğŸ”§ å¥–åŠ±å½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒ
        if len(rewards) > 1:
            reward_mean = rewards.mean()
            reward_std = rewards.std() + 1e-8
            rewards = (rewards - reward_mean) / reward_std
        
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)
        
        gae = 0
        for step in reversed(range(len(rewards))):
            delta = rewards[step] + gamma * values[step + 1] * (1 - dones[step]) - values[step]
            gae = delta + gamma * gae_lambda * (1 - dones[step]) * gae
            advantages[step] = gae
            returns[step] = advantages[step] + values[step]
        
        # ğŸ”§ ä¼˜åŠ¿å‡½æ•°å½’ä¸€åŒ–
        if len(advantages) > 1:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        self.advantages = advantages
        self.returns = returns
    
    def get_batch(self, batch_size):
        """è·å–æ‰¹é‡æ•°æ®"""
        indices = torch.randperm(len(self.joint_q))[:batch_size]
        
        batch = {
            'joint_q': torch.stack([self.joint_q[i] for i in indices]),
            'vertex_k': torch.stack([self.vertex_k[i] for i in indices]),
            'vertex_v': torch.stack([self.vertex_v[i] for i in indices]),
            'actions': torch.stack([self.actions[i] for i in indices]),
            'old_log_probs': torch.stack([self.log_probs[i] for i in indices]),
            'advantages': torch.stack([self.advantages[i] for i in indices]),
            'returns': torch.stack([self.returns[i] for i in indices]),
            'old_values': torch.stack([self.values[i] for i in indices])
        }
        
        return batch

class AttentionPPOWithBuffer:
    """ä¿®å¤ç‰ˆPPOç®—æ³•å®ç°"""
    def __init__(self, attn_model, action_dim, buffer_size=2048, batch_size=64, 
                 lr=1e-4, gamma=0.99, gae_lambda=0.95, clip_epsilon=0.2, 
                 entropy_coef=0.01, value_coef=0.5, max_grad_norm=0.5, 
                 device='cpu', env_type='reacher2d'):
        
        self.action_dim = action_dim
        self.device = device
        self.env_type = env_type
        self.batch_size = batch_size
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        self.max_grad_norm = max_grad_norm
        
        # åˆ›å»ºç½‘ç»œ
        self.actor = AttentionPPOActor(attn_model, action_dim, device=device)
        self.critic = AttentionPPOCritic(attn_model, device=device)
        
        # ğŸ”§ ä¿®å¤: ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr*0.5)  # Criticå­¦ä¹ ç‡é™ä½
        
        # ğŸ”§ ä¿®å¤: æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.actor_scheduler = torch.optim.lr_scheduler.StepLR(self.actor_optimizer, step_size=1000, gamma=0.95)
        self.critic_scheduler = torch.optim.lr_scheduler.StepLR(self.critic_optimizer, step_size=1000, gamma=0.95)
        
        # ç»éªŒç¼“å†²åŒº
        self.buffer = RolloutBuffer(buffer_size, device)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.update_count = 0
        self.recent_losses = deque(maxlen=10)
        
        print(f"ğŸ¯ PPOåˆå§‹åŒ–å®Œæˆ:")
        print(f"   Actionç»´åº¦: {action_dim}")
        print(f"   å­¦ä¹ ç‡: Actor={lr}, Critic={lr*0.5}")
        print(f"   Bufferå¤§å°: {buffer_size}")
        print(f"   Clip epsilon: {clip_epsilon}")
        print(f"   Value Scale: {self.critic.value_scale}")
    
    def get_action(self, obs, gnn_embeds, num_joints, deterministic=False):
        """è·å–åŠ¨ä½œ"""
        joint_q, vertex_k, vertex_v = self._prepare_inputs(obs, gnn_embeds, num_joints)
        
        with torch.no_grad():
            action, log_prob, _, _ = self.actor.get_action(
                joint_q, vertex_k, vertex_v, deterministic=deterministic
            )
            value = self.critic(joint_q, vertex_k, vertex_v)
        
        # ğŸ”§ æ·»åŠ åŠ¨ä½œç¼©æ”¾
        if self.env_type == 'reacher2d':
            action_scale = 10.0  # åŒ¹é…ç¯å¢ƒçš„max_torque
            scaled_action = torch.tanh(action) * action_scale  # å…ˆtanhé™åˆ¶åˆ°[-1,1]å†ç¼©æ”¾
            return scaled_action.squeeze(0), log_prob.squeeze(0) if log_prob is not None else None, value.squeeze(0)
        else:
            return action.squeeze(0), log_prob.squeeze(0) if log_prob is not None else None, value.squeeze(0)
    
    def store_experience(self, obs, gnn_embeds, action, reward, done, 
                        log_prob=None, value=None, num_joints=None):
        """å­˜å‚¨ç»éªŒ"""
        joint_q, vertex_k, vertex_v = self._prepare_inputs(obs, gnn_embeds, num_joints)
        
        if value is None:
            with torch.no_grad():
                value = self.critic(joint_q, vertex_k, vertex_v)
        
        self.buffer.store(
            joint_q.squeeze(0), vertex_k.squeeze(0), vertex_v.squeeze(0),
            action, reward, value, log_prob, done
        )
    
    def update(self, next_obs=None, next_gnn_embeds=None, num_joints=None, ppo_epochs=4):
        """PPOæ›´æ–°"""
        if len(self.buffer.joint_q) < self.batch_size:
            return None
        
        # è®¡ç®—æœ€åçŠ¶æ€çš„ä»·å€¼
        if next_obs is not None and next_gnn_embeds is not None:
            joint_q, vertex_k, vertex_v = self._prepare_inputs(next_obs, next_gnn_embeds, num_joints)
            with torch.no_grad():
                last_value = self.critic(joint_q, vertex_k, vertex_v)
        else:
            last_value = torch.zeros(1)
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        self.buffer.compute_advantages(last_value, self.gamma, self.gae_lambda)
        
        # PPOæ›´æ–°
        total_actor_loss = 0
        total_critic_loss = 0
        total_entropy = 0
        
        for epoch in range(ppo_epochs):
            batch = self.buffer.get_batch(min(self.batch_size, len(self.buffer.joint_q)))
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            for key in batch:
                if isinstance(batch[key], torch.Tensor):
                    batch[key] = batch[key].to(self.device)
            
            # å‰å‘ä¼ æ’­
            new_log_probs, entropy = self.actor.evaluate_action(
                batch['joint_q'], batch['vertex_k'], batch['vertex_v'], batch['actions']
            )
            new_values = self.critic(batch['joint_q'], batch['vertex_k'], batch['vertex_v']).squeeze(-1)
            
            # Actor loss (PPO clip)
            ratio = torch.exp(new_log_probs.squeeze(-1) - batch['old_log_probs'])
            surr1 = ratio * batch['advantages']
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch['advantages']
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # ğŸ”§ ä¿®å¤: æ”¹è¿›Critic Lossè®¡ç®—
            # ä½¿ç”¨Huber Lossæ›¿ä»£MSEï¼Œæ›´ç¨³å®š
            critic_loss = F.smooth_l1_loss(new_values, batch['returns'])
            
            # Entropy bonus
            entropy_loss = -entropy.mean()
            
            # æ€»æŸå¤±
            total_loss = actor_loss + self.value_coef * critic_loss + self.entropy_coef * entropy_loss
            
            # ğŸ”§ ä¿®å¤: åˆ†åˆ«ä¼˜åŒ–Actorå’ŒCritic
            # Actoræ›´æ–°
            self.actor_optimizer.zero_grad()
            actor_total_loss = actor_loss + self.entropy_coef * entropy_loss
            actor_total_loss.backward(retain_graph=True)
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
            self.actor_optimizer.step()
            
            # Criticæ›´æ–°
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
            self.critic_optimizer.step()
            
            total_actor_loss += actor_loss.item()
            total_critic_loss += critic_loss.item()
            total_entropy += entropy.mean().item()
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.actor_scheduler.step()
        self.critic_scheduler.step()
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.buffer.clear()
        self.update_count += 1
        
        # è®¡ç®—æŸå¤±è¶‹åŠ¿
        current_total_loss = total_actor_loss + total_critic_loss
        self.recent_losses.append(current_total_loss)
        
        loss_trend = "ğŸ“ˆ ä¸Šå‡"
        if len(self.recent_losses) >= 2:
            if self.recent_losses[-1] < self.recent_losses[-2]:
                loss_trend = "ğŸ“‰ ä¸‹é™"
        
        metrics = {
            'actor_loss': total_actor_loss / ppo_epochs,
            'critic_loss': total_critic_loss / ppo_epochs,
            'total_loss': current_total_loss / ppo_epochs,
            'entropy': total_entropy / ppo_epochs,
            'update_count': self.update_count,
            'loss_trend': loss_trend,
            'learning_rate': self.actor_optimizer.param_groups[0]['lr']
        }
        
        return metrics
    
    def _prepare_inputs(self, obs, gnn_embeds, num_joints):
        """å‡†å¤‡è¾“å…¥æ•°æ® - ä¿®å¤ç‰ˆ"""
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æœ‰batchç»´åº¦
        if obs.dim() == 1:
            obs_batch = obs.unsqueeze(0)  # [1, obs_dim]
        else:
            obs_batch = obs
            
        if gnn_embeds.dim() == 2:
            gnn_embeds_batch = gnn_embeds.unsqueeze(0)  # [1, N, 128]
        else:
            gnn_embeds_batch = gnn_embeds
        
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è°ƒç”¨æ•°æ®å‡†å¤‡å‡½æ•°ï¼Œä¼ å…¥æ‰€æœ‰å¿…éœ€å‚æ•°
        if self.env_type == 'reacher2d':
            joint_q = prepare_reacher2d_joint_q_input(obs_batch, gnn_embeds_batch, num_joints)
            vertex_v = prepare_dynamic_vertex_v(obs_batch, gnn_embeds_batch, num_joints, self.env_type)
        else:
            joint_q = prepare_joint_q_input(obs_batch, gnn_embeds_batch, num_joints)
            vertex_v = prepare_dynamic_vertex_v(obs_batch, gnn_embeds_batch, num_joints, self.env_type)
        
        vertex_k = gnn_embeds_batch  # [B, N, 128]
        
        # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        joint_q = joint_q.to(self.device)
        vertex_k = vertex_k.to(self.device)
        vertex_v = vertex_v.to(self.device)
        
        return joint_q, vertex_k, vertex_v
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'update_count': self.update_count,
            'action_dim': self.action_dim
        }, filepath)
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.update_count = checkpoint.get('update_count', 0)