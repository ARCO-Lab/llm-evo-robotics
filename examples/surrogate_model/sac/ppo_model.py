#!/usr/bin/env python3
"""
PPOç‰ˆæœ¬çš„Attentionæ¨¡å‹ - ä¿®å¤ç‰ˆ
åŸºäºAttnModelçš„PPOå®ç°ï¼Œé€‚åˆç»´æŒä»»åŠ¡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import sys
from collections import deque
from torch.distributions import Normal
import copy

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(__file__), "../../../")
sys.path.append(base_dir)
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_dataset"))
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_model"))

from data_utils import prepare_joint_q_input, prepare_reacher2d_joint_q_input, prepare_dynamic_vertex_v
from attn_model.attn_model import AttnModel

class AttentionPPOActor(nn.Module):
    """åŸºäºAttentionçš„PPO Actorç½‘ç»œ"""
    def __init__(self, attn_model, action_dim, log_std_init=-1.0):
        super(AttentionPPOActor, self).__init__()
        self.attn_model = attn_model
        self.action_dim = action_dim
        
        # ğŸ¯ PPOä½¿ç”¨å›ºå®šæˆ–å­¦ä¹ çš„log_std
        self.log_std = nn.Parameter(torch.ones(action_dim) * log_std_init)
        
    def forward(self, joint_q, vertex_k, vertex_v, vertex_mask=None):
        """å‰å‘ä¼ æ’­ï¼Œè¾“å‡ºåŠ¨ä½œåˆ†å¸ƒå‚æ•°"""
        # AttnModelè¾“å‡ºåŠ¨ä½œå‡å€¼
        mean = self.attn_model(joint_q, vertex_k, vertex_v, vertex_mask)  # [B, action_dim]
        
        # æ ‡å‡†å·®ï¼ˆå¯å­¦ä¹ æˆ–å›ºå®šï¼‰
        std = torch.exp(self.log_std.expand_as(mean))
        
        return mean, std
    
    def get_action(self, joint_q, vertex_k, vertex_v, vertex_mask=None, deterministic=False):
        """è·å–åŠ¨ä½œå’Œlogæ¦‚ç‡"""
        mean, std = self.forward(joint_q, vertex_k, vertex_v, vertex_mask)
        
        if deterministic:
            action = mean
            log_prob = None
        else:
            dist = Normal(mean, std)
            action = dist.sample()
            log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        
        return action, log_prob, mean, std
    
    def evaluate_action(self, joint_q, vertex_k, vertex_v, action, vertex_mask=None):
        """è¯„ä¼°ç»™å®šåŠ¨ä½œçš„logæ¦‚ç‡å’Œç†µ"""
        mean, std = self.forward(joint_q, vertex_k, vertex_v, vertex_mask)
        
        dist = Normal(mean, std)
        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        entropy = dist.entropy().sum(dim=-1, keepdim=True)
        
        return log_prob, entropy

class AttentionPPOCritic(nn.Module):
    """åŸºäºAttentionçš„PPO Criticç½‘ç»œ - ä¿®å¤ç‰ˆ"""
    def __init__(self, attn_model, action_dim, hidden_dim=256, device='cpu'):
        super(AttentionPPOCritic, self).__init__()
        self.attn_model = attn_model
        self.action_dim = action_dim  # ğŸ”§ æ”¹ä¸ºä½¿ç”¨action_dimè€Œä¸æ˜¯joint_embed_dim
        self.device = device
        
        # ğŸ”§ å€¼å‡½æ•°ç½‘ç»œ - ç›´æ¥ä»AttnModelè¾“å‡ºç»´åº¦ï¼ˆaction_dimï¼‰åˆ°å€¼å‡½æ•°
        self.value_net = nn.Sequential(
            nn.Linear(action_dim, hidden_dim),  # ğŸ”§ AttnModelè¾“å‡º[B, action_dim]
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, joint_q, vertex_k, vertex_v, vertex_mask=None):
        """å‰å‘ä¼ æ’­ï¼Œè¾“å‡ºçŠ¶æ€å€¼"""
        # ä½¿ç”¨AttnModelç¼–ç çŠ¶æ€ç‰¹å¾
        state_features = self.attn_model(joint_q, vertex_k, vertex_v, vertex_mask)  # [B, action_dim]
        
        # ğŸ”§ ç›´æ¥ä½¿ç”¨state_featuresï¼Œä¸éœ€è¦æŠ•å½±å’Œæ± åŒ–
        value = self.value_net(state_features)  # [B, 1]
        
        return value

class RolloutBuffer:
    """PPOçš„ç»éªŒæ”¶é›†ç¼“å†²åŒº"""
    def __init__(self, buffer_size, device='cpu'):
        self.buffer_size = buffer_size
        self.device = device
        self.clear()
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.joint_q = []
        self.vertex_k = []
        self.vertex_v = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
        self.advantages = []
        self.returns = []
        self.ptr = 0
    
    def store(self, joint_q, vertex_k, vertex_v, action, reward, value, log_prob, done):
        """å­˜å‚¨ä¸€æ­¥ç»éªŒ"""
        self.joint_q.append(joint_q.cpu())
        self.vertex_k.append(vertex_k.cpu())
        self.vertex_v.append(vertex_v.cpu())
        self.actions.append(action.cpu())
        self.rewards.append(reward)
        # ğŸ”§ ç¡®ä¿valueç»´åº¦ä¸€è‡´ï¼Œéƒ½æ˜¯æ ‡é‡
        self.values.append(value.cpu().squeeze())  # ğŸ”§ æ·»åŠ squeeze()ç¡®ä¿æ˜¯æ ‡é‡
        self.log_probs.append(log_prob.cpu() if log_prob is not None else torch.tensor(0.0))
        self.dones.append(done)
        self.ptr += 1
    
    def compute_advantages(self, last_value, gamma=0.99, gae_lambda=0.95):
        """è®¡ç®—GAEä¼˜åŠ¿å‡½æ•°"""
        rewards = torch.tensor(self.rewards, dtype=torch.float32)
        
        # ğŸ”§ ç¡®ä¿last_valueä¹Ÿæ˜¯æ ‡é‡
        last_value_scalar = last_value.cpu().squeeze()
        
        # ğŸ”§ å°†æ‰€æœ‰valuesè½¬æ¢ä¸ºæ ‡é‡tensoråˆ—è¡¨ï¼Œç„¶åæ‹¼æ¥
        value_tensors = []
        for v in self.values:
            if v.dim() == 0:  # å·²ç»æ˜¯æ ‡é‡
                value_tensors.append(v)
            else:  # éœ€è¦squeeze
                value_tensors.append(v.squeeze())
        
        # æ·»åŠ last_value
        value_tensors.append(last_value_scalar)
        
        # æ‹¼æ¥æˆä¸€ç»´tensor
        values = torch.stack(value_tensors)
        
        dones = torch.tensor(self.dones, dtype=torch.float32)
        
        advantages = torch.zeros_like(rewards)
        gae = 0
        
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]
            gae = delta + gamma * gae_lambda * (1 - dones[t]) * gae
            advantages[t] = gae
        
        returns = advantages + values[:-1]
        
        self.advantages = advantages
        self.returns = returns
    
    def get_batch(self, batch_size=None):
        """è·å–æ‰¹é‡æ•°æ®"""
        if batch_size is None:
            batch_size = len(self.joint_q)
        
        indices = torch.randperm(len(self.joint_q))[:batch_size]
        
        batch = {
            'joint_q': torch.stack([self.joint_q[i] for i in indices]).to(self.device),
            'vertex_k': torch.stack([self.vertex_k[i] for i in indices]).to(self.device),
            'vertex_v': torch.stack([self.vertex_v[i] for i in indices]).to(self.device),
            'actions': torch.stack([self.actions[i] for i in indices]).to(self.device),
            'old_log_probs': torch.stack([self.log_probs[i] for i in indices]).to(self.device),
            'advantages': self.advantages[indices].to(self.device),
            'returns': self.returns[indices].to(self.device),
        }
        
        return batch

class AttentionPPOWithBuffer:
    """åŸºäºAttentionçš„PPOç®—æ³•å®ç° - ä¿®å¤ç‰ˆ"""
    def __init__(self, attn_model, action_dim, buffer_size=2048, batch_size=64, 
                 lr=3e-4, gamma=0.99, gae_lambda=0.95, clip_epsilon=0.2, 
                 entropy_coef=0.01, value_coef=0.5, max_grad_norm=0.5, 
                 device='cpu', env_type='reacher2d'):
        
        self.device = device
        self.action_dim = action_dim
        self.env_type = env_type
        
        # è¶…å‚æ•°
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        self.max_grad_norm = max_grad_norm
        self.batch_size = batch_size
        
        # åˆ›å»ºç‹¬ç«‹çš„ç½‘ç»œ
        actor_model = copy.deepcopy(attn_model)
        critic_model = copy.deepcopy(attn_model)
        
        self.actor = AttentionPPOActor(actor_model, action_dim).to(device)
        self.critic = AttentionPPOCritic(critic_model, action_dim, device=device).to(device)  # ğŸ”§ ä¼ é€’action_dim
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)
        
        # ç»éªŒç¼“å†²åŒº
        self.buffer = RolloutBuffer(buffer_size, device)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.update_count = 0
    
    def get_action(self, obs, gnn_embeds, num_joints=12, deterministic=False):
        """è·å–åŠ¨ä½œ"""
        # å‡†å¤‡è¾“å…¥æ•°æ®
        if self.env_type == 'reacher2d':
            joint_q = prepare_reacher2d_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints)
        else:
            joint_q = prepare_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints)
        
        vertex_k = gnn_embeds.unsqueeze(0)
        vertex_v = prepare_dynamic_vertex_v(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints, self.env_type)
        
        with torch.no_grad():
            action, log_prob, mean, std = self.actor.get_action(
                joint_q, vertex_k, vertex_v, deterministic=deterministic
            )
            value = self.critic(joint_q, vertex_k, vertex_v)
        
        # ğŸ”§ åŠ¨ä½œç¼©æ”¾åˆ°ç¯å¢ƒèŒƒå›´
        if self.env_type == 'reacher2d':
            action_scale = 100.0  # ç¼©æ”¾åˆ°[-100, 100]
            scaled_action = torch.tanh(action.squeeze(0)) * action_scale
            return scaled_action, log_prob.squeeze(0) if log_prob is not None else None, value.squeeze(0)
        else:
            return torch.tanh(action.squeeze(0)), log_prob.squeeze(0) if log_prob is not None else None, value.squeeze(0)
    
    def store_experience(self, obs, gnn_embeds, action, reward, done, log_prob, value, num_joints=12):
        """å­˜å‚¨ç»éªŒ"""
        # å‡†å¤‡è¾“å…¥æ•°æ®
        if self.env_type == 'reacher2d':
            joint_q = prepare_reacher2d_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints)
        else:
            joint_q = prepare_joint_q_input(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints)
        
        vertex_k = gnn_embeds.unsqueeze(0)
        vertex_v = prepare_dynamic_vertex_v(obs.unsqueeze(0), gnn_embeds.unsqueeze(0), num_joints, self.env_type)
        
        self.buffer.store(joint_q.squeeze(0), vertex_k.squeeze(0), vertex_v.squeeze(0),
                         action, reward, value, log_prob, done)
    
    def update(self, next_obs=None, next_gnn_embeds=None, num_joints=12, ppo_epochs=4):
        """PPOæ›´æ–°"""
        # è®¡ç®—æœ€åä¸€ä¸ªçŠ¶æ€çš„å€¼å‡½æ•°ï¼ˆç”¨äºGAEï¼‰
        if next_obs is not None and next_gnn_embeds is not None:
            if self.env_type == 'reacher2d':
                next_joint_q = prepare_reacher2d_joint_q_input(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints)
            else:
                next_joint_q = prepare_joint_q_input(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints)
            
            next_vertex_k = next_gnn_embeds.unsqueeze(0)
            next_vertex_v = prepare_dynamic_vertex_v(next_obs.unsqueeze(0), next_gnn_embeds.unsqueeze(0), num_joints, self.env_type)
            
            with torch.no_grad():
                last_value = self.critic(next_joint_q, next_vertex_k, next_vertex_v)
        else:
            last_value = torch.zeros(1, 1)
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
        self.buffer.compute_advantages(last_value, self.gamma, self.gae_lambda)
        
        # PPOæ›´æ–°
        total_actor_loss = 0
        total_critic_loss = 0
        total_entropy = 0
        
        for epoch in range(ppo_epochs):
            batch = self.buffer.get_batch(self.batch_size)
            
            # è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡å’Œç†µ
            log_probs, entropy = self.actor.evaluate_action(
                batch['joint_q'], batch['vertex_k'], batch['vertex_v'], batch['actions']
            )
            
            # è®¡ç®—å½“å‰å€¼å‡½æ•°
            values = self.critic(batch['joint_q'], batch['vertex_k'], batch['vertex_v'])
            
            # PPO ActoræŸå¤±
            ratio = torch.exp(log_probs - batch['old_log_probs'])
            surr1 = ratio * batch['advantages']
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch['advantages']
            actor_loss = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy.mean()
            
            # CriticæŸå¤±
            critic_loss = F.mse_loss(values.squeeze(), batch['returns'])
            
            # æ›´æ–°Actor
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
            self.actor_optimizer.step()
            
            # æ›´æ–°Critic
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
            self.critic_optimizer.step()
            
            total_actor_loss += actor_loss.item()
            total_critic_loss += critic_loss.item()
            total_entropy += entropy.mean().item()
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.buffer.clear()
        self.update_count += 1
        
        return {
            'actor_loss': total_actor_loss / ppo_epochs,
            'critic_loss': total_critic_loss / ppo_epochs,
            'entropy': total_entropy / ppo_epochs,
            'update_count': self.update_count
        }
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'update_count': self.update_count
        }, filepath)
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        self.update_count = checkpoint.get('update_count', 0)

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸš€ æµ‹è¯•PPO + Attentionæ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹
    attn_model = AttnModel(128, 128, 130, 4)
    ppo = AttentionPPOWithBuffer(
        attn_model, action_dim=12, 
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•
    obs = torch.randn(40)
    gnn_embeds = torch.randn(12, 128)
    
    # æµ‹è¯•è·å–åŠ¨ä½œ
    action, log_prob, value = ppo.get_action(obs, gnn_embeds, deterministic=False)
    print(f"âœ… åŠ¨ä½œå½¢çŠ¶: {action.shape}")
    print(f"âœ… Logæ¦‚ç‡: {log_prob.item() if log_prob is not None else None}")
    print(f"âœ… çŠ¶æ€å€¼: {value.item()}")
    
    # æµ‹è¯•å­˜å‚¨ç»éªŒ
    ppo.store_experience(obs, gnn_embeds, action, 1.0, False, log_prob, value)
    print(f"âœ… ç»éªŒå­˜å‚¨æˆåŠŸ")
    
    print("ğŸ¯ PPOæ¨¡å‹åˆ›å»ºæˆåŠŸï¼")