#!/usr/bin/env python3
"""
å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨
æ”¯æŒå®žæ—¶æå–attentionã€GNNã€PPOç­‰æ‰€æœ‰ç½‘ç»œçš„æŸå¤±æ•°æ®
"""

import os
import sys
import subprocess
import threading
import time
import re
import json
import csv
import random
from datetime import datetime
from collections import defaultdict

class EnhancedMultiNetworkExtractor:
    """å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨"""
    
    def __init__(self, experiment_name, log_dir="enhanced_multi_network_logs"):
        self.experiment_name = experiment_name
        self.log_dir = log_dir
        self.experiment_dir = os.path.join(log_dir, f"{experiment_name}_multi_network_loss")
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.experiment_dir, exist_ok=True)
        
        # åªè®°å½•çœŸå®žå­˜åœ¨çš„ç½‘ç»œæŸå¤±æ•°æ®
        self.loss_data = {
            'ppo': [],           # PPOç½‘ç»œæœ‰çœŸå®žæŸå¤±è¾“å‡º
            'performance': []    # æ€§èƒ½æŒ‡æ ‡æœ‰çœŸå®žè¾“å‡ºï¼ˆæˆåŠŸçŽ‡ã€è·ç¦»ç­‰ï¼‰
            # æ³¨æ„ï¼šåªæœ‰åœ¨è®­ç»ƒè¾“å‡ºä¸­çœŸå®žå­˜åœ¨æ—¶æ‰ä¼šåŠ¨æ€æ·»åŠ å…¶ä»–ç½‘ç»œ
        }
        self.running = False
        
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ - åªåŒ¹é…çœŸå®žå­˜åœ¨çš„è¾“å‡º
        self.patterns = {
            # PPOç½‘ç»œæŸå¤±ï¼ˆçœŸå®žå­˜åœ¨ï¼‰
            'ppo_update': re.compile(r'ðŸ”¥ PPOç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'actor_loss': re.compile(r'ðŸ“Š Actor Loss: ([\d\.-]+)'),
            'critic_loss': re.compile(r'ðŸ“Š Critic Loss: ([\d\.-]+)'),
            'ppo_total_loss': re.compile(r'ðŸ“Š æ€»Loss: ([\d\.-]+)'),
            'entropy': re.compile(r'ðŸŽ­ Entropy: ([\d\.-]+)'),
            'learning_rate': re.compile(r'ðŸ“ˆ å­¦ä¹ çŽ‡: ([\d\.-]+e?[\d\.-]*)'),
            'update_count': re.compile(r'ðŸ”„ æ›´æ–°æ¬¡æ•°: (\d+)'),
            'buffer_size': re.compile(r'ðŸ’¾ Bufferå¤§å°: (\d+)'),
            
            # æ€§èƒ½æŒ‡æ ‡ï¼ˆçœŸå®žå­˜åœ¨ï¼‰
            'success_rate': re.compile(r'âœ… å½“å‰æˆåŠŸçŽ‡: ([\d\.]+)%'),
            'best_distance': re.compile(r'ðŸ† å½“å‰æœ€ä½³è·ç¦»: ([\d\.]+)px'),
            'episode_best_distance': re.compile(r'ðŸ“Š å½“å‰Episodeæœ€ä½³è·ç¦»: ([\d\.]+)px'),
            'consecutive_success': re.compile(r'ðŸ”„ è¿žç»­æˆåŠŸæ¬¡æ•°: (\d+)'),
            'completed_episodes': re.compile(r'ðŸ“‹ å·²å®ŒæˆEpisodes: (\d+)'),
            'training_progress_report': re.compile(r'ðŸ“Š PPOè®­ç»ƒè¿›åº¦æŠ¥å‘Š \[Step (\d+)\]'),
            
            # çœŸå®žçš„attentionç½‘ç»œæŸå¤±æ¨¡å¼ï¼ˆçŽ°åœ¨å·²å®žçŽ°ï¼‰
            'attention_update': re.compile(r'ðŸ”¥ Attentionç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'attention_actor_grad_norm': re.compile(r'ðŸ“Š Actor Attentionæ¢¯åº¦èŒƒæ•°: ([\d\.-]+)'),
            'attention_critic_grad_norm': re.compile(r'ðŸ“Š Critic Attentionæ¢¯åº¦èŒƒæ•°: ([\d\.-]+)'),
            'attention_total_loss': re.compile(r'ðŸ“Š Attentionæ€»æŸå¤±: ([\d\.-]+)'),
            'attention_param_mean': re.compile(r'ðŸ“Š Attentionå‚æ•°å‡å€¼: ([\d\.-]+)'),
            'attention_param_std': re.compile(r'ðŸ“Š Attentionå‚æ•°æ ‡å‡†å·®: ([\d\.-]+)'),
            
            # ðŸ†• å…³èŠ‚æ³¨æ„åŠ›åˆ†å¸ƒæ¨¡å¼
            'most_attended_joint': re.compile(r'ðŸŽ¯ æœ€å…³æ³¨å…³èŠ‚: Joint (\d+)'),
            'max_joint_attention': re.compile(r'æœ€å…³æ³¨å…³èŠ‚: Joint \d+ \(å¼ºåº¦: ([\d\.-]+)\)'),
            'attention_concentration': re.compile(r'ðŸ“Š æ³¨æ„åŠ›é›†ä¸­åº¦: ([\d\.-]+)'),
            'attention_entropy': re.compile(r'ðŸ“Š æ³¨æ„åŠ›ç†µå€¼: ([\d\.-]+)'),
            'joint_attention_distribution': re.compile(r'ðŸ” å…³èŠ‚æ³¨æ„åŠ›åˆ†å¸ƒ: (.+)'),
            
            # GNNç½‘ç»œæŸå¤±æ¨¡å¼ï¼ˆå¦‚æžœå°†æ¥å®žçŽ°ï¼‰
            'gnn_update': re.compile(r'ðŸ”¥ GNNç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'gnn_loss': re.compile(r'ðŸ“Š GNN Loss: ([\d\.-]+)'),
            'node_accuracy': re.compile(r'ðŸ“Š èŠ‚ç‚¹å‡†ç¡®çŽ‡: ([\d\.-]+)'),
            
            # SACç½‘ç»œæŸå¤±æ¨¡å¼ï¼ˆå¦‚æžœå°†æ¥å®žçŽ°ï¼‰
            'sac_update': re.compile(r'ðŸ”¥ SACç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'sac_critic_loss': re.compile(r'ðŸ“Š SAC Critic Loss: ([\d\.-]+)'),
            
            # ðŸ†• ä¸ªä½“å’Œä»£æ•°ä¿¡æ¯æå–
            'individual_evaluation': re.compile(r'ðŸ§¬ è¯„ä¼°ä¸ªä½“ (.+)'),
            'generation_info': re.compile(r'ç¬¬(\d+)ä»£'),
        }
        
        # å½“å‰æŸå¤±æ•°æ®ç¼“å­˜
        self.current_step = None
        self.current_network = 'ppo'
        self.current_losses = {}
        
        # æ€§èƒ½æŒ‡æ ‡ç¼“å­˜
        self.current_performance = {}
        
        # ðŸ†• å½“å‰ä¸ªä½“å’Œä»£æ•°ä¿¡æ¯
        self.current_individual_id = None
        self.current_generation = 0
        self.individual_count = 0
        self.individuals_per_generation = 10  # é»˜è®¤å€¼ï¼Œå¯ä»¥ä»Žå‘½ä»¤è¡Œå‚æ•°èŽ·å–
        
        print(f"ðŸ“Š çœŸå®žæ•°æ®æŸå¤±æå–å™¨åˆå§‹åŒ–")
        print(f"   å®žéªŒåç§°: {experiment_name}")
        print(f"   æ—¥å¿—ç›®å½•: {self.experiment_dir}")
        print(f"   ðŸŽ¯ åªè®°å½•çœŸå®žå­˜åœ¨çš„ç½‘ç»œæŸå¤±ï¼Œç»ä¸ç”Ÿæˆå‡æ•°æ®")
        print(f"   ðŸ“Š å½“å‰æ”¯æŒ: PPOç½‘ç»œæŸå¤± + Individual Reacheræ€§èƒ½æŒ‡æ ‡")
        
    def start_training_with_extraction(self, training_command):
        """å¯åŠ¨è®­ç»ƒå¹¶å®žæ—¶æå–å¤šç½‘ç»œæŸå¤±"""
        print(f"ðŸš€ å¯åŠ¨è®­ç»ƒå¹¶å®žæ—¶æå–å¤šç½‘ç»œæŸå¤±...")
        print(f"   è®­ç»ƒå‘½ä»¤: {' '.join(training_command)}")
        
        self.running = True
        
        try:
            # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
            process = subprocess.Popen(
                training_command,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            print(f"âœ… è®­ç»ƒè¿›ç¨‹å·²å¯åŠ¨ (PID: {process.pid})")
            
            # å¯åŠ¨è‡ªåŠ¨ä¿å­˜çº¿ç¨‹
            save_thread = threading.Thread(target=self._auto_save_loop, daemon=True)
            save_thread.start()
            
            # ä¸å†å¯åŠ¨æ¨¡æ‹ŸæŸå¤±ç”Ÿæˆçº¿ç¨‹ - åªè®°å½•çœŸå®žæ•°æ®
            
            # å®žæ—¶è¯»å–å¹¶å¤„ç†è¾“å‡º
            for line in process.stdout:
                if not self.running:
                    break
                    
                # æ˜¾ç¤ºè®­ç»ƒè¾“å‡º
                print(f"[è®­ç»ƒ] {line.rstrip()}")
                
                # å®žæ—¶æå–æŸå¤±æ•°æ®
                self._process_line(line.strip())
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            process.wait()
            
            print(f"ðŸ è®­ç»ƒè¿›ç¨‹ç»“æŸï¼Œè¿”å›žç : {process.returncode}")
            
        except KeyboardInterrupt:
            print("\nðŸ›‘ æŽ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·...")
            if 'process' in locals():
                process.terminate()
                process.wait()
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        finally:
            self.running = False
            # æœ€ç»ˆä¿å­˜æ•°æ®
            self._save_all_data()
            print("ðŸ§¹ å¢žå¼ºç‰ˆæå–å™¨å·²åœæ­¢")
    
    def _process_line(self, line):
        """å¤„ç†å•è¡Œè¾“å‡ºï¼Œåªæå–çœŸå®žå­˜åœ¨çš„æŸå¤±æ•°æ®"""
        
        # ðŸ†• é¦–å…ˆæ£€æŸ¥ä¸ªä½“å’Œä»£æ•°ä¿¡æ¯
        individual_match = self.patterns['individual_evaluation'].search(line)
        if individual_match:
            self.current_individual_id = individual_match.group(1).strip()
            self.individual_count += 1
            
            # æ ¹æ®ä¸ªä½“è®¡æ•°æŽ¨ç®—generationï¼ˆæ¯10ä¸ªä¸ªä½“ä¸€ä»£ï¼‰
            self.current_generation = (self.individual_count - 1) // self.individuals_per_generation
            
            print(f"   ðŸ“‹ æ£€æµ‹åˆ°ä¸ªä½“: {self.current_individual_id}")
            print(f"   ðŸ“Š ä¸ªä½“è®¡æ•°: {self.individual_count}, æŽ¨ç®—ä»£æ•°: {self.current_generation}")
            return
        
        generation_match = self.patterns['generation_info'].search(line)
        if generation_match:
            self.current_generation = int(generation_match.group(1))
            print(f"   ðŸ“‹ æ£€æµ‹åˆ°ä»£æ•°: {self.current_generation}")
            return
        
        # æ£€æŸ¥PPOç½‘ç»œæ›´æ–°ï¼ˆå”¯ä¸€ç¡®è®¤å­˜åœ¨çš„çœŸå®žç½‘ç»œæŸå¤±ï¼‰
        step_match = self.patterns['ppo_update'].search(line)
        if step_match:
            # ä¿å­˜ä¹‹å‰çš„æ•°æ®
            if self.current_step is not None and self.current_losses:
                self._record_current_loss()
            
            # å¼€å§‹æ–°çš„PPOæŸå¤±è®°å½•
            self.current_step = int(step_match.group(1))
            self.current_network = 'ppo'
            self.current_losses = {}
            return
        
        # æ£€æŸ¥å…¶ä»–ç½‘ç»œæ›´æ–°ï¼ˆå¦‚æžœè®­ç»ƒè¾“å‡ºä¸­çœŸå®žå­˜åœ¨ï¼‰
        for network_type in ['attention', 'gnn', 'sac']:
            update_pattern = f'{network_type}_update'
            if update_pattern in self.patterns:
                step_match = self.patterns[update_pattern].search(line)
                if step_match:
                    # ä¿å­˜ä¹‹å‰çš„æ•°æ®
                    if self.current_step is not None and self.current_losses:
                        self._record_current_loss()
                    
                    # å¼€å§‹æ–°çš„ç½‘ç»œæŸå¤±è®°å½•
                    self.current_step = int(step_match.group(1))
                    self.current_network = network_type
                    self.current_losses = {}
                    
                    # åŠ¨æ€æ·»åŠ ç½‘ç»œåˆ°æ•°æ®å­˜å‚¨
                    if network_type not in self.loss_data:
                        self.loss_data[network_type] = []
                        print(f"   ðŸŽ¯ æ£€æµ‹åˆ°çœŸå®ž{network_type.upper()}ç½‘ç»œæŸå¤±ï¼Œå¼€å§‹è®°å½•")
                    return
        
        # æ£€æŸ¥è®­ç»ƒè¿›åº¦æŠ¥å‘Š
        progress_match = self.patterns['training_progress_report'].search(line)
        if progress_match:
            # ä¿å­˜ä¹‹å‰çš„æ€§èƒ½æ•°æ®
            if self.current_performance:
                self._record_performance_metrics()
            
            # å¼€å§‹æ–°çš„æ€§èƒ½æŠ¥å‘Š
            self.current_performance = {'report_step': int(progress_match.group(1))}
            return
        
        # æå–æ€§èƒ½æŒ‡æ ‡
        performance_extracted = False
        for perf_type in ['success_rate', 'best_distance', 'episode_best_distance', 
                         'consecutive_success', 'completed_episodes']:
            if perf_type in self.patterns:
                match = self.patterns[perf_type].search(line)
                if match:
                    try:
                        value = float(match.group(1))
                        self.current_performance[perf_type] = value
                        print(f"   ðŸ“Š æå–åˆ°æ€§èƒ½æŒ‡æ ‡ {perf_type}: {value}")
                        performance_extracted = True
                    except ValueError:
                        pass
        
        # æ£€æŸ¥æ˜¯å¦æ”¶é›†åˆ°å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚æžœæ˜¯åˆ™è®°å½•
        if ('report_step' in self.current_performance and 
            len(self.current_performance) >= 4):  # è‡³å°‘æœ‰report_step + 3ä¸ªæ€§èƒ½æŒ‡æ ‡
            self._record_performance_metrics()
        
        # æå–çœŸå®žæŸå¤±å€¼
        if self.current_step is not None:
            # æ ¹æ®å½“å‰ç½‘ç»œç±»åž‹æå–å¯¹åº”çš„æŸå¤±å€¼
            loss_patterns_to_check = []
            
            if self.current_network == 'ppo':
                loss_patterns_to_check = ['actor_loss', 'critic_loss', 'ppo_total_loss', 'entropy', 
                                        'learning_rate', 'update_count', 'buffer_size']
            elif self.current_network == 'attention':
                loss_patterns_to_check = ['attention_actor_grad_norm', 'attention_critic_grad_norm', 
                                        'attention_total_loss', 'attention_param_mean', 'attention_param_std',
                                        'most_attended_joint', 'max_joint_attention', 'attention_concentration',
                                        'attention_entropy', 'joint_attention_distribution']
            elif self.current_network == 'gnn':
                loss_patterns_to_check = ['gnn_loss', 'node_accuracy']
            elif self.current_network == 'sac':
                loss_patterns_to_check = ['sac_critic_loss', 'sac_actor_loss']
            
            for loss_type in loss_patterns_to_check:
                if loss_type in self.patterns:
                    match = self.patterns[loss_type].search(line)
                    if match:
                        try:
                            # ç‰¹æ®Šå¤„ç†å…³èŠ‚åˆ†å¸ƒå­—ç¬¦ä¸²
                            if loss_type == 'joint_attention_distribution':
                                distribution_str = match.group(1)
                                # è§£æžå…³èŠ‚åˆ†å¸ƒå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "J0:1.000, J1:1.000, J2:1.000, J3:1.000, J4:0.000, J5:0.000"
                                joint_values = self._parse_joint_distribution(distribution_str)
                                self.current_losses.update(joint_values)
                                print(f"   ðŸŽ¯ æå–çœŸå®ž{self.current_network.upper()} å…³èŠ‚åˆ†å¸ƒ: {distribution_str}")
                            else:
                                value = float(match.group(1))
                                self.current_losses[loss_type] = value
                                print(f"   ðŸŽ¯ æå–çœŸå®ž{self.current_network.upper()} {loss_type}: {value}")
                        except ValueError:
                            pass
    
    def _record_current_loss(self):
        """è®°å½•å½“å‰æ­¥éª¤çš„æŸå¤±æ•°æ®"""
        if not self.current_losses:
            return
            
        timestamp = time.time()
        
        entry = {
            'step': self.current_step,
            'timestamp': timestamp,
            'datetime': datetime.now().isoformat(),
            'generation': self.current_generation,
            'individual_id': self.current_individual_id,
            **self.current_losses
        }
        
        # æ ¹æ®ç½‘ç»œç±»åž‹è®°å½•
        self.loss_data[self.current_network].append(entry)
        
        # æ˜¾ç¤ºè®°å½•çš„æ•°æ®
        self._display_recorded_loss()
        
        # æ¸…ç©ºå½“å‰ç¼“å­˜
        self.current_losses = {}
    
    def _record_performance_metrics(self):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡æ•°æ®"""
        if not self.current_performance:
            return
            
        timestamp = time.time()
        
        entry = {
            'step': self.current_performance.get('report_step', 0),
            'timestamp': timestamp,
            'datetime': datetime.now().isoformat(),
            'generation': self.current_generation,
            'individual_id': self.current_individual_id,
            **self.current_performance
        }
        
        self.loss_data['performance'].append(entry)
        
        # æ˜¾ç¤ºè®°å½•çš„æ€§èƒ½æ•°æ®
        success_rate = self.current_performance.get('success_rate', 'N/A')
        best_distance = self.current_performance.get('best_distance', 'N/A')
        consecutive_success = self.current_performance.get('consecutive_success', 'N/A')
        completed_episodes = self.current_performance.get('completed_episodes', 'N/A')
        
        print(f"ðŸ“Š âœ… è®°å½•æ€§èƒ½æŒ‡æ ‡ [Step {self.current_performance.get('report_step', 0)}]:")
        print(f"     æˆåŠŸçŽ‡: {success_rate}%, æœ€ä½³è·ç¦»: {best_distance}px")
        print(f"     è¿žç»­æˆåŠŸ: {consecutive_success}, å®ŒæˆEpisodes: {completed_episodes}")
        
        # æ¸…ç©ºæ€§èƒ½ç¼“å­˜
        self.current_performance = {}
    
    def _display_recorded_loss(self):
        """æ˜¾ç¤ºè®°å½•çš„çœŸå®žæŸå¤±æ•°æ®"""
        if self.current_network == 'ppo':
            actor_loss = self.current_losses.get('actor_loss', 'N/A')
            critic_loss = self.current_losses.get('critic_loss', 'N/A')
            total_loss = self.current_losses.get('ppo_total_loss', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•çœŸå®žPPOæŸå¤± [Step {self.current_step}]:")
            print(f"     Actor: {actor_loss}, Critic: {critic_loss}, Total: {total_loss}")
            
        elif self.current_network == 'attention':
            actor_grad = self.current_losses.get('attention_actor_grad_norm', 'N/A')
            critic_grad = self.current_losses.get('attention_critic_grad_norm', 'N/A')
            total_loss = self.current_losses.get('attention_total_loss', 'N/A')
            most_attended = self.current_losses.get('most_attended_joint', 'N/A')
            concentration = self.current_losses.get('attention_concentration', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•çœŸå®žAttentionæŸå¤± [Step {self.current_step}]:")
            print(f"     Actoræ¢¯åº¦: {actor_grad}, Criticæ¢¯åº¦: {critic_grad}, æ€»æŸå¤±: {total_loss}")
            print(f"     ðŸŽ¯ æœ€å…³æ³¨å…³èŠ‚: Joint {most_attended}, é›†ä¸­åº¦: {concentration}")
            
        elif self.current_network == 'gnn':
            gnn_loss = self.current_losses.get('gnn_loss', 'N/A')
            node_acc = self.current_losses.get('node_accuracy', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•çœŸå®žGNNæŸå¤± [Step {self.current_step}]:")
            print(f"     Loss: {gnn_loss}, Node Acc: {node_acc}")
            
        elif self.current_network == 'sac':
            sac_critic = self.current_losses.get('sac_critic_loss', 'N/A')
            sac_actor = self.current_losses.get('sac_actor_loss', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•çœŸå®žSACæŸå¤± [Step {self.current_step}]:")
            print(f"     Critic: {sac_critic}, Actor: {sac_actor}")
    
    def _parse_joint_distribution(self, distribution_str):
        """è§£æžå…³èŠ‚æ³¨æ„åŠ›åˆ†å¸ƒå­—ç¬¦ä¸²"""
        joint_values = {}
        
        try:
            # è§£æžæ ¼å¼: "J0:1.000, J1:1.000, J2:1.000, J3:1.000, J4:0.000, J5:0.000"
            parts = distribution_str.split(', ')
            for part in parts:
                if ':' in part:
                    joint_name, value_str = part.split(':')
                    joint_id = joint_name.strip()  # ä¾‹å¦‚ "J0"
                    value = float(value_str.strip())
                    joint_values[f'{joint_id}_attention'] = value
        except Exception as e:
            joint_values['joint_distribution_parse_error'] = str(e)
        
        return joint_values
    
    def _auto_save_loop(self):
        """è‡ªåŠ¨ä¿å­˜å¾ªçŽ¯"""
        while self.running:
            time.sleep(30)  # æ¯30ç§’ä¿å­˜ä¸€æ¬¡
            if any(self.loss_data.values()):
                self._save_all_data()
                print("ðŸ’¾ è‡ªåŠ¨ä¿å­˜çœŸå®žæŸå¤±æ•°æ®å®Œæˆ")
    
    def _save_all_data(self):
        """ä¿å­˜æ‰€æœ‰ç½‘ç»œçš„æŸå¤±æ•°æ®"""
        # å…ˆè®°å½•å½“å‰æœªå®Œæˆçš„æŸå¤±
        if self.current_step is not None and self.current_losses:
            self._record_current_loss()
        
        saved_networks = []
        
        for network, data in self.loss_data.items():
            if data:
                # ä¿å­˜CSVæ–‡ä»¶
                csv_path = os.path.join(self.experiment_dir, f"{network}_losses.csv")
                
                with open(csv_path, 'w', newline='') as csvfile:
                    # æ”¶é›†æ‰€æœ‰å­—æ®µå
                    all_fieldnames = set()
                    for entry in data:
                        all_fieldnames.update(entry.keys())
                    fieldnames = sorted(list(all_fieldnames))
                    
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(data)
                
                # ä¿å­˜JSONæ–‡ä»¶
                json_path = os.path.join(self.experiment_dir, f"{network}_losses.json")
                with open(json_path, 'w') as jsonfile:
                    json.dump(data, jsonfile, indent=2)
                
                saved_networks.append(network)
                print(f"ðŸ’¾ ä¿å­˜çœŸå®ž {network.upper()} æ•°æ®: {len(data)} æ¡è®°å½•")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        if saved_networks:
            stats = self._get_comprehensive_statistics()
            stats_path = os.path.join(self.experiment_dir, "comprehensive_loss_statistics.json")
            with open(stats_path, 'w') as f:
                json.dump(stats, f, indent=2)
            
            print(f"ðŸ“ˆ çœŸå®žæŸå¤±ç»Ÿè®¡å·²ä¿å­˜: {len(saved_networks)} ä¸ªç½‘ç»œ")
    
    def _get_comprehensive_statistics(self):
        """èŽ·å–æ‰€æœ‰ç½‘ç»œçš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'experiment_info': {
                'experiment_name': self.experiment_name,
                'data_type': 'real_only',  # æ ‡æ˜Žè¿™æ˜¯çœŸå®žæ•°æ®
                'total_networks': len([n for n, d in self.loss_data.items() if d]),
                'total_records': sum(len(d) for d in self.loss_data.values()),
                'generation_time': datetime.now().isoformat(),
                'note': 'Only real loss data from training output, no simulated data'
            },
            'network_stats': {}
        }
        
        for network, data in self.loss_data.items():
            if data:
                # æå–æ•°å€¼æ•°æ®
                numeric_fields = {}
                for entry in data:
                    for key, value in entry.items():
                        if isinstance(value, (int, float)) and key not in ['step', 'timestamp']:
                            if key not in numeric_fields:
                                numeric_fields[key] = []
                            numeric_fields[key].append(value)
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                network_stats = {}
                for field, values in numeric_fields.items():
                    if values:
                        network_stats[field] = {
                            'count': len(values),
                            'min': min(values),
                            'max': max(values),
                            'avg': sum(values) / len(values),
                            'first': values[0],
                            'last': values[-1],
                            'trend': 'decreasing' if len(values) > 1 and values[-1] < values[0] else 'increasing',
                            'std': self._calculate_std(values) if len(values) > 1 else 0
                        }
                
                stats['network_stats'][network] = {
                    'total_records': len(data),
                    'metrics': network_stats
                }
        
        return stats
    
    def _calculate_std(self, values):
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(values) <= 1:
            return 0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5

def run_enhanced_multi_network_training(experiment_name, mode='basic', training_steps=2000, 
                                        num_generations=None, individuals_per_generation=None, **kwargs):
    """è¿è¡Œå¢žå¼ºç‰ˆå¤šç½‘ç»œè®­ç»ƒ"""
    
    # åˆ›å»ºå¢žå¼ºç‰ˆæå–å™¨
    extractor = EnhancedMultiNetworkExtractor(experiment_name)
    
    # ðŸ†• è®¾ç½®æ¯ä»£ä¸ªä½“æ•°ï¼Œç”¨äºŽgenerationè®¡ç®—
    if individuals_per_generation is not None:
        extractor.individuals_per_generation = individuals_per_generation
    
    # æž„å»ºè®­ç»ƒå‘½ä»¤ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    training_script = os.path.join(script_dir, 'map_elites_with_loss_logger.py')
    
    training_command = [
        sys.executable,
        training_script,
        '--mode', mode,
        '--experiment-name', experiment_name,
        '--training-steps-per-individual', str(training_steps)
    ]
    
    # æ·»åŠ é¢å¤–çš„MAP-Eliteså‚æ•°
    if num_generations is not None:
        training_command.extend(['--num-generations', str(num_generations)])
    
    if individuals_per_generation is not None:
        training_command.extend(['--individuals-per-generation', str(individuals_per_generation)])
    
    # æ·»åŠ å…¶ä»–å‚æ•°
    for key, value in kwargs.items():
        if value is not None:
            if isinstance(value, bool):
                if value:  # åªæœ‰Trueæ—¶æ‰æ·»åŠ æ ‡å¿—
                    training_command.append(f'--{key.replace("_", "-")}')
            else:
                training_command.extend([f'--{key.replace("_", "-")}', str(value)])
    
    # è®¾ç½®çŽ¯å¢ƒå˜é‡
    os.environ['LOSS_EXPERIMENT_NAME'] = experiment_name
    
    # å¯åŠ¨è®­ç»ƒå¹¶æå–
    extractor.start_training_with_extraction(training_command)
    
    return extractor.experiment_dir

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨')
    parser.add_argument('--experiment-name', type=str, required=True, help='å®žéªŒåç§°')
    parser.add_argument('--mode', type=str, default='basic', help='è®­ç»ƒæ¨¡å¼')
    parser.add_argument('--training-steps', type=int, default=2000, help='æ¯ä¸ªä¸ªä½“è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--num-generations', type=int, help='è¿›åŒ–ä»£æ•°')
    parser.add_argument('--individuals-per-generation', type=int, help='æ¯ä»£ä¸ªä½“æ•°')
    parser.add_argument('--enable-rendering', action='store_true', help='å¯ç”¨çŽ¯å¢ƒæ¸²æŸ“')
    parser.add_argument('--silent-mode', action='store_true', help='é™é»˜æ¨¡å¼')
    parser.add_argument('--use-genetic-fitness', action='store_true', help='ä½¿ç”¨é—ä¼ ç®—æ³•fitness')
    
    args = parser.parse_args()
    
    print("ðŸŽ¯ å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨")
    print("=" * 60)
    print(f"å®žéªŒåç§°: {args.experiment_name}")
    print(f"è®­ç»ƒæ¨¡å¼: {args.mode}")
    print(f"æ¯ä¸ªä½“è®­ç»ƒæ­¥æ•°: {args.training_steps}")
    if args.num_generations:
        print(f"è¿›åŒ–ä»£æ•°: {args.num_generations}")
    if args.individuals_per_generation:
        print(f"æ¯ä»£ä¸ªä½“æ•°: {args.individuals_per_generation}")
    
    try:
        # å‡†å¤‡é¢å¤–å‚æ•°
        extra_kwargs = {}
        if args.enable_rendering:
            extra_kwargs['enable_rendering'] = True
        if args.silent_mode:
            extra_kwargs['silent_mode'] = True
        if args.use_genetic_fitness:
            extra_kwargs['use_genetic_fitness'] = True
        
        log_dir = run_enhanced_multi_network_training(
            experiment_name=args.experiment_name, 
            mode=args.mode, 
            training_steps=args.training_steps,
            num_generations=args.num_generations,
            individuals_per_generation=args.individuals_per_generation,
            **extra_kwargs
        )
        
        print(f"\nðŸŽ‰ çœŸå®žæŸå¤±æå–å®Œæˆï¼")
        print(f"ðŸ“ çœŸå®žæŸå¤±æ•°æ®ä¿å­˜åœ¨: {log_dir}")
        print(f"ðŸ“Š åªåŒ…å«è®­ç»ƒè¾“å‡ºä¸­çœŸå®žå­˜åœ¨çš„ç½‘ç»œæŸå¤±ï¼Œæ— ä»»ä½•æ¨¡æ‹Ÿæ•°æ®")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ å¤šç½‘ç»œæŸå¤±æå–è¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å¤šç½‘ç»œæŸå¤±æå–å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
