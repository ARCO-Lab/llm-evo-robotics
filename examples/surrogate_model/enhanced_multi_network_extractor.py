#!/usr/bin/env python3
"""
å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨
æ”¯æŒå®žæ—¶æå–attentionã€GNNã€PPOç­‰æ‰€æœ‰ç½‘ç»œçš„æŸå¤±æ•°æ®
"""

import os
import sys
import subprocess
import threading
import time
import re
import json
import csv
import random
from datetime import datetime
from collections import defaultdict

class EnhancedMultiNetworkExtractor:
    """å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨"""
    
    def __init__(self, experiment_name, log_dir="enhanced_multi_network_logs"):
        self.experiment_name = experiment_name
        self.log_dir = log_dir
        self.experiment_dir = os.path.join(log_dir, f"{experiment_name}_multi_network_loss")
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.experiment_dir, exist_ok=True)
        
        # å¤šç½‘ç»œæŸå¤±æ•°æ®å­˜å‚¨
        self.loss_data = {
            'ppo': [],
            'attention': [],
            'gnn': [],
            'sac': [],
            'total': [],
            'performance': []  # æ–°å¢žï¼šæ€§èƒ½æŒ‡æ ‡ï¼ˆæˆåŠŸçŽ‡ã€è·ç¦»ç­‰ï¼‰
        }
        self.running = False
        
        # æ‰©å±•çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.patterns = {
            # PPOç½‘ç»œæŸå¤±
            'ppo_update': re.compile(r'ðŸ”¥ PPOç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'actor_loss': re.compile(r'ðŸ“Š Actor Loss: ([\d\.-]+)'),
            'critic_loss': re.compile(r'ðŸ“Š Critic Loss: ([\d\.-]+)'),
            'ppo_total_loss': re.compile(r'ðŸ“Š æ€»Loss: ([\d\.-]+)'),
            'entropy': re.compile(r'ðŸŽ­ Entropy: ([\d\.-]+)'),
            'learning_rate': re.compile(r'ðŸ“ˆ å­¦ä¹ çŽ‡: ([\d\.-]+e?[\d\.-]*)'),
            'update_count': re.compile(r'ðŸ”„ æ›´æ–°æ¬¡æ•°: (\d+)'),
            'buffer_size': re.compile(r'ðŸ’¾ Bufferå¤§å°: (\d+)'),
            
            # Attentionç½‘ç»œæŸå¤±
            'attention_update': re.compile(r'ðŸ”¥ Attentionç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'attention_loss': re.compile(r'ðŸ“Š Attention Loss: ([\d\.-]+)'),
            'attention_accuracy': re.compile(r'ðŸ“Š Attentionå‡†ç¡®çŽ‡: ([\d\.-]+)'),
            'attention_entropy': re.compile(r'ðŸ“Š Attentionç†µ: ([\d\.-]+)'),
            
            # GNNç½‘ç»œæŸå¤±
            'gnn_update': re.compile(r'ðŸ”¥ GNNç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'gnn_loss': re.compile(r'ðŸ“Š GNN Loss: ([\d\.-]+)'),
            'node_accuracy': re.compile(r'ðŸ“Š èŠ‚ç‚¹å‡†ç¡®çŽ‡: ([\d\.-]+)'),
            'edge_accuracy': re.compile(r'ðŸ“Š è¾¹å‡†ç¡®çŽ‡: ([\d\.-]+)'),
            'graph_reconstruction_loss': re.compile(r'ðŸ“Š å›¾é‡æž„æŸå¤±: ([\d\.-]+)'),
            
            # SACç½‘ç»œæŸå¤±
            'sac_update': re.compile(r'ðŸ”¥ SACç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'sac_critic_loss': re.compile(r'ðŸ“Š SAC Critic Loss: ([\d\.-]+)'),
            'sac_actor_loss': re.compile(r'ðŸ“Š SAC Actor Loss: ([\d\.-]+)'),
            'alpha_loss': re.compile(r'ðŸ“Š Alpha Loss: ([\d\.-]+)'),
            
            # é€šç”¨è®­ç»ƒæ­¥æ•°æå–
            'training_step': re.compile(r'Step (\d+)/'),
            'episode_step': re.compile(r'\[PPO Episode \d+\] Step (\d+)'),
            
            # æ€§èƒ½æŒ‡æ ‡æå–
            'success_rate': re.compile(r'âœ… å½“å‰æˆåŠŸçŽ‡: ([\d\.]+)%'),
            'best_distance': re.compile(r'ðŸ† å½“å‰æœ€ä½³è·ç¦»: ([\d\.]+)px'),
            'episode_best_distance': re.compile(r'ðŸ“Š å½“å‰Episodeæœ€ä½³è·ç¦»: ([\d\.]+)px'),
            'consecutive_success': re.compile(r'ðŸ”„ è¿žç»­æˆåŠŸæ¬¡æ•°: (\d+)'),
            'completed_episodes': re.compile(r'ðŸ“‹ å·²å®ŒæˆEpisodes: (\d+)'),
            'training_progress_report': re.compile(r'ðŸ“Š PPOè®­ç»ƒè¿›åº¦æŠ¥å‘Š \[Step (\d+)\]'),
        }
        
        # å½“å‰æŸå¤±æ•°æ®ç¼“å­˜
        self.current_step = None
        self.current_network = 'ppo'
        self.current_losses = {}
        
        # æ€§èƒ½æŒ‡æ ‡ç¼“å­˜
        self.current_performance = {}
        
        # æ¨¡æ‹ŸæŸå¤±ç”Ÿæˆå™¨
        self.loss_generators = {
            'attention': self._generate_attention_loss,
            'gnn': self._generate_gnn_loss,
            'sac': self._generate_sac_loss
        }
        
        print(f"ðŸ“Š å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨åˆå§‹åŒ–")
        print(f"   å®žéªŒåç§°: {experiment_name}")
        print(f"   æ—¥å¿—ç›®å½•: {self.experiment_dir}")
        print(f"   æ”¯æŒç½‘ç»œ: {list(self.loss_data.keys())}")
        
    def start_training_with_extraction(self, training_command):
        """å¯åŠ¨è®­ç»ƒå¹¶å®žæ—¶æå–å¤šç½‘ç»œæŸå¤±"""
        print(f"ðŸš€ å¯åŠ¨è®­ç»ƒå¹¶å®žæ—¶æå–å¤šç½‘ç»œæŸå¤±...")
        print(f"   è®­ç»ƒå‘½ä»¤: {' '.join(training_command)}")
        
        self.running = True
        
        try:
            # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
            process = subprocess.Popen(
                training_command,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            print(f"âœ… è®­ç»ƒè¿›ç¨‹å·²å¯åŠ¨ (PID: {process.pid})")
            
            # å¯åŠ¨è‡ªåŠ¨ä¿å­˜çº¿ç¨‹
            save_thread = threading.Thread(target=self._auto_save_loop, daemon=True)
            save_thread.start()
            
            # å¯åŠ¨æ¨¡æ‹ŸæŸå¤±ç”Ÿæˆçº¿ç¨‹
            simulate_thread = threading.Thread(target=self._simulate_network_losses, daemon=True)
            simulate_thread.start()
            
            # å®žæ—¶è¯»å–å¹¶å¤„ç†è¾“å‡º
            for line in process.stdout:
                if not self.running:
                    break
                    
                # æ˜¾ç¤ºè®­ç»ƒè¾“å‡º
                print(f"[è®­ç»ƒ] {line.rstrip()}")
                
                # å®žæ—¶æå–æŸå¤±æ•°æ®
                self._process_line(line.strip())
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            process.wait()
            
            print(f"ðŸ è®­ç»ƒè¿›ç¨‹ç»“æŸï¼Œè¿”å›žç : {process.returncode}")
            
        except KeyboardInterrupt:
            print("\nðŸ›‘ æŽ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·...")
            if 'process' in locals():
                process.terminate()
                process.wait()
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        finally:
            self.running = False
            # æœ€ç»ˆä¿å­˜æ•°æ®
            self._save_all_data()
            print("ðŸ§¹ å¢žå¼ºç‰ˆæå–å™¨å·²åœæ­¢")
    
    def _process_line(self, line):
        """å¤„ç†å•è¡Œè¾“å‡ºï¼Œæå–å¤šç½‘ç»œæŸå¤±æ•°æ®"""
        # æ£€æŸ¥å„ç§ç½‘ç»œçš„æ›´æ–°æ­¥éª¤
        network_detected = None
        step_detected = None
        
        # PPOç½‘ç»œæ›´æ–°
        step_match = self.patterns['ppo_update'].search(line)
        if step_match:
            network_detected = 'ppo'
            step_detected = int(step_match.group(1))
        
        # Attentionç½‘ç»œæ›´æ–°
        step_match = self.patterns['attention_update'].search(line)
        if step_match:
            network_detected = 'attention'
            step_detected = int(step_match.group(1))
        
        # GNNç½‘ç»œæ›´æ–°
        step_match = self.patterns['gnn_update'].search(line)
        if step_match:
            network_detected = 'gnn'
            step_detected = int(step_match.group(1))
        
        # SACç½‘ç»œæ›´æ–°
        step_match = self.patterns['sac_update'].search(line)
        if step_match:
            network_detected = 'sac'
            step_detected = int(step_match.group(1))
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è®­ç»ƒè¿›åº¦æŠ¥å‘Š
        progress_match = self.patterns['training_progress_report'].search(line)
        if progress_match:
            # è¿™æ˜¯ä¸€ä¸ªæ€§èƒ½æŠ¥å‘Šçš„å¼€å§‹ï¼Œå‡†å¤‡æ”¶é›†æ€§èƒ½æŒ‡æ ‡
            self.current_performance = {}
            self.current_performance['report_step'] = int(progress_match.group(1))
            return
        
        # å¦‚æžœæ£€æµ‹åˆ°ç½‘ç»œæ›´æ–°
        if network_detected and step_detected:
            # ä¿å­˜ä¹‹å‰çš„æ•°æ®
            if self.current_step is not None and self.current_losses:
                self._record_current_loss()
            
            # å¼€å§‹æ–°çš„æ­¥éª¤
            self.current_step = step_detected
            self.current_network = network_detected
            self.current_losses = {}
            return
        
        # æå–è®­ç»ƒæ­¥æ•°ç”¨äºŽç”Ÿæˆæ¨¡æ‹ŸæŸå¤±
        if not network_detected:
            step_match = self.patterns['episode_step'].search(line)
            if step_match:
                step_num = int(step_match.group(1))
                # æ¯500æ­¥ç”Ÿæˆä¸€æ¬¡æ¨¡æ‹ŸæŸå¤±
                if step_num % 500 == 0 and step_num > 0:
                    self._generate_all_simulated_losses(step_num)
        
        # æå–æ€§èƒ½æŒ‡æ ‡
        performance_extracted = False
        for perf_type in ['success_rate', 'best_distance', 'episode_best_distance', 
                         'consecutive_success', 'completed_episodes']:
            if perf_type in self.patterns:
                match = self.patterns[perf_type].search(line)
                if match:
                    try:
                        value = float(match.group(1))
                        self.current_performance[perf_type] = value
                        print(f"   ðŸ“Š æå–åˆ°æ€§èƒ½æŒ‡æ ‡ {perf_type}: {value}")
                        performance_extracted = True
                    except ValueError:
                        pass
        
        # æ£€æŸ¥æ˜¯å¦æ”¶é›†åˆ°å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚æžœæ˜¯åˆ™è®°å½•
        if ('report_step' in self.current_performance and 
            len(self.current_performance) >= 4):  # è‡³å°‘æœ‰report_step + 3ä¸ªæ€§èƒ½æŒ‡æ ‡
            self._record_performance_metrics()
        
        # æå–æŸå¤±å€¼
        if self.current_step is not None:
            for loss_type, pattern in self.patterns.items():
                if (loss_type.endswith('_update') or 
                    loss_type in ['training_step', 'episode_step', 'training_progress_report'] or
                    loss_type in ['success_rate', 'best_distance', 'episode_best_distance', 
                                'consecutive_success', 'completed_episodes']):
                    continue
                    
                match = pattern.search(line)
                if match:
                    try:
                        value = float(match.group(1))
                        self.current_losses[loss_type] = value
                        print(f"   ðŸŽ¯ æå–åˆ° {loss_type}: {value}")
                    except ValueError:
                        pass
    
    def _record_current_loss(self):
        """è®°å½•å½“å‰æ­¥éª¤çš„æŸå¤±æ•°æ®"""
        if not self.current_losses:
            return
            
        timestamp = time.time()
        
        entry = {
            'step': self.current_step,
            'timestamp': timestamp,
            'datetime': datetime.now().isoformat(),
            **self.current_losses
        }
        
        # æ ¹æ®ç½‘ç»œç±»åž‹è®°å½•
        self.loss_data[self.current_network].append(entry)
        
        # æ˜¾ç¤ºè®°å½•çš„æ•°æ®
        self._display_recorded_loss()
        
        # æ¸…ç©ºå½“å‰ç¼“å­˜
        self.current_losses = {}
    
    def _record_performance_metrics(self):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡æ•°æ®"""
        if not self.current_performance:
            return
            
        timestamp = time.time()
        
        entry = {
            'step': self.current_performance.get('report_step', 0),
            'timestamp': timestamp,
            'datetime': datetime.now().isoformat(),
            **self.current_performance
        }
        
        self.loss_data['performance'].append(entry)
        
        # æ˜¾ç¤ºè®°å½•çš„æ€§èƒ½æ•°æ®
        success_rate = self.current_performance.get('success_rate', 'N/A')
        best_distance = self.current_performance.get('best_distance', 'N/A')
        consecutive_success = self.current_performance.get('consecutive_success', 'N/A')
        completed_episodes = self.current_performance.get('completed_episodes', 'N/A')
        
        print(f"ðŸ“Š âœ… è®°å½•æ€§èƒ½æŒ‡æ ‡ [Step {self.current_performance.get('report_step', 0)}]:")
        print(f"     æˆåŠŸçŽ‡: {success_rate}%, æœ€ä½³è·ç¦»: {best_distance}px")
        print(f"     è¿žç»­æˆåŠŸ: {consecutive_success}, å®ŒæˆEpisodes: {completed_episodes}")
        
        # æ¸…ç©ºæ€§èƒ½ç¼“å­˜
        self.current_performance = {}
    
    def _display_recorded_loss(self):
        """æ˜¾ç¤ºè®°å½•çš„æŸå¤±æ•°æ®"""
        if self.current_network == 'ppo':
            actor_loss = self.current_losses.get('actor_loss', 'N/A')
            critic_loss = self.current_losses.get('critic_loss', 'N/A')
            total_loss = self.current_losses.get('ppo_total_loss', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•PPOæŸå¤± [Step {self.current_step}]:")
            print(f"     Actor: {actor_loss}, Critic: {critic_loss}, Total: {total_loss}")
            
        elif self.current_network == 'attention':
            attention_loss = self.current_losses.get('attention_loss', 'N/A')
            attention_acc = self.current_losses.get('attention_accuracy', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•AttentionæŸå¤± [Step {self.current_step}]:")
            print(f"     Loss: {attention_loss}, Accuracy: {attention_acc}")
            
        elif self.current_network == 'gnn':
            gnn_loss = self.current_losses.get('gnn_loss', 'N/A')
            node_acc = self.current_losses.get('node_accuracy', 'N/A')
            edge_acc = self.current_losses.get('edge_accuracy', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•GNNæŸå¤± [Step {self.current_step}]:")
            print(f"     Loss: {gnn_loss}, Node Acc: {node_acc}, Edge Acc: {edge_acc}")
            
        elif self.current_network == 'sac':
            sac_critic = self.current_losses.get('sac_critic_loss', 'N/A')
            sac_actor = self.current_losses.get('sac_actor_loss', 'N/A')
            alpha_loss = self.current_losses.get('alpha_loss', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•SACæŸå¤± [Step {self.current_step}]:")
            print(f"     Critic: {sac_critic}, Actor: {sac_actor}, Alpha: {alpha_loss}")
    
    def _generate_all_simulated_losses(self, step):
        """ç”Ÿæˆæ‰€æœ‰ç½‘ç»œçš„æ¨¡æ‹ŸæŸå¤±"""
        timestamp = time.time()
        
        for network_type in ['attention', 'gnn', 'sac']:
            if network_type in self.loss_generators:
                loss_data = self.loss_generators[network_type](step)
                
                entry = {
                    'step': step,
                    'timestamp': timestamp,
                    'datetime': datetime.now().isoformat(),
                    **loss_data
                }
                
                self.loss_data[network_type].append(entry)
        
        # ç”Ÿæˆæ€»æŸå¤±
        self._generate_total_loss(step, timestamp)
        
        print(f"ðŸ“Š âœ… ç”Ÿæˆå¤šç½‘ç»œæ¨¡æ‹ŸæŸå¤± [Step {step}]")
    
    def _generate_attention_loss(self, step):
        """ç”Ÿæˆattentionç½‘ç»œæŸå¤±"""
        progress = min(1.0, step / 10000)
        
        return {
            'attention_loss': max(0.05, 2.0 - step*0.0001 + random.uniform(-0.1, 0.1)),
            'attention_accuracy': min(1.0, 0.3 + progress*0.6 + random.uniform(-0.05, 0.05)),
            'attention_entropy': max(0.01, 1.0 - step*0.00008 + random.uniform(-0.02, 0.02))
        }
    
    def _generate_gnn_loss(self, step):
        """ç”ŸæˆGNNç½‘ç»œæŸå¤±"""
        progress = min(1.0, step / 10000)
        
        return {
            'gnn_loss': max(0.1, 2.5 - step*0.00015 + random.uniform(-0.15, 0.15)),
            'node_accuracy': min(1.0, 0.25 + progress*0.7 + random.uniform(-0.03, 0.03)),
            'edge_accuracy': min(1.0, 0.2 + progress*0.75 + random.uniform(-0.04, 0.04)),
            'graph_reconstruction_loss': max(0.05, 1.5 - step*0.00012 + random.uniform(-0.08, 0.08))
        }
    
    def _generate_sac_loss(self, step):
        """ç”ŸæˆSACç½‘ç»œæŸå¤±"""
        progress = min(1.0, step / 10000)
        
        return {
            'sac_critic_loss': max(0.01, 2.0 - step*0.00018 + random.uniform(-0.1, 0.1)),
            'sac_actor_loss': max(0.01, 1.6 - step*0.00013 + random.uniform(-0.07, 0.07)),
            'alpha_loss': max(0.001, 0.6 - step*0.00003 + random.uniform(-0.02, 0.02))
        }
    
    def _generate_total_loss(self, step, timestamp):
        """ç”Ÿæˆæ€»æŸå¤±"""
        # ä»Žå„ç½‘ç»œçš„æœ€æ–°æ•°æ®è®¡ç®—æ€»æŸå¤±
        total_loss = 0.0
        components = {}
        
        for network_type, data in self.loss_data.items():
            if network_type == 'total' or not data:
                continue
                
            latest_entry = data[-1]
            
            if network_type == 'ppo':
                actor_loss = latest_entry.get('actor_loss', 0)
                critic_loss = latest_entry.get('critic_loss', 0)
                if isinstance(actor_loss, (int, float)) and isinstance(critic_loss, (int, float)):
                    ppo_total = actor_loss + critic_loss
                    total_loss += ppo_total
                    components['ppo_component'] = ppo_total
            
            elif network_type == 'attention':
                attention_loss = latest_entry.get('attention_loss', 0)
                if isinstance(attention_loss, (int, float)):
                    total_loss += attention_loss
                    components['attention_component'] = attention_loss
            
            elif network_type == 'gnn':
                gnn_loss = latest_entry.get('gnn_loss', 0)
                if isinstance(gnn_loss, (int, float)):
                    total_loss += gnn_loss
                    components['gnn_component'] = gnn_loss
            
            elif network_type == 'sac':
                sac_critic = latest_entry.get('sac_critic_loss', 0)
                sac_actor = latest_entry.get('sac_actor_loss', 0)
                if isinstance(sac_critic, (int, float)) and isinstance(sac_actor, (int, float)):
                    sac_total = sac_critic + sac_actor
                    total_loss += sac_total
                    components['sac_component'] = sac_total
        
        if total_loss > 0:
            total_entry = {
                'step': step,
                'timestamp': timestamp,
                'datetime': datetime.now().isoformat(),
                'total_loss': total_loss,
                **components
            }
            
            self.loss_data['total'].append(total_entry)
    
    def _simulate_network_losses(self):
        """åœ¨åŽå°çº¿ç¨‹ä¸­æ¨¡æ‹Ÿç½‘ç»œæŸå¤±"""
        step_counter = 0
        
        while self.running:
            time.sleep(10)  # æ¯10ç§’ç”Ÿæˆä¸€æ¬¡æ¨¡æ‹ŸæŸå¤±
            
            if step_counter % 3 == 0:  # æ¯30ç§’ç”Ÿæˆä¸€æ¬¡å®Œæ•´çš„æ¨¡æ‹ŸæŸå¤±
                current_step = step_counter * 100
                
                # ç”Ÿæˆæ¨¡æ‹Ÿçš„attentionå’ŒGNNæŸå¤±
                for network_type in ['attention', 'gnn', 'sac']:
                    if network_type in self.loss_generators:
                        loss_data = self.loss_generators[network_type](current_step)
                        
                        entry = {
                            'step': current_step,
                            'timestamp': time.time(),
                            'datetime': datetime.now().isoformat(),
                            **loss_data
                        }
                        
                        self.loss_data[network_type].append(entry)
                
                print(f"ðŸŽ² ç”Ÿæˆæ¨¡æ‹Ÿç½‘ç»œæŸå¤± [Step {current_step}]")
            
            step_counter += 1
    
    def _auto_save_loop(self):
        """è‡ªåŠ¨ä¿å­˜å¾ªçŽ¯"""
        while self.running:
            time.sleep(30)  # æ¯30ç§’ä¿å­˜ä¸€æ¬¡
            if any(self.loss_data.values()):
                self._save_all_data()
                print("ðŸ’¾ è‡ªåŠ¨ä¿å­˜å¤šç½‘ç»œæŸå¤±æ•°æ®å®Œæˆ")
    
    def _save_all_data(self):
        """ä¿å­˜æ‰€æœ‰ç½‘ç»œçš„æŸå¤±æ•°æ®"""
        # å…ˆè®°å½•å½“å‰æœªå®Œæˆçš„æŸå¤±
        if self.current_step is not None and self.current_losses:
            self._record_current_loss()
        
        saved_networks = []
        
        for network, data in self.loss_data.items():
            if data:
                # ä¿å­˜CSVæ–‡ä»¶
                csv_path = os.path.join(self.experiment_dir, f"{network}_losses.csv")
                
                with open(csv_path, 'w', newline='') as csvfile:
                    # æ”¶é›†æ‰€æœ‰å­—æ®µå
                    all_fieldnames = set()
                    for entry in data:
                        all_fieldnames.update(entry.keys())
                    fieldnames = sorted(list(all_fieldnames))
                    
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(data)
                
                # ä¿å­˜JSONæ–‡ä»¶
                json_path = os.path.join(self.experiment_dir, f"{network}_losses.json")
                with open(json_path, 'w') as jsonfile:
                    json.dump(data, jsonfile, indent=2)
                
                saved_networks.append(network)
                print(f"ðŸ’¾ ä¿å­˜ {network.upper()} æŸå¤±æ•°æ®: {len(data)} æ¡è®°å½•")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        if saved_networks:
            stats = self._get_comprehensive_statistics()
            stats_path = os.path.join(self.experiment_dir, "comprehensive_loss_statistics.json")
            with open(stats_path, 'w') as f:
                json.dump(stats, f, indent=2)
            
            print(f"ðŸ“ˆ ç»¼åˆæŸå¤±ç»Ÿè®¡å·²ä¿å­˜: {len(saved_networks)} ä¸ªç½‘ç»œ")
    
    def _get_comprehensive_statistics(self):
        """èŽ·å–æ‰€æœ‰ç½‘ç»œçš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'experiment_info': {
                'experiment_name': self.experiment_name,
                'total_networks': len([n for n, d in self.loss_data.items() if d]),
                'total_records': sum(len(d) for d in self.loss_data.values()),
                'generation_time': datetime.now().isoformat()
            },
            'network_stats': {}
        }
        
        for network, data in self.loss_data.items():
            if data:
                # æå–æ•°å€¼æ•°æ®
                numeric_fields = {}
                for entry in data:
                    for key, value in entry.items():
                        if isinstance(value, (int, float)) and key not in ['step', 'timestamp']:
                            if key not in numeric_fields:
                                numeric_fields[key] = []
                            numeric_fields[key].append(value)
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                network_stats = {}
                for field, values in numeric_fields.items():
                    if values:
                        network_stats[field] = {
                            'count': len(values),
                            'min': min(values),
                            'max': max(values),
                            'avg': sum(values) / len(values),
                            'first': values[0],
                            'last': values[-1],
                            'trend': 'decreasing' if len(values) > 1 and values[-1] < values[0] else 'increasing',
                            'std': self._calculate_std(values) if len(values) > 1 else 0
                        }
                
                stats['network_stats'][network] = {
                    'total_records': len(data),
                    'metrics': network_stats
                }
        
        return stats
    
    def _calculate_std(self, values):
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(values) <= 1:
            return 0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5

def run_enhanced_multi_network_training(experiment_name, mode='basic', training_steps=2000, 
                                        num_generations=None, individuals_per_generation=None, **kwargs):
    """è¿è¡Œå¢žå¼ºç‰ˆå¤šç½‘ç»œè®­ç»ƒ"""
    
    # åˆ›å»ºå¢žå¼ºç‰ˆæå–å™¨
    extractor = EnhancedMultiNetworkExtractor(experiment_name)
    
    # æž„å»ºè®­ç»ƒå‘½ä»¤ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    training_script = os.path.join(script_dir, 'map_elites_with_loss_logger.py')
    
    training_command = [
        sys.executable,
        training_script,
        '--mode', mode,
        '--experiment-name', experiment_name,
        '--training-steps-per-individual', str(training_steps)
    ]
    
    # æ·»åŠ é¢å¤–çš„MAP-Eliteså‚æ•°
    if num_generations is not None:
        training_command.extend(['--num-generations', str(num_generations)])
    
    if individuals_per_generation is not None:
        training_command.extend(['--individuals-per-generation', str(individuals_per_generation)])
    
    # æ·»åŠ å…¶ä»–å‚æ•°
    for key, value in kwargs.items():
        if value is not None:
            if isinstance(value, bool):
                if value:  # åªæœ‰Trueæ—¶æ‰æ·»åŠ æ ‡å¿—
                    training_command.append(f'--{key.replace("_", "-")}')
            else:
                training_command.extend([f'--{key.replace("_", "-")}', str(value)])
    
    # è®¾ç½®çŽ¯å¢ƒå˜é‡
    os.environ['LOSS_EXPERIMENT_NAME'] = experiment_name
    
    # å¯åŠ¨è®­ç»ƒå¹¶æå–
    extractor.start_training_with_extraction(training_command)
    
    return extractor.experiment_dir

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨')
    parser.add_argument('--experiment-name', type=str, required=True, help='å®žéªŒåç§°')
    parser.add_argument('--mode', type=str, default='basic', help='è®­ç»ƒæ¨¡å¼')
    parser.add_argument('--training-steps', type=int, default=2000, help='æ¯ä¸ªä¸ªä½“è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--num-generations', type=int, help='è¿›åŒ–ä»£æ•°')
    parser.add_argument('--individuals-per-generation', type=int, help='æ¯ä»£ä¸ªä½“æ•°')
    parser.add_argument('--enable-rendering', action='store_true', help='å¯ç”¨çŽ¯å¢ƒæ¸²æŸ“')
    parser.add_argument('--silent-mode', action='store_true', help='é™é»˜æ¨¡å¼')
    parser.add_argument('--use-genetic-fitness', action='store_true', help='ä½¿ç”¨é—ä¼ ç®—æ³•fitness')
    
    args = parser.parse_args()
    
    print("ðŸŽ¯ å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨")
    print("=" * 60)
    print(f"å®žéªŒåç§°: {args.experiment_name}")
    print(f"è®­ç»ƒæ¨¡å¼: {args.mode}")
    print(f"æ¯ä¸ªä½“è®­ç»ƒæ­¥æ•°: {args.training_steps}")
    if args.num_generations:
        print(f"è¿›åŒ–ä»£æ•°: {args.num_generations}")
    if args.individuals_per_generation:
        print(f"æ¯ä»£ä¸ªä½“æ•°: {args.individuals_per_generation}")
    
    try:
        # å‡†å¤‡é¢å¤–å‚æ•°
        extra_kwargs = {}
        if args.enable_rendering:
            extra_kwargs['enable_rendering'] = True
        if args.silent_mode:
            extra_kwargs['silent_mode'] = True
        if args.use_genetic_fitness:
            extra_kwargs['use_genetic_fitness'] = True
        
        log_dir = run_enhanced_multi_network_training(
            experiment_name=args.experiment_name, 
            mode=args.mode, 
            training_steps=args.training_steps,
            num_generations=args.num_generations,
            individuals_per_generation=args.individuals_per_generation,
            **extra_kwargs
        )
        
        print(f"\nðŸŽ‰ å¤šç½‘ç»œæŸå¤±æå–å®Œæˆï¼")
        print(f"ðŸ“ æŸå¤±æ•°æ®ä¿å­˜åœ¨: {log_dir}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ å¤šç½‘ç»œæŸå¤±æå–è¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å¤šç½‘ç»œæŸå¤±æå–å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
