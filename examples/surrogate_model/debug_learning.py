#!/usr/bin/env python3
"""
SACå­¦ä¹ èƒ½åŠ›è¯Šæ–­è„šæœ¬
ä¸€æ­¥æ­¥éªŒè¯æ¨¡å‹æ˜¯å¦èƒ½å­¦åˆ°æ­£ç¡®ç­–ç•¥
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import os
import sys

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(__file__), "../../")
sys.path.append(base_dir)
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/sac"))
sys.path.insert(0, os.path.join(base_dir, "examples/surrogate_model/attn_model"))
sys.path.insert(0, os.path.join(base_dir, "examples/2d_reacher/envs"))

from sac_model import AttentionSACWithBuffer
from attn_model import AttnModel
from reacher2d_env import Reacher2DEnv

class LearningDiagnostics:
    def __init__(self):
        print("ğŸ”¬ åˆå§‹åŒ–å­¦ä¹ è¯Šæ–­ç³»ç»Ÿ...")
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•ç¯å¢ƒ
        self.env = Reacher2DEnv(
            num_links=3, 
            link_lengths=[60, 60, 60],
            render_mode=None,  # ä¸æ¸²æŸ“ï¼Œæé«˜é€Ÿåº¦
            config_path="../2d_reacher/configs/reacher_with_zigzag_obstacles.yaml"
        )
        
        # åˆ›å»ºSACæ¨¡å‹
        self.attn_model = AttnModel(128, 128, 130, 4)
        self.sac = AttentionSACWithBuffer(
            self.attn_model, 
            action_dim=3,  # 3ä¸ªå…³èŠ‚
            lr=1e-4,  # æé«˜å­¦ä¹ ç‡
            batch_size=64,  # å°æ‰¹é‡ï¼Œå¿«é€Ÿæµ‹è¯•
            env_type='reacher2d'
        )
        
        print("âœ… åˆå§‹åŒ–å®Œæˆ")
    
    def step1_basic_functionality_test(self):
        """æ­¥éª¤1: æµ‹è¯•åŸºç¡€åŠŸèƒ½æ˜¯å¦æ­£å¸¸"""
        print("\n" + "="*60)
        print("ğŸ§ª æ­¥éª¤1: åŸºç¡€åŠŸèƒ½æµ‹è¯•")
        print("="*60)
        
        try:
            # 1.1 ç¯å¢ƒé‡ç½®æµ‹è¯•
            obs = self.env.reset()
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            print(f"âœ… ç¯å¢ƒé‡ç½®æˆåŠŸ: obs shape = {obs_tensor.shape}")
            
            # 1.2 GNNç¼–ç æµ‹è¯•
            gnn_embeds = torch.randn(3, 128)  # 3ä¸ªå…³èŠ‚çš„GNNåµŒå…¥
            print(f"âœ… GNNåµŒå…¥åˆ›å»ºæˆåŠŸ: {gnn_embeds.shape}")
            
            # 1.3 åŠ¨ä½œç”Ÿæˆæµ‹è¯•
            action = self.sac.get_action(obs_tensor, gnn_embeds, num_joints=3)
            print(f"âœ… åŠ¨ä½œç”ŸæˆæˆåŠŸ: {action.shape}, èŒƒå›´: [{action.min():.2f}, {action.max():.2f}]")
            
            # 1.4 ç¯å¢ƒäº¤äº’æµ‹è¯•
            next_obs, reward, done, info = self.env.step(action.numpy())
            print(f"âœ… ç¯å¢ƒäº¤äº’æˆåŠŸ: reward = {reward:.3f}, done = {done}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def step2_reward_signal_test(self):
        """æ­¥éª¤2: éªŒè¯å¥–åŠ±ä¿¡å·æ˜¯å¦åˆç†"""
        print("\n" + "="*60)
        print("ğŸ¯ æ­¥éª¤2: å¥–åŠ±ä¿¡å·æµ‹è¯•")
        print("="*60)
        
        rewards = []
        distances = []
        
        # æ”¶é›†ä¸åŒçŠ¶æ€ä¸‹çš„å¥–åŠ±
        for i in range(20):
            obs = self.env.reset()
            
            # éšæœºåŠ¨ä½œï¼Œè§‚å¯Ÿå¥–åŠ±å˜åŒ–
            for step in range(50):
                action = np.random.uniform(-50, 50, 3)  # ä¸­ç­‰å¼ºåº¦åŠ¨ä½œ
                next_obs, reward, done, info = self.env.step(action)
                
                # è·å–è·ç¦»ä¿¡æ¯
                end_pos = self.env._get_end_effector_position()
                distance = np.linalg.norm(np.array(end_pos) - self.env.goal_pos)
                
                rewards.append(reward)
                distances.append(distance)
                
                if done:
                    break
        
        # åˆ†æå¥–åŠ±ä¿¡å·
        rewards = np.array(rewards)
        distances = np.array(distances)
        
        print(f"ğŸ“Š å¥–åŠ±ç»Ÿè®¡:")
        print(f"   èŒƒå›´: [{rewards.min():.3f}, {rewards.max():.3f}]")
        print(f"   å¹³å‡: {rewards.mean():.3f}, æ ‡å‡†å·®: {rewards.std():.3f}")
        print(f"   è·ç¦»èŒƒå›´: [{distances.min():.1f}, {distances.max():.1f}]")
        
        # æ£€æŸ¥å¥–åŠ±ä¸è·ç¦»çš„ç›¸å…³æ€§
        correlation = np.corrcoef(rewards, distances)[0, 1]
        print(f"   å¥–åŠ±-è·ç¦»ç›¸å…³æ€§: {correlation:.3f} (åº”è¯¥æ˜¯è´Ÿå€¼)")
        
        if correlation < -0.1:
            print("âœ… å¥–åŠ±ä¿¡å·åˆç†ï¼šè·ç¦»è¶Šè¿‘ï¼Œå¥–åŠ±è¶Šé«˜")
            return True
        else:
            print("âŒ å¥–åŠ±ä¿¡å·å¯èƒ½æœ‰é—®é¢˜ï¼šç¼ºä¹æ˜ç¡®çš„è·ç¦»å¯¼å‘")
            return False
    
    def step3_action_effect_test(self):
        """æ­¥éª¤3: æµ‹è¯•åŠ¨ä½œæ˜¯å¦å¯¹ç¯å¢ƒäº§ç”Ÿé¢„æœŸæ•ˆæœ"""
        print("\n" + "="*60)
        print("ğŸ® æ­¥éª¤3: åŠ¨ä½œæ•ˆæœæµ‹è¯•")
        print("="*60)
        
        obs = self.env.reset()
        initial_pos = self.env._get_end_effector_position()
        
        # æµ‹è¯•ä¸åŒå¼ºåº¦çš„åŠ¨ä½œ
        action_effects = []
        
        for action_scale in [0, 25, 50, 100]:
            self.env.reset()  # é‡ç½®åˆ°ç›¸åŒåˆå§‹çŠ¶æ€
            
            # åº”ç”¨å›ºå®šåŠ¨ä½œ
            action = np.array([action_scale, 0, 0])  # åªåŠ¨ç¬¬ä¸€ä¸ªå…³èŠ‚
            
            positions = []
            for step in range(10):
                next_obs, reward, done, info = self.env.step(action)
                pos = self.env._get_end_effector_position()
                positions.append(pos)
            
            # è®¡ç®—ç§»åŠ¨è·ç¦»
            total_movement = 0
            for i in range(1, len(positions)):
                move = np.linalg.norm(np.array(positions[i]) - np.array(positions[i-1]))
                total_movement += move
            
            action_effects.append(total_movement)
            print(f"   åŠ¨ä½œå¼ºåº¦ {action_scale:3d}: æ€»ç§»åŠ¨è·ç¦» {total_movement:.1f}")
        
        # æ£€æŸ¥åŠ¨ä½œæ•ˆæœæ˜¯å¦é€’å¢
        is_increasing = all(action_effects[i] <= action_effects[i+1] for i in range(len(action_effects)-1))
        
        if is_increasing:
            print("âœ… åŠ¨ä½œæ•ˆæœæ­£å¸¸ï¼šæ›´å¤§çš„åŠ¨ä½œäº§ç”Ÿæ›´å¤§çš„ç§»åŠ¨")
            return True
        else:
            print("âŒ åŠ¨ä½œæ•ˆæœå¼‚å¸¸ï¼šåŠ¨ä½œå¼ºåº¦ä¸ç§»åŠ¨è·ç¦»ä¸æˆæ­£æ¯”")
            return False
    
    def step4_q_value_test(self):
        """æ­¥éª¤4: æµ‹è¯•Qå€¼ä¼°è®¡æ˜¯å¦åˆç†"""
        print("\n" + "="*60)
        print("ğŸ§  æ­¥éª¤4: Qå€¼ä¼°è®¡æµ‹è¯•")
        print("="*60)
        
        # æ”¶é›†ä¸€äº›ç»éªŒ
        print("ğŸ“š æ”¶é›†è®­ç»ƒæ•°æ®...")
        for episode in range(5):
            obs = self.env.reset()
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            gnn_embeds = torch.randn(3, 128)
            
            for step in range(20):
                action = self.sac.get_action(obs_tensor, gnn_embeds, num_joints=3)
                next_obs, reward, done, info = self.env.step(action.numpy())
                next_obs_tensor = torch.tensor(next_obs, dtype=torch.float32)
                next_gnn_embeds = torch.randn(3, 128)
                
                # å­˜å‚¨ç»éªŒ
                self.sac.store_experience(
                    obs_tensor, gnn_embeds, action, reward,
                    next_obs_tensor, next_gnn_embeds, done, num_joints=3
                )
                
                obs_tensor = next_obs_tensor
                gnn_embeds = next_gnn_embeds
                
                if done:
                    break
        
        print(f"ğŸ“Š Bufferå¤§å°: {len(self.sac.memory)}")
        
        # æµ‹è¯•Qå€¼è®­ç»ƒ
        if len(self.sac.memory) >= self.sac.batch_size:
            print("ğŸ”„ å¼€å§‹Qå€¼è®­ç»ƒæµ‹è¯•...")
            
            initial_losses = []
            final_losses = []
            
            # è®°å½•åˆå§‹æŸå¤±
            for _ in range(5):
                metrics = self.sac.update()
                if metrics:
                    initial_losses.append(metrics['critic_loss'])
            
            # å¤šæ¬¡æ›´æ–°
            for _ in range(100):
                metrics = self.sac.update()
            
            # è®°å½•æœ€ç»ˆæŸå¤±
            for _ in range(5):
                metrics = self.sac.update()
                if metrics:
                    final_losses.append(metrics['critic_loss'])
            
            if initial_losses and final_losses:
                initial_avg = np.mean(initial_losses)
                final_avg = np.mean(final_losses)
                
                print(f"ğŸ“ˆ Critic Losså˜åŒ–:")
                print(f"   åˆå§‹: {initial_avg:.3f}")
                print(f"   æœ€ç»ˆ: {final_avg:.3f}")
                print(f"   æ”¹å–„: {((initial_avg - final_avg) / initial_avg * 100):.1f}%")
                
                if final_avg < initial_avg:
                    print("âœ… Qå€¼å­¦ä¹ æ­£å¸¸ï¼šæŸå¤±åœ¨ä¸‹é™")
                    return True
                else:
                    print("âŒ Qå€¼å­¦ä¹ å¼‚å¸¸ï¼šæŸå¤±æ²¡æœ‰ä¸‹é™")
                    return False
            else:
                print("âŒ æ— æ³•è·å–æŸå¤±æ•°æ®")
                return False
        else:
            print("âŒ Bufferæ•°æ®ä¸è¶³ï¼Œæ— æ³•æµ‹è¯•")
            return False
    
    def step5_policy_improvement_test(self):
        """æ­¥éª¤5: æµ‹è¯•ç­–ç•¥æ˜¯å¦åœ¨æ”¹è¿›"""
        print("\n" + "="*60)
        print("ğŸ“ˆ æ­¥éª¤5: ç­–ç•¥æ”¹è¿›æµ‹è¯•")
        print("="*60)
        
        # æµ‹è¯•åˆå§‹ç­–ç•¥æ€§èƒ½
        initial_rewards = []
        for episode in range(5):
            obs = self.env.reset()
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            gnn_embeds = torch.randn(3, 128)
            episode_reward = 0
            
            for step in range(50):
                action = self.sac.get_action(obs_tensor, gnn_embeds, num_joints=3, deterministic=True)
                next_obs, reward, done, info = self.env.step(action.numpy())
                episode_reward += reward
                
                obs_tensor = torch.tensor(next_obs, dtype=torch.float32)
                gnn_embeds = torch.randn(3, 128)
                
                if done:
                    break
            
            initial_rewards.append(episode_reward)
        
        print(f"ğŸ¯ åˆå§‹ç­–ç•¥æ€§èƒ½: {np.mean(initial_rewards):.2f} Â± {np.std(initial_rewards):.2f}")
        
        # ç®€çŸ­è®­ç»ƒ
        print("ğŸƒâ€â™‚ï¸ è¿›è¡Œç®€çŸ­è®­ç»ƒ...")
        for episode in range(20):
            obs = self.env.reset()
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            gnn_embeds = torch.randn(3, 128)
            
            for step in range(30):
                action = self.sac.get_action(obs_tensor, gnn_embeds, num_joints=3)
                next_obs, reward, done, info = self.env.step(action.numpy())
                next_obs_tensor = torch.tensor(next_obs, dtype=torch.float32)
                next_gnn_embeds = torch.randn(3, 128)
                
                self.sac.store_experience(
                    obs_tensor, gnn_embeds, action, reward,
                    next_obs_tensor, next_gnn_embeds, done, num_joints=3
                )
                
                # æ¯å‡ æ­¥æ›´æ–°ä¸€æ¬¡
                if step % 4 == 0 and len(self.sac.memory) >= self.sac.batch_size:
                    self.sac.update()
                
                obs_tensor = next_obs_tensor
                gnn_embeds = next_gnn_embeds
                
                if done:
                    break
        
        # æµ‹è¯•è®­ç»ƒåç­–ç•¥æ€§èƒ½
        final_rewards = []
        for episode in range(5):
            obs = self.env.reset()
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            gnn_embeds = torch.randn(3, 128)
            episode_reward = 0
            
            for step in range(50):
                action = self.sac.get_action(obs_tensor, gnn_embeds, num_joints=3, deterministic=True)
                next_obs, reward, done, info = self.env.step(action.numpy())
                episode_reward += reward
                
                obs_tensor = torch.tensor(next_obs, dtype=torch.float32)
                gnn_embeds = torch.randn(3, 128)
                
                if done:
                    break
            
            final_rewards.append(episode_reward)
        
        print(f"ğŸ¯ è®­ç»ƒåç­–ç•¥æ€§èƒ½: {np.mean(final_rewards):.2f} Â± {np.std(final_rewards):.2f}")
        
        improvement = np.mean(final_rewards) - np.mean(initial_rewards)
        print(f"ğŸ“Š æ€§èƒ½æ”¹å–„: {improvement:.2f}")
        
        if improvement > 0:
            print("âœ… ç­–ç•¥æ­£åœ¨æ”¹è¿›")
            return True
        else:
            print("âŒ ç­–ç•¥æ²¡æœ‰æ˜æ˜¾æ”¹è¿›")
            return False
    
    def run_full_diagnostic(self):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("ğŸ”¬ å¼€å§‹SACå­¦ä¹ èƒ½åŠ›å…¨é¢è¯Šæ–­")
        print("="*80)
        
        test_results = {}
        
        # ä¾æ¬¡è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_results['basic'] = self.step1_basic_functionality_test()
        test_results['reward'] = self.step2_reward_signal_test()
        test_results['action'] = self.step3_action_effect_test()
        test_results['qvalue'] = self.step4_q_value_test()
        test_results['policy'] = self.step5_policy_improvement_test()
        
        # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        print("\n" + "="*80)
        print("ğŸ“‹ è¯Šæ–­æŠ¥å‘Š")
        print("="*80)
        
        passed = sum(test_results.values())
        total = len(test_results)
        
        for test_name, result in test_results.items():
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"{test_name:10s}: {status}")
        
        print(f"\nğŸ“Š æ€»ä½“è¯„ä¼°: {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡")
        
        if passed == total:
            print("ğŸ‰ æ­å–œï¼ä½ çš„æ¨¡å‹å­¦ä¹ èƒ½åŠ›æ­£å¸¸")
        elif passed >= total * 0.6:
            print("âš ï¸  æ¨¡å‹æœ‰å­¦ä¹ èƒ½åŠ›ï¼Œä½†å­˜åœ¨ä¸€äº›é—®é¢˜éœ€è¦è°ƒä¼˜")
        else:
            print("ğŸš¨ æ¨¡å‹å­¦ä¹ èƒ½åŠ›å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥æ¶æ„æˆ–è®­ç»ƒæ–¹æ³•")
        
        return test_results

def main():
    """ä¸»å‡½æ•°"""
    diagnostics = LearningDiagnostics()
    results = diagnostics.run_full_diagnostic()
    
    print("\nğŸ”§ å»ºè®®çš„åç»­è¡ŒåŠ¨:")
    if not results['basic']:
        print("   1. æ£€æŸ¥ç¯å¢ƒå’Œæ¨¡å‹çš„åŸºæœ¬é…ç½®")
    if not results['reward']:
        print("   2. è°ƒæ•´å¥–åŠ±å‡½æ•°ï¼Œç¡®ä¿ä¿¡å·æ˜ç¡®")
    if not results['action']:
        print("   3. æ£€æŸ¥åŠ¨ä½œç¼©æ”¾å’Œç¯å¢ƒäº¤äº’")
    if not results['qvalue']:
        print("   4. è°ƒæ•´å­¦ä¹ ç‡æˆ–ç½‘ç»œæ¶æ„")
    if not results['policy']:
        print("   5. å¢åŠ è®­ç»ƒæ—¶é—´æˆ–è°ƒæ•´æ¢ç´¢ç­–ç•¥")

if __name__ == "__main__":
    main()
