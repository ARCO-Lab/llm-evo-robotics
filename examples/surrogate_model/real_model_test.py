#!/usr/bin/env python3
"""
çœŸå®æ¨¡å‹æµ‹è¯•è„šæœ¬ - ä½¿ç”¨è®­ç»ƒå¥½çš„Actorç½‘ç»œæ¥ç”ŸæˆåŠ¨ä½œ
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os
import time

# è®¾ç½®è·¯å¾„
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../')
sys.path.append(base_dir)
sys.path.insert(0, os.path.join(base_dir, 'examples/2d_reacher/envs'))

from reacher2d_env import Reacher2DEnv


class SimpleActor(nn.Module):
    """ç®€åŒ–çš„Actorç½‘ç»œï¼Œç›´æ¥ä»observationç”Ÿæˆaction"""
    def __init__(self, obs_dim, action_dim, hidden_dim=256):
        super(SimpleActor, self).__init__()
        
        self.net = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim * 2)  # mean and log_std
        )
        
        self.action_dim = action_dim
        
    def forward(self, obs):
        x = self.net(obs)
        mean, log_std = torch.split(x, self.action_dim, dim=-1)
        
        # é™åˆ¶log_stdèŒƒå›´
        log_std = torch.clamp(log_std, -20, 2)
        
        return mean, log_std
    
    def get_action(self, obs, deterministic=False):
        """è·å–åŠ¨ä½œ"""
        mean, log_std = self.forward(obs)
        
        if deterministic:
            return torch.tanh(mean)
        else:
            std = torch.exp(log_std)
            dist = torch.distributions.Normal(mean, std)
            x_t = dist.rsample()
            action = torch.tanh(x_t)
            return action


def load_and_extract_actor_weights(model_path):
    """åŠ è½½æ¨¡å‹å¹¶æå–Actorç½‘ç»œçš„æƒé‡"""
    print(f"ğŸ” åˆ†ææ¨¡å‹æ–‡ä»¶: {model_path}")
    
    model_data = torch.load(model_path, map_location='cpu')
    
    print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    print(f"   è®­ç»ƒæ­¥éª¤: {model_data.get('step', 'N/A')}")
    print(f"   æˆåŠŸç‡: {model_data.get('final_success_rate', model_data.get('success_rate', 'N/A'))}")
    print(f"   æœ€å°è·ç¦»: {model_data.get('final_min_distance', model_data.get('min_distance', 'N/A'))}")
    print(f"   è®­ç»ƒå®Œæˆ: {model_data.get('training_completed', 'N/A')}")
    
    # æå–ActorçŠ¶æ€å­—å…¸
    actor_state_dict = model_data.get('actor_state_dict', {})
    print(f"\nğŸ§  Actorç½‘ç»œç»“æ„:")
    for key, tensor in actor_state_dict.items():
        print(f"   {key}: {tensor.shape}")
    
    return actor_state_dict, model_data


def test_with_real_model(model_path, num_episodes=3):
    """ä½¿ç”¨çœŸå®çš„è®­ç»ƒæ¨¡å‹è¿›è¡Œæµ‹è¯•"""
    
    print(f"ğŸ¯ ä½¿ç”¨çœŸå®æ¨¡å‹æµ‹è¯•: {model_path}")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return
    
    # åŠ è½½æ¨¡å‹æƒé‡
    actor_state_dict, model_info = load_and_extract_actor_weights(model_path)
    
    # åˆ›å»ºç¯å¢ƒ
    env_params = {
        'num_links': 4,
        'link_lengths': [80, 80, 80, 60],
        'render_mode': 'human',
        'config_path': "/home/xli149/Documents/repos/RoboGrammar/examples/2d_reacher/configs/reacher_with_zigzag_obstacles.yaml"
    }
    
    env = Reacher2DEnv(**env_params)
    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    
    print(f"ğŸ—ï¸ ç¯å¢ƒé…ç½®:")
    print(f"   è§‚å¯Ÿç»´åº¦: {obs_dim}")
    print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"   Action space: {env.action_space}")
    
    # æ–¹æ³•1: å°è¯•åˆ›å»ºç®€åŒ–çš„Actorç½‘ç»œ
    try:
        print(f"\nğŸ¤– å°è¯•åˆ›å»ºç®€åŒ–Actorç½‘ç»œ...")
        simple_actor = SimpleActor(obs_dim, action_dim)
        
        # å°è¯•åŠ è½½æƒé‡ï¼ˆå¯èƒ½ä¸å®Œå…¨åŒ¹é…ï¼Œä½†å¯ä»¥éƒ¨åˆ†åŠ è½½ï¼‰
        try:
            # è¿‡æ»¤å‡ºå¯ä»¥åŒ¹é…çš„æƒé‡
            filtered_state_dict = {}
            for key, value in actor_state_dict.items():
                if key in simple_actor.state_dict():
                    if simple_actor.state_dict()[key].shape == value.shape:
                        filtered_state_dict[key] = value
                        print(f"   âœ… åŒ¹é…æƒé‡: {key}")
                    else:
                        print(f"   âš ï¸ å½¢çŠ¶ä¸åŒ¹é…: {key} æœŸæœ›{simple_actor.state_dict()[key].shape} å¾—åˆ°{value.shape}")
                else:
                    print(f"   âŒ æœªæ‰¾åˆ°å¯¹åº”å±‚: {key}")
            
            if filtered_state_dict:
                simple_actor.load_state_dict(filtered_state_dict, strict=False)
                print(f"âœ… éƒ¨åˆ†åŠ è½½äº† {len(filtered_state_dict)} ä¸ªæƒé‡å±‚")
                use_trained_model = True
            else:
                print(f"âŒ æ²¡æœ‰åŒ¹é…çš„æƒé‡ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ç½‘ç»œ")
                use_trained_model = False
                
        except Exception as e:
            print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
            use_trained_model = False
            
    except Exception as e:
        print(f"âŒ åˆ›å»ºActorç½‘ç»œå¤±è´¥: {e}")
        print(f"ğŸ”„ å›é€€åˆ°å¯å‘å¼ç­–ç•¥")
        simple_actor = None
        use_trained_model = False
    
    # å¼€å§‹æµ‹è¯•
    print(f"\nğŸ® å¼€å§‹æµ‹è¯• {num_episodes} episodes")
    if use_trained_model and simple_actor is not None:
        print(f"   ä½¿ç”¨ç­–ç•¥: è®­ç»ƒå¥½çš„Actorç½‘ç»œ")
    else:
        print(f"   ä½¿ç”¨ç­–ç•¥: æ”¹è¿›çš„å¯å‘å¼ç­–ç•¥")
    
    success_count = 0
    total_rewards = []
    min_distances = []
    goal_threshold = 50.0
    
    for episode in range(num_episodes):
        obs = env.reset()
        episode_reward = 0
        step_count = 0
        max_steps = 500
        min_distance_this_episode = float('inf')
        
        print(f"\n--- Episode {episode + 1}/{num_episodes} ---")
        
        while step_count < max_steps:
            # è®¡ç®—å½“å‰è·ç¦»
            end_pos = env._get_end_effector_position()
            goal_pos = env.goal_pos
            distance = np.linalg.norm(np.array(end_pos) - goal_pos)
            min_distance_this_episode = min(min_distance_this_episode, distance)
            
            # ç”ŸæˆåŠ¨ä½œ
            if use_trained_model and simple_actor is not None:
                # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
                with torch.no_grad():
                    obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
                    action_tensor = simple_actor.get_action(obs_tensor, deterministic=True)
                    actions = action_tensor.squeeze(0).numpy()
                    
                    # ç¼©æ”¾åˆ°ç¯å¢ƒçš„åŠ¨ä½œèŒƒå›´
                    actions = actions * env.action_space.high[0]  # tanhè¾“å‡º[-1,1]ç¼©æ”¾åˆ°[-100,100]
            else:
                # ä½¿ç”¨æ”¹è¿›çš„å¯å‘å¼ç­–ç•¥
                direction = np.array(goal_pos) - np.array(end_pos)
                
                if distance > 1e-6:
                    direction_norm = direction / distance
                    
                    # æ ¹æ®è·ç¦»è°ƒæ•´åŠ¨ä½œå¼ºåº¦
                    if distance > 200:
                        action_magnitude = 30.0  # è¿œè·ç¦»æ—¶è¾ƒå¤§åŠ¨ä½œ
                    elif distance > 100:
                        action_magnitude = 20.0  # ä¸­ç­‰è·ç¦»
                    else:
                        action_magnitude = 10.0  # è¿‘è·ç¦»æ—¶å°å¿ƒåŠ¨ä½œ
                    
                    # ä¸ºæ¯ä¸ªå…³èŠ‚ç”ŸæˆåŠ¨ä½œ
                    actions = np.array([
                        action_magnitude * direction_norm[0] * 0.4,
                        action_magnitude * direction_norm[1] * 0.4,
                        action_magnitude * direction_norm[0] * 0.3,
                        action_magnitude * direction_norm[1] * 0.3,
                    ])
                    
                    # æ·»åŠ å°å¹…å™ªå£°
                    noise = np.random.normal(0, 2.0, size=actions.shape)
                    actions = actions + noise
                    
                    # è£å‰ªåˆ°åŠ¨ä½œç©ºé—´
                    actions = np.clip(actions, env.action_space.low, env.action_space.high)
                else:
                    # è·ç¦»å¾ˆè¿‘æ—¶ä½¿ç”¨å¾®è°ƒ
                    actions = np.random.uniform(-5, 5, size=action_dim)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = env.step(actions)
            episode_reward += reward
            step_count += 1
            
            # æ¸²æŸ“
            env.render()
            time.sleep(0.02)
            
            # æ¯50æ­¥æ‰“å°è¿›åº¦
            if step_count % 50 == 0:
                print(f"  æ­¥éª¤ {step_count}: è·ç¦» {distance:.1f}px, å¥–åŠ± {episode_reward:.1f}")
                if use_trained_model:
                    print(f"    åŠ¨ä½œ (æ¨¡å‹): [{actions[0]:+6.2f}, {actions[1]:+6.2f}, {actions[2]:+6.2f}, {actions[3]:+6.2f}]")
            
            # æ£€æŸ¥æˆåŠŸ
            if distance <= goal_threshold:
                success_count += 1
                print(f"  ğŸ‰ æˆåŠŸåˆ°è¾¾ç›®æ ‡! è·ç¦»: {distance:.1f}px, æ­¥éª¤: {step_count}")
                break
                
            if done:
                print(f"  Episode ç»“æŸ (done=True)")
                break
        
        # è®°å½•ç»“æœ
        total_rewards.append(episode_reward)
        min_distances.append(min_distance_this_episode)
        
        print(f"Episode {episode + 1} ç»“æœ:")
        print(f"  æœ€å°è·ç¦»: {min_distance_this_episode:.1f}px")
        print(f"  æ€»å¥–åŠ±: {episode_reward:.2f}")
        print(f"  æ­¥éª¤æ•°: {step_count}")
        print(f"  æˆåŠŸ: {'æ˜¯' if min_distance_this_episode <= goal_threshold else 'å¦'}")
    
    # è®¡ç®—ç»Ÿè®¡
    success_rate = success_count / num_episodes
    avg_reward = np.mean(total_rewards)
    avg_min_distance = np.mean(min_distances)
    
    # æ‰“å°æ€»ç»“
    strategy_name = "è®­ç»ƒæ¨¡å‹" if use_trained_model else "å¯å‘å¼ç­–ç•¥"
    print(f"\n{'='*60}")
    print(f"ğŸ† {strategy_name}æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"  æµ‹è¯•Episodes: {num_episodes}")
    print(f"  æˆåŠŸæ¬¡æ•°: {success_count}")
    print(f"  æˆåŠŸç‡: {success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"  å¹³å‡æœ€å°è·ç¦»: {avg_min_distance:.1f} pixels")
    print(f"  ç›®æ ‡é˜ˆå€¼: {goal_threshold:.1f} pixels")
    print(f"{'='*60}")
    
    if not use_trained_model:
        print(f"\nğŸ’¡ æ³¨æ„: ç”±äºæ¨¡å‹åŠ è½½é—®é¢˜ï¼Œä½¿ç”¨äº†å¯å‘å¼ç­–ç•¥")
        print(f"   è¦æµ‹è¯•çœŸå®æ¨¡å‹ï¼Œéœ€è¦:")
        print(f"   1. ä¿®å¤GNNç¼–ç å™¨çš„æ•°æ®ç±»å‹é—®é¢˜")
        print(f"   2. æˆ–è€…é‡æ–°æ„å»ºä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„ç½‘ç»œç»“æ„")
    
    env.close()
    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward,
        'avg_min_distance': avg_min_distance,
        'success_count': success_count,
        'total_episodes': num_episodes,
        'used_trained_model': use_trained_model
    }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ä½¿ç”¨çœŸå®è®­ç»ƒæ¨¡å‹è¿›è¡Œæµ‹è¯•")
    parser.add_argument('--model-path', type=str, required=True,
                        help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=3,
                        help='æµ‹è¯•çš„episodeæ•°é‡')
    
    args = parser.parse_args()
    
    test_with_real_model(args.model_path, args.episodes) 