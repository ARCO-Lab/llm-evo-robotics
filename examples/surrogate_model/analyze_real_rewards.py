#!/usr/bin/env python3
"""
åˆ†æçœŸå®å¥–åŠ±èŒƒå›´å’ŒCritic lossè®¡ç®—
æ‰¾å‡ºä¸ºä»€ä¹ˆCritic lossè¿˜æ˜¯åé«˜çš„åŸå› 
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import sys
import os

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(__file__), "../../")
sys.path.append(base_dir)
sys.path.append(os.path.join(base_dir, "examples/2d_reacher/envs"))
sys.path.append(os.path.join(base_dir, "examples/2d_reacher/utils"))

from reacher2d_env import Reacher2DEnv
from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
from sac.universal_ppo_model import UniversalAttnModel, UniversalPPOWithBuffer

def analyze_real_rewards_and_critic():
    """åˆ†æçœŸå®å¥–åŠ±èŒƒå›´å’ŒCriticé¢„æµ‹"""
    
    print("ğŸ” åˆ†æçœŸå®å¥–åŠ±èŒƒå›´å’ŒCritic Lossè®¡ç®—...")
    print("=" * 60)
    
    # === 1. åˆ›å»ºç¯å¢ƒå¹¶æ”¶é›†çœŸå®å¥–åŠ± ===
    print("\nğŸ“Š æ­¥éª¤1: æ”¶é›†çœŸå®ç¯å¢ƒå¥–åŠ±")
    
    env = Reacher2DEnv(num_links=5, render_mode=None)
    gnn_encoder = Reacher2D_GNN_Encoder()
    
    rewards_collected = []
    episode_returns = []
    
    # æ”¶é›†å¤šä¸ªepisodeçš„å¥–åŠ±
    for episode in range(3):
        obs = env.reset()
        episode_rewards = []
        episode_return = 0
        
        for step in range(100):  # æ¯ä¸ªepisode 100æ­¥
            # éšæœºåŠ¨ä½œ
            action = np.random.uniform(-10, 10, size=env.num_links)
            obs, reward, done, info = env.step(action)
            
            episode_rewards.append(reward)
            episode_return += reward
            rewards_collected.append(reward)
            
            if done:
                break
        
        episode_returns.append(episode_return)
        print(f"   Episode {episode+1}: å•æ­¥å¥–åŠ±èŒƒå›´ [{min(episode_rewards):.3f}, {max(episode_rewards):.3f}], ç´¯ç§¯å¥–åŠ±: {episode_return:.3f}")
    
    rewards_array = np.array(rewards_collected)
    returns_array = np.array(episode_returns)
    
    print(f"\nğŸ“Š çœŸå®å¥–åŠ±ç»Ÿè®¡:")
    print(f"   å•æ­¥å¥–åŠ±èŒƒå›´: [{rewards_array.min():.3f}, {rewards_array.max():.3f}]")
    print(f"   å•æ­¥å¥–åŠ±å‡å€¼: {rewards_array.mean():.3f}")
    print(f"   å•æ­¥å¥–åŠ±æ ‡å‡†å·®: {rewards_array.std():.3f}")
    print(f"   Episodeç´¯ç§¯å¥–åŠ±èŒƒå›´: [{returns_array.min():.3f}, {returns_array.max():.3f}]")
    print(f"   Episodeç´¯ç§¯å¥–åŠ±å‡å€¼: {returns_array.mean():.3f}")
    
    # === 2. åˆ†æCriticç½‘ç»œé¢„æµ‹èŒƒå›´ ===
    print(f"\nğŸ›ï¸ æ­¥éª¤2: åˆ†æCriticç½‘ç»œé¢„æµ‹èŒƒå›´")
    
    device = 'cpu'
    ppo_agent = UniversalPPOWithBuffer(
        buffer_size=1024,
        batch_size=32,
        device=device
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    obs = env.reset()
    gnn_embeds = gnn_encoder.encode(obs)
    joint_q, vertex_k, vertex_v = ppo_agent._prepare_inputs(obs, gnn_embeds, env.num_links)
    
    # æµ‹è¯•Criticé¢„æµ‹èŒƒå›´
    with torch.no_grad():
        critic_values = []
        for _ in range(50):  # å¤šæ¬¡éšæœºæµ‹è¯•
            # éšæœºæ‰°åŠ¨è¾“å…¥
            noise = torch.randn_like(joint_q) * 0.1
            noisy_input = joint_q + noise
            
            value = ppo_agent.critic(noisy_input, vertex_k, vertex_v)
            critic_values.append(value.item())
    
    critic_array = np.array(critic_values)
    print(f"   Criticé¢„æµ‹èŒƒå›´: [{critic_array.min():.3f}, {critic_array.max():.3f}]")
    print(f"   Criticé¢„æµ‹å‡å€¼: {critic_array.mean():.3f}")
    print(f"   Criticé¢„æµ‹æ ‡å‡†å·®: {critic_array.std():.3f}")
    print(f"   Critic Value Scale: {ppo_agent.critic.value_scale}")
    
    # === 3. æ¨¡æ‹ŸReturnsè®¡ç®— ===
    print(f"\nğŸ”„ æ­¥éª¤3: æ¨¡æ‹ŸReturnsè®¡ç®—è¿‡ç¨‹")
    
    # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„trajectory
    simulated_rewards = rewards_array[:10]  # å–å‰10ä¸ªçœŸå®å¥–åŠ±
    simulated_values = critic_array[:10]    # å–å‰10ä¸ªCriticé¢„æµ‹
    
    # è®¡ç®—returnsï¼ˆç®€åŒ–ç‰ˆGAEï¼‰
    gamma = 0.99
    gae_lambda = 0.95
    
    # å¥–åŠ±å½’ä¸€åŒ–ï¼ˆå’Œä»£ç ä¸­ä¸€æ ·ï¼‰
    if len(simulated_rewards) > 1:
        reward_mean = simulated_rewards.mean()
        reward_std = simulated_rewards.std() + 1e-8
        normalized_rewards = (simulated_rewards - reward_mean) / reward_std
    else:
        normalized_rewards = simulated_rewards
    
    print(f"   åŸå§‹å¥–åŠ±èŒƒå›´: [{simulated_rewards.min():.3f}, {simulated_rewards.max():.3f}]")
    print(f"   å½’ä¸€åŒ–åå¥–åŠ±èŒƒå›´: [{normalized_rewards.min():.3f}, {normalized_rewards.max():.3f}]")
    
    # è®¡ç®—returns
    returns = np.zeros_like(normalized_rewards)
    advantages = np.zeros_like(normalized_rewards)
    gae = 0
    
    for step in reversed(range(len(normalized_rewards))):
        if step == len(normalized_rewards) - 1:
            next_value = 0  # æœ€åä¸€æ­¥
        else:
            next_value = simulated_values[step + 1]
        
        delta = normalized_rewards[step] + gamma * next_value - simulated_values[step]
        gae = delta + gamma * gae_lambda * gae
        advantages[step] = gae
        returns[step] = advantages[step] + simulated_values[step]
    
    print(f"   è®¡ç®—å‡ºçš„ReturnsèŒƒå›´: [{returns.min():.3f}, {returns.max():.3f}]")
    print(f"   è®¡ç®—å‡ºçš„AdvantagesèŒƒå›´: [{advantages.min():.3f}, {advantages.max():.3f}]")
    
    # === 4. åˆ†æCritic Loss ===
    print(f"\nğŸ“Š æ­¥éª¤4: åˆ†æCritic Losså¤§å°")
    
    # æ¨¡æ‹Ÿcritic lossè®¡ç®—
    predicted_values = np.array(simulated_values[:len(returns)])
    actual_returns = returns
    
    # ä¸åŒlosså‡½æ•°çš„ç»“æœ
    mse_loss = np.mean((predicted_values - actual_returns) ** 2)
    mae_loss = np.mean(np.abs(predicted_values - actual_returns))
    smooth_l1_loss = np.mean(np.where(
        np.abs(predicted_values - actual_returns) < 1.0,
        0.5 * (predicted_values - actual_returns) ** 2,
        np.abs(predicted_values - actual_returns) - 0.5
    ))
    
    print(f"   é¢„æµ‹å€¼èŒƒå›´: [{predicted_values.min():.3f}, {predicted_values.max():.3f}]")
    print(f"   çœŸå®ReturnsèŒƒå›´: [{actual_returns.min():.3f}, {actual_returns.max():.3f}]")
    print(f"   é¢„æµ‹è¯¯å·®(ç»å¯¹å€¼): [{np.abs(predicted_values - actual_returns).min():.3f}, {np.abs(predicted_values - actual_returns).max():.3f}]")
    print(f"   MSE Loss: {mse_loss:.3f}")
    print(f"   MAE Loss: {mae_loss:.3f}")
    print(f"   Smooth L1 Loss: {smooth_l1_loss:.3f}")
    
    # === 5. è¯Šæ–­é—®é¢˜ ===
    print(f"\nğŸ” æ­¥éª¤5: é—®é¢˜è¯Šæ–­")
    
    issues = []
    
    # æ£€æŸ¥æ•°å€¼èŒƒå›´åŒ¹é…
    reward_range = rewards_array.max() - rewards_array.min()
    critic_range = critic_array.max() - critic_array.min()
    range_ratio = critic_range / reward_range if reward_range > 0 else float('inf')
    
    print(f"   ç¯å¢ƒå¥–åŠ±èŒƒå›´: {reward_range:.3f}")
    print(f"   Criticé¢„æµ‹èŒƒå›´: {critic_range:.3f}")
    print(f"   èŒƒå›´æ¯”ä¾‹: {range_ratio:.3f}")
    
    if range_ratio > 5 or range_ratio < 0.2:
        issues.append(f"Criticé¢„æµ‹èŒƒå›´ä¸ç¯å¢ƒå¥–åŠ±èŒƒå›´ä¸åŒ¹é… (æ¯”ä¾‹: {range_ratio:.2f})")
    
    # æ£€æŸ¥å¥–åŠ±å½’ä¸€åŒ–çš„å½±å“
    if reward_std > 1.0:
        issues.append(f"å¥–åŠ±æ ‡å‡†å·®è¿‡å¤§ ({reward_std:.3f})ï¼Œå½’ä¸€åŒ–å¯èƒ½å¯¼è‡´æ•°å€¼å¤±çœŸ")
    
    # æ£€æŸ¥losså¤§å°
    if smooth_l1_loss > 5.0:
        issues.append(f"Smooth L1 Lossä»ç„¶è¿‡å¤§ ({smooth_l1_loss:.3f})")
    
    if len(issues) == 0:
        print("   âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
    else:
        print("   âš ï¸ å‘ç°çš„é—®é¢˜:")
        for i, issue in enumerate(issues, 1):
            print(f"     {i}. {issue}")
    
    # === 6. æ¨èçš„è¿›ä¸€æ­¥ä¿®å¤ ===
    print(f"\nğŸ”§ æ¨èçš„è¿›ä¸€æ­¥ä¿®å¤:")
    
    if range_ratio > 5:
        print("   1. è¿›ä¸€æ­¥é™ä½value_scale")
        new_scale = max(5.0, reward_range * 1.5)
        print(f"      æ¨è: self.value_scale = {new_scale:.1f}")
    
    if smooth_l1_loss > 5.0:
        print("   2. è°ƒæ•´losså‡½æ•°æˆ–æ·»åŠ æ›´ä¸¥æ ¼çš„è¾“å‡ºé™åˆ¶")
        print("      - ä½¿ç”¨MSE lossæ›¿ä»£smooth_l1_loss")
        print("      - æˆ–è€…è¿›ä¸€æ­¥é™åˆ¶valueè¾“å‡ºèŒƒå›´")
    
    print("   3. æ£€æŸ¥å¥–åŠ±å½’ä¸€åŒ–æ˜¯å¦åˆé€‚")
    print("      - è€ƒè™‘ä¸è¿›è¡Œå¥–åŠ±å½’ä¸€åŒ–")
    print("      - æˆ–è€…ä½¿ç”¨æ›´æ¸©å’Œçš„å½’ä¸€åŒ–æ–¹æ³•")
    
    return {
        'reward_range': [rewards_array.min(), rewards_array.max()],
        'critic_range': [critic_array.min(), critic_array.max()],
        'predicted_loss': smooth_l1_loss,
        'issues': issues
    }

if __name__ == "__main__":
    try:
        results = analyze_real_rewards_and_critic()
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"ä¸»è¦å‘ç°: {len(results['issues'])} ä¸ªæ½œåœ¨é—®é¢˜")
    except Exception as e:
        print(f"\nâŒ åˆ†æå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
