#!/usr/bin/env python3
"""
ç¥ç»ç½‘ç»œè®­ç»ƒLosså¯è§†åŒ–å·¥å…·
åŠŸèƒ½ï¼š
1. å¯è§†åŒ–å„ä¸ªç½‘ç»œçš„losså˜åŒ–ï¼ˆAttentionã€GNNã€PPOã€SACç­‰ï¼‰
2. å®æ—¶ç›‘æ§è®­ç»ƒè¿‡ç¨‹
3. ç”Ÿæˆç»¼åˆlossåˆ†ææŠ¥å‘Š
4. æ”¯æŒå¤šç½‘ç»œå¯¹æ¯”åˆ†æ
"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation
try:
    import seaborn as sns
    SEABORN_AVAILABLE = True
except ImportError:
    SEABORN_AVAILABLE = False

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
from typing import Dict, List, Optional, Tuple, Any, Union
import argparse
from datetime import datetime
import glob
import pickle
from collections import defaultdict, deque
import threading
import time

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

class NetworkLossVisualizer:
    """ç¥ç»ç½‘ç»œè®­ç»ƒLosså¯è§†åŒ–å™¨"""
    
    def __init__(self, log_dir: str = "./training_logs", output_dir: str = "./loss_visualizations"):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        Args:
            log_dir: è®­ç»ƒæ—¥å¿—ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        """
        self.log_dir = log_dir
        self.output_dir = output_dir
        self.training_data = {}
        self.network_types = ['actor', 'critic', 'attention', 'gnn', 'ppo', 'sac', 'alpha']
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # è®¾ç½®matplotlibæ ·å¼
        try:
            if SEABORN_AVAILABLE:
                plt.style.use('seaborn-v0_8')
                sns.set_palette("husl")
            else:
                plt.style.use('default')
        except:
            plt.style.use('default')
        
        # é¢œè‰²æ˜ å°„
        self.color_map = {
            'actor_loss': '#FF6B6B',      # çº¢è‰²
            'critic_loss': '#4ECDC4',     # é’è‰²
            'attention_loss': '#45B7D1',  # è“è‰²
            'gnn_loss': '#96CEB4',        # ç»¿è‰²
            'ppo_loss': '#FFEAA7',        # é»„è‰²
            'sac_loss': '#DDA0DD',        # ç´«è‰²
            'alpha_loss': '#FFA07A',      # æ©™è‰²
            'total_loss': '#2C3E50',      # æ·±ç°è‰²
            'entropy': '#E17055',         # æ·±æ©™è‰²
            'learning_rate': '#00B894'    # æ·±ç»¿è‰²
        }
    
    def load_training_logs(self, experiment_path: Optional[str] = None) -> bool:
        """
        åŠ è½½è®­ç»ƒæ—¥å¿—æ•°æ®
        
        Args:
            experiment_path: å®éªŒè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°å®éªŒ
            
        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        if experiment_path is None:
            experiment_path = self._find_latest_experiment()
        
        if not experiment_path:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ—¥å¿—")
            return False
        
        print(f"ğŸ“Š æ­£åœ¨åŠ è½½è®­ç»ƒæ—¥å¿—: {experiment_path}")
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ—¥å¿—æ–‡ä»¶
            log_files = {
                'metrics': os.path.join(experiment_path, 'metrics.json'),
                'losses': os.path.join(experiment_path, 'losses.json'),
                'training_log': os.path.join(experiment_path, 'training_log.json'),
                'config': os.path.join(experiment_path, 'config.json')
            }
            
            # åŠ è½½æ¯ä¸ªæ–‡ä»¶
            for log_type, log_file in log_files.items():
                if os.path.exists(log_file):
                    with open(log_file, 'r') as f:
                        self.training_data[log_type] = json.load(f)
                    print(f"âœ… åŠ è½½ {log_type}: {len(self.training_data[log_type])} æ¡è®°å½•")
                else:
                    print(f"âš ï¸  æœªæ‰¾åˆ° {log_type} æ–‡ä»¶: {log_file}")
            
            # å°è¯•ä»pickleæ–‡ä»¶åŠ è½½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            pickle_files = glob.glob(os.path.join(experiment_path, '*.pkl'))
            for pickle_file in pickle_files:
                try:
                    with open(pickle_file, 'rb') as f:
                        data = pickle.load(f)
                        filename = os.path.basename(pickle_file).replace('.pkl', '')
                        self.training_data[filename] = data
                        print(f"âœ… åŠ è½½ {filename} (pickle)")
                except Exception as e:
                    print(f"âš ï¸  æ— æ³•åŠ è½½ {pickle_file}: {e}")
            
            return len(self.training_data) > 0
            
        except Exception as e:
            print(f"âŒ åŠ è½½è®­ç»ƒæ—¥å¿—å¤±è´¥: {e}")
            return False
    
    def _find_latest_experiment(self) -> Optional[str]:
        """æŸ¥æ‰¾æœ€æ–°çš„å®éªŒç›®å½•"""
        if not os.path.exists(self.log_dir):
            print(f"âŒ è®­ç»ƒæ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {self.log_dir}")
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰å®éªŒç›®å½•
        experiment_dirs = []
        for item in os.listdir(self.log_dir):
            item_path = os.path.join(self.log_dir, item)
            if os.path.isdir(item_path):
                experiment_dirs.append(item_path)
        
        if not experiment_dirs:
            print("âŒ æœªæ‰¾åˆ°å®éªŒç›®å½•")
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
        latest_dir = max(experiment_dirs, key=os.path.getmtime)
        print(f"ğŸ” æ‰¾åˆ°æœ€æ–°å®éªŒ: {latest_dir}")
        return latest_dir
    
    def create_loss_curves(self, save_path: Optional[str] = None, 
                          figsize: Tuple[int, int] = (16, 12),
                          networks: Optional[List[str]] = None) -> str:
        """
        åˆ›å»ºæŸå¤±æ›²çº¿å›¾
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            figsize: å›¾ç‰‡å¤§å°
            networks: è¦æ˜¾ç¤ºçš„ç½‘ç»œç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™æ˜¾ç¤ºæ‰€æœ‰
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not self.training_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return None
        
        # è§£æè®­ç»ƒæ•°æ®
        parsed_data = self._parse_training_data()
        if not parsed_data:
            print("âŒ æ— æ³•è§£æè®­ç»ƒæ•°æ®")
            return None
        
        networks = networks or self.network_types
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        fig.suptitle('ç¥ç»ç½‘ç»œè®­ç»ƒLossåˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. ä¸»è¦Lossæ›²çº¿
        ax1 = axes[0, 0]
        self._plot_main_losses(ax1, parsed_data, networks)
        
        # 2. å­¦ä¹ ç‡å’ŒAlphaå˜åŒ–
        ax2 = axes[0, 1]
        self._plot_hyperparameters(ax2, parsed_data)
        
        # 3. Lossè¶‹åŠ¿åˆ†æ
        ax3 = axes[1, 0]
        self._plot_loss_trends(ax3, parsed_data)
        
        # 4. ç½‘ç»œæ€§èƒ½æŒ‡æ ‡
        ax4 = axes[1, 1]
        self._plot_performance_metrics(ax4, parsed_data)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        if save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(self.output_dir, f'loss_curves_{timestamp}.png')
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Lossæ›²çº¿å›¾å·²ä¿å­˜: {save_path}")
        
        return save_path
    
    def _parse_training_data(self) -> Dict[str, Any]:
        """è§£æè®­ç»ƒæ•°æ®"""
        parsed = defaultdict(list)
        
        # ä»ä¸åŒçš„æ•°æ®æºè§£æ
        for data_type, data in self.training_data.items():
            if data_type == 'metrics' and isinstance(data, list):
                for record in data:
                    if isinstance(record, dict):
                        for key, value in record.items():
                            if isinstance(value, (int, float)):
                                parsed[key].append(value)
            
            elif data_type == 'losses' and isinstance(data, dict):
                for key, values in data.items():
                    if isinstance(values, list):
                        parsed[key].extend(values)
            
            elif data_type == 'training_log' and isinstance(data, list):
                for record in data:
                    if isinstance(record, dict) and 'metrics' in record:
                        metrics = record['metrics']
                        for key, value in metrics.items():
                            if isinstance(value, (int, float)):
                                parsed[key].append(value)
        
        # ç¡®ä¿æ‰€æœ‰åˆ—è¡¨é•¿åº¦ä¸€è‡´
        max_length = max(len(values) for values in parsed.values()) if parsed else 0
        for key in list(parsed.keys()):
            if len(parsed[key]) < max_length:
                # ç”¨æœ€åä¸€ä¸ªå€¼å¡«å……
                last_value = parsed[key][-1] if parsed[key] else 0
                parsed[key].extend([last_value] * (max_length - len(parsed[key])))
        
        return dict(parsed)
    
    def _plot_main_losses(self, ax, parsed_data, networks):
        """ç»˜åˆ¶ä¸»è¦Lossæ›²çº¿"""
        loss_keys = [key for key in parsed_data.keys() if 'loss' in key.lower()]
        
        if not loss_keys:
            ax.text(0.5, 0.5, 'æœªæ‰¾åˆ°Lossæ•°æ®', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('ä¸»è¦Lossæ›²çº¿')
            return
        
        for loss_key in loss_keys:
            if any(net in loss_key.lower() for net in networks):
                values = parsed_data[loss_key]
                if values:
                    color = self.color_map.get(loss_key, None)
                    steps = range(len(values))
                    ax.plot(steps, values, label=loss_key, color=color, alpha=0.8)
        
        ax.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax.set_ylabel('Losså€¼')
        ax.set_title('ä¸»è¦Lossæ›²çº¿')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åæ ‡ï¼Œæ›´å¥½åœ°æ˜¾ç¤ºlosså˜åŒ–
    
    def _plot_hyperparameters(self, ax, parsed_data):
        """ç»˜åˆ¶è¶…å‚æ•°å˜åŒ–"""
        param_keys = ['learning_rate', 'alpha', 'entropy']
        
        for param_key in param_keys:
            if param_key in parsed_data:
                values = parsed_data[param_key]
                if values:
                    color = self.color_map.get(param_key, None)
                    steps = range(len(values))
                    ax.plot(steps, values, label=param_key, color=color, alpha=0.8)
        
        ax.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax.set_ylabel('å‚æ•°å€¼')
        ax.set_title('è¶…å‚æ•°å˜åŒ–')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_loss_trends(self, ax, parsed_data):
        """ç»˜åˆ¶Lossè¶‹åŠ¿åˆ†æ"""
        # è®¡ç®—æ»‘åŠ¨å¹³å‡
        window_size = 100
        trend_data = {}
        
        loss_keys = [key for key in parsed_data.keys() if 'loss' in key.lower()]
        
        for loss_key in loss_keys:
            values = parsed_data[loss_key]
            if len(values) > window_size:
                # è®¡ç®—æ»‘åŠ¨å¹³å‡
                moving_avg = []
                for i in range(window_size, len(values)):
                    avg = np.mean(values[i-window_size:i])
                    moving_avg.append(avg)
                trend_data[loss_key] = moving_avg
        
        if trend_data:
            for loss_key, trend in trend_data.items():
                color = self.color_map.get(loss_key, None)
                steps = range(window_size, window_size + len(trend))
                ax.plot(steps, trend, label=f'{loss_key} (æ»‘åŠ¨å¹³å‡)', color=color, alpha=0.8)
        
        ax.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax.set_ylabel('Losså€¼ (æ»‘åŠ¨å¹³å‡)')
        ax.set_title(f'Lossè¶‹åŠ¿åˆ†æ (çª—å£å¤§å°: {window_size})')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')
    
    def _plot_performance_metrics(self, ax, parsed_data):
        """ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡"""
        perf_keys = ['buffer_size', 'update_count', 'grad_norm', 'clip_fraction']
        
        found_metrics = []
        for key in parsed_data.keys():
            if any(perf_key in key.lower() for perf_key in perf_keys):
                found_metrics.append(key)
        
        if not found_metrics:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ€§èƒ½æŒ‡æ ‡ï¼Œæ˜¾ç¤ºlossåˆ†å¸ƒ
            loss_keys = [key for key in parsed_data.keys() if 'loss' in key.lower()]
            if loss_keys:
                loss_values = []
                labels = []
                for loss_key in loss_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    values = parsed_data[loss_key]
                    if values:
                        loss_values.append(values)
                        labels.append(loss_key)
                
                if loss_values:
                    ax.boxplot(loss_values, labels=labels)
                    ax.set_ylabel('Losså€¼')
                    ax.set_title('Lossåˆ†å¸ƒ')
                    ax.tick_params(axis='x', rotation=45)
        else:
            # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
            for metric_key in found_metrics:
                values = parsed_data[metric_key]
                if values:
                    steps = range(len(values))
                    ax.plot(steps, values, label=metric_key, alpha=0.8)
            
            ax.set_xlabel('è®­ç»ƒæ­¥æ•°')
            ax.set_ylabel('æŒ‡æ ‡å€¼')
            ax.set_title('æ€§èƒ½æŒ‡æ ‡')
            ax.legend()
            ax.grid(True, alpha=0.3)
    
    def create_network_comparison(self, save_path: Optional[str] = None,
                                 figsize: Tuple[int, int] = (20, 10)) -> str:
        """
        åˆ›å»ºç½‘ç»œå¯¹æ¯”åˆ†æ
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            figsize: å›¾ç‰‡å¤§å°
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not self.training_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return None
        
        parsed_data = self._parse_training_data()
        if not parsed_data:
            print("âŒ æ— æ³•è§£æè®­ç»ƒæ•°æ®")
            return None
        
        # æ‰¾åˆ°æ‰€æœ‰lossç›¸å…³çš„é”®
        loss_keys = [key for key in parsed_data.keys() if 'loss' in key.lower()]
        
        if not loss_keys:
            print("âŒ æœªæ‰¾åˆ°lossæ•°æ®")
            return None
        
        # åˆ›å»ºå­å›¾ - æ¯ä¸ªç½‘ç»œä¸€ä¸ªå­å›¾
        n_networks = len(loss_keys)
        cols = 3
        rows = (n_networks + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=figsize)
        if n_networks == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes.reshape(1, -1)
        
        fig.suptitle('å„ç½‘ç»œLossè¯¦ç»†å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        for i, loss_key in enumerate(loss_keys):
            row = i // cols
            col = i % cols
            ax = axes[row, col] if rows > 1 else axes[col]
            
            values = parsed_data[loss_key]
            steps = range(len(values))
            
            # ç»˜åˆ¶åŸå§‹æ›²çº¿
            color = self.color_map.get(loss_key, f'C{i}')
            ax.plot(steps, values, alpha=0.3, color=color, linewidth=0.5, label='åŸå§‹æ•°æ®')
            
            # ç»˜åˆ¶æ»‘åŠ¨å¹³å‡
            if len(values) > 50:
                window_size = min(50, len(values) // 10)
                moving_avg = []
                for j in range(window_size, len(values)):
                    avg = np.mean(values[j-window_size:j])
                    moving_avg.append(avg)
                
                avg_steps = range(window_size, len(values))
                ax.plot(avg_steps, moving_avg, color=color, linewidth=2, label='æ»‘åŠ¨å¹³å‡')
            
            ax.set_title(f'{loss_key}')
            ax.set_xlabel('è®­ç»ƒæ­¥æ•°')
            ax.set_ylabel('Losså€¼')
            ax.grid(True, alpha=0.3)
            ax.legend()
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            if values:
                min_val = min(values)
                max_val = max(values)
                final_val = values[-1]
                ax.text(0.05, 0.95, f'æœ€å°: {min_val:.4f}\næœ€å¤§: {max_val:.4f}\næœ€ç»ˆ: {final_val:.4f}',
                       transform=ax.transAxes, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(n_networks, rows * cols):
            row = i // cols
            col = i % cols
            if rows > 1:
                axes[row, col].set_visible(False)
            else:
                axes[col].set_visible(False)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        if save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(self.output_dir, f'network_comparison_{timestamp}.png')
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ç½‘ç»œå¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
        
        return save_path
    
    def create_real_time_monitor(self, experiment_path: str, update_interval: int = 5):
        """
        åˆ›å»ºå®æ—¶ç›‘æ§ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰
        
        Args:
            experiment_path: å®éªŒè·¯å¾„
            update_interval: æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
        """
        print(f"ğŸ”„ å¯åŠ¨å®æ—¶ç›‘æ§: {experiment_path}")
        print(f"   æ›´æ–°é—´éš”: {update_interval}ç§’")
        print("   æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        
        # åˆ›å»ºå®æ—¶å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('å®æ—¶è®­ç»ƒç›‘æ§', fontsize=16, fontweight='bold')
        
        # æ•°æ®å­˜å‚¨
        real_time_data = defaultdict(list)
        
        def update_plots():
            """æ›´æ–°å›¾è¡¨"""
            # é‡æ–°åŠ è½½æ•°æ®
            self.load_training_logs(experiment_path)
            parsed_data = self._parse_training_data()
            
            if not parsed_data:
                return
            
            # æ¸…ç©ºæ‰€æœ‰å­å›¾
            for ax in axes.flat:
                ax.clear()
            
            # é‡æ–°ç»˜åˆ¶
            self._plot_main_losses(axes[0, 0], parsed_data, self.network_types)
            self._plot_hyperparameters(axes[0, 1], parsed_data)
            self._plot_loss_trends(axes[1, 0], parsed_data)
            self._plot_performance_metrics(axes[1, 1], parsed_data)
            
            plt.tight_layout()
            plt.draw()
        
        # è®¾ç½®å®šæ—¶æ›´æ–°
        def monitor_loop():
            while True:
                try:
                    update_plots()
                    time.sleep(update_interval)
                except KeyboardInterrupt:
                    print("\nâš ï¸  å®æ—¶ç›‘æ§å·²åœæ­¢")
                    break
                except Exception as e:
                    print(f"âš ï¸  æ›´æ–°å›¾è¡¨æ—¶å‡ºé”™: {e}")
                    time.sleep(update_interval)
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()
        
        # æ˜¾ç¤ºå›¾è¡¨
        plt.show()
    
    def generate_comprehensive_loss_report(self, report_path: Optional[str] = None) -> str:
        """
        ç”Ÿæˆç»¼åˆlossåˆ†ææŠ¥å‘Š
        
        Args:
            report_path: æŠ¥å‘Šä¿å­˜è·¯å¾„
            
        Returns:
            æŠ¥å‘Šç›®å½•è·¯å¾„
        """
        if not self.training_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return None
        
        print("ğŸ“Š æ­£åœ¨ç”Ÿæˆç»¼åˆLossåˆ†ææŠ¥å‘Š...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = os.path.join(self.output_dir, f'loss_report_{timestamp}')
        os.makedirs(report_dir, exist_ok=True)
        
        # 1. Lossæ›²çº¿å›¾
        curves_path = self.create_loss_curves(
            save_path=os.path.join(report_dir, 'loss_curves.png')
        )
        
        # 2. ç½‘ç»œå¯¹æ¯”å›¾
        comparison_path = self.create_network_comparison(
            save_path=os.path.join(report_dir, 'network_comparison.png')
        )
        
        # 3. ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        text_report_path = os.path.join(report_dir, 'loss_analysis_report.txt')
        self._generate_loss_text_report(text_report_path)
        
        # 4. å¯¼å‡ºåŸå§‹æ•°æ®
        data_path = os.path.join(report_dir, 'training_data.json')
        parsed_data = self._parse_training_data()
        with open(data_path, 'w') as f:
            # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            serializable_data = {}
            for key, values in parsed_data.items():
                if isinstance(values, np.ndarray):
                    serializable_data[key] = values.tolist()
                elif isinstance(values, list):
                    serializable_data[key] = values
                else:
                    serializable_data[key] = str(values)
            json.dump(serializable_data, f, indent=2)
        
        print(f"âœ… ç»¼åˆLossæŠ¥å‘Šå·²ç”Ÿæˆ: {report_dir}")
        return report_dir
    
    def _generate_loss_text_report(self, report_path: str):
        """ç”Ÿæˆlossåˆ†ææ–‡æœ¬æŠ¥å‘Š"""
        parsed_data = self._parse_training_data()
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("ç¥ç»ç½‘ç»œè®­ç»ƒLossåˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            f.write("ğŸ“Š è®­ç»ƒæ•°æ®æ¦‚è§ˆ\n")
            f.write("-" * 20 + "\n")
            f.write(f"æ•°æ®æºæ•°é‡: {len(self.training_data)}\n")
            f.write(f"è§£ææŒ‡æ ‡æ•°é‡: {len(parsed_data)}\n\n")
            
            # Lossç»Ÿè®¡åˆ†æ
            loss_keys = [key for key in parsed_data.keys() if 'loss' in key.lower()]
            
            if loss_keys:
                f.write("ğŸ”¥ Lossç»Ÿè®¡åˆ†æ\n")
                f.write("-" * 20 + "\n")
                
                for loss_key in loss_keys:
                    values = parsed_data[loss_key]
                    if values:
                        f.write(f"\n{loss_key}:\n")
                        f.write(f"  è®­ç»ƒæ­¥æ•°: {len(values)}\n")
                        f.write(f"  æœ€å°å€¼: {min(values):.6f}\n")
                        f.write(f"  æœ€å¤§å€¼: {max(values):.6f}\n")
                        f.write(f"  å¹³å‡å€¼: {np.mean(values):.6f}\n")
                        f.write(f"  æ ‡å‡†å·®: {np.std(values):.6f}\n")
                        f.write(f"  æœ€ç»ˆå€¼: {values[-1]:.6f}\n")
                        
                        # è¶‹åŠ¿åˆ†æ
                        if len(values) > 100:
                            first_100 = np.mean(values[:100])
                            last_100 = np.mean(values[-100:])
                            improvement = (first_100 - last_100) / first_100 * 100
                            f.write(f"  æ”¹å–„ç¨‹åº¦: {improvement:.2f}%\n")
            
            # è¶…å‚æ•°åˆ†æ
            param_keys = ['learning_rate', 'alpha', 'entropy']
            param_data = {key: parsed_data[key] for key in param_keys if key in parsed_data}
            
            if param_data:
                f.write(f"\nğŸ“ˆ è¶…å‚æ•°å˜åŒ–åˆ†æ\n")
                f.write("-" * 20 + "\n")
                
                for param_key, values in param_data.items():
                    if values:
                        f.write(f"\n{param_key}:\n")
                        f.write(f"  åˆå§‹å€¼: {values[0]:.6f}\n")
                        f.write(f"  æœ€ç»ˆå€¼: {values[-1]:.6f}\n")
                        f.write(f"  å¹³å‡å€¼: {np.mean(values):.6f}\n")
                        f.write(f"  å˜åŒ–èŒƒå›´: {min(values):.6f} - {max(values):.6f}\n")
            
            f.write(f"\n\næŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        print(f"âœ… Lossåˆ†ææ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç¥ç»ç½‘ç»œè®­ç»ƒLosså¯è§†åŒ–å·¥å…·')
    parser.add_argument('--log-dir', type=str, default='./training_logs', help='è®­ç»ƒæ—¥å¿—ç›®å½•')
    parser.add_argument('--output', type=str, default='./loss_visualizations', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--experiment', type=str, help='æŒ‡å®šå®éªŒè·¯å¾„')
    parser.add_argument('--mode', type=str, choices=['curves', 'comparison', 'monitor', 'all'], 
                       default='all', help='å¯è§†åŒ–æ¨¡å¼')
    parser.add_argument('--networks', nargs='+', help='æŒ‡å®šè¦åˆ†æçš„ç½‘ç»œç±»å‹')
    parser.add_argument('--real-time', action='store_true', help='å¯ç”¨å®æ—¶ç›‘æ§')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = NetworkLossVisualizer(args.log_dir, args.output)
    
    # åŠ è½½è®­ç»ƒæ—¥å¿—
    if not visualizer.load_training_logs(args.experiment):
        print("âŒ æ— æ³•åŠ è½½è®­ç»ƒæ—¥å¿—ï¼Œé€€å‡º")
        return
    
    # æ ¹æ®æ¨¡å¼ç”Ÿæˆå¯è§†åŒ–
    if args.real_time and args.experiment:
        visualizer.create_real_time_monitor(args.experiment)
    elif args.mode == 'curves':
        visualizer.create_loss_curves(networks=args.networks)
    elif args.mode == 'comparison':
        visualizer.create_network_comparison()
    elif args.mode == 'monitor':
        if args.experiment:
            visualizer.create_real_time_monitor(args.experiment)
        else:
            print("âŒ å®æ—¶ç›‘æ§éœ€è¦æŒ‡å®šå®éªŒè·¯å¾„")
    elif args.mode == 'all':
        visualizer.generate_comprehensive_loss_report()
    
    print("ğŸ‰ Losså¯è§†åŒ–å®Œæˆ!")


if __name__ == "__main__":
    main()
