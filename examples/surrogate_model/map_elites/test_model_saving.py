#!/usr/bin/env python3
"""
æµ‹è¯•å…±äº«PPOæ¨¡å‹ä¿å­˜åŠŸèƒ½
"""

import os
import sys
import time
import torch
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '../../..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

sys.path.insert(0, current_dir)
from shared_ppo_trainer import SharedPPOTrainer

def test_model_saving():
    """æµ‹è¯•æ¨¡å‹ä¿å­˜åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å…±äº«PPOæ¨¡å‹ä¿å­˜åŠŸèƒ½")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir = "./test_model_saving"
    os.makedirs(test_dir, exist_ok=True)
    
    # é…ç½®
    model_config = {
        'observation_dim': 10,
        'action_dim': 3,
        'hidden_dim': 64
    }
    
    training_config = {
        'lr': 1e-3,
        'buffer_size': 1000,
        'min_batch_size': 50,  # å¾ˆå°çš„æ‰¹æ¬¡å¤§å°ï¼Œå¿«é€Ÿè§¦å‘
        'model_path': f'{test_dir}/test_shared_ppo_model.pth',
        'update_interval': 10
    }
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   æ¨¡å‹ä¿å­˜è·¯å¾„: {training_config['model_path']}")
    print(f"   æœ€å°æ‰¹æ¬¡å¤§å°: {training_config['min_batch_size']}")
    print(f"   è§‚å¯Ÿç»´åº¦: {model_config['observation_dim']}")
    print(f"   åŠ¨ä½œç»´åº¦: {model_config['action_dim']}")
    
    # åˆ›å»ºå…±äº«PPOè®­ç»ƒå™¨
    shared_trainer = SharedPPOTrainer(model_config, training_config)
    
    try:
        print("\nğŸš€ å¯åŠ¨å…±äº«PPOè®­ç»ƒ...")
        shared_trainer.start_training()
        
        print("\nğŸ¯ å¼€å§‹æ·»åŠ æ¨¡æ‹Ÿç»éªŒ...")
        
        # æ·»åŠ æ¨¡æ‹Ÿç»éªŒæ•°æ®
        for i in range(200):  # æ·»åŠ 200ä¸ªç»éªŒï¼Œåº”è¯¥è§¦å‘4æ¬¡æ›´æ–°ï¼ˆ50*4=200ï¼‰
            experience = {
                'observation': np.random.randn(model_config['observation_dim']).astype(np.float32),
                'action': np.random.randn(model_config['action_dim']).astype(np.float32),
                'reward': float(np.random.randn()),
                'next_observation': np.random.randn(model_config['observation_dim']).astype(np.float32),
                'done': bool(np.random.rand() > 0.9),  # 10%æ¦‚ç‡ç»“æŸ
                'worker_id': 0,
                'robot_config': {'num_links': 3, 'link_lengths': [50, 50, 50]}
            }
            
            shared_trainer.add_experience(experience)
            
            # æ¯50ä¸ªç»éªŒæ£€æŸ¥ä¸€æ¬¡
            if (i + 1) % 50 == 0:
                print(f"âœ… å·²æ·»åŠ  {i+1} ä¸ªç»éªŒ")
                time.sleep(1)  # ç»™è®­ç»ƒè¿›ç¨‹æ—¶é—´å¤„ç†
        
        print("\nâ³ ç­‰å¾…è®­ç»ƒè¿›ç¨‹å¤„ç†æ‰€æœ‰ç»éªŒ...")
        time.sleep(10)  # ç­‰å¾…è¶³å¤Ÿæ—¶é—´è®©è®­ç»ƒè¿›ç¨‹å¤„ç†
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_path = training_config['model_path']
        if os.path.exists(model_path):
            print(f"âœ… æ¨¡å‹æ–‡ä»¶å·²ç”Ÿæˆ: {model_path}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(model_path)
            print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚ ({file_size/1024:.1f} KB)")
            
            # å°è¯•åŠ è½½æ¨¡å‹
            try:
                model_state = torch.load(model_path, map_location='cpu')
                print(f"ğŸ” æ¨¡å‹å†…å®¹:")
                for key in model_state.keys():
                    if isinstance(model_state[key], dict):
                        print(f"   - {key}: {len(model_state[key])} ä¸ªå‚æ•°")
                    else:
                        print(f"   - {key}: {model_state[key]}")
                        
                print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                
            except Exception as e:
                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        else:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶æœªç”Ÿæˆ: {model_path}")
            
        # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶
        backup_files = [f for f in os.listdir(test_dir) if f.startswith('test_shared_ppo_model_backup_')]
        if backup_files:
            print(f"ğŸ“¦ å¤‡ä»½æ–‡ä»¶: {len(backup_files)} ä¸ª")
            for backup in backup_files:
                print(f"   - {backup}")
        else:
            print("ğŸ“¦ æ²¡æœ‰ç”Ÿæˆå¤‡ä»½æ–‡ä»¶")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        print("\nğŸ›‘ åœæ­¢å…±äº«PPOè®­ç»ƒ...")
        shared_trainer.stop_training()
        
    print("\nğŸ§¹ æ¸…ç†æµ‹è¯•æ–‡ä»¶...")
    import shutil
    if os.path.exists(test_dir):
        shutil.rmtree(test_dir)
        print(f"âœ… å·²åˆ é™¤æµ‹è¯•ç›®å½•: {test_dir}")
        
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")

def test_directory_creation():
    """æµ‹è¯•ç›®å½•åˆ›å»ºåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•ç›®å½•åˆ›å»ºåŠŸèƒ½")
    print("-" * 30)
    
    # æµ‹è¯•æ·±å±‚ç›®å½•åˆ›å»º
    deep_path = "./test_deep/level1/level2/level3/model.pth"
    deep_dir = os.path.dirname(deep_path)
    
    print(f"ğŸ“ æµ‹è¯•è·¯å¾„: {deep_path}")
    print(f"ğŸ“ ç›®å½•è·¯å¾„: {deep_dir}")
    
    # æ¨¡æ‹Ÿshared_ppo_trainerä¸­çš„ç›®å½•åˆ›å»ºé€»è¾‘
    if deep_dir and not os.path.exists(deep_dir):
        os.makedirs(deep_dir, exist_ok=True)
        print(f"âœ… ç›®å½•åˆ›å»ºæˆåŠŸ: {deep_dir}")
    else:
        print(f"â„¹ï¸ ç›®å½•å·²å­˜åœ¨: {deep_dir}")
    
    # æµ‹è¯•æ–‡ä»¶å†™å…¥
    try:
        dummy_data = {'test': 'data', 'timestamp': datetime.now().isoformat()}
        torch.save(dummy_data, deep_path)
        print(f"âœ… æ–‡ä»¶å†™å…¥æˆåŠŸ: {deep_path}")
        
        # éªŒè¯æ–‡ä»¶
        loaded_data = torch.load(deep_path)
        print(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸ: {loaded_data}")
        
    except Exception as e:
        print(f"âŒ æ–‡ä»¶æ“ä½œå¤±è´¥: {e}")
    
    # æ¸…ç†
    import shutil
    if os.path.exists("./test_deep"):
        shutil.rmtree("./test_deep")
        print("ğŸ§¹ å·²æ¸…ç†æµ‹è¯•æ–‡ä»¶")

if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    import multiprocessing as mp
    mp.set_start_method('spawn', force=True)
    
    test_directory_creation()
    test_model_saving()
