"""
å¤šè¿›ç¨‹å…±äº«PPOè®­ç»ƒå™¨
æ”¯æŒå¤šä¸ªè¿›ç¨‹ååŒè®­ç»ƒä¸€ä¸ªPPOæ¨¡å‹
"""

import torch
import torch.multiprocessing as mp
import numpy as np
from queue import Queue, Empty
from threading import Thread
import time
from collections import deque
import pickle
import os
import sys
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '../../..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

class SharedExperienceBuffer:
    """å…±äº«ç»éªŒç¼“å†²åŒº"""
    def __init__(self, buffer_size=50000, min_batch_size=1000):
        self.buffer_size = buffer_size
        self.min_batch_size = min_batch_size
        
        # ä½¿ç”¨å¤šè¿›ç¨‹å®‰å…¨çš„é˜Ÿåˆ—
        self.experience_queue = mp.Queue(maxsize=buffer_size)
        self.batch_ready_event = mp.Event()
        self.shutdown_event = mp.Event()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_experiences = mp.Value('i', 0)
        self.total_batches = mp.Value('i', 0)
        
    def add_experience(self, experience):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        try:
            # åºåˆ—åŒ–ç»éªŒæ•°æ®
            serialized_exp = pickle.dumps(experience)
            self.experience_queue.put(serialized_exp, timeout=1.0)
            
            with self.total_experiences.get_lock():
                self.total_experiences.value += 1
                
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç»éªŒè¿›è¡Œè®­ç»ƒ
            if self.total_experiences.value >= self.min_batch_size:
                self.batch_ready_event.set()
                
        except Exception as e:
            print(f"âš ï¸ æ·»åŠ ç»éªŒå¤±è´¥: {e}")
    
    def get_batch(self, batch_size=None):
        """è·å–ä¸€æ‰¹ç»éªŒç”¨äºè®­ç»ƒ"""
        if batch_size is None:
            batch_size = min(self.min_batch_size, self.total_experiences.value)
            
        experiences = []
        for _ in range(min(batch_size, self.experience_queue.qsize())):
            try:
                serialized_exp = self.experience_queue.get_nowait()
                experience = pickle.loads(serialized_exp)
                experiences.append(experience)
            except:
                break
                
        if experiences:
            with self.total_experiences.get_lock():
                self.total_experiences.value -= len(experiences)
            
            with self.total_batches.get_lock():
                self.total_batches.value += 1
                
        return experiences
    
    def is_ready(self):
        """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç»éªŒè¿›è¡Œè®­ç»ƒ"""
        return self.batch_ready_event.is_set()
    
    def reset_ready(self):
        """é‡ç½®å‡†å¤‡çŠ¶æ€"""
        self.batch_ready_event.clear()
    
    def shutdown(self):
        """å…³é—­ç¼“å†²åŒº"""
        self.shutdown_event.set()


class SharedPPOTrainer:
    """å…±äº«PPOè®­ç»ƒå™¨"""
    def __init__(self, model_config, training_config):
        self.model_config = model_config
        self.training_config = training_config
        
        # åˆ›å»ºå…±äº«ç»éªŒç¼“å†²åŒº
        self.experience_buffer = SharedExperienceBuffer(
            buffer_size=training_config.get('buffer_size', 50000),
            min_batch_size=training_config.get('min_batch_size', 1000)
        )
        
        # æ¨¡å‹å‚æ•°å…±äº«ï¼ˆä½¿ç”¨æ–‡ä»¶æˆ–å…±äº«å†…å­˜ï¼‰
        self.model_path = training_config.get('model_path', './shared_ppo_model.pth')
        self.update_interval = training_config.get('update_interval', 100)
        
        # è®­ç»ƒè¿›ç¨‹
        self.trainer_process = None
        self.is_training = mp.Value('b', False)
        
    def start_training(self):
        """å¯åŠ¨è®­ç»ƒè¿›ç¨‹"""
        print("ğŸš€ å¯åŠ¨å…±äº«PPOè®­ç»ƒè¿›ç¨‹...")
        
        self.is_training.value = True
        self.trainer_process = mp.Process(
            target=self._training_loop,
            args=(self.experience_buffer, self.model_path, self.training_config)
        )
        self.trainer_process.start()
        print(f"âœ… è®­ç»ƒè¿›ç¨‹å·²å¯åŠ¨ (PID: {self.trainer_process.pid})")
    
    def stop_training(self):
        """åœæ­¢è®­ç»ƒè¿›ç¨‹"""
        print("ğŸ›‘ åœæ­¢å…±äº«PPOè®­ç»ƒ...")
        
        self.is_training.value = False
        self.experience_buffer.shutdown()
        
        if self.trainer_process and self.trainer_process.is_alive():
            self.trainer_process.join(timeout=5.0)
            if self.trainer_process.is_alive():
                self.trainer_process.terminate()
                self.trainer_process.join()
        
        print("âœ… è®­ç»ƒè¿›ç¨‹å·²åœæ­¢")
    
    @staticmethod
    def _training_loop(experience_buffer, model_path, config):
        """è®­ç»ƒå¾ªç¯ï¼ˆåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œï¼‰"""
        print("ğŸ¯ è¿›å…¥PPOè®­ç»ƒå¾ªç¯...")
        
        # åœ¨è®­ç»ƒè¿›ç¨‹ä¸­åˆå§‹åŒ–æ¨¡å‹
        try:
            # ğŸ”§ å¯¼å…¥å¿…è¦çš„æ¨¡å—
            import os
            import torch.nn as nn
            import torch.optim as optim
            import time
            from datetime import datetime
            
            # ğŸ”§ ç®€åŒ–çš„PPOæ¨¡å‹åˆå§‹åŒ–
            print("ğŸ¤– åˆå§‹åŒ–ç®€åŒ–PPOæ¨¡å‹...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„æ¨¡å‹
            load_existing_model = os.path.exists(model_path)
            if load_existing_model:
                print(f"ğŸ” å‘ç°å·²ä¿å­˜çš„æ¨¡å‹: {model_path}")
            else:
                print("ğŸ†• åˆ›å»ºæ–°çš„PPOæ¨¡å‹")
            
            # åˆ›å»ºç®€å•çš„Actor-Criticç½‘ç»œ
            
            class SimpleActor(nn.Module):
                def __init__(self, obs_dim, action_dim, hidden_dim=256):
                    super().__init__()
                    self.net = nn.Sequential(
                        nn.Linear(obs_dim, hidden_dim),
                        nn.ReLU(),
                        nn.Linear(hidden_dim, hidden_dim),
                        nn.ReLU(),
                        nn.Linear(hidden_dim, action_dim),
                        nn.Tanh()
                    )
                
                def forward(self, x):
                    return self.net(x)
            
            class SimpleCritic(nn.Module):
                def __init__(self, obs_dim, hidden_dim=256):
                    super().__init__()
                    self.net = nn.Sequential(
                        nn.Linear(obs_dim, hidden_dim),
                        nn.ReLU(),
                        nn.Linear(hidden_dim, hidden_dim),
                        nn.ReLU(),
                        nn.Linear(hidden_dim, 1)
                    )
                
                def forward(self, x):
                    return self.net(x)
            
            # ğŸ”§ åŠ¨æ€è·å–è§‚å¯Ÿç»´åº¦ï¼ˆä»ç¬¬ä¸€ä¸ªç»éªŒä¸­æ¨æ–­ï¼‰
            print("â³ ç­‰å¾…ç¬¬ä¸€ä¸ªç»éªŒä»¥ç¡®å®šè§‚å¯Ÿç»´åº¦...")
            first_batch = None
            while not experience_buffer.shutdown_event.is_set():
                experiences = experience_buffer.get_batch(batch_size=1)
                if experiences:
                    first_batch = experiences
                    break
                time.sleep(0.1)
            
            if first_batch:
                actual_obs_dim = len(first_batch[0]['observation'])
                actual_action_dim = len(first_batch[0]['action'])
                print(f"ğŸ“Š æ£€æµ‹åˆ°è§‚å¯Ÿç»´åº¦: {actual_obs_dim}, åŠ¨ä½œç»´åº¦: {actual_action_dim}")
            else:
                actual_obs_dim = config.get('observation_dim', 14)
                actual_action_dim = config.get('action_dim', 3)
                print(f"âš ï¸ ä½¿ç”¨é»˜è®¤ç»´åº¦: obs={actual_obs_dim}, action={actual_action_dim}")
            
            # åˆå§‹åŒ–ç½‘ç»œ
            obs_dim = actual_obs_dim
            action_dim = actual_action_dim
            hidden_dim = config.get('hidden_dim', 256)
            
            actor = SimpleActor(obs_dim, action_dim, hidden_dim)
            critic = SimpleCritic(obs_dim, hidden_dim)
            
            actor_optimizer = optim.Adam(actor.parameters(), lr=config.get('lr', 2e-4))
            critic_optimizer = optim.Adam(critic.parameters(), lr=config.get('lr', 2e-4))
            
            update_count = 0
            
            # ğŸ”§ åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if load_existing_model:
                try:
                    print(f"ğŸ”„ æ­£åœ¨åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹...")
                    checkpoint = torch.load(model_path, map_location='cpu')
                    
                    actor.load_state_dict(checkpoint['actor'])
                    critic.load_state_dict(checkpoint['critic'])
                    actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
                    critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
                    update_count = checkpoint.get('update_count', 0)
                    
                    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹ - å·²å®Œæˆ {update_count} æ¬¡æ›´æ–°")
                    print(f"ğŸ“Š æ¨¡å‹å‚æ•°:")
                    print(f"   è§‚å¯Ÿç»´åº¦: {obs_dim}")
                    print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")
                    print(f"   éšè—å±‚ç»´åº¦: {hidden_dim}")
                    
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–: {e}")
                    update_count = 0
            
            print("âœ… ç®€åŒ–PPOæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            
            # ğŸ”§ å¦‚æœæœ‰ç¬¬ä¸€æ‰¹ç»éªŒï¼Œå…ˆå¤„ç†å®ƒ
            if first_batch:
                print(f"ğŸ”„ å¤„ç†åˆå§‹ç»éªŒæ‰¹æ¬¡ ({len(first_batch)} ä¸ª)...")
                try:
                    # å¤„ç†ç¬¬ä¸€æ‰¹ç»éªŒ
                    obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []
                    
                    for exp in first_batch:
                        obs_batch.append(exp['observation'])
                        action_batch.append(exp['action'])
                        reward_batch.append(exp['reward'])
                        next_obs_batch.append(exp['next_observation'])
                        done_batch.append(exp['done'])
                    
                    # è½¬æ¢ä¸ºå¼ é‡
                    obs_batch = torch.FloatTensor(np.array(obs_batch))
                    action_batch = torch.FloatTensor(np.array(action_batch))
                    reward_batch = torch.FloatTensor(reward_batch)
                    next_obs_batch = torch.FloatTensor(np.array(next_obs_batch))
                    done_batch = torch.BoolTensor(done_batch)
                    
                    # ç®€åŒ–è®­ç»ƒ
                    actor.train()
                    critic.train()
                    values = critic(obs_batch).squeeze()
                    actions_pred = actor(obs_batch)
                    value_loss = torch.nn.functional.mse_loss(values, reward_batch)
                    action_loss = torch.nn.functional.mse_loss(actions_pred, action_batch)
                    total_loss = value_loss + action_loss
                    
                    actor_optimizer.zero_grad()
                    critic_optimizer.zero_grad()
                    total_loss.backward()
                    actor_optimizer.step()
                    critic_optimizer.step()
                    
                    update_count += 1
                    print(f"ğŸ“Š åˆå§‹æ‰¹æ¬¡æ›´æ–°å®Œæˆ - æŸå¤±: {total_loss.item():.4f}")
                    
                except Exception as e:
                    print(f"âš ï¸ åˆå§‹æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            
            while not experience_buffer.shutdown_event.is_set():
                # ç­‰å¾…è¶³å¤Ÿçš„ç»éªŒ
                if not experience_buffer.is_ready():
                    time.sleep(0.1)
                    continue
                
                # è·å–ç»éªŒæ‰¹æ¬¡
                experiences = experience_buffer.get_batch()
                if not experiences:
                    continue
                
                print(f"ğŸ”„ å¤„ç† {len(experiences)} ä¸ªç»éªŒ...")
                
                # å°†ç»éªŒè½¬æ¢ä¸ºPPOè®­ç»ƒæ ¼å¼
                obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []
                
                for exp in experiences:
                    obs_batch.append(exp['observation'])
                    action_batch.append(exp['action'])
                    reward_batch.append(exp['reward'])
                    next_obs_batch.append(exp['next_observation'])
                    done_batch.append(exp['done'])
                
                # è½¬æ¢ä¸ºå¼ é‡ï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼‰
                obs_batch = torch.FloatTensor(np.array(obs_batch))
                action_batch = torch.FloatTensor(np.array(action_batch))
                reward_batch = torch.FloatTensor(reward_batch)
                next_obs_batch = torch.FloatTensor(np.array(next_obs_batch))
                done_batch = torch.BoolTensor(done_batch)
                
                # æ‰§è¡Œç®€åŒ–çš„PPOæ›´æ–°
                try:
                    # ğŸ”§ ç®€åŒ–çš„PPOè®­ç»ƒé€»è¾‘
                    actor.train()
                    critic.train()
                    
                    # è®¡ç®—ä»·å€¼å’ŒåŠ¨ä½œ
                    values = critic(obs_batch).squeeze()
                    actions_pred = actor(obs_batch)
                    
                    # ç®€åŒ–çš„æŸå¤±è®¡ç®—
                    value_loss = torch.nn.functional.mse_loss(values, reward_batch)
                    action_loss = torch.nn.functional.mse_loss(actions_pred, action_batch)
                    
                    total_loss = value_loss + action_loss
                    
                    # åå‘ä¼ æ’­
                    actor_optimizer.zero_grad()
                    critic_optimizer.zero_grad()
                    total_loss.backward()
                    actor_optimizer.step()
                    critic_optimizer.step()
                    
                    print(f"ğŸ“Š PPOæ›´æ–°å®Œæˆ - æ‰¹æ¬¡: {len(experiences)}, æŸå¤±: {total_loss.item():.4f}")
                    
                    update_count += 1
                    
                    # ğŸ”§ æ›´é¢‘ç¹åœ°ä¿å­˜æ¨¡å‹ - æ¯æ¬¡æ›´æ–°éƒ½ä¿å­˜
                    model_state = {
                        'actor': actor.state_dict(),
                        'critic': critic.state_dict(),
                        'actor_optimizer': actor_optimizer.state_dict(),
                        'critic_optimizer': critic_optimizer.state_dict(),
                        'update_count': update_count
                    }
                    
                    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
                    import os
                    model_dir = os.path.dirname(model_path)
                    if model_dir and not os.path.exists(model_dir):
                        os.makedirs(model_dir, exist_ok=True)
                        print(f"ğŸ“ åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•: {model_dir}")
                    
                    torch.save(model_state, model_path)
                    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ (æ›´æ–°æ¬¡æ•°: {update_count}) -> {model_path}")
                    
                    # é¢å¤–ä¿å­˜ä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½
                    if update_count % 5 == 0:
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        backup_path = model_path.replace('.pth', f'_backup_{timestamp}.pth')
                        torch.save(model_state, backup_path)
                        print(f"ğŸ’¾ å¤‡ä»½æ¨¡å‹å·²ä¿å­˜: {backup_path}")
                        
                except Exception as e:
                    print(f"âŒ PPOæ›´æ–°å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                
                experience_buffer.reset_ready()
                
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¾ªç¯é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    
    def add_experience(self, experience):
        """æ·»åŠ ç»éªŒåˆ°å…±äº«ç¼“å†²åŒº"""
        self.experience_buffer.add_experience(experience)
    
    def load_latest_model(self):
        """åŠ è½½æœ€æ–°çš„æ¨¡å‹å‚æ•°"""
        try:
            import os
            if os.path.exists(self.model_path):
                return torch.load(self.model_path)
            else:
                print("âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                return None
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return None


class MultiProcessPPOWorker:
    """å¤šè¿›ç¨‹PPOå·¥ä½œå™¨"""
    def __init__(self, worker_id, shared_trainer, robot_config):
        self.worker_id = worker_id
        self.shared_trainer = shared_trainer
        self.robot_config = robot_config
        
        # æœ¬åœ°PPOæ¨¡å‹ï¼ˆç”¨äºæ¨ç†ï¼‰
        self.local_ppo = None
        self.last_model_update = 0
        self.model_update_interval = 100  # æ¯100æ­¥æ›´æ–°ä¸€æ¬¡æ¨¡å‹
        
    def initialize_local_model(self):
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹"""
        print(f"ğŸ¤– å·¥ä½œå™¨ {self.worker_id} åˆå§‹åŒ–æœ¬åœ°PPOæ¨¡å‹...")
        
        # åŠ è½½æœ€æ–°çš„å…±äº«æ¨¡å‹å‚æ•°
        shared_params = self.shared_trainer.load_latest_model()
        
        # åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„PPOå®ç°è°ƒæ•´
        # self.local_ppo = AttentionPPOWithBuffer(...)
        
        if shared_params:
            # self.local_ppo.load_state_dict(shared_params)
            print(f"âœ… å·¥ä½œå™¨ {self.worker_id} åŠ è½½äº†å…±äº«æ¨¡å‹å‚æ•°")
        else:
            print(f"âœ… å·¥ä½œå™¨ {self.worker_id} ä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    def collect_experience(self, num_steps=1000):
        """æ”¶é›†ç»éªŒå¹¶å‘é€åˆ°å…±äº«ç¼“å†²åŒº"""
        print(f"ğŸ¯ å·¥ä½œå™¨ {self.worker_id} å¼€å§‹æ”¶é›†ç»éªŒ...")
        
        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„ç¯å¢ƒå®ç°
        for step in range(num_steps):
            # æ¨¡æ‹Ÿç»éªŒæ”¶é›†
            experience = {
                'observation': np.random.randn(10),  # ç¤ºä¾‹è§‚å¯Ÿ
                'action': np.random.randn(3),        # ç¤ºä¾‹åŠ¨ä½œ
                'reward': np.random.randn(),         # ç¤ºä¾‹å¥–åŠ±
                'next_observation': np.random.randn(10),  # ç¤ºä¾‹ä¸‹ä¸€çŠ¶æ€
                'done': step % 100 == 99,           # ç¤ºä¾‹å®Œæˆæ ‡å¿—
                'worker_id': self.worker_id,
                'robot_config': self.robot_config
            }
            
            # å‘é€ç»éªŒåˆ°å…±äº«ç¼“å†²åŒº
            self.shared_trainer.add_experience(experience)
            
            # å®šæœŸæ›´æ–°æœ¬åœ°æ¨¡å‹
            if step % self.model_update_interval == 0:
                self.update_local_model()
        
        print(f"âœ… å·¥ä½œå™¨ {self.worker_id} å®Œæˆç»éªŒæ”¶é›†")
    
    def update_local_model(self):
        """æ›´æ–°æœ¬åœ°æ¨¡å‹å‚æ•°"""
        shared_params = self.shared_trainer.load_latest_model()
        if shared_params and self.local_ppo:
            # self.local_ppo.load_state_dict(shared_params)
            self.last_model_update += 1
            if self.last_model_update % 10 == 0:
                print(f"ğŸ”„ å·¥ä½œå™¨ {self.worker_id} æ›´æ–°äº†æ¨¡å‹å‚æ•°")


# ä½¿ç”¨ç¤ºä¾‹
def demo_shared_ppo_training():
    """æ¼”ç¤ºå…±äº«PPOè®­ç»ƒ"""
    print("ğŸš€ æ¼”ç¤ºå¤šè¿›ç¨‹å…±äº«PPOè®­ç»ƒ")
    
    # é…ç½®
    model_config = {
        'observation_dim': 10,
        'action_dim': 3,
        'hidden_dim': 256
    }
    
    training_config = {
        'lr': 2e-4,
        'buffer_size': 10000,
        'min_batch_size': 500,
        'model_path': './shared_ppo_demo.pth',
        'update_interval': 100
    }
    
    # åˆ›å»ºå…±äº«è®­ç»ƒå™¨
    shared_trainer = SharedPPOTrainer(model_config, training_config)
    
    try:
        # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
        shared_trainer.start_training()
        
        # åˆ›å»ºå¤šä¸ªå·¥ä½œå™¨
        workers = []
        worker_processes = []
        
        for i in range(3):  # 3ä¸ªå·¥ä½œå™¨
            robot_config = {
                'num_joints': 3 + i,
                'link_lengths': [90.0] * (3 + i)
            }
            
            worker = MultiProcessPPOWorker(i, shared_trainer, robot_config)
            workers.append(worker)
            
            # å¯åŠ¨å·¥ä½œå™¨è¿›ç¨‹
            process = mp.Process(
                target=worker.collect_experience,
                args=(500,)  # æ¯ä¸ªå·¥ä½œå™¨æ”¶é›†500æ­¥ç»éªŒ
            )
            worker_processes.append(process)
            process.start()
        
        # ç­‰å¾…æ‰€æœ‰å·¥ä½œå™¨å®Œæˆ
        for process in worker_processes:
            process.join()
        
        print("ğŸ‰ æ‰€æœ‰å·¥ä½œå™¨å®Œæˆç»éªŒæ”¶é›†")
        time.sleep(2)  # è®©è®­ç»ƒè¿›ç¨‹å¤„ç†å‰©ä½™ç»éªŒ
        
    finally:
        # åœæ­¢è®­ç»ƒ
        shared_trainer.stop_training()
        print("âœ… å…±äº«PPOè®­ç»ƒæ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    mp.set_start_method('spawn', force=True)
    demo_shared_ppo_training()
