import sys
import os
import numpy as np
import torch
from typing import Dict, Any, Optional
import time

# æ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from map_elites_core import Individual, RobotGenotype, RobotPhenotype, FeatureExtractor

# å¯¼å…¥çœŸå®è®­ç»ƒæ¥å£
try:
    from enhanced_train_interface import MAPElitesTrainingInterface
    REAL_TRAINING_AVAILABLE = True
except ImportError:
    print("âš ï¸  çœŸå®è®­ç»ƒæ¥å£ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ")
    REAL_TRAINING_AVAILABLE = False


class MAPElitesTrainingAdapter:
    """MAP-Elitesä¸SACè®­ç»ƒçš„é€‚é…å™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, base_args, base_save_dir: str = "./map_elites_experiments", 
                 use_real_training: bool = True,
                 enable_rendering: bool = False,  # ğŸ†• æ§åˆ¶æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–
                 silent_mode: bool = True):       # ğŸ†• æ§åˆ¶æ˜¯å¦é™é»˜
        self.base_args = base_args
        self.base_save_dir = base_save_dir
        self.use_real_training = use_real_training and REAL_TRAINING_AVAILABLE
        
        os.makedirs(base_save_dir, exist_ok=True)
        
        # ç‰¹å¾æå–å™¨
        self.feature_extractor = FeatureExtractor()
        
        # ğŸ”§ ä¼˜åŒ–ï¼šå¯é…ç½®çš„è®­ç»ƒæ¥å£
        if self.use_real_training:
            self.training_interface = MAPElitesTrainingInterface(
                silent_mode=silent_mode,
                enable_rendering=enable_rendering
            )
            print(f"ğŸ”§ MAP-Elitesè®­ç»ƒé€‚é…å™¨å·²åˆå§‹åŒ– (ä½¿ç”¨enhanced_train.py)")
            print(f"   ğŸ¨ æ¸²æŸ“: {'å¯ç”¨' if enable_rendering else 'ç¦ç”¨'}")
            print(f"   ğŸ”‡ é™é»˜: {'å¯ç”¨' if silent_mode else 'ç¦ç”¨'}")
        else:
            print("ğŸ”§ MAP-Elitesè®­ç»ƒé€‚é…å™¨å·²åˆå§‹åŒ– (ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ)")
    
    def evaluate_individual(self, individual: Individual, training_steps: int = 5000) -> Individual:
        """è¯„ä¼°å•ä¸ªä¸ªä½“ - å¢å¼ºç‰ˆ"""
        print(f"\nğŸ§¬ è¯„ä¼°ä¸ªä½“ {individual.individual_id}")
        print(f"ğŸ¤– åŸºå› å‹: num_links={individual.genotype.num_links}, "
              f"link_lengths={[f'{x:.1f}' for x in individual.genotype.link_lengths]}")
        print(f"ğŸ§  SACå‚æ•°: lr={individual.genotype.lr:.2e}, alpha={individual.genotype.alpha:.3f}")
        
        # 1. æ ¹æ®åŸºå› å‹åˆ›å»ºè®­ç»ƒå‚æ•°
        training_args = self._genotype_to_training_args(individual.genotype, training_steps)
        
        # 2. è¿è¡Œè®­ç»ƒ
        start_time = time.time()
        if self.use_real_training:
            print(f"   ğŸ¯ ä½¿ç”¨enhanced_train.pyè¿›è¡ŒçœŸå®è®­ç»ƒ ({training_steps} steps)")
            try:
                training_metrics = self.training_interface.train_individual(training_args)
            except Exception as e:
                print(f"   âŒ çœŸå®è®­ç»ƒå¤±è´¥: {e}")
                print(f"   ğŸ”„ å›é€€åˆ°æ¨¡æ‹Ÿè®­ç»ƒ")
                training_metrics = self._run_simulated_training(training_args)
        else:
            print(f"   ğŸ² ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒ ({training_steps} steps)")
            training_metrics = self._run_simulated_training(training_args)
        
        training_time = time.time() - start_time
        
        # 3. æå–è¡¨å‹ç‰¹å¾
        robot_config = {
            'num_links': individual.genotype.num_links,
            'link_lengths': individual.genotype.link_lengths
        }
        
        phenotype = self.feature_extractor.extract_from_training_data(training_metrics, robot_config)
        
        # 4. æ›´æ–°ä¸ªä½“
        individual.phenotype = phenotype
        individual.fitness = phenotype.avg_reward  # ä½¿ç”¨å¹³å‡å¥–åŠ±ä½œä¸ºé€‚åº”åº¦
        
        print(f"âœ… è¯„ä¼°å®Œæˆ: é€‚åº”åº¦={individual.fitness:.2f}, æˆåŠŸç‡={phenotype.success_rate:.2f}, è€—æ—¶={training_time:.1f}s")
        
        return individual
    
    def _genotype_to_training_args(self, genotype: RobotGenotype, training_steps: int):
        """å°†åŸºå› å‹è½¬æ¢ä¸ºè®­ç»ƒå‚æ•°"""
        # åˆ›å»ºå‚æ•°å¯¹è±¡
        args = type('Args', (), {})()
        
        # å¤åˆ¶åŸºç¡€å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
        if hasattr(self.base_args, '__dict__'):
            for key, value in vars(self.base_args).items():
                setattr(args, key, value)
        
        # ğŸ¤– è®¾ç½®æœºå™¨äººå½¢æ€å‚æ•°
        args.num_joints = genotype.num_links
        args.link_lengths = genotype.link_lengths
        
        # ğŸ§  è®¾ç½®SACè¶…å‚æ•°
        args.lr = genotype.lr
        args.alpha = genotype.alpha
        args.tau = genotype.tau
        args.gamma = genotype.gamma
        args.batch_size = genotype.batch_size
        args.buffer_capacity = genotype.buffer_capacity
        args.warmup_steps = genotype.warmup_steps
        args.target_entropy_factor = genotype.target_entropy_factor
        
        # ğŸ¯ è®¾ç½®è®­ç»ƒæ­¥æ•°
        args.total_steps = training_steps
        
        # ğŸ“ è®¾ç½®ä¿å­˜ç›®å½•
        individual_dir = os.path.join(self.base_save_dir, f"individual_{int(time.time() * 1000) % 100000}")
        args.save_dir = individual_dir
        
        # ğŸ”§ è®¾ç½®å…¶ä»–è®­ç»ƒå‚æ•°
        args.update_frequency = getattr(self.base_args, 'update_frequency', 1)
        args.num_processes = 1  # MAP-Elitesä½¿ç”¨å•è¿›ç¨‹
        args.seed = getattr(self.base_args, 'seed', 42)
        
        return args
    
    def _run_simulated_training(self, args) -> Dict[str, Any]:
        """è¿è¡Œæ¨¡æ‹Ÿè®­ç»ƒï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        # åŸºäºåŸºå› å‹é¢„æµ‹å¤§è‡´æ€§èƒ½
        num_links = getattr(args, 'num_joints', 4)
        link_lengths = getattr(args, 'link_lengths', [80.0] * num_links)
        
        # ç®€å•çš„å¯å‘å¼è¯„ä¼°
        total_reach = sum(link_lengths)
        complexity_penalty = abs(num_links - 4) * 10  # 4é“¾èŠ‚æœ€ä¼˜
        length_variance_penalty = np.var(link_lengths) / 10
        
        # åŸºäºè¶…å‚æ•°çš„æ€§èƒ½é¢„æµ‹
        lr_factor = 1.0 if 1e-5 <= args.lr <= 1e-3 else 0.5
        alpha_factor = 1.0 if 0.1 <= args.alpha <= 0.5 else 0.7
        
        base_reward = min(100, total_reach / 5) - complexity_penalty - length_variance_penalty
        base_reward *= lr_factor * alpha_factor
        
        # æ·»åŠ éšæœºæ€§
        noise = np.random.normal(0, 15)
        final_reward = base_reward + noise
        
        return {
            'avg_reward': final_reward,
            'success_rate': max(0, min(1, (final_reward + 50) / 150)),
            'min_distance': max(10, 200 - final_reward * 2),
            'trajectory_smoothness': np.random.uniform(0.3, 0.8),
            'collision_rate': np.random.uniform(0.0, 0.3),
            'exploration_area': np.random.uniform(100, 500),
            'action_variance': np.random.uniform(0.1, 0.5),
            'learning_rate': np.random.uniform(0.1, 0.9),
            'final_critic_loss': np.random.uniform(0.1, 5.0),
            'final_actor_loss': np.random.uniform(0.1, 2.0),
            'training_stability': np.random.uniform(0.3, 0.9)
        }


# ğŸ§ª æµ‹è¯•å‡½æ•°
def test_training_adapter():
    """æµ‹è¯•è®­ç»ƒé€‚é…å™¨"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•MAP-Elitesè®­ç»ƒé€‚é…å™¨\n")
    
    # 1. åˆ›å»ºæ¨¡æ‹Ÿçš„åŸºç¡€å‚æ•°
    print("ğŸ“Š æµ‹è¯•1: åˆ›å»ºåŸºç¡€å‚æ•°")
    import argparse
    base_args = argparse.Namespace()
    base_args.env_type = 'reacher2d'
    base_args.num_processes = 1
    base_args.seed = 42
    base_args.save_dir = './test_map_elites_results'
    base_args.lr = 1e-4
    base_args.alpha = 0.2
    base_args.tau = 0.005
    base_args.gamma = 0.99
    base_args.update_frequency = 1
    print(f"âœ… åŸºç¡€å‚æ•°åˆ›å»ºå®Œæˆ: {base_args.env_type}")
    
    # 2. æµ‹è¯•ä¸¤ç§æ¨¡å¼
    for use_real in [False, True]:
        print(f"\n{'='*30}")
        print(f"ğŸ“Š æµ‹è¯•æ¨¡å¼: {'çœŸå®è®­ç»ƒ(enhanced_train.py)' if use_real else 'æ¨¡æ‹Ÿè®­ç»ƒ'}")
        print(f"{'='*30}")
        
        adapter = MAPElitesTrainingAdapter(
            base_args, 
            "./test_adapter_results", 
            use_real_training=use_real
        )
        
        # åˆ›å»ºæµ‹è¯•ä¸ªä½“
        from map_elites_core import RobotGenotype, RobotPhenotype, Individual
        
        test_genotype = RobotGenotype(
            num_links=3,
            link_lengths=[70, 50, 30],
            lr=2e-4,
            alpha=0.25
        )
        
        test_individual = Individual(
            genotype=test_genotype,
            phenotype=RobotPhenotype()
        )
        
        # è¯„ä¼°ä¸ªä½“ï¼ˆä½¿ç”¨è¾ƒå°‘æ­¥æ•°è¿›è¡Œå¿«é€Ÿæµ‹è¯•ï¼‰
        test_steps = 500 if use_real else 100
        start_time = time.time()
        
        evaluated = adapter.evaluate_individual(test_individual, training_steps=test_steps)
        
        end_time = time.time()
        
        print(f"âœ… è¯„ä¼°ç»“æœ (è€—æ—¶: {end_time - start_time:.1f}ç§’):")
        print(f"   é€‚åº”åº¦: {evaluated.fitness:.2f}")
        print(f"   æˆåŠŸç‡: {evaluated.phenotype.success_rate:.2f}")
        print(f"   æ€»ä¼¸å±•: {evaluated.phenotype.total_reach:.1f}")
        print(f"   è®­ç»ƒç¨³å®šæ€§: {evaluated.phenotype.learning_efficiency:.2f}")
        
        if use_real and REAL_TRAINING_AVAILABLE:
            print(f"âœ… çœŸå®è®­ç»ƒæ¨¡å¼å·¥ä½œæ­£å¸¸!")
        elif use_real:
            print(f"âš ï¸  çœŸå®è®­ç»ƒä¸å¯ç”¨ï¼Œå·²å›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼")


if __name__ == "__main__":
    test_training_adapter()