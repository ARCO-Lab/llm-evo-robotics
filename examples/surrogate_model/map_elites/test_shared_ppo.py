#!/usr/bin/env python3
"""
æµ‹è¯•å…±äº«PPOè®­ç»ƒå™¨çš„ç‹¬ç«‹æ¼”ç¤ºè„šæœ¬
"""

import os
import sys
import time
import torch
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

def test_shared_ppo_basic():
    """æµ‹è¯•åŸºç¡€å…±äº«PPOåŠŸèƒ½"""
    print("ğŸš€ æµ‹è¯•å…±äº«PPOè®­ç»ƒå™¨ - åŸºç¡€åŠŸèƒ½")
    print("=" * 50)
    
    try:
        from shared_ppo_trainer import SharedPPOTrainer
        
        # é…ç½®
        model_config = {
            'observation_dim': 10,
            'action_dim': 3,
            'hidden_dim': 128
        }
        
        training_config = {
            'lr': 1e-3,
            'buffer_size': 5000,
            'min_batch_size': 100,  # è¾ƒå°çš„æ‰¹æ¬¡å¤§å°
            'model_path': './test_shared_ppo_model.pth',
            'update_interval': 20
        }
        
        # åˆ›å»ºè®­ç»ƒå™¨
        print("ğŸ¤– åˆ›å»ºå…±äº«PPOè®­ç»ƒå™¨...")
        trainer = SharedPPOTrainer(model_config, training_config)
        
        # å¯åŠ¨è®­ç»ƒ
        print("ğŸš€ å¯åŠ¨è®­ç»ƒè¿›ç¨‹...")
        trainer.start_training()
        
        # æ¨¡æ‹Ÿæ·»åŠ ä¸€äº›ç»éªŒ
        print("ğŸ“Š æ·»åŠ æ¨¡æ‹Ÿç»éªŒæ•°æ®...")
        for i in range(150):  # æ·»åŠ 150ä¸ªç»éªŒï¼ˆè¶…è¿‡min_batch_sizeï¼‰
            experience = {
                'observation': np.random.randn(10).astype(np.float32),
                'action': np.random.randn(3).astype(np.float32),
                'reward': float(np.random.randn()),  # ğŸ”§ ä¿®å¤ï¼šè½¬æ¢ä¸ºPython float
                'next_observation': np.random.randn(10).astype(np.float32),
                'done': i % 50 == 49,  # æ¯50æ­¥ç»“æŸä¸€ä¸ªepisode
                'step': i
            }
            trainer.add_experience(experience)
            
            if i % 50 == 0:
                print(f"   ğŸ“ˆ å·²æ·»åŠ  {i+1} ä¸ªç»éªŒ...")
        
        # ç­‰å¾…è®­ç»ƒå¤„ç†
        print("â³ ç­‰å¾…è®­ç»ƒå¤„ç†ç»éªŒ...")
        time.sleep(5)
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¿å­˜
        if os.path.exists(training_config['model_path']):
            print("âœ… æ¨¡å‹æ–‡ä»¶å·²åˆ›å»º")
            model_state = torch.load(training_config['model_path'])
            print(f"   ğŸ“Š æ¨¡å‹åŒ…å«: {list(model_state.keys())}")
        else:
            print("âš ï¸ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°")
        
        # åœæ­¢è®­ç»ƒ
        print("ğŸ›‘ åœæ­¢è®­ç»ƒ...")
        trainer.stop_training()
        
        print("âœ… å…±äº«PPOåŸºç¡€åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_shared_ppo_multi_worker():
    """æµ‹è¯•å¤šå·¥ä½œå™¨å…±äº«PPOï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    print("\nğŸš€ æµ‹è¯•å…±äº«PPOè®­ç»ƒå™¨ - å¤šå·¥ä½œå™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰")
    print("=" * 50)
    
    try:
        from shared_ppo_trainer import SharedPPOTrainer
        
        # é…ç½®
        model_config = {
            'observation_dim': 8,
            'action_dim': 2,
            'hidden_dim': 64
        }
        
        training_config = {
            'lr': 2e-3,
            'buffer_size': 3000,
            'min_batch_size': 50,
            'model_path': './test_multi_worker_ppo.pth',
            'update_interval': 10
        }
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SharedPPOTrainer(model_config, training_config)
        trainer.start_training()
        
        # ğŸ”§ ç®€åŒ–ï¼šåœ¨ä¸»è¿›ç¨‹ä¸­æ¨¡æ‹Ÿå¤šä¸ªå·¥ä½œå™¨æ·»åŠ ç»éªŒ
        print("ğŸ‘¥ æ¨¡æ‹Ÿ3ä¸ªå·¥ä½œå™¨æ·»åŠ ç»éªŒ...")
        for worker_id in range(3):
            print(f"   ğŸ¤– å·¥ä½œå™¨ {worker_id} æ·»åŠ ç»éªŒ...")
            for step in range(60):  # æ¯ä¸ªå·¥ä½œå™¨60æ­¥
                experience = {
                    'observation': np.random.randn(8).astype(np.float32),
                    'action': np.random.randn(2).astype(np.float32),
                    'reward': float(np.random.randn() * 0.1),
                    'next_observation': np.random.randn(8).astype(np.float32),
                    'done': step % 20 == 19,
                    'worker_id': worker_id,
                    'step': step
                }
                trainer.add_experience(experience)
            
            print(f"   âœ… å·¥ä½œå™¨ {worker_id} å®Œæˆ (60ä¸ªç»éªŒ)")
        
        # ç­‰å¾…è®­ç»ƒå¤„ç†
        print("â³ ç­‰å¾…è®­ç»ƒå¤„ç†æ‰€æœ‰ç»éªŒ...")
        time.sleep(3)
        
        # åœæ­¢è®­ç»ƒ
        trainer.stop_training()
        
        print("âœ… å¤šå·¥ä½œå™¨æ¨¡æ‹Ÿæµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ å¤šå·¥ä½œå™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def cleanup_test_files():
    """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    test_files = [
        './test_shared_ppo_model.pth',
        './test_multi_worker_ppo.pth',
        './shared_ppo_demo.pth'
    ]
    
    for file_path in test_files:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"ğŸ§¹ æ¸…ç†æ–‡ä»¶: {file_path}")

if __name__ == "__main__":
    print("ğŸ§ª å…±äº«PPOè®­ç»ƒå™¨æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    import multiprocessing as mp
    mp.set_start_method('spawn', force=True)
    
    try:
        # è¿è¡Œæµ‹è¯•
        test_shared_ppo_basic()
        time.sleep(1)
        test_shared_ppo_multi_worker()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æµ‹è¯•è¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¥—ä»¶å¤±è´¥: {e}")
    finally:
        # æ¸…ç†
        print("\nğŸ§¹ æ¸…ç†æµ‹è¯•æ–‡ä»¶...")
        cleanup_test_files()
        print("âœ… æ¸…ç†å®Œæˆ")
