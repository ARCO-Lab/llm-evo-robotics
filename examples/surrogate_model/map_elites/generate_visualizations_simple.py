#!/usr/bin/env python3
"""
ç®€å•çš„MAP-Eliteså¯è§†åŒ–ç”Ÿæˆå™¨ï¼ˆä¸ä¾èµ–pandasï¼‰
"""

import os
import sys
import csv
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import argparse
from collections import defaultdict

def read_csv_data(csv_file: str) -> dict:
    """
    è¯»å–CSVæ–‡ä»¶å¹¶è¿”å›æ•°æ®å­—å…¸
    """
    data = defaultdict(list)
    
    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                for key, value in row.items():
                    # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                    try:
                        if '.' in value:
                            data[key].append(float(value))
                        else:
                            data[key].append(int(value))
                    except (ValueError, TypeError):
                        data[key].append(value)
        
        print(f"ğŸ“Š æˆåŠŸè¯»å–CSVæ–‡ä»¶: {len(data[list(data.keys())[0]])} æ¡è®°å½•")
        print(f"ğŸ“Š åˆ—å: {list(data.keys())}")
        return data
        
    except Exception as e:
        print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
        return {}

def create_map_elites_heatmap(data: dict, output_path: str) -> bool:
    """
    åˆ›å»ºMAP-Elitesçƒ­åŠ›å›¾
    """
    try:
        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        required_cols = ['num_links', 'total_length', 'fitness']
        if not all(col in data for col in required_cols):
            print(f"âš ï¸ ç¼ºå°‘å¿…éœ€çš„åˆ—: {required_cols}")
            return False
        
        num_links = np.array(data['num_links'])
        total_length = np.array(data['total_length'])
        fitness = np.array(data['fitness'])
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'MAP-Elites è®­ç»ƒç»“æœçƒ­åŠ›å›¾åˆ†æ\\nä¸ªä½“æ•°: {len(num_links)}', 
                    fontsize=16, fontweight='bold')
        
        # 1. å…³èŠ‚æ•° vs æ€»é•¿åº¦ æ•£ç‚¹å›¾
        scatter1 = ax1.scatter(num_links, total_length, c=fitness, 
                              cmap='viridis', s=100, alpha=0.7, edgecolors='white')
        ax1.set_xlabel('å…³èŠ‚æ•°')
        ax1.set_ylabel('æ€»é•¿åº¦ (px)')
        ax1.set_title('å…³èŠ‚æ•° vs æ€»é•¿åº¦')
        ax1.grid(True, alpha=0.3)
        plt.colorbar(scatter1, ax=ax1, label='é€‚åº”åº¦')
        
        # 2. å­¦ä¹ ç‡åˆ†æï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if 'lr' in data and 'alpha' in data:
            lr = np.array(data['lr'])
            alpha = np.array(data['alpha'])
            scatter2 = ax2.scatter(lr, alpha, c=fitness, 
                                 cmap='plasma', s=100, alpha=0.7, edgecolors='white')
            ax2.set_xlabel('å­¦ä¹ ç‡')
            ax2.set_ylabel('Alpha')
            ax2.set_title('å­¦ä¹ ç‡ vs Alpha')
            ax2.set_xscale('log')
            ax2.grid(True, alpha=0.3)
            plt.colorbar(scatter2, ax=ax2, label='é€‚åº”åº¦')
        else:
            ax2.text(0.5, 0.5, 'å­¦ä¹ ç‡æ•°æ®ä¸å¯ç”¨', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=14)
        
        # 3. é€‚åº”åº¦åˆ†å¸ƒ
        ax3.hist(fitness, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax3.set_xlabel('é€‚åº”åº¦')
        ax3.set_ylabel('ä¸ªä½“æ•°é‡')
        ax3.set_title('é€‚åº”åº¦åˆ†å¸ƒ')
        ax3.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_fitness = np.mean(fitness)
        max_fitness = np.max(fitness)
        ax3.axvline(mean_fitness, color='red', linestyle='--', 
                   label=f'å¹³å‡å€¼: {mean_fitness:.3f}')
        ax3.axvline(max_fitness, color='green', linestyle='--', 
                   label=f'æœ€å¤§å€¼: {max_fitness:.3f}')
        ax3.legend()
        
        # 4. å…³èŠ‚æ•°åˆ†å¸ƒ
        unique_links, counts = np.unique(num_links, return_counts=True)
        ax4.bar(unique_links, counts, alpha=0.7, color='lightcoral', edgecolor='black')
        ax4.set_xlabel('å…³èŠ‚æ•°')
        ax4.set_ylabel('ä¸ªä½“æ•°é‡')
        ax4.set_title('å…³èŠ‚æ•°åˆ†å¸ƒ')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… MAP-Elitesçƒ­åŠ›å›¾å·²ä¿å­˜: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ç”ŸæˆMAP-Elitesçƒ­åŠ›å›¾å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_training_metrics_plot(data: dict, output_path: str) -> bool:
    """
    åˆ›å»ºè®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–
    """
    try:
        if 'fitness' not in data:
            return False
        
        fitness = np.array(data['fitness'])
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'è®­ç»ƒæŒ‡æ ‡åˆ†æ\\næ€»ä¸ªä½“æ•°: {len(fitness)}', 
                    fontsize=16, fontweight='bold')
        
        # 1. é€‚åº”åº¦vsæˆåŠŸç‡
        if 'success_rate' in data:
            success_rate = np.array(data['success_rate'])
            ax1.scatter(fitness, success_rate, s=100, alpha=0.7, color='blue', edgecolors='white')
            ax1.set_xlabel('é€‚åº”åº¦')
            ax1.set_ylabel('æˆåŠŸç‡')
            ax1.set_title('é€‚åº”åº¦ vs æˆåŠŸç‡')
            ax1.grid(True, alpha=0.3)
            
            # æˆåŠŸç‡åˆ†å¸ƒ
            ax2.hist(success_rate, bins=20, alpha=0.7, color='green', edgecolor='black')
            ax2.set_xlabel('æˆåŠŸç‡')
            ax2.set_ylabel('ä¸ªä½“æ•°é‡')
            ax2.set_title('æˆåŠŸç‡åˆ†å¸ƒ')
            ax2.grid(True, alpha=0.3)
        else:
            ax1.text(0.5, 0.5, 'æˆåŠŸç‡æ•°æ®ä¸å¯ç”¨', ha='center', va='center', transform=ax1.transAxes)
            ax2.text(0.5, 0.5, 'æˆåŠŸç‡æ•°æ®ä¸å¯ç”¨', ha='center', va='center', transform=ax2.transAxes)
        
        # 3. å¥–åŠ±åˆ†æ
        if 'avg_reward' in data:
            avg_reward = np.array(data['avg_reward'])
            ax3.scatter(fitness, avg_reward, s=100, alpha=0.7, color='red', edgecolors='white')
            ax3.set_xlabel('é€‚åº”åº¦')
            ax3.set_ylabel('å¹³å‡å¥–åŠ±')
            ax3.set_title('é€‚åº”åº¦ vs å¹³å‡å¥–åŠ±')
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, 'å¥–åŠ±æ•°æ®ä¸å¯ç”¨', ha='center', va='center', transform=ax3.transAxes)
        
        # 4. ä»£é™…è¿›åŒ–ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if 'generation' in data:
            generation = np.array(data['generation'])
            
            # æŒ‰ä»£æ•°åˆ†ç»„è®¡ç®—ç»Ÿè®¡
            generations = {}
            for i, gen in enumerate(generation):
                if gen not in generations:
                    generations[gen] = []
                generations[gen].append(fitness[i])
            
            gen_nums = sorted(generations.keys())
            gen_max = [max(generations[gen]) for gen in gen_nums]
            gen_mean = [np.mean(generations[gen]) for gen in gen_nums]
            
            ax4.plot(gen_nums, gen_max, 'b-o', label='æœ€ä½³é€‚åº”åº¦', linewidth=2, markersize=6)
            ax4.plot(gen_nums, gen_mean, 'r--s', label='å¹³å‡é€‚åº”åº¦', linewidth=2, markersize=6)
            ax4.set_xlabel('ä»£æ•°')
            ax4.set_ylabel('é€‚åº”åº¦')
            ax4.set_title('ä»£é™…è¿›åŒ–è¶‹åŠ¿')
            ax4.grid(True, alpha=0.3)
            ax4.legend()
        else:
            ax4.text(0.5, 0.5, 'ä»£æ•°æ•°æ®ä¸å¯ç”¨', ha='center', va='center', transform=ax4.transAxes)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–å·²ä¿å­˜: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_neural_network_loss_simulation(data: dict, output_path: str) -> bool:
    """
    åˆ›å»ºæ¨¡æ‹Ÿçš„ç¥ç»ç½‘ç»œLosså¯è§†åŒ–
    """
    try:
        if 'fitness' not in data:
            return False
        
        fitness = np.array(data['fitness'])
        n_individuals = len(fitness)
        
        # æ¨¡æ‹Ÿä¸åŒç½‘ç»œç»„ä»¶çš„Losså˜åŒ–
        np.random.seed(42)  # ç¡®ä¿ç»“æœå¯é‡ç°
        
        # ä¸ºæ¯ä¸ªä¸ªä½“æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        training_steps = 100
        steps = np.arange(training_steps)
        
        # æ¨¡æ‹Ÿä¸åŒç½‘ç»œçš„loss
        actor_losses = []
        critic_losses = []
        attention_losses = []
        gnn_losses = []
        
        for i in range(n_individuals):
            # åŸºäºfitnessç”Ÿæˆä¸åŒçš„lossæ›²çº¿
            base_fitness = fitness[i]
            
            # Actor loss: ä»é«˜å¼€å§‹ï¼Œé€æ¸ä¸‹é™
            actor_loss = 10.0 * (1 - base_fitness) * np.exp(-steps / 50) + np.random.normal(0, 0.1, training_steps)
            actor_losses.append(actor_loss)
            
            # Critic loss: ç±»ä¼¼ä½†ç¨å¾®ä¸åŒçš„æ¨¡å¼
            critic_loss = 8.0 * (1 - base_fitness) * np.exp(-steps / 40) + np.random.normal(0, 0.15, training_steps)
            critic_losses.append(critic_loss)
            
            # Attention loss: æ›´å¿«æ”¶æ•›
            attention_loss = 5.0 * (1 - base_fitness) * np.exp(-steps / 30) + np.random.normal(0, 0.08, training_steps)
            attention_losses.append(attention_loss)
            
            # GNN loss: ä¸­ç­‰æ”¶æ•›é€Ÿåº¦
            gnn_loss = 6.0 * (1 - base_fitness) * np.exp(-steps / 45) + np.random.normal(0, 0.12, training_steps)
            gnn_losses.append(gnn_loss)
        
        # è®¡ç®—å¹³å‡lossæ›²çº¿
        avg_actor_loss = np.mean(actor_losses, axis=0)
        avg_critic_loss = np.mean(critic_losses, axis=0)
        avg_attention_loss = np.mean(attention_losses, axis=0)
        avg_gnn_loss = np.mean(gnn_losses, axis=0)
        
        # åˆ›å»ºå¯è§†åŒ–
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'ç¥ç»ç½‘ç»œLosså˜åŒ–åˆ†æï¼ˆåŸºäº{n_individuals}ä¸ªä¸ªä½“æ¨¡æ‹Ÿï¼‰', 
                    fontsize=16, fontweight='bold')
        
        # 1. ä¸»è¦ç½‘ç»œLossæ›²çº¿
        ax1.plot(steps, avg_actor_loss, 'r-', linewidth=2, label='Actor Loss', alpha=0.8)
        ax1.plot(steps, avg_critic_loss, 'b-', linewidth=2, label='Critic Loss', alpha=0.8)
        ax1.plot(steps, avg_attention_loss, 'g-', linewidth=2, label='Attention Loss', alpha=0.8)
        ax1.plot(steps, avg_gnn_loss, 'm-', linewidth=2, label='GNN Loss', alpha=0.8)
        ax1.set_xlabel('è®­ç»ƒæ­¥æ•°')
        ax1.set_ylabel('Losså€¼')
        ax1.set_title('ä¸»è¦ç½‘ç»œLosså˜åŒ–')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_yscale('log')
        
        # 2. Lossæ”¶æ•›åˆ†æ
        final_losses = [avg_actor_loss[-1], avg_critic_loss[-1], 
                       avg_attention_loss[-1], avg_gnn_loss[-1]]
        network_names = ['Actor', 'Critic', 'Attention', 'GNN']
        colors = ['red', 'blue', 'green', 'magenta']
        
        bars = ax2.bar(network_names, final_losses, color=colors, alpha=0.7, edgecolor='black')
        ax2.set_ylabel('æœ€ç»ˆLosså€¼')
        ax2.set_title('å„ç½‘ç»œæœ€ç»ˆLosså¯¹æ¯”')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, loss in zip(bars, final_losses):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{loss:.3f}', ha='center', va='bottom')
        
        # 3. Lossä¸‹é™é€Ÿåº¦åˆ†æ
        # è®¡ç®—æ¯ä¸ªç½‘ç»œè¾¾åˆ°50%åˆå§‹lossçš„æ­¥æ•°
        convergence_steps = []
        initial_losses = [avg_actor_loss[0], avg_critic_loss[0], 
                         avg_attention_loss[0], avg_gnn_loss[0]]
        all_losses = [avg_actor_loss, avg_critic_loss, avg_attention_loss, avg_gnn_loss]
        
        for i, loss_curve in enumerate(all_losses):
            target = initial_losses[i] * 0.5
            convergence_step = np.argmax(loss_curve <= target)
            convergence_steps.append(convergence_step if convergence_step > 0 else training_steps)
        
        bars = ax3.bar(network_names, convergence_steps, color=colors, alpha=0.7, edgecolor='black')
        ax3.set_ylabel('æ”¶æ•›æ­¥æ•°')
        ax3.set_title('æ”¶æ•›é€Ÿåº¦å¯¹æ¯”ï¼ˆè¾¾åˆ°50%åˆå§‹Lossï¼‰')
        ax3.grid(True, alpha=0.3)
        
        for bar, steps in zip(bars, convergence_steps):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height,
                    f'{steps}', ha='center', va='bottom')
        
        # 4. Lossç¨³å®šæ€§åˆ†æ
        # è®¡ç®—æœ€å20æ­¥çš„æ ‡å‡†å·®
        stability_scores = []
        for loss_curve in all_losses:
            last_20_steps = loss_curve[-20:]
            stability = np.std(last_20_steps)
            stability_scores.append(stability)
        
        bars = ax4.bar(network_names, stability_scores, color=colors, alpha=0.7, edgecolor='black')
        ax4.set_ylabel('Lossç¨³å®šæ€§ (æ ‡å‡†å·®)')
        ax4.set_title('è®­ç»ƒç¨³å®šæ€§åˆ†æï¼ˆæœ€å20æ­¥ï¼‰')
        ax4.grid(True, alpha=0.3)
        
        for bar, stability in zip(bars, stability_scores):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height,
                    f'{stability:.4f}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ç¥ç»ç½‘ç»œLosså¯è§†åŒ–å·²ä¿å­˜: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆç¥ç»ç½‘ç»œLosså¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_comprehensive_report(data: dict, output_path: str) -> bool:
    """
    åˆ›å»ºç»¼åˆç»Ÿè®¡æŠ¥å‘Š
    """
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("MAP-Elites ç»¼åˆè®­ç»ƒç»“æœæŠ¥å‘Š\\n")
            f.write("=" * 50 + "\\n\\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
            f.write(f"æ€»ä¸ªä½“æ•°: {len(data[list(data.keys())[0]])}\\n\\n")
            
            # åŸºç¡€ç»Ÿè®¡
            f.write("ğŸ“Š åŸºç¡€ç»Ÿè®¡ä¿¡æ¯:\\n")
            f.write("-" * 30 + "\\n")
            
            if 'fitness' in data:
                fitness = np.array(data['fitness'])
                f.write(f"é€‚åº”åº¦ç»Ÿè®¡:\\n")
                f.write(f"  å¹³å‡å€¼: {np.mean(fitness):.4f}\\n")
                f.write(f"  æœ€å¤§å€¼: {np.max(fitness):.4f}\\n")
                f.write(f"  æœ€å°å€¼: {np.min(fitness):.4f}\\n")
                f.write(f"  æ ‡å‡†å·®: {np.std(fitness):.4f}\\n\\n")
            
            if 'success_rate' in data:
                success_rate = np.array(data['success_rate'])
                f.write(f"æˆåŠŸç‡ç»Ÿè®¡:\\n")
                f.write(f"  å¹³å‡æˆåŠŸç‡: {np.mean(success_rate):.4f}\\n")
                f.write(f"  æœ€é«˜æˆåŠŸç‡: {np.max(success_rate):.4f}\\n")
                f.write(f"  æˆåŠŸç‡>0.5çš„ä¸ªä½“: {np.sum(success_rate > 0.5)}ä¸ª\\n\\n")
            
            if 'num_links' in data:
                num_links = np.array(data['num_links'])
                f.write(f"æœºå™¨äººç»“æ„ç»Ÿè®¡:\\n")
                f.write(f"  å…³èŠ‚æ•°èŒƒå›´: {np.min(num_links)}-{np.max(num_links)}\\n")
                unique_links, counts = np.unique(num_links, return_counts=True)
                for links, count in zip(unique_links, counts):
                    f.write(f"  {int(links)}å…³èŠ‚: {count}ä¸ª ({count/len(num_links)*100:.1f}%)\\n")
                f.write("\\n")
            
            if 'total_length' in data:
                total_length = np.array(data['total_length'])
                f.write(f"æœºå™¨äººé•¿åº¦ç»Ÿè®¡:\\n")
                f.write(f"  å¹³å‡é•¿åº¦: {np.mean(total_length):.1f}px\\n")
                f.write(f"  é•¿åº¦èŒƒå›´: {np.min(total_length):.1f}-{np.max(total_length):.1f}px\\n\\n")
            
            # ä»£é™…åˆ†æ
            if 'generation' in data:
                generation = np.array(data['generation'])
                fitness = np.array(data['fitness'])
                
                f.write("ğŸ§¬ ä»£é™…è¿›åŒ–åˆ†æ:\\n")
                f.write("-" * 30 + "\\n")
                
                generations = {}
                for i, gen in enumerate(generation):
                    if gen not in generations:
                        generations[gen] = []
                    generations[gen].append(fitness[i])
                
                for gen in sorted(generations.keys()):
                    gen_fitness = generations[gen]
                    f.write(f"ç¬¬{gen}ä»£: {len(gen_fitness)}ä¸ªä¸ªä½“, ")
                    f.write(f"æœ€ä½³é€‚åº”åº¦={max(gen_fitness):.4f}, ")
                    f.write(f"å¹³å‡é€‚åº”åº¦={np.mean(gen_fitness):.4f}\\n")
                f.write("\\n")
            
            # æœ€ä½³ä¸ªä½“ä¿¡æ¯
            if 'fitness' in data:
                fitness = np.array(data['fitness'])
                best_idx = np.argmax(fitness)
                f.write("ğŸ† æœ€ä½³ä¸ªä½“ä¿¡æ¯:\\n")
                f.write("-" * 30 + "\\n")
                f.write(f"é€‚åº”åº¦: {fitness[best_idx]:.4f}\\n")
                
                if 'num_links' in data:
                    f.write(f"å…³èŠ‚æ•°: {data['num_links'][best_idx]}\\n")
                if 'total_length' in data:
                    f.write(f"æ€»é•¿åº¦: {data['total_length'][best_idx]:.1f}px\\n")
                if 'success_rate' in data:
                    f.write(f"æˆåŠŸç‡: {data['success_rate'][best_idx]:.4f}\\n")
                if 'lr' in data:
                    f.write(f"å­¦ä¹ ç‡: {data['lr'][best_idx]:.2e}\\n")
                if 'alpha' in data:
                    f.write(f"Alpha: {data['alpha'][best_idx]:.4f}\\n")
        
        print(f"âœ… ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆç»¼åˆæŠ¥å‘Šå¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç”ŸæˆMAP-Elitesç®€å•å¯è§†åŒ–")
    parser.add_argument("--csv-file", type=str, 
                       default="./experiment_results/session_20250917_160838/results.csv",
                       help="CSVç»“æœæ–‡ä»¶")
    parser.add_argument("--output-dir", type=str, 
                       default="./map_elites_shared_ppo_results/visualizations",
                       help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()
    
    print("ğŸ¨ ç”ŸæˆMAP-Elitesç®€å•å¯è§†åŒ–")
    print("=" * 60)
    
    if not os.path.exists(args.csv_file):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {args.csv_file}")
        return
    
    # è¯»å–æ•°æ®
    data = read_csv_data(args.csv_file)
    if not data:
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    success_count = 0
    
    # 1. ç”ŸæˆMAP-Elitesçƒ­åŠ›å›¾
    print("\\nğŸ“Š ç”ŸæˆMAP-Elitesçƒ­åŠ›å›¾...")
    heatmap_path = os.path.join(args.output_dir, "map_elites_heatmap.png")
    if create_map_elites_heatmap(data, heatmap_path):
        success_count += 1
    
    # 2. ç”Ÿæˆè®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–
    print("\\nğŸ“ˆ ç”Ÿæˆè®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–...")
    metrics_path = os.path.join(args.output_dir, "training_metrics.png")
    if create_training_metrics_plot(data, metrics_path):
        success_count += 1
    
    # 3. ç”Ÿæˆç¥ç»ç½‘ç»œLosså¯è§†åŒ–ï¼ˆæ¨¡æ‹Ÿï¼‰
    print("\\nğŸ§  ç”Ÿæˆç¥ç»ç½‘ç»œLosså¯è§†åŒ–...")
    loss_path = os.path.join(args.output_dir, "neural_network_losses.png")
    if create_neural_network_loss_simulation(data, loss_path):
        success_count += 1
    
    # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\\nğŸ“ ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
    report_path = os.path.join(args.output_dir, "comprehensive_report.txt")
    if create_comprehensive_report(data, report_path):
        success_count += 1
    
    print("\\n" + "=" * 60)
    print(f"ğŸ‰ å¯è§†åŒ–ç”Ÿæˆå®Œæˆ! æˆåŠŸ: {success_count}/4")
    print(f"ğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {args.output_dir}")

if __name__ == "__main__":
    main()

