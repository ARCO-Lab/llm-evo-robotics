#!/usr/bin/env python3
"""
åˆ†æç°æœ‰è®­ç»ƒæ—¥å¿—è„šæœ¬
ä»è®­ç»ƒçš„æ§åˆ¶å°è¾“å‡ºä¸­æå–æŸå¤±ä¿¡æ¯å¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
"""

import re
import numpy as np
import matplotlib.pyplot as plt
import os
import json
from collections import defaultdict
import argparse

def parse_training_log(log_file_path):
    """ä»è®­ç»ƒæ—¥å¿—æ–‡ä»¶ä¸­è§£ææŸå¤±ä¿¡æ¯"""
    
    print(f"ğŸ“– è§£æè®­ç»ƒæ—¥å¿—: {log_file_path}")
    
    # ç”¨äºå­˜å‚¨è§£æçš„æ•°æ®
    training_data = {
        'steps': [],
        'critic_loss': [],
        'actor_loss': [],
        'alpha_loss': [],
        'alpha': [],
        'q1_mean': [],
        'q2_mean': [],
        'buffer_size': [],
        'entropy_term': [],
        'q_term': [],
        'log_probs_mean': []
    }
    
    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    patterns = {
        'step_pattern': r'Step (\d+).*?Critic Loss: ([\d\.-]+), Actor Loss: ([\d\.-]+), Alpha: ([\d\.-]+), Buffer Size: (\d+)',
        'entropy_pattern': r'Entropy Term \(Î±\*log_Ï€\): ([\d\.-]+)',
        'q_term_pattern': r'Q Term \(Qå€¼\): ([\d\.-]+)',
        'q_means_pattern': r'q1_mean.*?: ([\d\.-]+).*?q2_mean.*?: ([\d\.-]+)'
    }
    
    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # æå–ä¸»è¦æŸå¤±ä¿¡æ¯
        step_matches = re.findall(patterns['step_pattern'], content)
        
        for match in step_matches:
            step, critic_loss, actor_loss, alpha, buffer_size = match
            training_data['steps'].append(int(step))
            training_data['critic_loss'].append(float(critic_loss))
            training_data['actor_loss'].append(float(actor_loss))
            training_data['alpha'].append(float(alpha))
            training_data['buffer_size'].append(int(buffer_size))
        
        # æå–ç†µé¡¹ä¿¡æ¯
        entropy_matches = re.findall(patterns['entropy_pattern'], content)
        training_data['entropy_term'] = [float(match) for match in entropy_matches]
        
        # æå–Qé¡¹ä¿¡æ¯
        q_term_matches = re.findall(patterns['q_term_pattern'], content)
        training_data['q_term'] = [float(match) for match in q_term_matches]
        
        print(f"âœ… è§£æå®Œæˆ:")
        print(f"   æ‰¾åˆ° {len(training_data['steps'])} ä¸ªè®­ç»ƒæ­¥éª¤è®°å½•")
        print(f"   æ­¥éª¤èŒƒå›´: {min(training_data['steps']) if training_data['steps'] else 0} - {max(training_data['steps']) if training_data['steps'] else 0}")
        
        return training_data
        
    except Exception as e:
        print(f"âŒ è§£ææ—¥å¿—å¤±è´¥: {e}")
        return None


def plot_training_analysis(training_data, save_dir):
    """ç”Ÿæˆè®­ç»ƒåˆ†æå›¾è¡¨"""
    
    if not training_data or not training_data['steps']:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä»¥ç»˜åˆ¶")
        return
    
    steps = training_data['steps']
    
    # åˆ›å»ºå¤šå­å›¾
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    # 1. ä¸»è¦æŸå¤±æ›²çº¿
    ax = axes[0]
    if training_data['critic_loss']:
        ax.plot(steps, training_data['critic_loss'], 'b-', label='Critic Loss', linewidth=2)
    if training_data['actor_loss']:
        ax.plot(steps, training_data['actor_loss'], 'r-', label='Actor Loss', linewidth=2)
    if training_data.get('alpha_loss'):
        ax.plot(steps, training_data['alpha_loss'], 'g-', label='Alpha Loss', linewidth=2)
    
    ax.set_title('Training Losses', fontsize=14, fontweight='bold')
    ax.set_xlabel('Training Step')
    ax.set_ylabel('Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. Alpha å˜åŒ–
    ax = axes[1]
    if training_data['alpha']:
        ax.plot(steps, training_data['alpha'], 'purple', linewidth=2)
        ax.set_title('Alpha (Temperature) Evolution', fontsize=14)
        ax.set_xlabel('Training Step')
        ax.set_ylabel('Alpha Value')
        ax.grid(True, alpha=0.3)
    
    # 3. Buffer Size
    ax = axes[2]
    if training_data['buffer_size']:
        ax.plot(steps, training_data['buffer_size'], 'orange', linewidth=2)
        ax.set_title('Replay Buffer Size', fontsize=14)
        ax.set_xlabel('Training Step')
        ax.set_ylabel('Buffer Size')
        ax.grid(True, alpha=0.3)
    
    # 4. Actor Loss ç»„ä»¶åˆ†æ
    ax = axes[3]
    if training_data['entropy_term'] and training_data['q_term']:
        # åŒ¹é…é•¿åº¦
        min_len = min(len(training_data['entropy_term']), len(training_data['q_term']), len(steps))
        entropy_steps = steps[:min_len]
        
        ax.plot(entropy_steps, training_data['entropy_term'][:min_len], 'cyan', 
               label='Entropy Term (Î±*log_Ï€)', linewidth=2)
        ax.plot(entropy_steps, training_data['q_term'][:min_len], 'magenta', 
               label='Q Term', linewidth=2)
        
        ax.set_title('Actor Loss Components', fontsize=14)
        ax.set_xlabel('Training Step')
        ax.set_ylabel('Value')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    # 5. æŸå¤±è¶‹åŠ¿åˆ†æï¼ˆç§»åŠ¨å¹³å‡ï¼‰
    ax = axes[4]
    if len(training_data['critic_loss']) > 10:
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        window = min(50, len(training_data['critic_loss']) // 10)
        critic_smooth = np.convolve(training_data['critic_loss'], np.ones(window)/window, mode='valid')
        actor_smooth = np.convolve(training_data['actor_loss'], np.ones(window)/window, mode='valid')
        smooth_steps = steps[window-1:]
        
        ax.plot(smooth_steps, critic_smooth, 'b-', label=f'Critic Loss (MA{window})', linewidth=3)
        ax.plot(smooth_steps, actor_smooth, 'r-', label=f'Actor Loss (MA{window})', linewidth=3)
        
        ax.set_title('Loss Trends (Moving Average)', fontsize=14)
        ax.set_xlabel('Training Step')
        ax.set_ylabel('Loss')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    # 6. å­¦ä¹ è¿›åº¦æ€»ç»“
    ax = axes[5]
    if training_data['critic_loss'] and training_data['actor_loss']:
        # è®¡ç®—å­¦ä¹ è¿›åº¦æŒ‡æ ‡
        early_critic = np.mean(training_data['critic_loss'][:min(len(training_data['critic_loss'])//4, 10)])
        late_critic = np.mean(training_data['critic_loss'][-min(len(training_data['critic_loss'])//4, 10):])
        
        early_actor = np.mean(training_data['actor_loss'][:min(len(training_data['actor_loss'])//4, 10)])
        late_actor = np.mean(training_data['actor_loss'][-min(len(training_data['actor_loss'])//4, 10):])
        
        metrics = ['Early Critic', 'Late Critic', 'Early Actor', 'Late Actor']
        values = [early_critic, late_critic, early_actor, late_actor]
        colors = ['lightblue', 'darkblue', 'lightcoral', 'darkred']
        
        bars = ax.bar(metrics, values, color=colors)
        ax.set_title('Learning Progress Summary', fontsize=14)
        ax.set_ylabel('Average Loss')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{value:.3f}', ha='center', va='bottom')
    
    plt.suptitle('Training Loss Analysis Report', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    save_path = os.path.join(save_dir, 'training_analysis.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“ˆ è®­ç»ƒåˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()
    
    return save_path


def generate_summary_report(training_data, save_dir):
    """ç”Ÿæˆè®­ç»ƒæ€»ç»“æŠ¥å‘Š"""
    
    if not training_data or not training_data['steps']:
        print("âŒ æ²¡æœ‰æ•°æ®ç”ŸæˆæŠ¥å‘Š")
        return
    
    report_path = os.path.join(save_dir, 'training_analysis_report.txt')
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Training Analysis Report\n")
        f.write("=" * 50 + "\n\n")
        
        # åŸºæœ¬ä¿¡æ¯
        f.write("åŸºæœ¬è®­ç»ƒä¿¡æ¯:\n")
        f.write(f"  æ€»è®­ç»ƒæ­¥æ•°: {training_data['steps'][-1] if training_data['steps'] else 0}\n")
        f.write(f"  è®°å½•çš„æ­¥éª¤æ•°: {len(training_data['steps'])}\n")
        f.write(f"  æ­¥éª¤èŒƒå›´: {min(training_data['steps'])} - {max(training_data['steps'])}\n\n")
        
        # æŸå¤±ç»Ÿè®¡
        for metric_name in ['critic_loss', 'actor_loss', 'alpha']:
            if metric_name in training_data and training_data[metric_name]:
                values = training_data[metric_name]
                f.write(f"{metric_name} ç»Ÿè®¡:\n")
                f.write(f"  æœ€ç»ˆå€¼: {values[-1]:.6f}\n")
                f.write(f"  å¹³å‡å€¼: {np.mean(values):.6f}\n")
                f.write(f"  æ ‡å‡†å·®: {np.std(values):.6f}\n")
                f.write(f"  æœ€å°å€¼: {np.min(values):.6f}\n")
                f.write(f"  æœ€å¤§å€¼: {np.max(values):.6f}\n")
                
                # è®¡ç®—æ”¹è¿›æƒ…å†µ
                if len(values) > 10:
                    early_avg = np.mean(values[:len(values)//4])
                    late_avg = np.mean(values[-len(values)//4:])
                    improvement = (early_avg - late_avg) / early_avg * 100
                    f.write(f"  æ”¹è¿›ç¨‹åº¦: {improvement:.2f}%\n")
                f.write("\n")
        
        # Bufferä½¿ç”¨æƒ…å†µ
        if training_data['buffer_size']:
            f.write("Replay Buffer ä½¿ç”¨æƒ…å†µ:\n")
            f.write(f"  æœ€ç»ˆBufferå¤§å°: {training_data['buffer_size'][-1]}\n")
            f.write(f"  å¹³å‡Bufferå¤§å°: {np.mean(training_data['buffer_size']):.0f}\n")
            f.write(f"  Bufferå¢é•¿ç‡: {(training_data['buffer_size'][-1] - training_data['buffer_size'][0]) / len(training_data['buffer_size']):.1f} per step\n\n")
        
        # å­¦ä¹ ç¨³å®šæ€§åˆ†æ
        if len(training_data['critic_loss']) > 20:
            recent_critic = training_data['critic_loss'][-10:]
            critic_stability = np.std(recent_critic) / np.mean(recent_critic)
            f.write("å­¦ä¹ ç¨³å®šæ€§åˆ†æ:\n")
            f.write(f"  æœ€è¿‘10æ­¥Critic Losså˜å¼‚ç³»æ•°: {critic_stability:.4f}\n")
            if critic_stability < 0.1:
                f.write("  -> è®­ç»ƒå·²æ”¶æ•›ï¼ŒæŸå¤±ç¨³å®š\n")
            elif critic_stability < 0.3:
                f.write("  -> è®­ç»ƒåŸºæœ¬ç¨³å®šï¼Œä»æœ‰å°å¹…æ³¢åŠ¨\n")
            else:
                f.write("  -> è®­ç»ƒå°šä¸ç¨³å®šï¼ŒæŸå¤±æ³¢åŠ¨è¾ƒå¤§\n")
    
    print(f"ğŸ“‹ è®­ç»ƒåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    return report_path


def main():
    parser = argparse.ArgumentParser(description="åˆ†æè®­ç»ƒæ—¥å¿—å¹¶ç”Ÿæˆå›¾è¡¨")
    parser.add_argument('--log-file', type=str, required=True,
                        help='è®­ç»ƒæ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str, default='./log_analysis_output',
                        help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # è§£ææ—¥å¿—
    training_data = parse_training_log(args.log_file)
    
    if training_data:
        # ç”Ÿæˆå›¾è¡¨
        plot_training_analysis(training_data, args.output_dir)
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_summary_report(training_data, args.output_dir)
        
        # ä¿å­˜è§£æçš„æ•°æ®
        data_path = os.path.join(args.output_dir, 'parsed_training_data.json')
        with open(data_path, 'w') as f:
            json.dump(training_data, f, indent=2)
        print(f"ğŸ’¾ è§£æçš„æ•°æ®å·²ä¿å­˜åˆ°: {data_path}")
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼Œæ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
    else:
        print("âŒ æ—¥å¿—è§£æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆåˆ†æç»“æœ")


if __name__ == "__main__":
    main() 