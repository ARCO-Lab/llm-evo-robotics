# è®­ç»ƒæŸå¤±ç›‘æ§ç³»ç»Ÿä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬ä¸ºä½ åˆ›å»ºçš„è®­ç»ƒæŸå¤±è®°å½•å’Œç›‘æ§ç³»ç»Ÿã€‚

## ğŸ¯ **ç³»ç»ŸåŠŸèƒ½æ¦‚è§ˆ**

### ä¸»è¦åŠŸèƒ½
- âœ… **å®æ—¶æŸå¤±è®°å½•** - è‡ªåŠ¨è®°å½•å„ç½‘ç»œçš„è®­ç»ƒæŸå¤±
- âœ… **å¯è§†åŒ–å›¾è¡¨** - ç”ŸæˆæŸå¤±æ›²çº¿å’Œè¶‹åŠ¿åˆ†æ
- âœ… **å¼‚å¸¸ç›‘æ§** - å®æ—¶ç›‘æ§å¼‚å¸¸æƒ…å†µå¹¶å‘å‡ºè­¦æŠ¥
- âœ… **å¤šæ ¼å¼å¯¼å‡º** - æ”¯æŒCSVã€JSONã€PNGç­‰å¤šç§æ ¼å¼
- âœ… **ç»Ÿè®¡åˆ†æ** - æä¾›è¯¦ç»†çš„è®­ç»ƒç»Ÿè®¡æŠ¥å‘Š

### è„šæœ¬æ–‡ä»¶è¯´æ˜
```
training_logger.py          # æ ¸å¿ƒè®­ç»ƒç›‘æ§ç³»ç»Ÿ
enhanced_train.py          # é›†æˆç›‘æ§çš„å¢å¼ºè®­ç»ƒè„šæœ¬
analyze_existing_logs.py    # åˆ†æç°æœ‰æ—¥å¿—çš„å·¥å…·è„šæœ¬
```

---

## ğŸš€ **æ–¹æ³•1: ä½¿ç”¨å¢å¼ºç‰ˆè®­ç»ƒè„šæœ¬**

### åŸºæœ¬ä½¿ç”¨
```bash
# è¿›å…¥è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# è¿è¡Œå¢å¼ºç‰ˆè®­ç»ƒï¼ˆReacher2Dï¼‰
cd examples/surrogate_model
python enhanced_train.py --test-reacher2d
```

### è‡ªå®šä¹‰é…ç½®
```bash
# è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
python enhanced_train.py --test-reacher2d \
    --num-processes 4 \
    --lr 1e-3 \
    --seed 123
```

### è¾“å‡ºç»“æœ
è¿è¡Œåä¼šåœ¨ä»¥ä¸‹ä½ç½®ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒè®°å½•ï¼š
```
./trained_models/reacher2d/enhanced_test/[timestamp]/
â”œâ”€â”€ training_logs/
â”‚   â””â”€â”€ reacher2d_sac_[timestamp]/
â”‚       â”œâ”€â”€ training_log.csv         # CSVæ ¼å¼çš„è¯¦ç»†è®°å½•
â”‚       â”œâ”€â”€ training_log.json        # JSONæ ¼å¼çš„æ•°æ®
â”‚       â”œâ”€â”€ training_curves_*.png    # æŸå¤±æ›²çº¿å›¾
â”‚       â”œâ”€â”€ training_report.txt      # è®­ç»ƒæ€»ç»“æŠ¥å‘Š
â”‚       â””â”€â”€ config.json              # å®éªŒé…ç½®
â””â”€â”€ best_models/                     # ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶
    â”œâ”€â”€ final_model_step_*.pth
    â””â”€â”€ checkpoint_step_*.pth
```

---

## ğŸ”§ **æ–¹æ³•2: æ‰‹åŠ¨é›†æˆåˆ°ç°æœ‰è®­ç»ƒè„šæœ¬**

### æ­¥éª¤1: å¯¼å…¥ç›‘æ§ç³»ç»Ÿ
```python
from training_logger import TrainingLogger, RealTimeMonitor

# åˆå§‹åŒ–logger
experiment_name = f"my_experiment_{time.strftime('%Y%m%d_%H%M%S')}"
logger = TrainingLogger(
    log_dir="training_logs",
    experiment_name=experiment_name
)

# è®¾ç½®ç›‘æ§é˜ˆå€¼
monitor = RealTimeMonitor(logger, alert_thresholds={
    'critic_loss': {'max': 50.0, 'nan_check': True},
    'actor_loss': {'max': 10.0, 'nan_check': True},
    'alpha_loss': {'max': 5.0, 'nan_check': True},
})
```

### æ­¥éª¤2: åœ¨è®­ç»ƒå¾ªç¯ä¸­è®°å½•æŸå¤±
```python
for step in range(num_steps):
    # ... è®­ç»ƒä»£ç  ...
    
    # æ›´æ–°ç½‘ç»œå¹¶è·å–æŸå¤±
    if should_update:
        metrics = sac.update()  # è¿”å›æŸå¤±å­—å…¸
        
        if metrics:
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            enhanced_metrics = metrics.copy()
            enhanced_metrics.update({
                'step': step,
                'buffer_size': len(sac.memory),
                'learning_rate': optimizer.param_groups[0]['lr']
            })
            
            # è®°å½•åˆ°ç›‘æ§ç³»ç»Ÿ
            logger.log_step(step, enhanced_metrics, episode=episode_count)
            alerts = monitor.check_alerts(step, enhanced_metrics)
            
            # å®šæœŸæ‰“å°ç»Ÿè®¡å’Œç”Ÿæˆå›¾è¡¨
            if step % 100 == 0:
                logger.print_current_stats(step, detailed=True)
            
            if step % 1000 == 0:
                logger.plot_losses(recent_steps=2000, show=False)
```

### æ­¥éª¤3: è®­ç»ƒç»“æŸæ—¶ç”ŸæˆæŠ¥å‘Š
```python
# è®­ç»ƒå®Œæˆå
logger.generate_report()
logger.plot_losses(show=False)
print(f"ğŸ“Š å®Œæ•´è®­ç»ƒæ—¥å¿—: {logger.experiment_dir}")
```

---

## ğŸ“Š **æ–¹æ³•3: åˆ†æç°æœ‰è®­ç»ƒæ—¥å¿—**

å¦‚æœä½ å·²ç»æœ‰è®­ç»ƒè¾“å‡ºçš„æ–‡æœ¬æ—¥å¿—ï¼Œå¯ä»¥ç”¨åˆ†æè„šæœ¬æå–æŸå¤±ä¿¡æ¯ï¼š

```bash
# åˆ†æç°æœ‰çš„è®­ç»ƒæ—¥å¿—æ–‡ä»¶
python analyze_existing_logs.py \
    --log-file /path/to/training_output.log \
    --output-dir ./analysis_results

# ç¤ºä¾‹ï¼šåˆ†æä¸Šæ¬¡è®­ç»ƒçš„æ—¥å¿—
python analyze_existing_logs.py \
    --log-file ../../trained_models/reacher2d/test/*/logs.txt \
    --output-dir ./log_analysis
```

---

## ğŸ“ˆ **ç”Ÿæˆçš„å›¾è¡¨å’ŒæŠ¥å‘Š**

### 1. è®­ç»ƒæ›²çº¿å›¾ (`training_curves_*.png`)
åŒ…å«6ä¸ªå­å›¾ï¼š
- **SAC Losses**: Critic Loss, Actor Loss, Alpha Loss
- **Q Values**: Q1 Mean, Q2 Mean  
- **Policy Metrics**: Alpha, Entropy Term, Q Term
- **Episode Metrics**: Episode Reward, Episode Length
- **Loss Trends**: ç§»åŠ¨å¹³å‡è¶‹åŠ¿çº¿
- **Learning Progress**: æ—©æœŸvsåæœŸæ€§èƒ½å¯¹æ¯”

### 2. è®­ç»ƒæŠ¥å‘Š (`training_report.txt`)
åŒ…å«ï¼š
- å®éªŒåŸºæœ¬ä¿¡æ¯ï¼ˆæ­¥æ•°ã€æ—¶é—´ã€é€Ÿåº¦ï¼‰
- å„ç½‘ç»œæŸå¤±çš„è¯¦ç»†ç»Ÿè®¡
- Replay Bufferä½¿ç”¨æƒ…å†µ
- å­¦ä¹ ç¨³å®šæ€§åˆ†æ

### 3. æ•°æ®æ–‡ä»¶
- **CSVæ ¼å¼** (`training_log.csv`): é€‚åˆExcelåˆ†æ
- **JSONæ ¼å¼** (`training_log.json`): é€‚åˆç¨‹åºåŒ–å¤„ç†
- **Pickleæ ¼å¼** (`training_logger.pkl`): å¯é‡æ–°åŠ è½½å®Œæ•´Loggerå¯¹è±¡

---

## ğŸš¨ **å®æ—¶ç›‘æ§å’Œè­¦æŠ¥**

### ç›‘æ§æŒ‡æ ‡
ç³»ç»Ÿä¼šè‡ªåŠ¨ç›‘æ§ä»¥ä¸‹å¼‚å¸¸æƒ…å†µï¼š
- **NaN/Infå€¼**: æ£€æµ‹æ•°å€¼å¼‚å¸¸
- **æŸå¤±çˆ†ç‚¸**: è¶…å‡ºè®¾å®šé˜ˆå€¼
- **è¶‹åŠ¿å¼‚å¸¸**: æŒç»­ä¸Šå‡çš„æŸå¤±

### è­¦æŠ¥ç¤ºä¾‹
```
ğŸš¨ Step 1500 ç›‘æ§è­¦æŠ¥:
   âš ï¸ critic_loss è¶…å‡ºæœ€å¤§é˜ˆå€¼: 15.2341 > 10.0
   ğŸ“ˆ actor_loss æŒç»­ä¸Šå‡è¶‹åŠ¿ï¼Œå½“å‰å‡å€¼: 2.3456
```

### è‡ªå®šä¹‰é˜ˆå€¼
```python
custom_thresholds = {
    'critic_loss': {'max': 20.0, 'min': 0.0, 'nan_check': True},
    'actor_loss': {'max': 5.0, 'nan_check': True},
    'alpha': {'min': 0.01, 'max': 1.0},
}
monitor = RealTimeMonitor(logger, alert_thresholds=custom_thresholds)
```

---

## ğŸ’¡ **æœ€ä½³å®è·µå»ºè®®**

### 1. å®éªŒå‘½å
```python
# ä½¿ç”¨æè¿°æ€§çš„å®éªŒåç§°
experiment_name = f"reacher2d_sac_lr{args.lr}_bs{batch_size}_{timestamp}"
```

### 2. è®°å½•é¢‘ç‡
```python
# æŸå¤±è®°å½•ï¼šæ¯æ¬¡æ›´æ–°éƒ½è®°å½•
logger.log_step(step, metrics)

# å›¾è¡¨ç”Ÿæˆï¼šé€‚ä¸­é¢‘ç‡é¿å…æ€§èƒ½å½±å“
if step % 1000 == 0:
    logger.plot_losses(recent_steps=2000, show=False)

# ç»Ÿè®¡æ‰“å°ï¼šæŸ¥çœ‹è®­ç»ƒè¿›åº¦
if step % 100 == 0:
    logger.print_current_stats(step)
```

### 3. å­˜å‚¨ç®¡ç†
```python
# å®šæœŸä¿å­˜é¿å…æ•°æ®ä¸¢å¤±
logger.save_logs()  # æ¯100æ­¥è‡ªåŠ¨ä¿å­˜

# å¤§å‹å®éªŒå»ºè®®è®¾ç½®æ›´å¤§çš„ä¿å­˜é—´éš”
logger.save_interval = 500  # æ¯500æ­¥ä¿å­˜ä¸€æ¬¡
```

---

## ğŸ” **æ•…éšœæ’é™¤**

### å¸¸è§é—®é¢˜

**Q: å›¾è¡¨æ— æ³•æ˜¾ç¤º**
```bash
# è®¾ç½®éäº¤äº’å¼åç«¯
export MPLBACKEND=Agg
python your_script.py
```

**Q: å†…å­˜ä½¿ç”¨è¿‡å¤š**
```python
# å‡å°‘recent_lossesä¿å­˜çš„æ•°æ®é‡
logger.max_recent_size = 50  # é»˜è®¤100
```

**Q: Pickleåºåˆ—åŒ–å¤±è´¥**
```python
# é¿å…ä½¿ç”¨lambdaå‡½æ•°ï¼Œå·²åœ¨å½“å‰ç‰ˆæœ¬ä¿®å¤
```

### è°ƒè¯•æ¨¡å¼
```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
logger.debug_mode = True
logger.print_current_stats(step, detailed=True)
```

---

## ğŸ“ **ç¤ºä¾‹é…ç½®æ–‡ä»¶**

åˆ›å»º `training_config.py`:
```python
# è®­ç»ƒç›‘æ§é…ç½®
LOGGING_CONFIG = {
    'log_dir': 'training_logs',
    'save_interval': 100,
    'plot_interval': 1000,
    'alert_thresholds': {
        'critic_loss': {'max': 50.0, 'nan_check': True},
        'actor_loss': {'max': 10.0, 'nan_check': True},
        'alpha_loss': {'max': 5.0, 'nan_check': True},
    }
}
```

---

## ğŸŠ **æ€»ç»“**

ä½ ç°åœ¨æœ‰äº†ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæŸå¤±ç›‘æ§ç³»ç»Ÿï¼

**å¿«é€Ÿå¼€å§‹**:
```bash
# 1. ç›´æ¥ä½¿ç”¨å¢å¼ºç‰ˆè®­ç»ƒè„šæœ¬
python enhanced_train.py --test-reacher2d

# 2. æˆ–è€…åˆ†æç°æœ‰æ—¥å¿—
python analyze_existing_logs.py --log-file your_log.txt

# 3. æŸ¥çœ‹ç»“æœ
ls training_logs/  # æŸ¥çœ‹ç”Ÿæˆçš„æ‰€æœ‰è®°å½•
```

è¿™ä¸ªç³»ç»Ÿå°†å¸®åŠ©ä½ ï¼š
- ğŸ“Š **å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹** - æ¸…æ™°çœ‹åˆ°æŸå¤±å˜åŒ–
- ğŸš¨ **åŠæ—¶å‘ç°é—®é¢˜** - è‡ªåŠ¨ç›‘æ§å¼‚å¸¸æƒ…å†µ  
- ğŸ“ˆ **åˆ†æè®­ç»ƒæ•ˆæœ** - è¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Š
- ğŸ’¾ **ä¿å­˜è®­ç»ƒè®°å½•** - å¤šæ ¼å¼æ•°æ®å¯¼å‡º

äº«å—ä½ çš„è®­ç»ƒç›‘æ§ä¹‹æ—…ï¼ğŸš€ 