#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆè®­ç»ƒè„šæœ¬ - é›†æˆæŸå¤±ç›‘æ§ç³»ç»Ÿ
åŸºäºåŸå§‹train.pyï¼Œæ·»åŠ äº†å®Œæ•´çš„æŸå¤±è®°å½•å’Œç›‘æ§åŠŸèƒ½
"""

# ä¿æŒåŸæœ‰çš„æ‰€æœ‰å¯¼å…¥
import sys
import os
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../')
sys.path.append(base_dir)

sys.path.insert(0, os.path.join(base_dir, 'examples/2d_reacher/envs'))
sys.path.insert(0, os.path.join(base_dir, 'examples/surrogate_model/gnn_encoder'))
sys.path.insert(0, os.path.join(base_dir, 'examples/rl/train'))  
sys.path.insert(0, os.path.join(base_dir, 'examples/rl/common'))
sys.path.insert(0, os.path.join(base_dir, 'examples/rl/environments'))
sys.path.append(os.path.join(base_dir, 'examples/rl'))

import numpy as np
import time
from collections import deque
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import gym
import argparse

gym.logger.set_level(40)

# å¯¼å…¥è®­ç»ƒç›‘æ§ç³»ç»Ÿ
from training_logger import TrainingLogger, RealTimeMonitor

# åŸæœ‰çš„è®­ç»ƒå¯¼å…¥
from gnn_encoder import GNN_Encoder

if not hasattr(np, 'bool'):
    np.bool = bool

import environments
from arguments import get_parser
from utils import solve_argv_conflict
from common import *
from evaluation import render
from a2c_ppo_acktr import algo, utils
from a2c_ppo_acktr.envs import make_vec_envs, make_env
from a2c_ppo_acktr.model import Policy
from a2c_ppo_acktr.storage import RolloutStorage
from attn_dataset.sim_data_handler import DataHandler
from attn_model.attn_model import AttnModel
from sac.sac_model import AttentionSACWithBuffer
from env_config.env_wrapper import make_reacher2d_vec_envs, make_smart_reacher2d_vec_envs
from reacher2d_env import Reacher2DEnv
sys.path.insert(0, os.path.join(base_dir, 'examples/2d_reacher/envs'))
from async_renderer import AsyncRenderer, StateExtractor
import logging


SILENT_MODE = True

def smart_print(*args, **kwargs):
    if not SILENT_MODE:
        print(*args, **kwargs)

def check_goal_reached(env, goal_threshold=50.0):
    """æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡"""
    try:
        if hasattr(env, '_get_end_effector_position') and hasattr(env, 'goal_pos'):
            end_pos = env._get_end_effector_position()
            goal_pos = env.goal_pos
            distance = np.linalg.norm(np.array(end_pos) - goal_pos)
            return distance <= goal_threshold, distance
    except Exception as e:
        print(f"ç›®æ ‡æ£€æµ‹å¤±è´¥: {e}")
    return False, float('inf')


def save_best_model(sac, model_save_path, success_rate, min_distance, step):
    """ä¿å­˜æœ€ä½³æ¨¡å‹"""
    try:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        model_data = {
            'step': step,
            'success_rate': success_rate,
            'min_distance': min_distance,
            'timestamp': timestamp,
            'actor_state_dict': sac.actor.state_dict(),
            'critic1_state_dict': sac.critic1.state_dict(),
            'critic2_state_dict': sac.critic2.state_dict(),
            'target_critic1_state_dict': sac.target_critic1.state_dict(),
            'target_critic2_state_dict': sac.target_critic2.state_dict(),
            'actor_optimizer_state_dict': sac.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': sac.critic_optimizer.state_dict(),
            'alpha_optimizer_state_dict': sac.alpha_optimizer.state_dict(),
            'alpha': sac.alpha.item() if torch.is_tensor(sac.alpha) else sac.alpha,
            'log_alpha': sac.log_alpha.item() if torch.is_tensor(sac.log_alpha) else sac.log_alpha,
        }
        
        model_file = os.path.join(model_save_path, f'best_model_step_{step}_{timestamp}.pth')
        torch.save(model_data, model_file)
        
        latest_file = os.path.join(model_save_path, 'latest_best_model.pth')
        torch.save(model_data, latest_file)
        
        smart_print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹: {model_file}")
        smart_print(f"   æˆåŠŸç‡: {success_rate:.3f}, æœ€å°è·ç¦»: {min_distance:.1f}, æ­¥éª¤: {step}")
        
        return True
    except Exception as e:
        smart_print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
        return False
    

# def load_checkpoint(sac, checkpoint_path, device='cpu'):
#     """ä»æ£€æŸ¥ç‚¹åŠ è½½SACæ¨¡å‹"""
#     try:
#         if not os.path.exists(checkpoint_path):
#             print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
#             return 0
#         else:
#             print(f"ğŸ”„ Loading checkpoint from: {checkpoint_path}")

#         smart_print(f"ğŸ”„ Loading checkpoint from: {checkpoint_path}")
#         checkpoint = torch.load(checkpoint_path, map_location=device)
        
#         # æ‰“å°checkpointå†…å®¹
#         smart_print(f"ğŸ“‹ Checkpoint contains: {list(checkpoint.keys())}")
        
#         # åŠ è½½å„ä¸ªç½‘ç»œçš„çŠ¶æ€
#         if 'actor_state_dict' in checkpoint:
#             sac.actor.load_state_dict(checkpoint['actor_state_dict'])
#             smart_print("âœ… Actor loaded")
        
#         if 'critic1_state_dict' in checkpoint:
#             sac.critic1.load_state_dict(checkpoint['critic1_state_dict'])
#             smart_print("âœ… Critic1 loaded")
            
#         if 'critic2_state_dict' in checkpoint:
#             sac.critic2.load_state_dict(checkpoint['critic2_state_dict'])
#             smart_print("âœ… Critic2 loaded")
        
#         # åŠ è½½target networksï¼ˆå¦‚æœå­˜åœ¨ï¼‰
#         if 'target_critic1_state_dict' in checkpoint:
#             sac.target_critic1.load_state_dict(checkpoint['target_critic1_state_dict'])
#             smart_print("âœ… Target Critic1 loaded")
#         else:
#             # å¦‚æœæ²¡æœ‰ä¿å­˜target networksï¼Œä»main networkså¤åˆ¶
#             sac.target_critic1.load_state_dict(sac.critic1.state_dict())
#             smart_print("âš ï¸ Target Critic1 copied from Critic1")
            
#         if 'target_critic2_state_dict' in checkpoint:
#             sac.target_critic2.load_state_dict(checkpoint['target_critic2_state_dict'])
#             smart_print("âœ… Target Critic2 loaded")
#         else:
#             sac.target_critic2.load_state_dict(sac.critic2.state_dict())
#             smart_print("âš ï¸ Target Critic2 copied from Critic2")
        
#         # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
#         load_optimizers = True  # è®¾ä¸ºFalseå¦‚æœä½ æƒ³ç”¨æ–°çš„å­¦ä¹ ç‡
#         if load_optimizers:
#             if 'actor_optimizer_state_dict' in checkpoint:
#                 sac.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
#                 print("âœ… Actor optimizer loaded")
            
#             if 'critic_optimizer_state_dict' in checkpoint:
#                 sac.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
#                 smart_print("âœ… Critic optimizer loaded")
                
#             if 'alpha_optimizer_state_dict' in checkpoint:
#                 sac.alpha_optimizer.load_state_dict(checkpoint['alpha_optimizer_state_dict'])
#                 smart_print("âœ… Alpha optimizer loaded")
        
#         # åŠ è½½alphaå€¼
#         if 'alpha' in checkpoint:
#             if isinstance(checkpoint['alpha'], torch.Tensor):
#                 sac.alpha = checkpoint['alpha'].item()
#             else:
#                 sac.alpha = checkpoint['alpha']
#             smart_print(f"âœ… Alpha loaded: {sac.alpha}")
            
#         if 'log_alpha' in checkpoint:
#             if isinstance(checkpoint['log_alpha'], torch.Tensor):
#                 sac.log_alpha.data.fill_(checkpoint['log_alpha'].item())
#             else:
#                 sac.log_alpha.data.fill_(checkpoint['log_alpha'])
#             smart_print(f"âœ… Log Alpha loaded: {sac.log_alpha.item()}")
        
#         # è¿”å›è®­ç»ƒæ­¥æ•°
#         start_step = checkpoint.get('step', 0)
#         smart_print(f"âœ… Checkpoint loaded successfully! Starting from step: {start_step}")
        
#         return start_step
        
#     except Exception as e:
#         print(f"âŒ Failed to load checkpoint: {e}")
#         print("Training will start from scratch...")
#         return 0

def load_checkpoint(sac, checkpoint_path, device='cpu'):
    """å®Œå…¨ä¿®å¤proj_dictçš„checkpointåŠ è½½"""
    try:
        if not os.path.exists(checkpoint_path):
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return 0
        
        print(f"ğŸ”„ Loading checkpoint from: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        print(f"ğŸ“‹ Checkpoint contains: {list(checkpoint.keys())}")
        
        # å…ˆåŠ è½½éproj_dictçš„éƒ¨åˆ†
        for network_name in ['critic1', 'critic2', 'target_critic1', 'target_critic2']:
            state_dict_key = f'{network_name}_state_dict'
            if state_dict_key in checkpoint:
                state_dict = checkpoint[state_dict_key]
                network = getattr(sac, network_name)
                
                # åˆ†ç¦»mainéƒ¨åˆ†å’Œproj_dictéƒ¨åˆ†
                main_state = {}
                proj_dict_state = {}
                
                for key, value in state_dict.items():
                    if key.startswith('proj_dict.'):
                        proj_dict_state[key] = value
                    else:
                        main_state[key] = value
                
                # åŠ è½½ä¸»è¦éƒ¨åˆ†
                missing_keys, unexpected_keys = network.load_state_dict(main_state, strict=False)
                if unexpected_keys:
                    print(f"âš ï¸ {network_name}: å¿½ç•¥ä¸åŒ¹é…çš„å±‚: {unexpected_keys}")
                print(f"âœ… {network_name} main networks loaded")
                
                # é‡å»ºproj_dict
                proj_layers = {}
                for key, tensor in proj_dict_state.items():
                    if '.weight' in key:
                        # ä» "proj_dict.8.weight" æå–ç»´åº¦ "8"
                        dim_key = key.split('.')[1]
                        input_dim = tensor.shape[1]
                        output_dim = tensor.shape[0]
                        
                        # åˆ›å»ºæ–°çš„çº¿æ€§å±‚
                        proj_layers[dim_key] = nn.Linear(input_dim, output_dim).to(device)
                        proj_layers[dim_key].weight.data = tensor.clone()
                        
                        print(f"  ğŸ“Œ é‡å»º proj_dict['{dim_key}']: {input_dim} â†’ {output_dim}")
                
                # åŠ è½½bias
                for key, tensor in proj_dict_state.items():
                    if '.bias' in key:
                        dim_key = key.split('.')[1]
                        if dim_key in proj_layers:
                            proj_layers[dim_key].bias.data = tensor.clone()
                
                # å°†é‡å»ºçš„proj_dictèµ‹å€¼ç»™ç½‘ç»œ
                if proj_layers:
                    network.proj_dict = nn.ModuleDict(proj_layers)
                    print(f"âœ… {network_name} proj_dict å®Œå…¨æ¢å¤: {list(proj_layers.keys())}")
                else:
                    print(f"â„¹ï¸ {network_name}: æ²¡æœ‰æ‰¾åˆ°proj_dictå±‚")
        
        # åŠ è½½Actorï¼ˆé€šå¸¸æ²¡æœ‰proj_dicté—®é¢˜ï¼‰
        if 'actor_state_dict' in checkpoint:
            missing_keys, unexpected_keys = sac.actor.load_state_dict(checkpoint['actor_state_dict'], strict=False)
            if unexpected_keys:
                print(f"âš ï¸ Actor: å¿½ç•¥ä¸åŒ¹é…çš„å±‚: {unexpected_keys}")
            print("âœ… Actor loaded")
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
        load_optimizers = True  # è®¾ä¸ºFalseå¦‚æœä½ æƒ³ç”¨æ–°çš„å­¦ä¹ ç‡
        if load_optimizers:
            if 'actor_optimizer_state_dict' in checkpoint:
                try:
                    sac.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
                    print("âœ… Actor optimizer loaded")
                except Exception as e:
                    print(f"âš ï¸ Actor optimizer åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨çŠ¶æ€: {e}")
            
            if 'critic_optimizer_state_dict' in checkpoint:
                try:
                    sac.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
                    print("âœ… Critic optimizer loaded")
                except Exception as e:
                    print(f"âš ï¸ Critic optimizer åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨çŠ¶æ€: {e}")
                
            if 'alpha_optimizer_state_dict' in checkpoint:
                try:
                    sac.alpha_optimizer.load_state_dict(checkpoint['alpha_optimizer_state_dict'])
                    print("âœ… Alpha optimizer loaded")
                except Exception as e:
                    print(f"âš ï¸ Alpha optimizer åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨çŠ¶æ€: {e}")
        
        # åŠ è½½alphaå€¼
        if 'alpha' in checkpoint:
            if isinstance(checkpoint['alpha'], torch.Tensor):
                sac.alpha = checkpoint['alpha'].item()
            else:
                sac.alpha = checkpoint['alpha']
            print(f"âœ… Alpha loaded: {sac.alpha}")
            
        if 'log_alpha' in checkpoint:
            if isinstance(checkpoint['log_alpha'], torch.Tensor):
                sac.log_alpha.data.fill_(checkpoint['log_alpha'].item())
            else:
                sac.log_alpha.data.fill_(checkpoint['log_alpha'])
            print(f"âœ… Log Alpha loaded: {sac.log_alpha.item()}")
        
        # è¿”å›è®­ç»ƒæ­¥æ•°
        start_step = checkpoint.get('step', 0)
        print(f"âœ… Checkpoint å®Œå…¨ä¿®å¤åŠ è½½! Starting from step: {start_step}")
        
        return start_step
        
    except Exception as e:
        print(f"âŒ Failed to load checkpoint: {e}")
        import traceback
        traceback.print_exc()
        print("Training will start from scratch...")
        return 0

def setup_logging(args):
    train_log_level = getattr(args, 'train_log_level',  'INFO')
    if args.quiet:
        train_log_level = 'SILENT'
    logger = logging.getLogger('TrainingScript')
    logger.handlers.clear()

    if train_log_level != 'SILENT':
        level_map = {
            'DEBUG': logging.DEBUG,
            'INFO': logging.INFO,
            'WARNING': logging.WARNING,
            'ERROR': logging.ERROR
        }
        log_level = level_map.get(train_log_level.upper(), logging.INFO)
        logger.setLevel(log_level)

        handler = logging.StreamHandler()
        handler.setLevel(log_level)
        formatter = logging.Formatter('%(levelname)s: %(message)s')
        handler.setFormatter(formatter)
    else:
        logger.setLevel(logging.CRITICAL + 10)
    
    return logger, train_log_level == 'SILENT'


def main(args):
    torch.manual_seed(args.seed)
    torch.set_num_threads(1)
    device = torch.device('cpu')

    os.makedirs(args.save_dir, exist_ok=True)

    # ğŸš€ NEW: åˆå§‹åŒ–è®­ç»ƒç›‘æ§ç³»ç»Ÿ
    # experiment_name = f"reacher2d_sac_{time.strftime('%Y%m%d_%H%M%S')}"
    # logger = TrainingLogger(
    #     log_dir=os.path.join(args.save_dir, 'training_logs'),
    #     experiment_name=experiment_name
    # )
    
    # # è®¾ç½®ç›‘æ§é˜ˆå€¼
    # monitor = RealTimeMonitor(logger, alert_thresholds={
    #     'critic_loss': {'max': 50.0, 'nan_check': True},
    #     'actor_loss': {'max': 10.0, 'nan_check': True},
    #     'alpha_loss': {'max': 5.0, 'nan_check': True},
    #     'alpha': {'min': 0.01, 'max': 2.0, 'nan_check': True}
    # })
    
    # smart_print(f"ğŸ“Š è®­ç»ƒç›‘æ§ç³»ç»Ÿå·²åˆå§‹åŒ–: {logger.experiment_dir}")


    hyperparams = {
        'learning_rate': args.lr,
        'alpha': args.alpha,
        'warmup_steps': args.warmup_steps,
        'target_entropy_factor': args.target_entropy_factor,
        'batch_size': 64,  # ä»SACåˆå§‹åŒ–ä¸­è·å–
        'buffer_capacity': 10000,
        'gamma': args.gamma,
        'seed': args.seed,
        'num_processes': args.num_processes,
        'update_frequency': args.update_frequency,
        'total_steps': 120000,  # num_stepä¼šåœ¨åé¢å®šä¹‰
        'optimizer': 'Adam',
        'network_architecture': {
            'attn_model_dims': '128-130-130-4',
            'action_dim': 'dynamic',  # ä¼šåœ¨åé¢æ›´æ–°
            'critic_hidden_layers': 'å¾…ç¡®å®š',
            'actor_hidden_layers': 'å¾…ç¡®å®š'
        }
    }


    training_log_path = os.path.join(args.save_dir, 'logs.txt')
    fp_log = open(training_log_path, 'w')
    fp_log.close()

    if args.env_name == 'reacher2d':
        smart_print("use reacher2d env")

        if hasattr(args, 'num_joints') and hasattr(args, 'link_lengths'):
            # ä½¿ç”¨MAP-Elitesä¼ å…¥çš„é…ç½®
            num_links = args.num_joints
            link_lengths = args.link_lengths
            smart_print(f"ğŸ¤– ä½¿ç”¨MAP-Elitesé…ç½®: {num_links}å…³èŠ‚, é•¿åº¦={link_lengths}")
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            num_links = 4
            link_lengths = [80, 80, 80, 60]
            smart_print(f"ğŸ¤– ä½¿ç”¨é»˜è®¤é…ç½®: {num_links}å…³èŠ‚, é•¿åº¦={link_lengths}")


        env_params = {
            'num_links': num_links,
            'link_lengths': link_lengths,
            'render_mode': 'human',
            'config_path': "/home/xli149/Documents/repos/RoboGrammar/examples/2d_reacher/configs/reacher_with_zigzag_obstacles.yaml"
        }
        smart_print(f"num links: {env_params['num_links']}")
        smart_print(f"link lengths: {env_params['link_lengths']}")

        env_config = {
            'env_name': args.env_name,
            'env_type': 'reacher2d',
            'num_links': env_params['num_links'],
            'link_lengths': env_params['link_lengths'],
            'config_path': env_params.get('config_path', 'N/A'),
            'render_mode': env_params.get('render_mode', 'human'),
            'reward_function': 'è·ç¦»+è¿›åº¦+æˆåŠŸ+ç¢°æ’+æ–¹å‘å¥–åŠ±',
            'physics_engine': 'PyMunk',
            'goal_threshold': 35.0,
            'obstacle_type': 'zigzag',
            'action_space': 'continuous',
            'observation_space': 'joint_angles_and_positions'
        }

        # ğŸ¨ å¼‚æ­¥æ¸²æŸ“æ¨¡å¼ï¼šå¤šè¿›ç¨‹è®­ç»ƒ + ç‹¬ç«‹æ¸²æŸ“
        async_renderer = None
        sync_env = None
        
        if args.num_processes > 1:
            smart_print("ğŸš€ å¤šè¿›ç¨‹æ¨¡å¼ï¼šå¯ç”¨å¼‚æ­¥æ¸²æŸ“")
            
            train_env_params = env_params.copy()
            train_env_params['render_mode'] = None
            
            envs = make_reacher2d_vec_envs(
                env_params=train_env_params,
                seed=args.seed,
                num_processes=args.num_processes,
                gamma=args.gamma,
                log_dir=None,
                device=device,
                allow_early_resets=False,
            )
            
            async_renderer = AsyncRenderer(env_params)
            async_renderer.start()
            
            sync_env = Reacher2DEnv(**train_env_params)
            smart_print(f"âœ… å¼‚æ­¥æ¸²æŸ“å™¨å·²å¯åŠ¨ (PID: {async_renderer.render_process.pid})")
            
        else:   
            smart_print("ğŸƒ å•è¿›ç¨‹æ¨¡å¼ï¼šç›´æ¥æ¸²æŸ“")
            envs = make_reacher2d_vec_envs(
                env_params=env_params,
                seed=args.seed,
                num_processes=args.num_processes,
                gamma=args.gamma,
                log_dir=None,
                device=device,
                allow_early_resets=False
            )

        smart_print("âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        args.env_type = 'reacher2d'
        
    else:
        smart_print(f"use bullet env: {args.env_name}")
        envs = make_vec_envs(args.env_name, args.seed, args.num_processes, 
                            args.gamma, None, device, False, args=args)
        render_env = gym.make(args.env_name, args=args)
        render_env.seed(args.seed)
        args.env_type = 'bullet'


        env_config = {
            'env_name': args.env_name,
            'env_type': 'bullet',
            'action_space': 'continuous',
            'physics_engine': 'Bullet'
        }


    num_joints = envs.action_space.shape[0]
    smart_print(f"Number of joints: {num_joints}")
    num_updates = 5
    num_step = 120000
    data_handler = DataHandler(num_joints, args.env_type)

    hyperparams['network_architecture']['action_dim'] = num_joints
    hyperparams['total_steps'] = num_step
    env_config['action_dim'] = num_joints

    if args.env_type == 'reacher2d':
        sys.path.append(os.path.join(os.path.dirname(__file__), '../2d_reacher/utils'))
        from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
        
        smart_print("ğŸ¤– åˆå§‹åŒ– Reacher2D GNN ç¼–ç å™¨...")
        reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
        single_gnn_embed = reacher2d_encoder.get_gnn_embeds(
            num_links=num_joints, 
            link_lengths=env_params['link_lengths']
        )
        smart_print(f"âœ… Reacher2D GNN åµŒå…¥ç”ŸæˆæˆåŠŸï¼Œå½¢çŠ¶: {single_gnn_embed.shape}")
    else:
        rule_sequence = [int(s.strip(",")) for s in args.rule_sequence]
        gnn_encoder = GNN_Encoder(args.grammar_file, rule_sequence, 70, num_joints)
        gnn_graph = gnn_encoder.get_graph(rule_sequence)
        single_gnn_embed = gnn_encoder.get_gnn_embeds(gnn_graph)

    action_dim = num_joints
    attn_model = AttnModel(128, 130, 130, 4)
    sac = AttentionSACWithBuffer(attn_model, action_dim, 
                                buffer_capacity=10000, batch_size=64,
                                lr=args.lr,
                                env_type=args.env_type)
    
    
    sac.warmup_steps = args.warmup_steps
    # sac.alpha = args.alpha
    sac.alpha = torch.tensor(0.1)

    if hasattr(sac, 'target_entropy'):
        sac.target_entropy = -action_dim * args.target_entropy_factor


    sac.min_alpha = 0.05  # è®¾ç½®ä¸‹é™ä¸º0.05
    print(f"ğŸ”’ Alphaè¡°å‡ä¸‹é™è®¾ç½®ä¸º: {sac.min_alpha}")
    
    experiment_name = f"reacher2d_sac_{time.strftime('%Y%m%d_%H%M%S')}"
    logger = TrainingLogger(
        log_dir=os.path.join(args.save_dir, 'training_logs'),
        experiment_name=experiment_name,
        hyperparams=hyperparams,  # â† ã€æ–°å¢å‚æ•°ã€‘
        env_config=env_config     # â† ã€æ–°å¢å‚æ•°ã€‘
    )
    
    # è®¾ç½®ç›‘æ§é˜ˆå€¼
    monitor = RealTimeMonitor(logger, alert_thresholds={
        'critic_loss': {'max': 50.0, 'nan_check': True},
        'actor_loss': {'max': 10.0, 'nan_check': True},
        'alpha_loss': {'max': 5.0, 'nan_check': True},
        'alpha': {'min': 0.01, 'max': 2.0, 'nan_check': True}
    })
    
    smart_print(f"ğŸ“Š è®­ç»ƒç›‘æ§ç³»ç»Ÿå·²åˆå§‹åŒ–: {logger.experiment_dir}")

    start_step = 0
    if hasattr(args,  'resume_checkpoint') and  args.resume_checkpoint:
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume_checkpoint}")
        start_step = load_checkpoint(sac, args.resume_checkpoint)

        if start_step > 0:
            print(f"æˆåŠŸåŠ è½½checkpoint, ä»step {start_step} å¼€å§‹è®­ç»ƒ")
            sac.warmup_steps = 0

            if hasattr(args, 'resume_lr') and args.resume_lr:
                for param_group in sac.actor_optimizer.param_groups:
                    param_group['lr'] = args.resume_lr
                for param_group in sac.critic_optimizer.param_groups:
                    param_group['lr'] = args.resume_lr
                print(f"æ›´æ–°å­¦ä¹ ç‡ä¸º {args.resume_lr}")

            if hasattr(args, 'resume_alpha') and args.resume_alpha:
                sac.alpha = args.resume_alpha
                print(f"æ›´æ–°alphaä¸º {args.resume_alpha}")
            
            resume_hyperparams = {
                'is_resumed_training': True,
                'resume_checkpoint': args.resume_checkpoint,
                'resume_step': start_step,
                'resume_lr': getattr(args, 'resume_lr', None),
                'resume_alpha': getattr(args, 'resume_alpha', None),
                'additional_steps': getattr(args, 'additional_steps', None),
                'original_lr': args.lr,
                'original_alpha': args.alpha,
                'warmup_skipped': True
            }
            logger.update_hyperparams(resume_hyperparams)
            
            experiment_name = f"resumed_{experiment_name}_from_step_{start_step}"
            logger.experiment_name = experiment_name
        else:
            print("âŒ æ— æ³•åŠ è½½checkpoint, ä»å¼€å§‹è®­ç»ƒ")
            start_step = 0
    
    current_obs = envs.reset()
    current_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)
    total_steps = 0
    episode_rewards = [0.0] * args.num_processes
    eval_frequency = 200
    
    # ğŸ† æ·»åŠ æœ€ä½³æ¨¡å‹ä¿å­˜ç›¸å…³å˜é‡
    best_success_rate = 0.0
    best_min_distance = float('inf')
    goal_threshold = 35.0
    consecutive_success_count = 0
    min_consecutive_successes = 3
    model_save_path = os.path.join(args.save_dir, 'best_models')
    os.makedirs(model_save_path, exist_ok=True)
    
    if not hasattr(args, 'update_frequency'):
        args.update_frequency = args.update_frequency

    # smart_print(f"start training, warmup {sac.warmup_steps} steps")
    # smart_print(f"Total training steps: {num_step}, Update frequency: {args.update_frequency}")
    # smart_print(f"Expected warmup completion at step: {sac.warmup_steps}")

    # if start_step > 0 and hasattr(args, 'additional_steps') and args.additional_steps:
    #     num_step = start_step + args.additional_steps
    #     smart_print(f"ğŸ”„ ä»step {start_step} å¼€å§‹è®­ç»ƒ, é¢å¤–è®­ç»ƒ {args.additional_steps} æ­¥")
    # else:
    #     num_step = 120000
    print(f"start training, warmup {sac.warmup_steps} steps")
    print(f"Total training steps: {num_step}, Update frequency: {args.update_frequency}")
    if start_step > 0:
        print(f"Resume from step: {start_step}")
    else:
        print(f"Expected warmup completion at step: {sac.warmup_steps}")

    training_completed = False
    early_termination_reason = ""

    try:
        for step in range(start_step, num_step):
            
            # ğŸš€ NEW: è®°å½•æ¯ä¸ªepisodeçš„æ€§èƒ½æŒ‡æ ‡
            if step % 100 == 0:
                if step < sac.warmup_steps:
                    smart_print(f"Step {step}/{num_step}: Warmup phase ({step}/{sac.warmup_steps})")
                else:
                    smart_print(f"Step {step}/{num_step}: Training phase, Buffer size: {len(sac.memory)}")

                if async_renderer:
                    stats = async_renderer.get_stats()
                    smart_print(f"   ğŸ¨ æ¸²æŸ“FPS: {stats.get('fps', 0):.1f}")

            if step < sac.warmup_steps:
                action_batch = torch.from_numpy(np.array([envs.action_space.sample() for _ in range(args.num_processes)]))
            else:
                actions = []
                for proc_id in range(args.num_processes):
                    action = sac.get_action(current_obs[proc_id],
                                            current_gnn_embeds[proc_id],
                                            num_joints=envs.action_space.shape[0],
                                            deterministic=False)
                    actions.append(action)
                action_batch = torch.stack(actions)

            # ğŸ” Actionç›‘æ§
            if step % 50 == 0 or step < 20:
                if hasattr(envs, 'envs') and len(envs.envs) > 0:
                    env_goal = getattr(envs.envs[0], 'goal_pos', 'NOT FOUND')
                    print(f"ğŸ¯ [è®­ç»ƒ] Step {step} - ç¯å¢ƒgoal_pos: {env_goal}")
                smart_print(f"\nğŸ¯ Step {step} Action Analysis:")
                action_numpy = action_batch.cpu().numpy() if hasattr(action_batch, 'cpu') else action_batch.numpy()
                
                for proc_id in range(min(args.num_processes, 2)):
                    action_values = action_numpy[proc_id]
                    # smart_print(f"  Process {proc_id}: Actions = [{action_values[0]:+6.2f}, {action_values[1]:+6.2f}, {action_values[2]:+6.2f}, {action_values[3]:+6.2f}]")
                    action_str = ', '.join([f"{val:+6.2f}" for val in action_values])
                    smart_print(f"  Process {proc_id}: Actions = [{action_str}]")
                    smart_print(f"    Max action: {np.max(np.abs(action_values)):6.2f}, Mean abs: {np.mean(np.abs(action_values)):6.2f}")

            next_obs, reward, done, infos = envs.step(action_batch)
            
            # ğŸ¨ å¼‚æ­¥æ¸²æŸ“
            if async_renderer and sync_env:
                sync_action = action_batch[0].cpu().numpy() if hasattr(action_batch, 'cpu') else action_batch[0]
                sync_env.step(sync_action)
                robot_state = StateExtractor.extract_robot_state(sync_env, step)
                async_renderer.render_frame(robot_state)

            next_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)

            for proc_id in range(args.num_processes):
                sac.store_experience(
                        obs=current_obs[proc_id],
                        gnn_embeds=current_gnn_embeds[proc_id],
                        action=action_batch[proc_id],
                        reward=reward[proc_id],
                        next_obs=next_obs[proc_id],
                        next_gnn_embeds=next_gnn_embeds[proc_id],
                        done=done[proc_id],
                        num_joints=num_joints
                )
                episode_rewards[proc_id] += reward[proc_id].item()

            current_obs = next_obs.clone()
            current_gnn_embeds = next_gnn_embeds.clone()

            for proc_id in range(args.num_processes):
                is_done = done[proc_id].item() if torch.is_tensor(done[proc_id]) else bool(done[proc_id])
                if is_done:
                    print(f"Episode {step} finished with reward {episode_rewards[proc_id]:.2f}")

                    # ğŸ”§ ä»infosä¸­è·å–ç¯å¢ƒçŠ¶æ€ä¿¡æ¯
                    if len(infos) > proc_id and isinstance(infos[proc_id], dict):
                        info = infos[proc_id]
                        
                        # æ£€æŸ¥goalç›¸å…³ä¿¡æ¯
                        if 'goal' in info:
                            goal_info = info['goal']
                            distance = goal_info.get('distance_to_goal', float('inf'))
                            goal_reached = goal_info.get('goal_reached', False)
                            
                            print(f"  ğŸ¯ è·ç¦»ç›®æ ‡: {distance:.1f} pixels")
                            
                            if goal_reached or distance <= 35.0:
                                print(f"ğŸ‰ æˆåŠŸåˆ°è¾¾ç›®æ ‡! è·ç¦»: {distance:.1f}")
                                consecutive_success_count += 1
                                
                                # è®¡ç®—æˆåŠŸç‡
                                if hasattr(envs, 'episode_count'):
                                    envs.episode_count += 1
                                else:
                                    envs.episode_count = 1
                                
                                success_rate = consecutive_success_count / max(envs.episode_count, 1)
                                
                                # ğŸ† ä¿å­˜æˆåŠŸæ¨¡å‹
                                if success_rate > best_success_rate or distance < best_min_distance:
                                    best_success_rate = max(success_rate, best_success_rate)
                                    best_min_distance = min(distance, best_min_distance)
                                    save_best_model(sac, model_save_path, success_rate, distance, step)
                                
                                # ğŸ¯ æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
                                if consecutive_success_count >= min_consecutive_successes:
                                    print(f"ğŸ è¿ç»­æˆåŠŸ{consecutive_success_count}æ¬¡ï¼Œè®­ç»ƒè¾¾åˆ°ç›®æ ‡!")
                                    print(f"   æˆåŠŸç‡: {success_rate:.3f}")
                                    print(f"   æœ€å°è·ç¦»: {best_min_distance:.1f}")
                                    
                                    # ä¿å­˜æœ€ç»ˆæˆåŠŸæ¨¡å‹
                                    final_model_path = os.path.join(model_save_path, f'final_successful_model_step_{step}.pth')
                                    final_model_data = {
                                        'step': step,
                                        'final_success_rate': success_rate,
                                        'final_min_distance': best_min_distance,
                                        'consecutive_successes': consecutive_success_count,
                                        'training_completed': True,
                                        'reason': 'Reached target consecutive successes',
                                        'actor_state_dict': sac.actor.state_dict(),
                                        'critic1_state_dict': sac.critic1.state_dict(),
                                        'critic2_state_dict': sac.critic2.state_dict(),
                                    }
                                    torch.save(final_model_data, final_model_path)
                                    print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæˆåŠŸæ¨¡å‹: {final_model_path}")
                                    
                                    # å¯ä»¥é€‰æ‹©æ˜¯å¦é€€å‡ºè®­ç»ƒ
                                    if step > 5000:  # è‡³å°‘è®­ç»ƒ5000æ­¥åæ‰å…è®¸æå‰é€€å‡º
                                        print("ğŸ¯ æå‰ç»“æŸè®­ç»ƒ - å·²è¾¾åˆ°è®­ç»ƒç›®æ ‡")
                                        training_completed = True
                                        early_termination_reason = f"è¿ç»­æˆåŠŸ{consecutive_success_count}æ¬¡ï¼Œè¾¾åˆ°è®­ç»ƒç›®æ ‡"
                                        break  # è¿™ä¼šé€€å‡ºå½“å‰forå¾ªç¯ï¼Œä½†ä¸ä¼šé€€å‡ºæ•´ä¸ªè®­ç»ƒå¾ªç¯
                            else:
                                # é‡ç½®è¿ç»­æˆåŠŸè®¡æ•°
                                consecutive_success_count = 0
                        
                        # å¯é€‰ï¼šæ˜¾ç¤ºå…¶ä»–æœ‰ç”¨ä¿¡æ¯
                        if 'robot' in info:
                            robot_info = info['robot']
                            print(f"  ğŸ¤– æ­¥éª¤: {robot_info.get('step_count', 0)}")
                    
                    # ğŸš€ NEW: è®°å½•episodeæŒ‡æ ‡
                    episode_metrics = {
                        'reward': episode_rewards[proc_id],
                        'length': step,
                        'distance_to_goal': distance if 'distance' in locals() else float('inf'),
                        'goal_reached': goal_reached if 'goal_reached' in locals() else False
                    }
                    logger.log_episode(step // 100, episode_metrics)
                    
                    episode_rewards[proc_id] = 0.0
                    
                    if hasattr(envs, 'reset_one'):
                        current_obs[proc_id] = envs.reset_one(proc_id)
                        current_gnn_embeds[proc_id] = single_gnn_embed
            if step % 100 == 0:  # æ¯100æ­¥æ£€æŸ¥ä¸€æ¬¡
                    # è®¡ç®—æœ€è¿‘çš„æˆåŠŸç‡
                recent_successes = 0
                recent_episodes = 0
                
                # è¿™é‡Œéœ€è¦ç»´æŠ¤ä¸€ä¸ªæ»‘åŠ¨çª—å£çš„æˆåŠŸè®°å½•
                if hasattr(envs, 'recent_success_history'):
                    recent_successes = sum(envs.recent_success_history[-100:])  # æœ€è¿‘100æ­¥
                    recent_episodes = len(envs.recent_success_history[-100:])
                    recent_success_rate = recent_successes / max(recent_episodes, 1)
                    
                    # å¦‚æœæœ€è¿‘æˆåŠŸç‡å¾ˆé«˜ï¼Œå¯ä»¥è€ƒè™‘æš‚åœè®­ç»ƒ
                    if recent_success_rate >= 0.8 and step > 10000:  # è‡³å°‘è®­ç»ƒ10000æ­¥
                        print(f"ï¿½ï¿½ æœ€è¿‘100æ­¥æˆåŠŸç‡: {recent_success_rate:.3f} >= 0.8")
                        print(f"   å»ºè®®æš‚åœè®­ç»ƒï¼Œä¿å­˜å½“å‰æ¨¡å‹")
                        
                        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
                        final_model_path = os.path.join(model_save_path, f'final_successful_model_step_{step}.pth')
                        final_model_data = {
                            'step': step,
                            'final_success_rate': recent_success_rate,
                            'training_completed': True,
                            'reason': 'High success rate achieved',
                            'actor_state_dict': sac.actor.state_dict(),
                            'critic1_state_dict': sac.critic1.state_dict(),
                            'critic2_state_dict': sac.critic2.state_dict(),
                        }
                        torch.save(final_model_data, final_model_path)
                        print(f"ğŸ’¾ ä¿å­˜æˆåŠŸæ¨¡å‹: {final_model_path}")

            # ğŸš€ NEW: è®­ç»ƒæ›´æ–°å’ŒæŸå¤±è®°å½•
            if (step >= sac.warmup_steps and 
                step % args.update_frequency == 0 and 
                sac.memory.can_sample(sac.batch_size)):
                
                metrics = sac.update()
                
                if metrics:
                    # ğŸš€ NEW: è®°å½•æŸå¤±åˆ°ç›‘æ§ç³»ç»Ÿ
                    enhanced_metrics = metrics.copy()
                    enhanced_metrics.update({
                        'step': step,
                        'buffer_size': len(sac.memory),
                        'learning_rate': sac.actor_optimizer.param_groups[0]['lr'],
                        'warmup_progress': min(1.0, step / max(sac.warmup_steps, 1))  # ğŸš€ FIX: é˜²æ­¢é™¤é›¶
                    })
                    
                    logger.log_step(step, enhanced_metrics, episode=step//100)
                    alerts = monitor.check_alerts(step, enhanced_metrics)
                    
                    if step % 100 == 0:
                        print(f"Step {step} (total_steps {total_steps}): "
                            f"Critic Loss: {metrics['critic_loss']:.4f}, "
                            f"Actor Loss: {metrics['actor_loss']:.4f}, "
                            f"Alpha: {metrics['alpha']:.4f}, "
                            f"Buffer Size: {len(sac.memory)}")
                        
                        # ğŸš€ NEW: ä½¿ç”¨æ–°çš„ç»Ÿè®¡æ‰“å°
                        logger.print_current_stats(step, detailed=(step % 500 == 0))
                        
                        if 'entropy_term' in metrics:
                            smart_print(f"  Actor Loss ç»„ä»¶åˆ†æ:")
                            smart_print(f"    Entropy Term (Î±*log_Ï€): {metrics['entropy_term']:.4f}")
                            smart_print(f"    Q Term (Qå€¼): {metrics['q_term']:.4f}")
                            smart_print(f"    Actor Loss = {metrics['entropy_term']:.4f} - {metrics['q_term']:.4f} = {metrics['actor_loss']:.4f}")
                            
                            if metrics['actor_loss'] < 0:
                                smart_print(f"    âœ“ è´Ÿæ•°Actor Loss = é«˜Qå€¼ = å¥½çš„ç­–ç•¥!")
                            else:
                                smart_print(f"    âš  æ­£æ•°Actor Loss = ä½Qå€¼ = ç­–ç•¥éœ€è¦æ”¹è¿›")
            
            # ğŸš€ NEW: å®šæœŸç”ŸæˆæŸå¤±æ›²çº¿
            if step % 1000 == 0 and step > 0:
                logger.plot_losses(recent_steps=2000, show=False)
                
                checkpoint_path = os.path.join(model_save_path, f'checkpoint_step_{step}.pth')
                checkpoint_data = {
                    'step': step,
                    'best_success_rate': best_success_rate,
                    'best_min_distance': best_min_distance,
                    'consecutive_success_count': consecutive_success_count,
                    'actor_state_dict': sac.actor.state_dict(),
                    'critic1_state_dict': sac.critic1.state_dict(),
                    'critic2_state_dict': sac.critic2.state_dict(),
                }
                torch.save(checkpoint_data, checkpoint_path)
                smart_print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹æ¨¡å‹: {checkpoint_path}")
            
            total_steps += args.num_processes
            if training_completed:
                print(f"ğŸ è®­ç»ƒæå‰ç»ˆæ­¢: {early_termination_reason}")
                break

    except Exception as e:
        smart_print(f"ğŸ”´ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        
        # ğŸš€ NEW: åœ¨å¼‚å¸¸æ—¶ä¹Ÿä¿å­˜æ—¥å¿—
        logger.save_logs()
        logger.generate_report()
        raise e

    finally:
        # æ¸…ç†èµ„æº
        if 'async_renderer' in locals() and async_renderer:
            async_renderer.stop()
        if 'sync_env' in locals() and sync_env:
            sync_env.close()
            
        # ğŸš€ NEW: æœ€ç»ˆæŠ¥å‘Šå’Œå›¾è¡¨ç”Ÿæˆ
        print(f"\n{'='*60}")
        print(f"ğŸ è®­ç»ƒå®Œæˆæ€»ç»“:")
        print(f"  æ€»æ­¥æ•°: {step}")
        print(f"  æœ€ä½³æˆåŠŸç‡: {best_success_rate:.3f}")
        print(f"  æœ€ä½³æœ€å°è·ç¦»: {best_min_distance:.1f} pixels")
        print(f"  å½“å‰è¿ç»­æˆåŠŸæ¬¡æ•°: {consecutive_success_count}")
        
        # ğŸš€ NEW: ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒæŠ¥å‘Š
        logger.generate_report()
        logger.plot_losses(show=False)
        
        print(f"ğŸ“Š å®Œæ•´è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {logger.experiment_dir}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(model_save_path, f'final_model_step_{step}.pth')
        final_model_data = {
            'step': step,
            'final_success_rate': best_success_rate,
            'final_min_distance': best_min_distance,
            'final_consecutive_successes': consecutive_success_count,
            'training_completed': True,
            'experiment_name': experiment_name,  # ğŸš€ NEW: å…³è”å®éªŒåç§°
            'actor_state_dict': sac.actor.state_dict(),
            'critic1_state_dict': sac.critic1.state_dict(),
            'critic2_state_dict': sac.critic2.state_dict(),
            'target_critic1_state_dict': sac.target_critic1.state_dict(),
            'target_critic2_state_dict': sac.target_critic2.state_dict(),
        }
        torch.save(final_model_data, final_model_path)
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_model_path}")
        print(f"{'='*60}")
def test_trained_model(model_path, num_episodes=10, render=True):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½"""
    print(f"ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_path}")
    
    # ç¯å¢ƒé…ç½® - ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
    env_params = {
        'num_links': 4,
        'link_lengths': [80, 80, 80, 60],
        'render_mode': 'human' if render else None,
        'config_path': "/home/xli149/Documents/repos/RoboGrammar/examples/2d_reacher/configs/reacher_with_zigzag_obstacles.yaml",
        'debug_level': 'SILENT'
    }
    
    # åˆ›å»ºç¯å¢ƒ
    env = Reacher2DEnv(**env_params)
    num_joints = env.action_space.shape[0]
    
    # åˆ›å»ºGNNç¼–ç å™¨ - ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
    sys.path.append(os.path.join(base_dir, 'examples/2d_reacher/utils'))
    from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
    
    reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
    gnn_embed = reacher2d_encoder.get_gnn_embeds(
        num_links=num_joints, 
        link_lengths=env_params['link_lengths']
    )
    
    # åˆ›å»ºSACæ¨¡å‹ - ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
    attn_model = AttnModel(128, 130, 130, 4)
    sac = AttentionSACWithBuffer(attn_model, num_joints, 
                                buffer_capacity=10000, batch_size=64,
                                lr=1e-5, env_type='reacher2d')
    
    # åŠ è½½æ¨¡å‹
    try:
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
            
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
        model_data = torch.load(model_path, map_location='cpu')
        
        # åŠ è½½ç½‘ç»œçŠ¶æ€
        if 'actor_state_dict' in model_data:
            sac.actor.load_state_dict(model_data['actor_state_dict'], strict=False)
            print("âœ… Actor åŠ è½½æˆåŠŸ")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"   è®­ç»ƒæ­¥æ•°: {model_data.get('step', 'N/A')}")
        print(f"   æ—¶é—´æˆ³: {model_data.get('timestamp', 'N/A')}")
        if 'success_rate' in model_data:
            print(f"   è®­ç»ƒæ—¶æˆåŠŸç‡: {model_data.get('success_rate', 'N/A'):.3f}")
        if 'min_distance' in model_data:
            print(f"   è®­ç»ƒæ—¶æœ€å°è·ç¦»: {model_data.get('min_distance', 'N/A'):.1f}")
            
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None
    
    # æµ‹è¯•å¤šä¸ªepisode
    success_count = 0
    total_rewards = []
    min_distances = []
    episode_lengths = []
    goal_threshold = 35.0  # ä¸å¥–åŠ±å‡½æ•°ä¸­çš„é˜ˆå€¼ä¿æŒä¸€è‡´
    
    print(f"\nğŸ® å¼€å§‹æµ‹è¯• {num_episodes} ä¸ªepisodes...")
    print(f"ğŸ¯ ç›®æ ‡é˜ˆå€¼: {goal_threshold} pixels")
    
    for episode in range(num_episodes):
        obs = env.reset()
        episode_reward = 0
        step_count = 0
        max_steps = 500  # é™åˆ¶æœ€å¤§æ­¥æ•°
        min_distance_this_episode = float('inf')
        episode_success = False
        
        print(f"\nğŸ“ Episode {episode + 1}/{num_episodes}")
        
        while step_count < max_steps:
            # è·å–åŠ¨ä½œï¼ˆä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼‰
            action = sac.get_action(
                torch.from_numpy(obs).float(),
                gnn_embed.squeeze(0),
                num_joints=num_joints,
                deterministic=True  # æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            )
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = env.step(action.cpu().numpy())
            episode_reward += reward
            step_count += 1
            
            # æ£€æŸ¥è·ç¦»
            end_pos = env._get_end_effector_position()
            goal_pos = env.goal_pos
            distance = np.linalg.norm(np.array(end_pos) - goal_pos)
            min_distance_this_episode = min(min_distance_this_episode, distance)
            
            # æ¸²æŸ“
            if render:
                env.render()
                time.sleep(0.02)  # ç¨å¾®å‡æ…¢æ¸²æŸ“é€Ÿåº¦ä»¥ä¾¿è§‚å¯Ÿ
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
            if distance <= goal_threshold:
                if not episode_success:  # é¿å…é‡å¤è®¡æ•°
                    success_count += 1
                    episode_success = True
                    print(f"  ğŸ‰ ç›®æ ‡åˆ°è¾¾! è·ç¦»: {distance:.1f} pixels, æ­¥éª¤: {step_count}")
                    if not render:  # å¦‚æœæ²¡æœ‰æ¸²æŸ“ï¼Œåœ¨åˆ°è¾¾ç›®æ ‡åç­‰å¾…å‡ æ­¥å†ç»“æŸ
                        if step_count > max_steps - 50:  # æœ€å50æ­¥å†…åˆ°è¾¾å°±ç»“æŸ
                            break
                    else:
                        break
                        
            if done:
                break
        
        total_rewards.append(episode_reward)
        min_distances.append(min_distance_this_episode)
        episode_lengths.append(step_count)
        
        print(f"  ğŸ“Š Episode {episode + 1} ç»“æœ:")
        print(f"    å¥–åŠ±: {episode_reward:.2f}")
        print(f"    æœ€å°è·ç¦»: {min_distance_this_episode:.1f} pixels")
        print(f"    æ­¥éª¤æ•°: {step_count}")
        print(f"    æˆåŠŸ: {'âœ… æ˜¯' if episode_success else 'âŒ å¦'}")
    
    # æµ‹è¯•æ€»ç»“
    success_rate = success_count / num_episodes
    avg_reward = np.mean(total_rewards)
    avg_min_distance = np.mean(min_distances)
    avg_episode_length = np.mean(episode_lengths)
    
    print(f"\n{'='*60}")
    print(f"ğŸ† æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"  æµ‹è¯•Episodes: {num_episodes}")
    print(f"  æˆåŠŸæ¬¡æ•°: {success_count}")
    print(f"  æˆåŠŸç‡: {success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"  å¹³å‡æœ€å°è·ç¦»: {avg_min_distance:.1f} pixels")
    print(f"  å¹³å‡Episodeé•¿åº¦: {avg_episode_length:.1f} steps")
    print(f"  ç›®æ ‡é˜ˆå€¼: {goal_threshold:.1f} pixels")
    
    # æ€§èƒ½è¯„ä»·
    print(f"\nğŸ“‹ æ€§èƒ½è¯„ä»·:")
    if success_rate >= 0.8:
        print(f"  ğŸ† ä¼˜ç§€! æˆåŠŸç‡ >= 80%")
    elif success_rate >= 0.5:
        print(f"  ğŸ‘ è‰¯å¥½! æˆåŠŸç‡ >= 50%")
    elif success_rate >= 0.2:
        print(f"  âš ï¸  ä¸€èˆ¬! æˆåŠŸç‡ >= 20%")
    else:
        print(f"  âŒ éœ€è¦æ”¹è¿›! æˆåŠŸç‡ < 20%")
        
    if avg_min_distance <= goal_threshold:
        print(f"  âœ… å¹³å‡æœ€å°è·ç¦»è¾¾åˆ°ç›®æ ‡é˜ˆå€¼")
    else:
        print(f"  âš ï¸  å¹³å‡æœ€å°è·ç¦»è¶…å‡ºç›®æ ‡é˜ˆå€¼ {avg_min_distance - goal_threshold:.1f} pixels")
    
    print(f"{'='*60}")
    
    env.close()
    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward, 
        'avg_min_distance': avg_min_distance,
        'avg_episode_length': avg_episode_length,
        'success_count': success_count,
        'total_episodes': num_episodes
    }


def find_latest_model():
    """æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶"""
    base_path = "./trained_models/reacher2d/enhanced_test"
    
    if not os.path.exists(base_path):
        print(f"âŒ è®­ç»ƒæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {base_path}")
        return None
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹æ–‡ä»¶
    model_candidates = []
    
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.pth') and ('final_model' in file or 'checkpoint' in file):
                full_path = os.path.join(root, file)
                # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                mtime = os.path.getmtime(full_path)
                model_candidates.append((full_path, mtime))
    
    if not model_candidates:
        print(f"âŒ åœ¨ {base_path} ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
    model_candidates.sort(key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ” æ‰¾åˆ° {len(model_candidates)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
    for i, (path, mtime) in enumerate(model_candidates[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
        import datetime
        time_str = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        print(f"  {i+1}. {os.path.basename(path)} ({time_str})")
    
    latest_model = model_candidates[0][0]
    print(f"âœ… é€‰æ‹©æœ€æ–°æ¨¡å‹: {latest_model}")
    return latest_model

 
if __name__ == "__main__":
    torch.set_default_dtype(torch.float64)
    
    # ğŸš€ NEW: æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == '--test':
        print("ğŸ§ª è¿›å…¥æµ‹è¯•æ¨¡å¼")
        
        # è§£ææµ‹è¯•å‚æ•°
        model_path = None
        num_episodes = 5
        render = True
        
        for i, arg in enumerate(sys.argv):
            if arg == '--model-path' and i + 1 < len(sys.argv):
                model_path = sys.argv[i + 1]
            elif arg == '--episodes' and i + 1 < len(sys.argv):
                num_episodes = int(sys.argv[i + 1])
            elif arg == '--no-render':
                render = False
            elif arg == '--latest':
                model_path = 'latest'  # æ ‡è®°ä½¿ç”¨æœ€æ–°æ¨¡å‹
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
        if model_path is None or model_path == 'latest':
            print("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹...")
            model_path = find_latest_model()
        
        if model_path:
            print(f"ğŸ¯ æµ‹è¯•å‚æ•°: episodes={num_episodes}, render={render}")
            result = test_trained_model(model_path, num_episodes, render)
            
            if result:
                print(f"\nğŸ¯ å¿«é€Ÿç»“è®º:")
                if result['success_rate'] >= 0.8:
                    print(f"  âœ… æ¨¡å‹è¡¨ç°ä¼˜ç§€! ç»§ç»­å½“å‰è®­ç»ƒç­–ç•¥")
                elif result['success_rate'] >= 0.3:
                    print(f"  âš ï¸  æ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´å‚æ•°")
                else:
                    print(f"  âŒ æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦é‡æ–°å®¡è§†å¥–åŠ±å‡½æ•°æˆ–ç½‘ç»œç»“æ„")
        else:
            print("âŒ æœªæ‰¾åˆ°å¯æµ‹è¯•çš„æ¨¡å‹")
        
        exit(0)
    
    # ğŸš€ NEW: æ£€æŸ¥æ˜¯å¦æ˜¯è®­ç»ƒæ¨¡å¼ (é»˜è®¤)
    elif len(sys.argv) > 1 and sys.argv[1] == '--train':
        print("ğŸš€ è¿›å…¥è®­ç»ƒæ¨¡å¼")
        # ç§»é™¤ --train å‚æ•°ï¼Œç»§ç»­ä½¿ç”¨åŸæœ‰çš„å‚æ•°è§£æ
        sys.argv.pop(1)  # ç§»é™¤ --train
    
    # ğŸ”§ MODIFIED: åŸæœ‰çš„æµ‹è¯•å‘½ä»¤ä¿æŒå…¼å®¹æ€§
    if len(sys.argv) > 1 and sys.argv[1] == '--test-reacher2d':
        print("ğŸ¤– å¯åŠ¨ Reacher2D ç¯å¢ƒè®­ç»ƒ (å…¼å®¹æ¨¡å¼)")
        args_list = ['--env-name', 'reacher2d',
                     '--num-processes', '2',
                     '--lr', '3e-4',
                     '--gamma', '0.99',
                     '--seed', '42',
                     '--save-dir', './trained_models/reacher2d/enhanced_test/',
                     '--grammar-file', '/home/xli149/Documents/repos/RoboGrammar/data/designs/grammar_jan21.dot',
                     '--rule-sequence', '0']
        test_args = sys.argv[2:] if len(sys.argv) > 2 else []
    
    # ğŸš€ NEW: é»˜è®¤è®­ç»ƒæ¨¡å¼ - ä½¿ç”¨ reacher2d é…ç½®
    elif len(sys.argv) == 1 or (len(sys.argv) > 1 and sys.argv[1] not in ['--test', '--train', '--test-reacher2d']):
        print("ğŸ¤– é»˜è®¤å¯åŠ¨ Reacher2D ç¯å¢ƒè®­ç»ƒ")
        args_list = ['--env-name', 'reacher2d',
                     '--num-processes', '2',
                     '--lr', '3e-4',
                     '--gamma', '0.99',
                     '--seed', '42',
                     '--save-dir', './trained_models/reacher2d/enhanced_test/',
                     '--grammar-file', '/home/xli149/Documents/repos/RoboGrammar/data/designs/grammar_jan21.dot',
                     '--rule-sequence', '0']
        test_args = sys.argv[1:] if len(sys.argv) > 1 else []
    
    else:
        # å…¶ä»–ç¯å¢ƒçš„è®­ç»ƒé…ç½® (bullet envç­‰)
        args_list = ['--env-name', 'RobotLocomotion-v0',
                     '--task', 'FlatTerrainTask',
                     '--grammar-file', '../../data/designs/grammar_jan21.dot',
                     '--algo', 'ppo',
                     '--use-gae',
                     '--log-interval', '5',
                     '--num-steps', '1024',
                     '--num-processes', '2',
                     '--lr', '3e-4',
                     '--entropy-coef', '0',
                     '--value-loss-coef', '0.5',
                     '--ppo-epoch', '10',
                     '--num-mini-batch', '32',
                     '--gamma', '0.995',
                     '--gae-lambda', '0.95',
                     '--num-env-steps', '30000000',
                     '--use-linear-lr-decay',
                     '--use-proper-time-limits',
                     '--save-interval', '100',
                     '--seed', '2',
                     '--save-dir', './trained_models/RobotLocomotion-v0/enhanced_test/',
                     '--render-interval', '80']
        test_args = sys.argv[1:]
    
    parser = get_parser()

    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--resume-checkpoint', type=str, default=None,
                       help='ä»æŒ‡å®šcheckpointç»§ç»­è®­ç»ƒçš„æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume-lr', type=float, default=None,
                       help='æ¢å¤è®­ç»ƒæ—¶ä½¿ç”¨çš„æ–°å­¦ä¹ ç‡')
    parser.add_argument('--resume-alpha', type=float, default=None,
                       help='æ¢å¤è®­ç»ƒæ—¶ä½¿ç”¨çš„æ–°alphaå€¼')
    parser.add_argument('--additional-steps', type=int, default=None,
                       help='æ¢å¤è®­ç»ƒæ—¶é¢å¤–çš„è®­ç»ƒæ­¥æ•°')
    

    parser.add_argument('--batch-size', type=int, default=64,
                    help='è®­ç»ƒæ‰¹æ¬¡å¤§å°')
    parser.add_argument('--buffer-capacity', type=int, default=10000,
                    help='ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°')
    parser.add_argument('--warmup-steps', type=int, default=1000,
                    help='çƒ­èº«æ­¥æ•°')
    parser.add_argument('--target-entropy-factor', type=float, default=0.8,
                    help='ç›®æ ‡ç†µç³»æ•° (target_entropy = -action_dim * factor)')
    parser.add_argument('--update-frequency', type=int, default=2,
                    help='ç½‘ç»œæ›´æ–°é¢‘ç‡ (æ¯Næ­¥æ›´æ–°ä¸€æ¬¡)')
    
    args = parser.parse_args(args_list + test_args)

    solve_argv_conflict(args_list)
    args = parser.parse_args(args_list + test_args)

    args.cuda = not args.no_cuda and torch.cuda.is_available()

    args.save_dir = os.path.join(args.save_dir, get_time_stamp())
    try:
        os.makedirs(args.save_dir, exist_ok=True)
    except OSError:
        pass

    fp = open(os.path.join(args.save_dir, 'args.txt'), 'w')
    fp.write(str(args_list + test_args))
    fp.close()

    main(args)