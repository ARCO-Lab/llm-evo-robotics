#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆè®­ç»ƒè„šæœ¬ - é‡æ„ç‰ˆ
ä»£ç ç»“æ„æ¸…æ™°ï¼ŒåŠŸèƒ½æ¨¡å—åŒ–ï¼Œä¿æŒæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
"""

import sys
import os
import time
import numpy as np
import torch
import argparse
import logging
from collections import deque

# === è·¯å¾„è®¾ç½® ===
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../')
sys.path.extend([
    base_dir,
    os.path.join(base_dir, 'examples/2d_reacher/envs'),
    os.path.join(base_dir, 'examples/surrogate_model/gnn_encoder'),
    os.path.join(base_dir, 'examples/rl/train'),
    os.path.join(base_dir, 'examples/rl/common'),
    os.path.join(base_dir, 'examples/rl/environments'),
    os.path.join(base_dir, 'examples/rl')
])

# === æ ¸å¿ƒå¯¼å…¥ ===
import gym
gym.logger.set_level(40)

if not hasattr(np, 'bool'):
    np.bool = bool

from training_logger import TrainingLogger, RealTimeMonitor
from gnn_encoder import GNN_Encoder
from attn_model.attn_model import AttnModel
from sac.sac_model import AttentionSACWithBuffer
from env_config.env_wrapper import make_reacher2d_vec_envs
from reacher2d_env import Reacher2DEnv

# RLç›¸å…³å¯¼å…¥
import environments
from common import *
from attn_dataset.sim_data_handler import DataHandler

# === é…ç½®å¸¸é‡ ===
SILENT_MODE = True
GOAL_THRESHOLD = 20.0
DEFAULT_CONFIG = {
    'num_links': 4,
    'link_lengths': [80, 80, 80, 60],
    'config_path': "/home/xli149/Documents/repos/test_robo/examples/2d_reacher/configs/reacher_with_zigzag_obstacles.yaml"
}

# === è‡ªå®šä¹‰å‚æ•°è§£æå™¨ ===
def create_training_parser():
    """åˆ›å»ºè®­ç»ƒä¸“ç”¨çš„å‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='Enhanced SAC Training for Reacher2D')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--env-name', default='reacher2d', help='ç¯å¢ƒåç§°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--num-processes', type=int, default=2, help='å¹¶è¡Œè¿›ç¨‹æ•°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--gamma', type=float, default=0.99, help='æŠ˜æ‰£å› å­')
    parser.add_argument('--alpha', type=float, default=0.1, help='SACç†µç³»æ•°')
    parser.add_argument('--save-dir', default='./trained_models/reacher2d/enhanced_test/', help='ä¿å­˜ç›®å½•')
    
      # ğŸ†• æ·»åŠ æ¸²æŸ“æ§åˆ¶å‚æ•°
    parser.add_argument('--render', action='store_true', default=False, help='æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–çª—å£')
    parser.add_argument('--no-render', action='store_true', default=False, help='å¼ºåˆ¶ç¦ç”¨å¯è§†åŒ–çª—å£')
    # SACç‰¹å®šå‚æ•°
    parser.add_argument('--warmup-steps', type=int, default=1000, help='çƒ­èº«æ­¥æ•°')
    parser.add_argument('--target-entropy-factor', type=float, default=0.8, help='ç›®æ ‡ç†µç³»æ•°')
    parser.add_argument('--update-frequency', type=int, default=2, help='ç½‘ç»œæ›´æ–°é¢‘ç‡')
    parser.add_argument('--batch-size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--buffer-capacity', type=int, default=10000, help='ç¼“å†²åŒºå®¹é‡')
    
    # æ¢å¤è®­ç»ƒå‚æ•°
    parser.add_argument('--resume-checkpoint', type=str, default=None, help='æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--resume-lr', type=float, default=None, help='æ¢å¤æ—¶çš„å­¦ä¹ ç‡')
    parser.add_argument('--resume-alpha', type=float, default=None, help='æ¢å¤æ—¶çš„alphaå€¼')
    
    # å…¼å®¹æ€§å‚æ•°ï¼ˆç”¨äºå…¶ä»–ç¯å¢ƒï¼‰
    parser.add_argument('--grammar-file', type=str, default='/home/xli149/Documents/repos/RoboGrammar/data/designs/grammar_jan21.dot', help='è¯­æ³•æ–‡ä»¶')
    parser.add_argument('--rule-sequence', nargs='+', default=['0'], help='è§„åˆ™åºåˆ—')
    parser.add_argument('--no-cuda', action='store_true', default=False, help='ç¦ç”¨CUDA')
    
    return parser

# === å·¥å…·å‡½æ•° ===
def smart_print(*args, **kwargs):
    """æ™ºèƒ½æ‰“å°å‡½æ•°"""
    if not SILENT_MODE:
        print(*args, **kwargs)

def get_time_stamp():
    """è·å–æ—¶é—´æˆ³"""
    return time.strftime('%m-%d-%Y-%H-%M-%S')

# === æ¨¡å‹ç®¡ç†å™¨ ===
class ModelManager:
    """æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç®¡ç†å™¨"""
    
    def __init__(self, save_dir):
        self.save_dir = save_dir
        self.best_models_dir = os.path.join(save_dir, 'best_models')
        os.makedirs(self.best_models_dir, exist_ok=True)
    
    def save_best_model(self, sac, success_rate, min_distance, step):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        try:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            
            model_data = {
                'step': step,
                'success_rate': success_rate,
                'min_distance': min_distance,
                'timestamp': timestamp,
                'actor_state_dict': sac.actor.state_dict(),
                'critic1_state_dict': sac.critic1.state_dict(),
                'critic2_state_dict': sac.critic2.state_dict(),
                'target_critic1_state_dict': sac.target_critic1.state_dict(),
                'target_critic2_state_dict': sac.target_critic2.state_dict(),
                'actor_optimizer_state_dict': sac.actor_optimizer.state_dict(),
                'critic_optimizer_state_dict': sac.critic_optimizer.state_dict(),
                'alpha_optimizer_state_dict': sac.alpha_optimizer.state_dict(),
                'alpha': sac.alpha.item() if torch.is_tensor(sac.alpha) else sac.alpha,
                'log_alpha': sac.log_alpha.item() if torch.is_tensor(sac.log_alpha) else sac.log_alpha,
            }
            
            model_file = os.path.join(self.best_models_dir, f'best_model_step_{step}_{timestamp}.pth')
            torch.save(model_data, model_file)
            
            latest_file = os.path.join(self.best_models_dir, 'latest_best_model.pth')
            torch.save(model_data, latest_file)
            
            print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹: æˆåŠŸç‡ {success_rate:.3f}, è·ç¦» {min_distance:.1f}, æ­¥éª¤ {step}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    # ä¿®å¤ save_checkpoint æ–¹æ³• (ç¬¬145-159è¡Œ)
    def save_checkpoint(self, sac, step, **kwargs):
        """ä¿å­˜æ£€æŸ¥ç‚¹ - å®Œæ•´ç‰ˆ"""
        try:
            checkpoint_data = {
                'step': step,
                'actor_state_dict': sac.actor.state_dict(),
                'critic1_state_dict': sac.critic1.state_dict(),
                'critic2_state_dict': sac.critic2.state_dict(),
                'target_critic1_state_dict': sac.target_critic1.state_dict(),
                'target_critic2_state_dict': sac.target_critic2.state_dict(),
                # ğŸ”§ æ·»åŠ ä¼˜åŒ–å™¨çŠ¶æ€
                'actor_optimizer_state_dict': sac.actor_optimizer.state_dict(),
                'critic_optimizer_state_dict': sac.critic_optimizer.state_dict(),
                'alpha_optimizer_state_dict': sac.alpha_optimizer.state_dict(),
                # ğŸ”§ æ·»åŠ alphaå€¼
                'alpha': sac.alpha.item() if torch.is_tensor(sac.alpha) else sac.alpha,
                'log_alpha': sac.log_alpha.item() if torch.is_tensor(sac.log_alpha) else sac.log_alpha,
                # ğŸ”§ æ·»åŠ è®­ç»ƒçŠ¶æ€
                'buffer_size': len(sac.memory),
                'warmup_steps': sac.warmup_steps,
                **kwargs
            }
            
            checkpoint_path = os.path.join(self.best_models_dir, f'checkpoint_step_{step}.pth')
            torch.save(checkpoint_data, checkpoint_path)
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: step {step}, buffer size: {len(sac.memory)}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False

    # ä¿®å¤ save_final_model æ–¹æ³• (ç¬¬161-176è¡Œ)
    def save_final_model(self, sac, step, **kwargs):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹ - å®Œæ•´ç‰ˆ"""
        try:
            final_model_data = {
                'step': step,
                'training_completed': True,
                'actor_state_dict': sac.actor.state_dict(),
                'critic1_state_dict': sac.critic1.state_dict(),
                'critic2_state_dict': sac.critic2.state_dict(),
                'target_critic1_state_dict': sac.target_critic1.state_dict(),
                'target_critic2_state_dict': sac.target_critic2.state_dict(),
                # ğŸ”§ æ·»åŠ ä¼˜åŒ–å™¨çŠ¶æ€
                'actor_optimizer_state_dict': sac.actor_optimizer.state_dict(),
                'critic_optimizer_state_dict': sac.critic_optimizer.state_dict(),
                'alpha_optimizer_state_dict': sac.alpha_optimizer.state_dict(),
                # ğŸ”§ æ·»åŠ alphaå€¼
                'alpha': sac.alpha.item() if torch.is_tensor(sac.alpha) else sac.alpha,
                'log_alpha': sac.log_alpha.item() if torch.is_tensor(sac.log_alpha) else sac.log_alpha,
                **kwargs
            }
            
            final_path = os.path.join(self.best_models_dir, f'final_model_step_{step}.pth')
            torch.save(final_model_data, final_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_path}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æœ€ç»ˆæ¨¡å‹å¤±è´¥: {e}")
            return False

    # ä¿®å¤ load_checkpoint æ–¹æ³• (ç¬¬178-226è¡Œ)
    def load_checkpoint(self, sac, checkpoint_path, device='cpu'):
        """åŠ è½½æ£€æŸ¥ç‚¹ - å¢å¼ºç‰ˆ"""
        try:
            if not os.path.exists(checkpoint_path):
                print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
                return 0
            
            print(f"ğŸ”„ Loading checkpoint from: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device)
            
            print(f"ğŸ“‹ Checkpoint contains: {list(checkpoint.keys())}")
            
            # ğŸ”§ åŠ è½½ç½‘ç»œçŠ¶æ€ - å¢å¼ºé”™è¯¯å¤„ç†
            networks = ['actor', 'critic1', 'critic2', 'target_critic1', 'target_critic2']
            for network_name in networks:
                state_dict_key = f'{network_name}_state_dict'
                if state_dict_key in checkpoint:
                    try:
                        network = getattr(sac, network_name)
                        state_dict = checkpoint[state_dict_key]
                        
                        # ğŸ”§ å¤„ç†proj_dicté—®é¢˜
                        if 'proj_dict' in str(state_dict.keys()):
                            print(f"âš ï¸ {network_name} åŒ…å«proj_dictï¼Œä½¿ç”¨strict=FalseåŠ è½½")
                            missing_keys, unexpected_keys = network.load_state_dict(state_dict, strict=False)
                            if unexpected_keys:
                                print(f"   å¿½ç•¥çš„é”®: {unexpected_keys[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
                        else:
                            network.load_state_dict(state_dict)
                        
                        print(f"âœ… {network_name} loaded")
                    except Exception as e:
                        print(f"âš ï¸ {network_name} åŠ è½½å¤±è´¥: {e}")
                        print("   å°†è·³è¿‡è¯¥ç½‘ç»œï¼Œä½¿ç”¨åˆå§‹åŒ–æƒé‡")
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ° {state_dict_key}")
            
            # ğŸ”§ åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ - å¯é€‰åŠ è½½
            load_optimizers = True  # å¯ä»¥è®¾ä¸ºFalseå¦‚æœæƒ³ç”¨æ–°çš„å­¦ä¹ ç‡
            if load_optimizers:
                optimizers = ['actor_optimizer', 'critic_optimizer', 'alpha_optimizer']
                for opt_name in optimizers:
                    opt_key = f'{opt_name}_state_dict'
                    if opt_key in checkpoint:
                        try:
                            optimizer = getattr(sac, opt_name)
                            optimizer.load_state_dict(checkpoint[opt_key])
                            print(f"âœ… {opt_name} loaded")
                        except Exception as e:
                            print(f"âš ï¸ {opt_name} åŠ è½½å¤±è´¥: {e}")
                            print("   å°†ä½¿ç”¨å½“å‰ä¼˜åŒ–å™¨çŠ¶æ€")
            
            # ğŸ”§ åŠ è½½alphaå€¼
            if 'alpha' in checkpoint:
                try:
                    alpha_val = checkpoint['alpha']
                    if isinstance(alpha_val, (int, float)):
                        sac.alpha = alpha_val
                    else:
                        sac.alpha = alpha_val.item()
                    print(f"âœ… Alpha loaded: {sac.alpha}")
                except Exception as e:
                    print(f"âš ï¸ Alpha åŠ è½½å¤±è´¥: {e}")
                    
            if 'log_alpha' in checkpoint:
                try:
                    log_alpha_val = checkpoint['log_alpha']
                    if isinstance(log_alpha_val, (int, float)):
                        sac.log_alpha.data.fill_(log_alpha_val)
                    else:
                        sac.log_alpha.data.fill_(log_alpha_val.item())
                    print(f"âœ… Log Alpha loaded: {sac.log_alpha.item()}")
                except Exception as e:
                    print(f"âš ï¸ Log Alpha åŠ è½½å¤±è´¥: {e}")
            
            # ğŸ”§ æ˜¾ç¤ºé¢å¤–ä¿¡æ¯
            start_step = checkpoint.get('step', 0)
            buffer_size = checkpoint.get('buffer_size', 'N/A')
            warmup_steps = checkpoint.get('warmup_steps', 'N/A')
            
            print(f"âœ… Checkpoint loaded successfully!")
            print(f"   Starting step: {start_step}")
            print(f"   Buffer size: {buffer_size}")
            print(f"   Warmup steps: {warmup_steps}")
            
            return start_step
            
        except Exception as e:
            print(f"âŒ Failed to load checkpoint: {e}")
            import traceback
            traceback.print_exc()
            print("Training will start from scratch...")
            return 0

# === ç¯å¢ƒè®¾ç½®ç®¡ç†å™¨ ===
class EnvironmentSetup:
    """ç¯å¢ƒè®¾ç½®ç®¡ç†å™¨"""
    
    @staticmethod
    def create_reacher2d_env(args):
        """åˆ›å»ºReacher2Dç¯å¢ƒ"""
        # è·å–ç¯å¢ƒé…ç½®
        if hasattr(args, 'num_joints') and hasattr(args, 'link_lengths'):
            num_links = args.num_joints
            link_lengths = args.link_lengths
            print(f"ğŸ¤– ä½¿ç”¨MAP-Elitesé…ç½®: {num_links}å…³èŠ‚, é•¿åº¦={link_lengths}")
        else:
            num_links = DEFAULT_CONFIG['num_links']
            link_lengths = DEFAULT_CONFIG['link_lengths']
            print(f"ğŸ¤– ä½¿ç”¨é»˜è®¤é…ç½®: {num_links}å…³èŠ‚, é•¿åº¦={link_lengths}")


        should_render = False
        if hasattr(args, 'render') and args.render:
            should_render = True
        elif hasattr(args, 'no_render') and args.no_render:
            should_render = False
        else:
            should_render = False  # ğŸ†• é»˜è®¤ä¸æ˜¾ç¤ºï¼Œé™¤éæ˜ç¡®æŒ‡å®š
            print("ğŸ¨ æ¸²æŸ“è®¾ç½®: é»˜è®¤ç¦ç”¨ (æ— æ¸²æŸ“å‚æ•°)")

        env_params = {
            'num_links': num_links,
            'link_lengths': link_lengths,
            'render_mode': 'human' if args.num_processes == 1 else None,
            'config_path': DEFAULT_CONFIG['config_path']
        }
        
        print(f"ç¯å¢ƒå‚æ•°: {env_params}")
        print(f"ğŸ¨ æ¸²æŸ“è®¾ç½®: {'å¯ç”¨' if should_render else 'ç¦ç”¨'}")
        # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
        envs = make_reacher2d_vec_envs(
            env_params=env_params,
            seed=args.seed,
            num_processes=args.num_processes,
            gamma=args.gamma,
            log_dir=None,
            device=torch.device('cpu'),
            allow_early_resets=False,
        )
        
        # ğŸ†• æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åˆ›å»ºæ¸²æŸ“ç¯å¢ƒ
        if should_render:
            render_env_params = env_params.copy()
            render_env_params['render_mode'] = 'human'
            sync_env = Reacher2DEnv(**render_env_params)
            print(f"âœ… è®­ç»ƒç¯å¢ƒå·²åˆ›å»ºï¼ˆè¿›ç¨‹æ•°: {args.num_processes}ï¼Œå¸¦æ¸²æŸ“ï¼‰")
        else:
                sync_env = None
                print(f"âœ… è®­ç»ƒç¯å¢ƒå·²åˆ›å»ºï¼ˆè¿›ç¨‹æ•°: {args.num_processes}ï¼Œæ— æ¸²æŸ“ï¼‰")
            
        return envs, sync_env, env_params

# === è®­ç»ƒç®¡ç†å™¨ ===
class TrainingManager:
    """è®­ç»ƒè¿‡ç¨‹ç®¡ç†å™¨"""
    
    def __init__(self, args, sac, logger, model_manager):
        self.args = args
        self.sac = sac
        self.logger = logger
        self.model_manager = model_manager
        
        # è®­ç»ƒçŠ¶æ€
        self.best_success_rate = 0.0
        self.best_min_distance = float('inf')
        self.consecutive_success_count = 0
        self.min_consecutive_successes = 3
        
    def handle_episode_end(self, proc_id, step, episode_rewards, infos):
        """å¤„ç†episodeç»“æŸé€»è¾‘"""
        if len(infos) <= proc_id or not isinstance(infos[proc_id], dict):
            episode_rewards[proc_id] = 0.0
            return False
        
        info = infos[proc_id]
        goal_reached = False
        distance = float('inf')
        
        # æ£€æŸ¥ç›®æ ‡ä¿¡æ¯
        if 'goal' in info:
            goal_info = info['goal']
            distance = goal_info.get('distance_to_goal', float('inf'))
            goal_reached = goal_info.get('goal_reached', False)
            
            print(f"Episode {step} ç»“æŸ: å¥–åŠ± {episode_rewards[proc_id]:.2f}, è·ç¦» {distance:.1f}")
            
            if goal_reached:
                print(f"ğŸ‰ æˆåŠŸåˆ°è¾¾ç›®æ ‡! è·ç¦»: {distance:.1f}")
                self.consecutive_success_count += 1
                
                # ğŸ”§ ç»Ÿä¸€çš„ä¿å­˜é€»è¾‘
                if distance < self.best_min_distance:
                    self.best_min_distance = distance
                    success_rate = self.consecutive_success_count / max(1, step // 100)
                    self.best_success_rate = max(success_rate, self.best_success_rate)
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŒ…å«å®Œæ•´çŠ¶æ€ï¼‰
                    self.model_manager.save_best_model(
                        self.sac, success_rate, distance, step
                    )
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è®­ç»ƒç›®æ ‡
                if self.consecutive_success_count >= self.min_consecutive_successes and step > 5000:
                    print(f"ğŸ è¿ç»­æˆåŠŸ{self.consecutive_success_count}æ¬¡ï¼Œè®­ç»ƒè¾¾åˆ°ç›®æ ‡!")
                    
                    # ğŸ”§ åªéœ€è¦æ ‡è®°è®­ç»ƒå®Œæˆï¼Œä¸éœ€è¦é‡å¤ä¿å­˜
                    print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼Œè®­ç»ƒç›®æ ‡è¾¾æˆï¼")
                    return True  # ç»“æŸè®­ç»ƒ
        else:
                self.consecutive_success_count = 0
        
        # è®°å½•episodeæŒ‡æ ‡
        episode_metrics = {
            'reward': episode_rewards[proc_id],
            'length': step,
            'distance_to_goal': distance,
            'goal_reached': goal_reached
        }
        self.logger.log_episode(step // 100, episode_metrics)
        episode_rewards[proc_id] = 0.0
        
        return False
    
    def should_update_model(self, step):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°æ¨¡å‹"""
        return (step >= self.sac.warmup_steps and 
                step % self.args.update_frequency == 0 and 
                self.sac.memory.can_sample(self.sac.batch_size))
    
    def update_and_log(self, step, total_steps):
        """æ›´æ–°æ¨¡å‹å¹¶è®°å½•"""
        metrics = self.sac.update()
        
        if metrics:
            enhanced_metrics = metrics.copy()
            enhanced_metrics.update({
                'step': step,
                'buffer_size': len(self.sac.memory),
                'learning_rate': self.sac.actor_optimizer.param_groups[0]['lr'],
                'warmup_progress': min(1.0, step / max(self.sac.warmup_steps, 1))
            })
            
            self.logger.log_step(step, enhanced_metrics, episode=step//100)
            
            if step % 100 == 0:
                print(f"Step {step} (total_steps {total_steps}): "
                      f"Learning Rate: {metrics['lr']:.6f}, "
                      f"Critic Loss: {metrics['critic_loss']:.4f}, "
                      f"Actor Loss: {metrics['actor_loss']:.4f}, "
                      f"Alpha: {metrics['alpha']:.4f}, "
                      f"Buffer Size: {len(self.sac.memory)}")



def main(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    # è®¾ç½®éšæœºç§å­å’Œè®¾å¤‡
    torch.manual_seed(args.seed)
    torch.set_num_threads(1)
    device = torch.device('cpu')
    os.makedirs(args.save_dir, exist_ok=True)
    
    # åˆ›å»ºè®­ç»ƒæ—¥å¿—
    training_log_path = os.path.join(args.save_dir, 'logs.txt')
    with open(training_log_path, 'w') as f:
        pass
    
    # åˆ›å»ºç¯å¢ƒ
    if args.env_name == 'reacher2d':
        print("ä½¿ç”¨ reacher2d ç¯å¢ƒ")
        env_setup = EnvironmentSetup()
        envs, sync_env, env_params = env_setup.create_reacher2d_env(args)
        args.env_type = 'reacher2d'
    else:
        print(f"ä½¿ç”¨ bullet ç¯å¢ƒ: {args.env_name}")
        from a2c_ppo_acktr.envs import make_vec_envs
        envs = make_vec_envs(args.env_name, args.seed, args.num_processes, 
                            args.gamma, None, device, False, args=args)
        sync_env = None
        args.env_type = 'bullet'

    # è·å–å…³èŠ‚æ•°é‡å’Œåˆ›å»ºæ•°æ®å¤„ç†å™¨
    num_joints = envs.action_space.shape[0]
    print(f"å…³èŠ‚æ•°é‡: {num_joints}")
    data_handler = DataHandler(num_joints, args.env_type)

    # åˆ›å»ºGNNç¼–ç å™¨
    if args.env_type == 'reacher2d':
        sys.path.append(os.path.join(base_dir, 'examples/2d_reacher/utils'))
        from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
        
        print("ğŸ¤– åˆå§‹åŒ– Reacher2D GNN ç¼–ç å™¨...")
        reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
        single_gnn_embed = reacher2d_encoder.get_gnn_embeds(
            num_links=num_joints, 
            link_lengths=env_params['link_lengths']
        )
        print(f"âœ… Reacher2D GNN åµŒå…¥ç”ŸæˆæˆåŠŸï¼Œå½¢çŠ¶: {single_gnn_embed.shape}")
    else:
        rule_sequence = [int(s.strip(",")) for s in args.rule_sequence]
        gnn_encoder = GNN_Encoder(args.grammar_file, rule_sequence, 70, num_joints)
        gnn_graph = gnn_encoder.get_graph(rule_sequence)
        single_gnn_embed = gnn_encoder.get_gnn_embeds(gnn_graph)

    # åˆ›å»ºSACæ¨¡å‹
    attn_model = AttnModel(128, 130, 130, 4)
    sac = AttentionSACWithBuffer(
        attn_model, num_joints, 
        buffer_capacity=args.buffer_capacity, batch_size=args.batch_size,
        lr=args.lr, env_type=args.env_type
    )
    
    # æ·»åŠ SACç‰¹å®šå‚æ•°
    sac.warmup_steps = args.warmup_steps
    sac.alpha = torch.tensor(args.alpha)
    sac.min_alpha = 0.05
    print(f"ğŸ”’ Alphaè¡°å‡ä¸‹é™è®¾ç½®ä¸º: {sac.min_alpha}")

    if hasattr(sac, 'target_entropy'):
        sac.target_entropy = -num_joints * args.target_entropy_factor
    
    # åˆ›å»ºè®­ç»ƒç›‘æ§ç³»ç»Ÿ
    experiment_name = f"reacher2d_sac_{time.strftime('%Y%m%d_%H%M%S')}"
    
    # é…ç½®ä¿¡æ¯
    hyperparams = {
        'learning_rate': args.lr,
        'alpha': args.alpha,
        'warmup_steps': args.warmup_steps,
        'target_entropy_factor': args.target_entropy_factor,
        'batch_size': args.batch_size,
        'buffer_capacity': args.buffer_capacity,
        'gamma': args.gamma,
        'seed': args.seed,
        'num_processes': args.num_processes,
        'update_frequency': args.update_frequency,
        'total_steps': 120000,
        'optimizer': 'Adam',
        'network_architecture': {
            'attn_model_dims': '128-130-130-4',
            'action_dim': num_joints,
        }
    }
    
    env_config = {
        'env_name': args.env_name,
        'env_type': args.env_type,
        'goal_threshold': GOAL_THRESHOLD,
        'action_dim': num_joints,
    }
    
    if args.env_type == 'reacher2d':
        env_config.update({
            'num_links': env_params['num_links'],
            'link_lengths': env_params['link_lengths'],
            'config_path': env_params.get('config_path', 'N/A'),
            'render_mode': env_params.get('render_mode', 'human'),
            'reward_function': 'è·ç¦»+è¿›åº¦+æˆåŠŸ+ç¢°æ’+æ–¹å‘å¥–åŠ±',
            'physics_engine': 'PyMunk',
            'obstacle_type': 'zigzag',
            'action_space': 'continuous',
            'observation_space': 'joint_angles_and_positions'
        })
    
    logger = TrainingLogger(
        log_dir=os.path.join(args.save_dir, 'training_logs'),
        experiment_name=experiment_name,
        hyperparams=hyperparams,
        env_config=env_config
    )
    
    # monitor = RealTimeMonitor(logger, alert_thresholds={
    #     'critic_loss': {'max': 50.0, 'nan_check': True},
    #     'actor_loss': {'max': 10.0, 'nan_check': True},
    #     'alpha_loss': {'max': 5.0, 'nan_check': True},
    #     'alpha': {'min': 0.01, 'max': 2.0, 'nan_check': True}
    # })
    
    print(f"ğŸ“Š è®­ç»ƒç›‘æ§ç³»ç»Ÿå·²åˆå§‹åŒ–: {logger.experiment_dir}")
    
    # åˆ›å»ºç®¡ç†å™¨
    model_manager = ModelManager(args.save_dir)
    training_manager = TrainingManager(args, sac, logger, model_manager)
    
    # å¤„ç†checkpointæ¢å¤
    start_step = 0
    if args.resume_checkpoint:
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume_checkpoint}")
        start_step = model_manager.load_checkpoint(sac, args.resume_checkpoint)

        if start_step > 0:
            print(f"æˆåŠŸåŠ è½½checkpoint, ä»step {start_step} å¼€å§‹è®­ç»ƒ")
            sac.warmup_steps = 0

            # æ›´æ–°å­¦ä¹ ç‡å’Œalpha
            if args.resume_lr:
                for param_group in sac.actor_optimizer.param_groups:
                    param_group['lr'] = args.resume_lr
                for param_group in sac.critic_optimizer.param_groups:
                    param_group['lr'] = args.resume_lr
                print(f"æ›´æ–°å­¦ä¹ ç‡ä¸º {args.resume_lr}")

            if args.resume_alpha:
                sac.alpha = args.resume_alpha
                print(f"æ›´æ–°alphaä¸º {args.resume_alpha}")
            
    # è¿è¡Œè®­ç»ƒå¾ªç¯
    run_training_loop(args, envs, sync_env, sac, single_gnn_embed, training_manager, num_joints, start_step)
    
    # æ¸…ç†èµ„æº
    cleanup_resources(sync_env, logger, model_manager, training_manager)

def run_training_loop(args, envs, sync_env, sac, single_gnn_embed, training_manager, num_joints, start_step=0):
    """è¿è¡Œè®­ç»ƒå¾ªç¯"""
    current_obs = envs.reset()
    print(f"åˆå§‹è§‚å¯Ÿ: {current_obs.shape}")
    
    # é‡ç½®æ¸²æŸ“ç¯å¢ƒ
    if sync_env:
        sync_env.reset()
        print("ğŸ”§ sync_env å·²é‡ç½®")
    
    current_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)
    episode_rewards = [0.0] * args.num_processes
    num_step = 120000
    total_steps = 0
    
    print(f"å¼€å§‹è®­ç»ƒ: warmup {sac.warmup_steps} æ­¥")
    print(f"æ€»è®­ç»ƒæ­¥æ•°: {num_step}, æ›´æ–°é¢‘ç‡: {args.update_frequency}")
    if start_step > 0:
        print(f"ä»æ­¥éª¤ {start_step} æ¢å¤è®­ç»ƒ")
    else:
        print(f"é¢„æœŸwarmupå®Œæˆæ­¥éª¤: {sac.warmup_steps}")

    training_completed = False
    early_termination_reason = ""

    try:
        for step in range(start_step, num_step):
            # è¿›åº¦æ˜¾ç¤º
            if step % 100 == 0:
                if step < sac.warmup_steps:
                    smart_print(f"Step {step}/{num_step}: Warmup phase ({step}/{sac.warmup_steps})")
                else:
                    smart_print(f"Step {step}/{num_step}: Training phase, Buffer size: {len(sac.memory)}")

            # è·å–åŠ¨ä½œ
            if step < sac.warmup_steps:
                action_batch = torch.from_numpy(np.array([
                    envs.action_space.sample() for _ in range(args.num_processes)
                ]))
            else:
                actions = []
                for proc_id in range(args.num_processes):
                    action = sac.get_action(
                        current_obs[proc_id],
                                            current_gnn_embeds[proc_id],
                                            num_joints=envs.action_space.shape[0],
                        deterministic=False
                    )
                    actions.append(action)
                action_batch = torch.stack(actions)

            # åŠ¨ä½œåˆ†æï¼ˆè°ƒè¯•ç”¨ï¼‰
            if step % 50 == 0 or step < 20:
                if hasattr(envs, 'envs') and len(envs.envs) > 0:
                    env_goal = getattr(envs.envs[0], 'goal_pos', 'NOT FOUND')
                    print(f"ğŸ¯ [è®­ç»ƒ] Step {step} - ç¯å¢ƒgoal_pos: {env_goal}")
                
                smart_print(f"\nğŸ¯ Step {step} Action Analysis:")
                action_numpy = action_batch.cpu().numpy() if hasattr(action_batch, 'cpu') else action_batch.numpy()
                
                for proc_id in range(min(args.num_processes, 2)):
                    action_values = action_numpy[proc_id]
                    action_str = ', '.join([f"{val:+6.2f}" for val in action_values])
                    smart_print(f"  Process {proc_id}: Actions = [{action_str}]")
                    smart_print(f"    Max action: {np.max(np.abs(action_values)):6.2f}, Mean abs: {np.mean(np.abs(action_values)):6.2f}")

            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done, infos = envs.step(action_batch)

            # æ¸²æŸ“å¤„ç†
            if sync_env:
                sync_action = action_batch[0].cpu().numpy() if hasattr(action_batch, 'cpu') else action_batch[0]
                sync_env.step(sync_action)
                sync_env.render()

            next_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)

            # å­˜å‚¨ç»éªŒ
            for proc_id in range(args.num_processes):
                sac.store_experience(
                        obs=current_obs[proc_id],
                        gnn_embeds=current_gnn_embeds[proc_id],
                        action=action_batch[proc_id],
                        reward=reward[proc_id],
                        next_obs=next_obs[proc_id],
                        next_gnn_embeds=next_gnn_embeds[proc_id],
                        done=done[proc_id],
                        num_joints=num_joints
                )
                episode_rewards[proc_id] += reward[proc_id].item()

            current_obs = next_obs.clone()
            current_gnn_embeds = next_gnn_embeds.clone()

            # å¤„ç†episodeç»“æŸ
            for proc_id in range(args.num_processes):
                is_done = done[proc_id].item() if torch.is_tensor(done[proc_id]) else bool(done[proc_id])
                if is_done:
                    should_end = training_manager.handle_episode_end(proc_id, step, episode_rewards, infos)
                    if should_end:
                        training_completed = True
                        early_termination_reason = f"è¿ç»­æˆåŠŸ{training_manager.consecutive_success_count}æ¬¡ï¼Œè¾¾åˆ°è®­ç»ƒç›®æ ‡"
                        break
                    
                    if hasattr(envs, 'reset_one'):
                        current_obs[proc_id] = envs.reset_one(proc_id)
                        current_gnn_embeds[proc_id] = single_gnn_embed
            
            # æ¨¡å‹æ›´æ–°
            if training_manager.should_update_model(step):
                training_manager.update_and_log(step, total_steps)
            
            # å®šæœŸä¿å­˜å’Œç»˜å›¾
            if step % 1000 == 0 and step > 0:
                training_manager.logger.plot_losses(recent_steps=2000, show=False)
                training_manager.model_manager.save_checkpoint(
                    sac, step,
                    best_success_rate=training_manager.best_success_rate,
                    best_min_distance=training_manager.best_min_distance,
                    consecutive_success_count=training_manager.consecutive_success_count
                )
            
            total_steps += args.num_processes
            
            if training_completed:
                print(f"ğŸ è®­ç»ƒæå‰ç»ˆæ­¢: {early_termination_reason}")
                break

    except Exception as e:
        print(f"ğŸ”´ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        training_manager.logger.save_logs()
        training_manager.logger.generate_report()
        raise e

def cleanup_resources(sync_env, logger, model_manager, training_manager):
    """æ¸…ç†èµ„æº"""
    if sync_env:
            sync_env.close()
            
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print(f"\n{'='*60}")
    print(f"ğŸ è®­ç»ƒå®Œæˆæ€»ç»“:")
    print(f"  æœ€ä½³æˆåŠŸç‡: {training_manager.best_success_rate:.3f}")
    print(f"  æœ€ä½³æœ€å°è·ç¦»: {training_manager.best_min_distance:.1f} pixels")
    print(f"  å½“å‰è¿ç»­æˆåŠŸæ¬¡æ•°: {training_manager.consecutive_success_count}")
    
    logger.generate_report()
    logger.plot_losses(show=False)
    print(f"ğŸ“Š å®Œæ•´è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {logger.experiment_dir}")
    print(f"{'='*60}")

# === æµ‹è¯•åŠŸèƒ½ ===
def test_trained_model(model_path, num_episodes=10, render=True):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½"""
    print(f"ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_path}")
    
    # ç¯å¢ƒé…ç½®
    env_params = {
        'num_links': DEFAULT_CONFIG['num_links'],
        'link_lengths': DEFAULT_CONFIG['link_lengths'],
        'render_mode': 'human' if render else None,
        'config_path': DEFAULT_CONFIG['config_path']
    }
    
    # åˆ›å»ºç¯å¢ƒ
    env = Reacher2DEnv(**env_params)
    num_joints = env.action_space.shape[0]
    
    # åˆ›å»ºGNNç¼–ç å™¨
    sys.path.append(os.path.join(base_dir, 'examples/2d_reacher/utils'))
    from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
    
    reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
    gnn_embed = reacher2d_encoder.get_gnn_embeds(
        num_links=num_joints, 
        link_lengths=env_params['link_lengths']
    )
    
    # åˆ›å»ºSACæ¨¡å‹
    attn_model = AttnModel(128, 130, 130, 4)
    sac = AttentionSACWithBuffer(attn_model, num_joints, 
                                buffer_capacity=10000, batch_size=64,
                                lr=1e-5, env_type='reacher2d')
    
    # åŠ è½½æ¨¡å‹
    try:
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
            
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
        model_data = torch.load(model_path, map_location='cpu')
        
        if 'actor_state_dict' in model_data:
            sac.actor.load_state_dict(model_data['actor_state_dict'], strict=False)
            print("âœ… Actor åŠ è½½æˆåŠŸ")
        
        print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"   è®­ç»ƒæ­¥æ•°: {model_data.get('step', 'N/A')}")
        print(f"   æ—¶é—´æˆ³: {model_data.get('timestamp', 'N/A')}")
        if 'success_rate' in model_data:
            print(f"   è®­ç»ƒæ—¶æˆåŠŸç‡: {model_data.get('success_rate', 'N/A'):.3f}")
        if 'min_distance' in model_data:
            print(f"   è®­ç»ƒæ—¶æœ€å°è·ç¦»: {model_data.get('min_distance', 'N/A'):.1f}")
            
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None
    
    # æµ‹è¯•å¤šä¸ªepisode
    success_count = 0
    total_rewards = []
    min_distances = []
    episode_lengths = []
    
    print(f"\nğŸ® å¼€å§‹æµ‹è¯• {num_episodes} ä¸ªepisodes...")
    print(f"ğŸ¯ ç›®æ ‡é˜ˆå€¼: {GOAL_THRESHOLD} pixels")
    
    for episode in range(num_episodes):
        obs = env.reset()
        episode_reward = 0
        step_count = 0
        max_steps = 2500
        min_distance_this_episode = float('inf')
        episode_success = False
        
        print(f"\nğŸ“ Episode {episode + 1}/{num_episodes}")
        
        while step_count < max_steps:
            # è·å–åŠ¨ä½œï¼ˆä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼‰
            action = sac.get_action(
                torch.from_numpy(obs).float(),
                gnn_embed.squeeze(0),
                num_joints=num_joints,
                deterministic=True
            )
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = env.step(action.cpu().numpy())
            episode_reward += reward
            step_count += 1
            
            # æ£€æŸ¥è·ç¦»
            end_pos = env._get_end_effector_position()
            goal_pos = env.goal_pos
            distance = np.linalg.norm(np.array(end_pos) - goal_pos)
            min_distance_this_episode = min(min_distance_this_episode, distance)
            
            # æ¸²æŸ“
            if render:
                env.render()
                time.sleep(0.02)
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
            if done:
                if not episode_success:
                    success_count += 1
                    episode_success = True
                    print(f"  ğŸ‰ ç›®æ ‡åˆ°è¾¾! è·ç¦»: {distance:.1f} pixels, æ­¥éª¤: {step_count}")
                break
        
        total_rewards.append(episode_reward)
        min_distances.append(min_distance_this_episode)
        episode_lengths.append(step_count)
        
        print(f"  ğŸ“Š Episode {episode + 1} ç»“æœ:")
        print(f"    å¥–åŠ±: {episode_reward:.2f}")
        print(f"    æœ€å°è·ç¦»: {min_distance_this_episode:.1f} pixels")
        print(f"    æ­¥éª¤æ•°: {step_count}")
        print(f"    æˆåŠŸ: {'âœ… æ˜¯' if episode_success else 'âŒ å¦'}")
    
    # æµ‹è¯•æ€»ç»“
    success_rate = success_count / num_episodes
    avg_reward = np.mean(total_rewards)
    avg_min_distance = np.mean(min_distances)
    avg_episode_length = np.mean(episode_lengths)
    
    print(f"\n{'='*60}")
    print(f"ğŸ† æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"  æµ‹è¯•Episodes: {num_episodes}")
    print(f"  æˆåŠŸæ¬¡æ•°: {success_count}")
    print(f"  æˆåŠŸç‡: {success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"  å¹³å‡æœ€å°è·ç¦»: {avg_min_distance:.1f} pixels")
    print(f"  å¹³å‡Episodeé•¿åº¦: {avg_episode_length:.1f} steps")
    print(f"  ç›®æ ‡é˜ˆå€¼: {GOAL_THRESHOLD:.1f} pixels")
    
    # æ€§èƒ½è¯„ä»·
    print(f"\nğŸ“‹ æ€§èƒ½è¯„ä»·:")
    if success_rate >= 0.8:
        print(f"  ğŸ† ä¼˜ç§€! æˆåŠŸç‡ >= 80%")
    elif success_rate >= 0.5:
        print(f"  ğŸ‘ è‰¯å¥½! æˆåŠŸç‡ >= 50%")
    elif success_rate >= 0.2:
        print(f"  âš ï¸  ä¸€èˆ¬! æˆåŠŸç‡ >= 20%")
    else:
        print(f"  âŒ éœ€è¦æ”¹è¿›! æˆåŠŸç‡ < 20%")
        
    if avg_min_distance <= GOAL_THRESHOLD:
        print(f"  âœ… å¹³å‡æœ€å°è·ç¦»è¾¾åˆ°ç›®æ ‡é˜ˆå€¼")
    else:
        print(f"  âš ï¸  å¹³å‡æœ€å°è·ç¦»è¶…å‡ºç›®æ ‡é˜ˆå€¼ {avg_min_distance - GOAL_THRESHOLD:.1f} pixels")
    
    print(f"{'='*60}")
    
    env.close()
    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward, 
        'avg_min_distance': avg_min_distance,
        'avg_episode_length': avg_episode_length,
        'success_count': success_count,
        'total_episodes': num_episodes
    }

def find_latest_model():
    """æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶"""
    base_path = "./trained_models/reacher2d/enhanced_test"
    
    if not os.path.exists(base_path):
        print(f"âŒ è®­ç»ƒæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {base_path}")
        return None
    
    model_candidates = []
    
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.pth') and ('final_model' in file or 'checkpoint' in file):
                full_path = os.path.join(root, file)
                mtime = os.path.getmtime(full_path)
                model_candidates.append((full_path, mtime))
    
    if not model_candidates:
        print(f"âŒ åœ¨ {base_path} ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        return None
    
    model_candidates.sort(key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ” æ‰¾åˆ° {len(model_candidates)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
    for i, (path, mtime) in enumerate(model_candidates[:5]):
        import datetime
        time_str = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        print(f"  {i+1}. {os.path.basename(path)} ({time_str})")
    
    latest_model = model_candidates[0][0]
    print(f"âœ… é€‰æ‹©æœ€æ–°æ¨¡å‹: {latest_model}")
    return latest_model

# === ä¸»ç¨‹åºå…¥å£ ===
if __name__ == "__main__":
    torch.set_default_dtype(torch.float64)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == '--test':
        print("ğŸ§ª è¿›å…¥æµ‹è¯•æ¨¡å¼")
        
        model_path = None
        num_episodes = 5
        render = True
        
        for i, arg in enumerate(sys.argv):
            if arg == '--model-path' and i + 1 < len(sys.argv):
                model_path = sys.argv[i + 1]
            elif arg == '--episodes' and i + 1 < len(sys.argv):
                num_episodes = int(sys.argv[i + 1])
            elif arg == '--no-render':
                render = False
            elif arg == '--latest':
                model_path = 'latest'
        
        if model_path is None or model_path == 'latest':
            print("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹...")
            model_path = find_latest_model()
        
        if model_path:
            print(f"ğŸ¯ æµ‹è¯•å‚æ•°: episodes={num_episodes}, render={render}")
            result = test_trained_model(model_path, num_episodes, render)
            
            if result:
                print(f"\nğŸ¯ å¿«é€Ÿç»“è®º:")
                if result['success_rate'] >= 0.8:
                    print(f"  âœ… æ¨¡å‹è¡¨ç°ä¼˜ç§€! ç»§ç»­å½“å‰è®­ç»ƒç­–ç•¥")
                elif result['success_rate'] >= 0.3:
                    print(f"  âš ï¸  æ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´å‚æ•°")
                else:
                    print(f"  âŒ æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦é‡æ–°å®¡è§†å¥–åŠ±å‡½æ•°æˆ–ç½‘ç»œç»“æ„")
        else:
            print("âŒ æœªæ‰¾åˆ°å¯æµ‹è¯•çš„æ¨¡å‹")
        
        exit(0)
    
    # è®­ç»ƒæ¨¡å¼ - å‚æ•°è§£æ
    parser = create_training_parser()
    args = parser.parse_args()

    args.cuda = not args.no_cuda and torch.cuda.is_available()
    args.save_dir = os.path.join(args.save_dir, get_time_stamp())
    os.makedirs(args.save_dir, exist_ok=True)
    
    # ä¿å­˜å‚æ•°
    with open(os.path.join(args.save_dir, 'args.txt'), 'w') as f:
        f.write(str(sys.argv))

    main(args)