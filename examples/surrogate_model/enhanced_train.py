#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆè®­ç»ƒè„šæœ¬ - PPOç‰ˆæœ¬
ä»£ç ç»“æ„æ¸…æ™°ï¼ŒåŠŸèƒ½æ¨¡å—åŒ–ï¼Œä¿æŒæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
"""

import sys
import os
import time
import numpy as np
import torch
import argparse
import logging
from collections import deque

# === è·¯å¾„è®¾ç½® ===
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../')
sys.path.extend([
    base_dir,
    os.path.join(base_dir, 'examples/2d_reacher/envs'),
    os.path.join(base_dir, 'examples/surrogate_model/gnn_encoder'),
    os.path.join(base_dir, 'examples/rl/train'),
    os.path.join(base_dir, 'examples/rl/common'),
    os.path.join(base_dir, 'examples/rl/environments'),
    os.path.join(base_dir, 'examples/rl')
])

# === æ ¸å¿ƒå¯¼å…¥ ===
import gym
gym.logger.set_level(40)

if not hasattr(np, 'bool'):
    np.bool = bool

from training_logger import TrainingLogger, RealTimeMonitor
from gnn_encoder import GNN_Encoder
from attn_model.attn_model import AttnModel
# from sac.ppo_model import AttentionPPOWithBuffer
from sac.universal_ppo_model import UniversalPPOWithBuffer
from env_config.env_wrapper import make_reacher2d_vec_envs
from reacher2d_env import Reacher2DEnv

# RLç›¸å…³å¯¼å…¥
import environments
from common import *
from attn_dataset.sim_data_handler import DataHandler

# === é…ç½®å¸¸é‡ ===
SILENT_MODE = True
GOAL_THRESHOLD = 20.0
DEFAULT_CONFIG = {
    'num_links': 3,
    'link_lengths': [90,90,90],
    'config_path': None
}

# === è‡ªå®šä¹‰å‚æ•°è§£æå™¨ ===
def create_training_parser():
    """åˆ›å»ºè®­ç»ƒä¸“ç”¨çš„å‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='Enhanced PPO Training for Reacher2D')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--env-name', default='reacher2d', help='ç¯å¢ƒåç§°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--num-processes', type=int, default=1, help='å¹¶è¡Œè¿›ç¨‹æ•°')
    parser.add_argument('--lr', type=float, default=2e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--gamma', type=float, default=0.99, help='æŠ˜æ‰£å› å­')
    parser.add_argument('--save-dir', default='./trained_models/reacher2d/enhanced_test/', help='ä¿å­˜ç›®å½•')
    
    # æ¸²æŸ“æ§åˆ¶å‚æ•°
    parser.add_argument('--render', action='store_true', default=False, help='æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–çª—å£')
    parser.add_argument('--no-render', action='store_true', default=False, help='å¼ºåˆ¶ç¦ç”¨å¯è§†åŒ–çª—å£')
    
    # PPOç‰¹å®šå‚æ•°
    parser.add_argument('--clip-epsilon', type=float, default=0.1, help='PPOè£å‰ªå‚æ•°')
    parser.add_argument('--entropy-coef', type=float, default=0.01, help='ç†µç³»æ•°')
    parser.add_argument('--value-coef', type=float, default=0.25, help='å€¼å‡½æ•°æŸå¤±ç³»æ•°')
    parser.add_argument('--ppo-epochs', type=int, default=4, help='PPOæ›´æ–°è½®æ•°')
    parser.add_argument('--batch-size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--buffer-size', type=int, default=2048, help='ç¼“å†²åŒºå®¹é‡')
    
    # æ¢å¤è®­ç»ƒå‚æ•°
    parser.add_argument('--resume-checkpoint', type=str, default=None, help='æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--resume-lr', type=float, default=None, help='æ¢å¤æ—¶çš„å­¦ä¹ ç‡')
    
    # ğŸ”§ MAP-Elitesæœºå™¨äººé…ç½®å‚æ•°
    parser.add_argument('--num-joints', type=int, default=3, help='æœºå™¨äººå…³èŠ‚æ•°é‡')
    parser.add_argument('--link-lengths', nargs='+', type=float, default=[90.0, 90.0, 90.0], help='æœºå™¨äººé“¾èŠ‚é•¿åº¦')
    parser.add_argument('--total-steps', type=int, default=10000, help='æ€»è®­ç»ƒæ­¥æ•°')
    
    # å…¼å®¹æ€§å‚æ•°ï¼ˆç”¨äºå…¶ä»–ç¯å¢ƒï¼‰
    parser.add_argument('--grammar-file', type=str, default='/home/xli149/Documents/repos/RoboGrammar/data/designs/grammar_jan21.dot', help='è¯­æ³•æ–‡ä»¶')
    parser.add_argument('--rule-sequence', nargs='+', default=['0'], help='è§„åˆ™åºåˆ—')
    parser.add_argument('--no-cuda', action='store_true', default=False, help='ç¦ç”¨CUDA')
    
    return parser

# === å·¥å…·å‡½æ•° ===
def smart_print(*args, **kwargs):
    """æ™ºèƒ½æ‰“å°å‡½æ•°"""
    if not SILENT_MODE:
        print(*args, **kwargs)

def get_time_stamp():
    """è·å–æ—¶é—´æˆ³"""
    return time.strftime('%m-%d-%Y-%H-%M-%S')

# === æ¨¡å‹ç®¡ç†å™¨ ===
class ModelManager:
    """æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ç®¡ç†å™¨"""
    
    def __init__(self, save_dir):
        self.save_dir = save_dir
        self.best_models_dir = os.path.join(save_dir, 'best_models')
        os.makedirs(self.best_models_dir, exist_ok=True)
    
    def save_best_model(self, ppo, success_rate, min_distance, step):
        """ä¿å­˜æœ€ä½³PPOæ¨¡å‹"""
        try:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            
            model_data = {
                'step': step,
                'success_rate': success_rate,
                'min_distance': min_distance,
                'timestamp': timestamp,
                'actor_state_dict': ppo.actor.state_dict(),
                'critic_state_dict': ppo.critic.state_dict(),
                'actor_optimizer_state_dict': ppo.actor_optimizer.state_dict(),
                'critic_optimizer_state_dict': ppo.critic_optimizer.state_dict(),
                'update_count': ppo.update_count,
                'model_type': 'PPO'
            }
            
            model_file = os.path.join(self.best_models_dir, f'best_ppo_model_step_{step}_{timestamp}.pth')
            torch.save(model_data, model_file)
            
            latest_file = os.path.join(self.best_models_dir, 'latest_best_model.pth')
            torch.save(model_data, latest_file)
            
            print(f"ğŸ† ä¿å­˜æœ€ä½³PPOæ¨¡å‹: æˆåŠŸç‡ {success_rate:.3f}, è·ç¦» {min_distance:.1f}, æ­¥éª¤ {step}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜PPOæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def save_checkpoint(self, ppo, step, **kwargs):
        """æ™ºèƒ½ä¿å­˜æ£€æŸ¥ç‚¹ - åªä¿å­˜æœ€ä½³æ¨¡å‹"""
        try:
            current_best_distance = kwargs.get('current_best_distance', float('inf'))
            best_min_distance = kwargs.get('best_min_distance', float('inf'))
            
            # åªæœ‰åœ¨æ€§èƒ½æ”¹å–„æ—¶æ‰ä¿å­˜
            if current_best_distance < best_min_distance:
                success_rate = kwargs.get('best_success_rate', 0.0)
                print(f"ğŸ† å‘ç°æ›´å¥½æ€§èƒ½ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹: {current_best_distance:.1f}px")
                return self.save_best_model(ppo, success_rate, current_best_distance, step)
            else:
                print(f"â­ï¸  æ€§èƒ½æœªæ”¹å–„ ({current_best_distance:.1f}px >= {best_min_distance:.1f}px)ï¼Œè·³è¿‡ä¿å­˜")
                return False
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False

    def save_final_model(self, ppo, step, **kwargs):
        """ä¿å­˜æœ€ç»ˆPPOæ¨¡å‹"""
        try:
            final_model_data = {
                'step': step,
                'training_completed': True,
                'actor_state_dict': ppo.actor.state_dict(),
                'critic_state_dict': ppo.critic.state_dict(),
                'actor_optimizer_state_dict': ppo.actor_optimizer.state_dict(),
                'critic_optimizer_state_dict': ppo.critic_optimizer.state_dict(),
                'update_count': ppo.update_count,
                'model_type': 'PPO',
                **kwargs
            }
            
            final_path = os.path.join(self.best_models_dir, f'final_ppo_model_step_{step}.pth')
            torch.save(final_model_data, final_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆPPOæ¨¡å‹: {final_path}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æœ€ç»ˆPPOæ¨¡å‹å¤±è´¥: {e}")
            return False

    def load_checkpoint(self, ppo, checkpoint_path, device='cpu'):
        """åŠ è½½PPOæ£€æŸ¥ç‚¹"""
        try:
            if not os.path.exists(checkpoint_path):
                print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
                return 0
            
            print(f"ğŸ”„ Loading PPO checkpoint from: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=device)
            
            # åŠ è½½ç½‘ç»œçŠ¶æ€
            if 'actor_state_dict' in checkpoint:
                ppo.actor.load_state_dict(checkpoint['actor_state_dict'], strict=False)
                print("âœ… PPO Actor loaded")
            
            if 'critic_state_dict' in checkpoint:
                ppo.critic.load_state_dict(checkpoint['critic_state_dict'], strict=False)
                print("âœ… PPO Critic loaded")
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            if 'actor_optimizer_state_dict' in checkpoint:
                ppo.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
                print("âœ… Actor optimizer loaded")
            
            if 'critic_optimizer_state_dict' in checkpoint:
                ppo.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
                print("âœ… Critic optimizer loaded")
            
            # åŠ è½½æ›´æ–°è®¡æ•°
            if 'update_count' in checkpoint:
                ppo.update_count = checkpoint['update_count']
                print(f"âœ… Update count loaded: {ppo.update_count}")
            
            start_step = checkpoint.get('step', 0)
            print(f"âœ… PPO Checkpoint loaded successfully! Starting step: {start_step}")
            
            return start_step
            
        except Exception as e:
            print(f"âŒ Failed to load PPO checkpoint: {e}")
            import traceback
            traceback.print_exc()
            return 0

# === ç¯å¢ƒè®¾ç½®ç®¡ç†å™¨ ===
class EnvironmentSetup:
    """ç¯å¢ƒè®¾ç½®ç®¡ç†å™¨"""
    
    @staticmethod
    def create_reacher2d_env(args):
        """åˆ›å»ºReacher2Dç¯å¢ƒ"""
        # è·å–ç¯å¢ƒé…ç½®
        if hasattr(args, 'num_joints') and hasattr(args, 'link_lengths'):
            num_links = args.num_joints
            link_lengths = args.link_lengths
            print(f"ğŸ¤– ä½¿ç”¨MAP-Elitesé…ç½®: {num_links}å…³èŠ‚, é•¿åº¦={link_lengths}")
        else:
            num_links = DEFAULT_CONFIG['num_links']
            link_lengths = DEFAULT_CONFIG['link_lengths']
            print(f"ğŸ¤– ä½¿ç”¨é»˜è®¤é…ç½®: {num_links}å…³èŠ‚, é•¿åº¦={link_lengths}")

        should_render = False
        if hasattr(args, 'render') and args.render:
            should_render = True
        elif hasattr(args, 'no_render') and args.no_render:
            should_render = False
        else:
            should_render = False
            print("ğŸ¨ æ¸²æŸ“è®¾ç½®: é»˜è®¤ç¦ç”¨ (æ— æ¸²æŸ“å‚æ•°)")

        env_params = {
            'num_links': num_links,
            'link_lengths': link_lengths,
            'render_mode': 'human' if args.num_processes == 1 else None,
            'config_path': DEFAULT_CONFIG['config_path']
        }
        
        print(f"ç¯å¢ƒå‚æ•°: {env_params}")
        print(f"ğŸ¨ æ¸²æŸ“è®¾ç½®: {'å¯ç”¨' if should_render else 'ç¦ç”¨'}")
        
        # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
        envs = make_reacher2d_vec_envs(
            env_params=env_params,
            seed=args.seed,
            num_processes=args.num_processes,
            gamma=args.gamma,
            log_dir=None,
            device=torch.device('cpu'),
            allow_early_resets=False,
        )
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åˆ›å»ºæ¸²æŸ“ç¯å¢ƒ
        if should_render:
            render_env_params = env_params.copy()
            render_env_params['render_mode'] = 'human'
            sync_env = Reacher2DEnv(**render_env_params)
            print(f"âœ… è®­ç»ƒç¯å¢ƒå·²åˆ›å»ºï¼ˆè¿›ç¨‹æ•°: {args.num_processes}ï¼Œå¸¦æ¸²æŸ“ï¼‰")
        else:
            sync_env = None
            print(f"âœ… è®­ç»ƒç¯å¢ƒå·²åˆ›å»ºï¼ˆè¿›ç¨‹æ•°: {args.num_processes}ï¼Œæ— æ¸²æŸ“ï¼‰")
            
        return envs, sync_env, env_params

# === è®­ç»ƒç®¡ç†å™¨ ===
class TrainingManager:
    """è®­ç»ƒè¿‡ç¨‹ç®¡ç†å™¨ - PPOç‰ˆæœ¬"""
    
    def __init__(self, args, ppo, logger, model_manager):
        self.args = args
        self.ppo = ppo
        self.logger = logger
        self.model_manager = model_manager
        
        # è®­ç»ƒçŠ¶æ€
        self.best_success_rate = 0.0
        self.best_min_distance = float('inf')
        self.consecutive_success_count = 0
        self.min_consecutive_successes = 2

        # Episodesæ§åˆ¶ - 2ä¸ªepisodes Ã— 120kæ­¥
        self.current_episodes = 0
        self.max_episodes = 2
        self.steps_per_episode = 120000
        self.current_episode_steps = 0
        self.total_training_steps = 0
        self.episode_results = []
        self.current_episode_start_step = 0
        self.current_episode_start_time = time.time()
        
        # è¿½è¸ªæ¯ä¸ªepisodeçš„æœ€ä½³è¡¨ç°
        self.current_episode_best_distance = float('inf')
        self.current_episode_best_reward = float('-inf')
        self.current_episode_min_distance_step = 0
        
        print(f"ğŸ¯ PPOè®­ç»ƒé…ç½®: 2ä¸ªepisodes Ã— 120,000æ­¥/episode = æ€»è®¡240,000æ­¥")

    def update_episode_tracking(self, episode_step, infos, episode_rewards):
        """åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æ›´æ–°episodeè¿½è¸ª"""
        for proc_id in range(len(infos)):
            if len(infos) > proc_id and isinstance(infos[proc_id], dict):
                info = infos[proc_id]
                
                # æå–å½“å‰è·ç¦»
                current_distance = float('inf')
                if 'goal' in info:
                    current_distance = info['goal'].get('distance_to_goal', float('inf'))
                elif 'distance' in info:
                    current_distance = info['distance']
                
                # æ›´æ–°æœ€ä½³è·ç¦»
                if current_distance < self.current_episode_best_distance:
                    self.current_episode_best_distance = current_distance
                    self.current_episode_best_reward = episode_rewards[proc_id]
                    self.current_episode_min_distance_step = episode_step

    def _classify_episode_result(self, goal_reached, distance, steps, reward):
        """åˆ†ç±»episodeç»“æœ"""
        if goal_reached:
            if steps < 200:
                return {
                    'type': 'PERFECT_SUCCESS',
                    'score': 1.0,
                    'description': 'å¿«é€Ÿç²¾ç¡®åˆ°è¾¾'
                }
            elif steps < 400:
                return {
                    'type': 'GOOD_SUCCESS', 
                    'score': 0.9,
                    'description': 'æ­£å¸¸é€Ÿåº¦åˆ°è¾¾'
                }
            else:
                return {
                    'type': 'SLOW_SUCCESS',
                    'score': 0.7,
                    'description': 'ç¼“æ…¢ä½†æˆåŠŸåˆ°è¾¾'
                }
        else:
            if distance < 50:
                return {
                    'type': 'NEAR_SUCCESS',
                    'score': 0.5,
                    'description': 'æ¥è¿‘æˆåŠŸ'
                }
            elif distance < 100:
                return {
                    'type': 'TIMEOUT_CLOSE',
                    'score': 0.3,
                    'description': 'è¶…æ—¶ä½†è¾ƒæ¥è¿‘'
                }
            elif reward > -100:
                return {
                    'type': 'TIMEOUT_MEDIUM',
                    'score': 0.2,
                    'description': 'è¶…æ—¶ä¸­ç­‰è¡¨ç°'
                }
            else:
                return {
                    'type': 'COMPLETE_FAILURE',
                    'score': 0.0,
                    'description': 'å®Œå…¨å¤±è´¥'
                }

    def _check_episode_stopping_conditions(self, step):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ"""
        # ğŸ”§ ä¿®å¤ï¼šå¢åŠ episodesæ•°é‡ï¼Œé€‚åˆMAP-Elitesè®­ç»ƒ
        if self.current_episodes >= 20:  # ä»2å¢åŠ åˆ°20ä¸ªepisodes
            print(f"ğŸ å®Œæˆ{self.current_episodes}ä¸ªepisodesï¼Œè®­ç»ƒç»“æŸ")
            return True
        
        # ğŸ”§ ä¿®å¤ï¼šå‡å°‘å•ä¸ªepisodeæ­¥æ•°é™åˆ¶ï¼Œé€‚åˆå¿«é€Ÿè¯„ä¼°
        episode_steps = step - self.current_episode_start_step
        if episode_steps >= 500:  # ä»120,000å‡å°‘åˆ°500æ­¥
            print(f"â° å½“å‰episodeè¾¾åˆ°500æ­¥é™åˆ¶")
            return False  # ä¸æ˜¯æ•´ä½“ç»“æŸï¼Œåªæ˜¯å½“å‰episodeç»“æŸ
        
        return False

    def _generate_final_fitness_report(self):
        """ç”Ÿæˆæœ€ç»ˆfitnessæŠ¥å‘Š"""
        if len(self.episode_results) == 0:
            print("âš ï¸ æ²¡æœ‰episodeç»“æœæ•°æ®")
            return
        
        print("\n" + "="*50)
        print("ğŸ¯ æœ€ç»ˆPPOè®­ç»ƒç»“æœæŠ¥å‘Š")
        print("="*50)
        
        # è®¡ç®—ç»¼åˆæŒ‡æ ‡ï¼ˆåŸºäºæœ€ä½³è·ç¦»ï¼‰
        success_count = sum(1 for ep in self.episode_results if ep['success'])
        success_rate = success_count / len(self.episode_results)
        avg_best_distance = sum(ep['best_distance'] for ep in self.episode_results) / len(self.episode_results)
        avg_end_distance = sum(ep['end_distance'] for ep in self.episode_results) / len(self.episode_results)
        avg_steps = sum(ep['steps'] for ep in self.episode_results) / len(self.episode_results)
        avg_score = sum(ep['score'] for ep in self.episode_results) / len(self.episode_results)
        
        print(f"ğŸ“Š Episodeså®Œæˆ: {len(self.episode_results)}/2")
        print(f"ğŸ¯ æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{len(self.episode_results)})")
        print(f"ğŸ† å¹³å‡æœ€ä½³è·ç¦»: {avg_best_distance:.1f}px")
        print(f"ğŸ“ å¹³å‡ç»“æŸè·ç¦»: {avg_end_distance:.1f}px")
        print(f"â±ï¸  å¹³å‡æ­¥æ•°: {avg_steps:.0f}")
        print(f"â­ å¹³å‡å¾—åˆ†: {avg_score:.2f}")
        
        # è¯¦ç»†episodeä¿¡æ¯
        print("\nğŸ“‹ è¯¦ç»†Episodeç»“æœ:")
        for i, ep in enumerate(self.episode_results, 1):
            status = "âœ…" if ep['success'] else "âŒ"
            print(f"   Episode {i}: {status} {ep['type']} - "
                  f"æœ€ä½³è·ç¦»:{ep['best_distance']:.1f}px@{ep['best_distance_step']}æ­¥, "
                  f"ç»“æŸè·ç¦»:{ep['end_distance']:.1f}px, "
                  f"å¾—åˆ†:{ep['score']:.2f}")
        
        print("="*50)

    def handle_episode_end(self, proc_id, step, episode_rewards, infos):
        """å¤„ç†episodeç»“æŸé€»è¾‘ - æ”¯æŒç»´æŒæ£€æŸ¥"""
        if len(infos) <= proc_id or not isinstance(infos[proc_id], dict):
            episode_rewards[proc_id] = 0.0
            return False
        
        info = infos[proc_id]
        
        # è®¡ç®—episodeè¯¦ç»†ä¿¡æ¯
        episode_steps = step - self.current_episode_start_step
        episode_duration = time.time() - self.current_episode_start_time if hasattr(self, 'current_episode_start_time') else 0
        episode_reward = episode_rewards[proc_id]
        
        # ä½¿ç”¨æœ€ä½³è·ç¦»è€Œä¸æ˜¯ç»“æŸæ—¶è·ç¦»
        best_distance = self.current_episode_best_distance
        best_reward = self.current_episode_best_reward
        goal_reached = best_distance < 20.0
        
        # æ£€æŸ¥ç»´æŒå®Œæˆæƒ…å†µ
        maintain_completed = False
        maintain_counter = 0
        maintain_target = 500
        
        # å°è¯•ä»ç¯å¢ƒè·å–ç»´æŒä¿¡æ¯
        try:
            maintain_completed = info.get('maintain_completed', False)
            maintain_counter = info.get('maintain_counter', 0)
        except:
            pass
        
        # è·å–ç»“æŸæ—¶çš„è·ç¦»ç”¨äºå¯¹æ¯”
        end_distance = float('inf')
        if 'goal' in info:
            end_distance = info['goal'].get('distance_to_goal', float('inf'))
        elif 'distance' in info:
            end_distance = info['distance']
        
        # åˆ†ç±»episodeç»“æœï¼ˆåŸºäºç»´æŒå®Œæˆæƒ…å†µï¼‰
        if maintain_completed:
            episode_type = {
                'type': 'MAINTAIN_SUCCESS',
                'score': 1.0,
                'description': f'æˆåŠŸç»´æŒ{maintain_counter}æ­¥'
            }
            goal_reached = True  # ç»´æŒå®Œæˆç®—ä½œæˆåŠŸ
        else:
            episode_type = self._classify_episode_result(goal_reached, best_distance, episode_steps, best_reward)
        
        # å­˜å‚¨episodeç»“æœ
        episode_result = {
            'episode_num': self.current_episodes + 1,
            'type': episode_type['type'],
            'success': maintain_completed or goal_reached,
            'maintain_completed': maintain_completed,
            'maintain_counter': maintain_counter,
            'best_distance': best_distance,
            'end_distance': end_distance,
            'best_distance_step': self.current_episode_min_distance_step,
            'steps': episode_steps,
            'duration': episode_duration,
            'reward': episode_reward,
            'best_reward': best_reward,
            'score': episode_type['score'],
            'description': episode_type['description']
        }
        
        self.episode_results.append(episode_result)
        self.current_episodes += 1
        
        # æ‰“å°episodeç»“æœï¼ˆæ˜¾ç¤ºç»´æŒä¿¡æ¯ï¼‰
        print(f"ğŸ“Š Episode {self.current_episodes}/2 å®Œæˆ:")
        print(f"   ç±»å‹: {episode_type['type']} ({episode_type['description']})")
        print(f"   æˆåŠŸ: {'âœ…' if episode_result['success'] else 'âŒ'}")
        if maintain_completed:
            print(f"   ğŸŠ ç»´æŒå®Œæˆ: {maintain_counter}/{maintain_target} æ­¥")
        print(f"   æœ€ä½³è·ç¦»: {best_distance:.1f}px (æ­¥æ•°: {self.current_episode_min_distance_step})")
        print(f"   ç»“æŸè·ç¦»: {end_distance:.1f}px")
        print(f"   æ€»æ­¥æ•°: {episode_steps}")
        print(f"   æœ€ç»ˆå¥–åŠ±: {episode_reward:.2f}")
        print(f"   å¾—åˆ†: {episode_type['score']:.2f}")
        
        # é‡ç½®episodeè¿½è¸ª
        self.current_episode_best_distance = float('inf')
        self.current_episode_best_reward = float('-inf')
        self.current_episode_min_distance_step = 0
        self.current_episode_start_step = step
        self.current_episode_start_time = time.time()
        
        # ğŸ”§ ä¿®å¤ï¼šæ›´æ–°å…¨å±€æœ€ä½³æˆåŠŸç‡å’Œè·ç¦»
        if episode_result['success']:
            self.consecutive_success_count += 1
            # æ›´æ–°å…¨å±€æœ€ä½³è·ç¦»
            if best_distance < self.best_min_distance:
                self.best_min_distance = best_distance
            
            # è®¡ç®—å½“å‰æˆåŠŸç‡
            success_count = sum(1 for ep in self.episode_results if ep['success'])
            current_success_rate = success_count / len(self.episode_results)
            
            # æ›´æ–°å…¨å±€æœ€ä½³æˆåŠŸç‡
            if current_success_rate > self.best_success_rate:
                self.best_success_rate = current_success_rate
                print(f"ğŸ¯ æ›´æ–°æœ€ä½³æˆåŠŸç‡: {self.best_success_rate:.1%}")
        else:
            self.consecutive_success_count = 0
        
        # æ£€æŸ¥åœæ­¢æ¡ä»¶
        should_stop = self._check_episode_stopping_conditions(step)
        if should_stop:
            self._generate_final_fitness_report()
        
        episode_rewards[proc_id] = 0.0
        return should_stop
    
    def should_update_model(self, step):
        """PPOæ¯ä¸ªepisodeç»“æŸåæ›´æ–°"""
        # PPOä¸éœ€è¦warmupï¼Œbufferæ»¡äº†å°±å¯ä»¥æ›´æ–°
        # return len(self.ppo.buffer.joint_q) >= self.ppo.batch_size
        return len(self.ppo.buffer.experiences) >= self.ppo.batch_size
    # def update_and_log(self, step, next_obs=None, next_gnn_embeds=None, num_joints=12):
    #     """PPOæ›´æ–°å¹¶è®°å½•"""
    #     metrics = self.ppo.update(next_obs, next_gnn_embeds, num_joints)
        
    #     if metrics:
    #         enhanced_metrics = metrics.copy()
    #         enhanced_metrics.update({
    #             'step': step,
    #             'buffer_size': len(self.ppo.buffer.joint_q),
    #             'learning_rate': self.ppo.actor_optimizer.param_groups[0]['lr'],
    #             'update_count': metrics['update_count']
    #         })
            
    #         self.logger.log_step(step, enhanced_metrics, episode=step//100)
            
    #         if step % 100 == 0:
    #             print(f"Step {step}: "
    #                   f"Actor Loss: {metrics['actor_loss']:.4f}, "
    #                   f"Critic Loss: {metrics['critic_loss']:.4f}, "
    #                   f"Entropy: {metrics['entropy']:.4f}")


    def update_and_log(self, step, next_obs=None, next_gnn_embeds=None, num_joints=12):
        """PPOæ›´æ–°å¹¶è®°å½• - å¢å¼ºç‰ˆlossæ‰“å°"""
        metrics = self.ppo.update(next_obs, next_gnn_embeds, num_joints)
        
        if metrics:
            enhanced_metrics = metrics.copy()
            enhanced_metrics.update({
                'step': step,
                # 'buffer_size': len(self.ppo.buffer.joint_q),
                'buffer_size': len(self.ppo.buffer.experiences),
                'learning_rate': self.ppo.actor_optimizer.param_groups[0]['lr'],
                'update_count': metrics['update_count']
            })
            
            self.logger.log_step(step, enhanced_metrics, episode=step//100)
            
            # ğŸ”§ å¢å¼ºç‰ˆlossæ‰“å° - æ¯æ¬¡æ›´æ–°éƒ½æ‰“å°
            print(f"\nğŸ”¥ PPOç½‘ç»œLossæ›´æ–° [Step {step}]:")
            print(f"   ğŸ“Š Actor Loss: {metrics['actor_loss']:.6f}")
            print(f"   ğŸ“Š Critic Loss: {metrics['critic_loss']:.6f}")
            print(f"   ğŸ“Š æ€»Loss: {metrics['actor_loss'] + metrics['critic_loss']:.6f}")
            print(f"   ğŸ­ Entropy: {metrics['entropy']:.6f}")
            print(f"   ğŸ“ˆ å­¦ä¹ ç‡: {self.ppo.actor_optimizer.param_groups[0]['lr']:.2e}")
            print(f"   ğŸ”„ æ›´æ–°æ¬¡æ•°: {metrics['update_count']}")
            # print(f"   ğŸ’¾ Bufferå¤§å°: {len(self.ppo.buffer.joint_q)}")
            print(f"   ğŸ’¾ Bufferå¤§å°: {len(self.ppo.buffer.experiences)}")
            
            # ğŸ”§ æ·»åŠ æ¢¯åº¦èŒƒæ•°ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if 'actor_grad_norm' in metrics:
                print(f"   âš¡ Actoræ¢¯åº¦èŒƒæ•°: {metrics['actor_grad_norm']:.6f}")
            if 'critic_grad_norm' in metrics:
                print(f"   âš¡ Criticæ¢¯åº¦èŒƒæ•°: {metrics['critic_grad_norm']:.6f}")
            
            # ğŸ”§ æ·»åŠ PPOç‰¹å®šæŒ‡æ ‡
            if 'policy_ratio' in metrics:
                print(f"   ğŸ¯ ç­–ç•¥æ¯”ç‡: {metrics['policy_ratio']:.4f}")
            if 'clip_fraction' in metrics:
                print(f"   âœ‚ï¸  è£å‰ªæ¯”ä¾‹: {metrics['clip_fraction']:.4f}")
            if 'kl_divergence' in metrics:
                print(f"   ğŸ“ KLæ•£åº¦: {metrics['kl_divergence']:.6f}")
            if 'explained_variance' in metrics:
                print(f"   ğŸ“Š è§£é‡Šæ–¹å·®: {metrics['explained_variance']:.4f}")
            
            # ğŸ”§ Lossè¶‹åŠ¿åˆ†æ
            if hasattr(self, 'loss_history'):
                if len(self.loss_history) >= 3:
                    recent_losses = self.loss_history[-3:]
                    if recent_losses[-1] < recent_losses[0]:
                        trend = "ğŸ“‰ ä¸‹é™"
                    elif recent_losses[-1] > recent_losses[0]:
                        trend = "ğŸ“ˆ ä¸Šå‡"
                    else:
                        trend = "â¡ï¸  å¹³ç¨³"
                    print(f"   ğŸ“ˆ Lossè¶‹åŠ¿: {trend}")
            else:
                self.loss_history = []
            
            # è®°å½•losså†å²
            total_loss = metrics['actor_loss'] + metrics['critic_loss']
            self.loss_history.append(total_loss)
            if len(self.loss_history) > 10:  # åªä¿ç•™æœ€è¿‘10æ¬¡
                self.loss_history = self.loss_history[-10:]
            
            print(f"   {'='*50}")

def main(args):
    """ä¸»è®­ç»ƒå‡½æ•° - PPOç‰ˆæœ¬"""
    print("ğŸš€ å¼€å§‹PPOè®­ç»ƒ...")
    
    # è®¾ç½®éšæœºç§å­å’Œè®¾å¤‡
    torch.manual_seed(args.seed)
    torch.set_num_threads(1)
    device = torch.device('cpu')
    os.makedirs(args.save_dir, exist_ok=True)
    
    # åˆ›å»ºè®­ç»ƒæ—¥å¿—
    training_log_path = os.path.join(args.save_dir, 'logs.txt')
    with open(training_log_path, 'w') as f:
        pass
    
    # åˆ›å»ºç¯å¢ƒ
    if args.env_name == 'reacher2d':
        print("ä½¿ç”¨ reacher2d ç¯å¢ƒ")
        env_setup = EnvironmentSetup()
        envs, sync_env, env_params = env_setup.create_reacher2d_env(args)
        args.env_type = 'reacher2d'
    else:
        print(f"ä½¿ç”¨ bullet ç¯å¢ƒ: {args.env_name}")
        from a2c_ppo_acktr.envs import make_vec_envs
        envs = make_vec_envs(args.env_name, args.seed, args.num_processes, 
                            args.gamma, None, device, False, args=args)
        sync_env = None
        args.env_type = 'bullet'

    # è·å–å…³èŠ‚æ•°é‡å’Œåˆ›å»ºæ•°æ®å¤„ç†å™¨
    num_joints = envs.action_space.shape[0]
    print(f"å…³èŠ‚æ•°é‡: {num_joints}")
    data_handler = DataHandler(num_joints, args.env_type)

    # åˆ›å»ºGNNç¼–ç å™¨
    if args.env_type == 'reacher2d':
        sys.path.append(os.path.join(base_dir, 'examples/2d_reacher/utils'))
        from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
        
        print("ğŸ¤– åˆå§‹åŒ– Reacher2D GNN ç¼–ç å™¨...")
        # reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
        reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=num_joints, num_joints=num_joints)
        single_gnn_embed = reacher2d_encoder.get_gnn_embeds(
            num_links=num_joints, 
            link_lengths=env_params['link_lengths']
        )
        print(f"âœ… Reacher2D GNN åµŒå…¥ç”ŸæˆæˆåŠŸï¼Œå½¢çŠ¶: {single_gnn_embed.shape}")
    else:
        rule_sequence = [int(s.strip(",")) for s in args.rule_sequence]
        gnn_encoder = GNN_Encoder(args.grammar_file, rule_sequence, 70, num_joints)
        gnn_graph = gnn_encoder.get_graph(rule_sequence)
        single_gnn_embed = gnn_encoder.get_gnn_embeds(gnn_graph)

    # åˆ›å»ºPPOæ¨¡å‹
    attn_model = AttnModel(128, 130, 130, 4)
    # ppo = AttentionPPOWithBuffer(
    #     attn_model, num_joints, 
    #     buffer_size=args.buffer_size, 
    #     batch_size=args.batch_size,
    #     lr=args.lr, 
    #     gamma=args.gamma,
    #     clip_epsilon=args.clip_epsilon,
    #     entropy_coef=args.entropy_coef,
    #     value_coef=args.value_coef,
    #     env_type=args.env_type
    #     # ğŸ”§ ç§»é™¤äº† joint_embed_dim å‚æ•°
    # )
    # åˆ›å»ºé€šç”¨PPOæ¨¡å‹
    print("ğŸ¯ åˆå§‹åŒ–é€šç”¨PPOæ¨¡å‹...")
    ppo = UniversalPPOWithBuffer(
        buffer_size=args.buffer_size, 
        batch_size=args.batch_size,
        lr=args.lr, 
        gamma=args.gamma,
        gae_lambda=0.95,
        clip_epsilon=args.clip_epsilon,
        entropy_coef=args.entropy_coef,
        value_coef=args.value_coef,
        max_grad_norm=0.5,
        device=device,
        env_type=args.env_type
    )
    print("âœ… é€šç”¨PPOæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    # PPOç‰¹å®šå‚æ•°è®¾ç½®
    print(f"ğŸ¯ PPOé…ç½®: clip_epsilon={args.clip_epsilon}, entropy_coef={args.entropy_coef}")
    
    # åˆ›å»ºè®­ç»ƒç›‘æ§ç³»ç»Ÿ
    experiment_name = f"reacher2d_ppo_{time.strftime('%Y%m%d_%H%M%S')}"
    
    # é…ç½®ä¿¡æ¯
    hyperparams = {
        'learning_rate': args.lr,
        'clip_epsilon': args.clip_epsilon,
        'entropy_coef': args.entropy_coef,
        'value_coef': args.value_coef,
        'ppo_epochs': args.ppo_epochs,
        'batch_size': args.batch_size,
        'buffer_size': args.buffer_size,
        'gamma': args.gamma,
        'seed': args.seed,
        'num_processes': args.num_processes,
        'total_steps': 240000,
        'optimizer': 'Adam',
        'algorithm': 'PPO',
        'network_architecture': {
            'attn_model_dims': '128-130-130-4',
            'action_dim': num_joints,
        }
    }
    
    env_config = {
        'env_name': args.env_name,
        'env_type': args.env_type,
        'goal_threshold': GOAL_THRESHOLD,
        'action_dim': num_joints,
    }
    
    if args.env_type == 'reacher2d':
        env_config.update({
            'num_links': env_params['num_links'],
            'link_lengths': env_params['link_lengths'],
            'config_path': env_params.get('config_path', 'N/A'),
            'render_mode': env_params.get('render_mode', 'human'),
            'reward_function': 'è·ç¦»+è¿›åº¦+æˆåŠŸ+ç¢°æ’+æ–¹å‘å¥–åŠ±',
            'physics_engine': 'PyMunk',
            'obstacle_type': 'zigzag',
            'action_space': 'continuous',
            'observation_space': 'joint_angles_and_positions'
        })
    
    logger = TrainingLogger(
        log_dir=os.path.join(args.save_dir, 'training_logs'),
        experiment_name=experiment_name,
        hyperparams=hyperparams,
        env_config=env_config
    )
    
    print(f"ğŸ“Š PPOè®­ç»ƒç›‘æ§ç³»ç»Ÿå·²åˆå§‹åŒ–: {logger.experiment_dir}")
    
    # åˆ›å»ºç®¡ç†å™¨
    model_manager = ModelManager(args.save_dir)
    training_manager = TrainingManager(args, ppo, logger, model_manager)
    
    # å¤„ç†checkpointæ¢å¤
    start_step = 0
    if args.resume_checkpoint:
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤PPOè®­ç»ƒ: {args.resume_checkpoint}")
        start_step = model_manager.load_checkpoint(ppo, args.resume_checkpoint)

        if start_step > 0:
            print(f"æˆåŠŸåŠ è½½PPO checkpoint, ä»step {start_step} å¼€å§‹è®­ç»ƒ")

            # æ›´æ–°å­¦ä¹ ç‡
            if args.resume_lr:
                for param_group in ppo.actor_optimizer.param_groups:
                    param_group['lr'] = args.resume_lr
                for param_group in ppo.critic_optimizer.param_groups:
                    param_group['lr'] = args.resume_lr
                print(f"æ›´æ–°å­¦ä¹ ç‡ä¸º {args.resume_lr}")
            
    # è¿è¡Œè®­ç»ƒå¾ªç¯
    run_training_loop(args, envs, sync_env, ppo, single_gnn_embed, training_manager, num_joints, start_step)
    training_results = collect_training_results(training_manager)
    
    # æ¸…ç†èµ„æº
    cleanup_resources(sync_env, logger, model_manager, training_manager)
    return training_results

def collect_training_results(training_manager):
    """æ”¶é›†è®­ç»ƒç»“æœç”¨äºfitnessè®¡ç®—"""
    import numpy as np
    
    if not hasattr(training_manager, 'episode_results') or not training_manager.episode_results:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°episode_resultsï¼Œè¿”å›é»˜è®¤ç»“æœ")
        return {
            'success': False,
            'error': 'No episode results available',
            'episodes_completed': 0,
            'success_rate': 0.0,
            'avg_best_distance': float('inf'),
            'avg_score': 0.0,
            'total_training_time': 0.0,
            'episode_details': [],
            'learning_progress': 0.0,
            'avg_steps_to_best': 120000
        }
    
    episodes = training_manager.episode_results
    print(f"ğŸ“Š æ”¶é›†åˆ° {len(episodes)} ä¸ªepisodeçš„ç»“æœ")
    
    # è®¡ç®—åŸºç¡€ç»Ÿè®¡
    success_count = sum(1 for ep in episodes if ep.get('success', False))
    total_episodes = len(episodes)
    
    # è®¡ç®—å¹³å‡æœ€ä½³è·ç¦»
    distances = [ep.get('best_distance', float('inf')) for ep in episodes]
    avg_best_distance = np.mean([d for d in distances if d != float('inf')]) if distances else float('inf')
    
    # è®¡ç®—å­¦ä¹ è¿›æ­¥
    if len(episodes) >= 2:
        first_score = episodes[0].get('score', 0)
        last_score = episodes[-1].get('score', 0)
        learning_progress = last_score - first_score
    else:
        learning_progress = 0.0
    
    # è®¡ç®—å¹³å‡åˆ°è¾¾æœ€ä½³è·ç¦»çš„æ­¥æ•°
    steps_to_best = [ep.get('best_distance_step', 120000) for ep in episodes]
    avg_steps_to_best = np.mean(steps_to_best)
    
    # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
    durations = [ep.get('duration', 0) for ep in episodes]
    total_training_time = sum(durations)
    
    result = {
        'success': True,
        'episodes_completed': total_episodes,
        'success_rate': success_count / total_episodes if total_episodes > 0 else 0.0,
        'avg_best_distance': avg_best_distance,
        'avg_score': np.mean([ep.get('score', 0) for ep in episodes]),
        'total_training_time': total_training_time,
        'episode_details': episodes,
        'learning_progress': learning_progress,
        'avg_steps_to_best': avg_steps_to_best,
        'episode_results': episodes
    }
    
    print(f"âœ… PPOè®­ç»ƒç»“æœæ”¶é›†å®Œæˆ:")
    print(f"   Episodes: {total_episodes}")
    print(f"   æˆåŠŸç‡: {result['success_rate']:.1%}")
    print(f"   å¹³å‡æœ€ä½³è·ç¦»: {result['avg_best_distance']:.1f}px")
    print(f"   å­¦ä¹ è¿›æ­¥: {result['learning_progress']:+.3f}")
    
    return result

def run_training_loop(args, envs, sync_env, ppo, single_gnn_embed, training_manager, num_joints, start_step=0):
    """è¿è¡ŒPPOè®­ç»ƒå¾ªç¯ - Episodesç‰ˆæœ¬"""
    current_obs = envs.reset()
    print(f"åˆå§‹è§‚å¯Ÿ: {current_obs.shape}")
    
    # é‡ç½®æ¸²æŸ“ç¯å¢ƒ
    if sync_env:
        sync_env.reset()
        print("ğŸ”§ sync_env å·²é‡ç½®")
    
    current_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)
    episode_rewards = [0.0] * args.num_processes
    
    # Episodesæ§åˆ¶å‚æ•°
    max_episodes = 2
    steps_per_episode = 120000
    
    print(f"å¼€å§‹PPOè®­ç»ƒ: {max_episodes}ä¸ªepisodes Ã— {steps_per_episode}æ­¥/episode")

    training_completed = False
    early_termination_reason = ""
    global_step = start_step  # å…¨å±€æ­¥æ•°è®¡æ•°å™¨

    try:
        # Episodeså¾ªç¯
        for episode_num in range(max_episodes):
            print(f"\nğŸ¯ å¼€å§‹PPO Episode {episode_num + 1}/{max_episodes}")
            
            print(f"ğŸ”„ é‡ç½®ç¯å¢ƒå¼€å§‹Episode {episode_num + 1}...")
            current_obs = envs.reset()
            if sync_env:
                sync_env.reset()
                print("ğŸ”§ sync_env å·²é‡ç½®")
            current_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)
            episode_rewards = [0.0] * args.num_processes
            
            # é‡ç½®episodeè¿½è¸ª
            training_manager.current_episode_start_step = global_step
            training_manager.current_episode_start_time = time.time()
            training_manager.current_episode_best_distance = float('inf')
            training_manager.current_episode_best_reward = float('-inf')
            training_manager.current_episode_min_distance_step = 0
            
            episode_step = 0
            episode_completed = False
            
            # å•ä¸ªEpisodeçš„è®­ç»ƒå¾ªç¯
            while episode_step < steps_per_episode and not episode_completed:
                # è¿›åº¦æ˜¾ç¤º
                if episode_step % 100 == 0:
                    # smart_print(f"PPO Episode {episode_num+1}, Step {episode_step}/{steps_per_episode}: Buffer size: {len(ppo.buffer.joint_q)}")
                    smart_print(f"PPO Episode {episode_num+1}, Step {episode_step}/{steps_per_episode}: Buffer size: {len(ppo.buffer.experiences)}")

                # è·å–åŠ¨ä½œ - PPOç‰ˆæœ¬
                if global_step < 1000:  # PPOçš„ç®€å•warmup
                    action_batch = torch.from_numpy(np.array([
                        envs.action_space.sample() for _ in range(args.num_processes)
                    ]))
                    log_prob_batch = None
                    value_batch = None
                else:
                    actions = []
                    log_probs = []
                    values = []
                    for proc_id in range(args.num_processes):
                        action, log_prob, value = ppo.get_action(
                            current_obs[proc_id],
                            current_gnn_embeds[proc_id],
                            num_joints=envs.action_space.shape[0],
                            deterministic=False
                        )
                        actions.append(action)
                        log_probs.append(log_prob)
                        values.append(value)
                    
                    action_batch = torch.stack(actions)
                    log_prob_batch = torch.stack(log_probs) if log_probs[0] is not None else None
                    value_batch = torch.stack(values)

                # åŠ¨ä½œåˆ†æï¼ˆè°ƒè¯•ç”¨ï¼‰
                if episode_step % 50 == 0 or episode_step < 20:
                    if hasattr(envs, 'envs') and len(envs.envs) > 0:
                        env_goal = getattr(envs.envs[0], 'goal_pos', 'NOT FOUND')
                        print(f"ğŸ¯ [PPO Episode {episode_num+1}] Step {episode_step} - ç¯å¢ƒgoal_pos: {env_goal}")

                # æ‰§è¡ŒåŠ¨ä½œ
                next_obs, reward, done, infos = envs.step(action_batch)

                # æ¸²æŸ“å¤„ç†
                if sync_env:
                    sync_action = action_batch[0].cpu().numpy() if hasattr(action_batch, 'cpu') else action_batch[0]
                    sync_env.step(sync_action)
                    sync_env.render()

                next_gnn_embeds = single_gnn_embed.repeat(args.num_processes, 1, 1)

                # PPOç»éªŒå­˜å‚¨
                for proc_id in range(args.num_processes):
                    if global_step >= 1000:  # åªåœ¨éwarmupæœŸé—´å­˜å‚¨
                        ppo.store_experience(
                            obs=current_obs[proc_id],
                            gnn_embeds=current_gnn_embeds[proc_id],
                            action=action_batch[proc_id],
                            reward=reward[proc_id].item(),
                            done=done[proc_id].item(),
                            log_prob=log_prob_batch[proc_id] if log_prob_batch is not None else None,
                            value=value_batch[proc_id] if value_batch is not None else None,
                            num_joints=num_joints
                        )
                    episode_rewards[proc_id] += reward[proc_id].item()

                current_obs = next_obs.clone()
                current_gnn_embeds = next_gnn_embeds.clone()

                # æ›´æ–°episodeè¿½è¸ª
                training_manager.update_episode_tracking(global_step, infos, episode_rewards)

                # å¤„ç†episodeç»“æŸ
                for proc_id in range(args.num_processes):
                    is_done = done[proc_id].item() if torch.is_tensor(done[proc_id]) else bool(done[proc_id])
                    if is_done:
                        print(f"ğŸ” [DEBUG] PPO Episodeç»“æŸæ£€æµ‹: proc_id={proc_id}, å½“å‰episodes={training_manager.current_episodes}")
                        
                        should_end = training_manager.handle_episode_end(proc_id, episode_step, episode_rewards, infos)
                        print(f"ğŸ” [DEBUG] handle_episode_endè¿”å›: should_end={should_end}, æ–°çš„current_episodes={training_manager.current_episodes}")
                        
                        # æ£€æŸ¥ç»´æŒå®Œæˆæƒ…å†µï¼ˆä»ç¯å¢ƒç›´æ¥è·å–ï¼‰
                        maintain_completed = False
                        if len(infos) > proc_id and isinstance(infos[proc_id], dict):
                            # ä»ç¯å¢ƒinfoè·å–ç»´æŒä¿¡æ¯
                            maintain_info = infos[proc_id].get('maintain', {})
                            maintain_completed = maintain_info.get('maintain_completed', False)
                            maintain_counter = maintain_info.get('maintain_counter', 0)
                            maintain_target = maintain_info.get('maintain_target', 500)
                            
                            print(f"ğŸ† [DEBUG] ç»´æŒæ£€æŸ¥: {maintain_counter}/{maintain_target} æ­¥, å®Œæˆ: {maintain_completed}")
                        
                        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡ï¼ˆä½†æœªç»´æŒå¤Ÿæ—¶é—´ï¼‰
                        goal_reached = infos[proc_id].get('goal', {}).get('distance_to_goal', float('inf')) < 20.0
                        print(f"ğŸ” [DEBUG] ç›®æ ‡æ£€æŸ¥: goal_reached={goal_reached}")
                        
                        if should_end:  # å®Œæˆ2ä¸ªè®­ç»ƒepisodes
                            print(f"ğŸ” [DEBUG] è§¦å‘should_endï¼Œæ•´ä¸ªPPOè®­ç»ƒç»“æŸ")
                            training_completed = True
                            early_termination_reason = f"å®Œæˆ{training_manager.current_episodes}ä¸ªepisodes"
                            episode_completed = True
                            break
                        elif maintain_completed:  # ç»´æŒ10ç§’å®Œæˆï¼Œç»“æŸå½“å‰è®­ç»ƒepisode
                            print(f"ğŸ” [DEBUG] è§¦å‘maintain_completedï¼Œå½“å‰PPOè®­ç»ƒepisodeç»“æŸ")
                            print(f"ğŸŠ PPOè®­ç»ƒEpisode {episode_num+1} ç»´æŒæˆåŠŸå®Œæˆï¼å¼€å§‹ä¸‹ä¸€ä¸ªepisode...")
                            episode_completed = True  # ç»“æŸå½“å‰è®­ç»ƒepisode
                            break
                        elif goal_reached:  # åˆ°è¾¾ç›®æ ‡ä½†æœªç»´æŒå¤Ÿæ—¶é—´ï¼Œç»§ç»­è®­ç»ƒ
                            print(f"ğŸ¯ [DEBUG] åˆ°è¾¾ç›®æ ‡ä½†éœ€ç»§ç»­ç»´æŒï¼Œç»§ç»­å½“å‰PPO episode")
                            # å…³é”®ï¼šä¸breakï¼Œè®©æœºå™¨äººç»§ç»­å­¦ä¹ ç»´æŒ
                            pass
                        
                        # ç¯å¢ƒé‡ç½®ï¼ˆç»§ç»­å½“å‰è®­ç»ƒepisodeï¼‰
                        if hasattr(envs, 'reset_one'):
                            current_obs[proc_id] = envs.reset_one(proc_id)
                            current_gnn_embeds[proc_id] = single_gnn_embed

                # PPOæ¨¡å‹æ›´æ–° - åœ¨episodeç»“æŸæ—¶æ›´æ–°
                if training_manager.should_update_model(global_step):
                    # PPOéœ€è¦ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼å‡½æ•°
                    training_manager.update_and_log(
                        global_step, 
                        next_obs=current_obs[0] if args.num_processes > 0 else None,
                        next_gnn_embeds=current_gnn_embeds[0] if args.num_processes > 0 else None,
                        num_joints=num_joints
                    )
                
                # å®šæœŸä¿å­˜å’Œç»˜å›¾
                if global_step % 200 == 0 and global_step > 0:
                    # è·å–å½“å‰æœ€ä½³è·ç¦»
                    current_best_distance = training_manager.current_episode_best_distance
                    
                    # ä¼ é€’å½“å‰è·ç¦»ç”¨äºæ¯”è¾ƒ
                    saved = training_manager.model_manager.save_checkpoint(
                        ppo, global_step,
                        best_success_rate=training_manager.best_success_rate,
                        best_min_distance=training_manager.best_min_distance,
                        current_best_distance=current_best_distance,
                        consecutive_success_count=training_manager.consecutive_success_count,
                        current_episode=episode_num + 1,
                        episode_step=episode_step
                    )
                    
                    # å¦‚æœä¿å­˜æˆåŠŸï¼Œæ›´æ–°æœ€ä½³è®°å½•
                    if saved:
                        training_manager.best_min_distance = current_best_distance
                        print(f"ğŸ“ˆ æ›´æ–°å…¨å±€æœ€ä½³è·ç¦»: {current_best_distance:.1f}px")

                # ä½é¢‘æ—¥å¿—è®°å½•
                if global_step % 2000 == 0 and global_step > 0:
                    training_manager.logger.plot_losses(recent_steps=2000, show=False)
                    print(f"ğŸ“Š PPO Step {global_step}: å½“å‰æœ€ä½³è·ç¦» {training_manager.best_min_distance:.1f}px")
                
                episode_step += 1  # episodeå†…æ­¥æ•°é€’å¢
                global_step += args.num_processes  # å…¨å±€æ­¥æ•°é€’å¢
                
                if training_completed:
                    break
            
            print(f"ğŸ“Š PPO Episode {episode_num + 1} å®Œæˆ: {episode_step} æ­¥")
            
            if training_completed:
                print(f"ğŸ PPOè®­ç»ƒæå‰ç»ˆæ­¢: {early_termination_reason}")
                break

    except Exception as e:
        print(f"ğŸ”´ PPOè®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        training_manager.logger.save_logs()
        training_manager.logger.generate_report()
        raise e

def cleanup_resources(sync_env, logger, model_manager, training_manager):
    """æ¸…ç†èµ„æº"""
    if sync_env:
        sync_env.close()
    
    # ğŸ”§ ä¿®å¤ï¼šä¿å­˜æœ€ç»ˆæˆåŠŸçš„æ¨¡å‹
    if training_manager.best_success_rate > 0:
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæˆåŠŸæ¨¡å‹...")
        model_manager.save_final_model(
            training_manager.ppo, 
            step=training_manager.total_training_steps,
            final_success_rate=training_manager.best_success_rate,
            final_min_distance=training_manager.best_min_distance,
            final_consecutive_successes=training_manager.consecutive_success_count,
            episode_results=training_manager.episode_results
        )
            
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print(f"\n{'='*60}")
    print(f"ğŸ PPOè®­ç»ƒå®Œæˆæ€»ç»“:")
    print(f"  æœ€ä½³æˆåŠŸç‡: {training_manager.best_success_rate:.3f}")
    print(f"  æœ€ä½³æœ€å°è·ç¦»: {training_manager.best_min_distance:.1f} pixels")
    print(f"  å½“å‰è¿ç»­æˆåŠŸæ¬¡æ•°: {training_manager.consecutive_success_count}")
    
    logger.generate_report()
    logger.plot_losses(show=False)
    print(f"ğŸ“Š å®Œæ•´PPOè®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {logger.experiment_dir}")
    print(f"{'='*60}")

def test_trained_model(model_path, num_episodes=10, render=True):
    """æµ‹è¯•è®­ç»ƒå¥½çš„PPOæ¨¡å‹æ€§èƒ½"""
    print(f"ğŸ§ª å¼€å§‹æµ‹è¯•PPOæ¨¡å‹: {model_path}")
    
    # ç¯å¢ƒé…ç½®
    env_params = {
        'num_links': DEFAULT_CONFIG['num_links'],
        'link_lengths': DEFAULT_CONFIG['link_lengths'],
        'render_mode': 'human' if render else None,
        'config_path': DEFAULT_CONFIG['config_path']
    }
    
    print(f"ğŸ”§ æµ‹è¯•ç¯å¢ƒé…ç½®: {env_params}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = Reacher2DEnv(**env_params)
    num_joints = env.action_space.shape[0]
    
    print(f"ğŸ” æµ‹è¯•ç¯å¢ƒéªŒè¯:")
    print(f"   èµ·å§‹ä½ç½®: {getattr(env, 'anchor_point', 'N/A')}")
    print(f"   ç›®æ ‡ä½ç½®: {getattr(env, 'goal_pos', 'N/A')}")
    print(f"   å…³èŠ‚æ•°é‡: {num_joints}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
    
    # åˆ›å»ºGNNç¼–ç å™¨
    sys.path.append(os.path.join(base_dir, 'examples/2d_reacher/utils'))
    from reacher2d_gnn_encoder import Reacher2D_GNN_Encoder
    
    reacher2d_encoder = Reacher2D_GNN_Encoder(max_nodes=20, num_joints=num_joints)
    gnn_embed = reacher2d_encoder.get_gnn_embeds(
        num_links=num_joints, 
        link_lengths=env_params['link_lengths']
    )
    
    print(f"   GNNåµŒå…¥å½¢çŠ¶: {gnn_embed.shape}")
    
    # åˆ›å»ºPPOæ¨¡å‹ - ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„å­¦ä¹ ç‡
    attn_model = AttnModel(128, 130, 130, 4)
    # ppo = AttentionPPOWithBuffer(attn_model, num_joints, 
    #                             buffer_size=2048, batch_size=64,
    #                             lr=2e-4, env_type='reacher2d')  # ä¿®å¤: ä½¿ç”¨è®­ç»ƒæ—¶çš„å­¦ä¹ ç‡
    # åˆ›å»ºé€šç”¨PPOæ¨¡å‹ç”¨äºæµ‹è¯•
    print("ğŸ¯ åˆå§‹åŒ–é€šç”¨PPOæ¨¡å‹ç”¨äºæµ‹è¯•...")
    ppo = UniversalPPOWithBuffer(
        buffer_size=2048, 
        batch_size=64,
        lr=2e-4, 
        device=torch.device('cpu'),
        env_type='reacher2d'
    )
    print("âœ… é€šç”¨PPOæµ‹è¯•æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    # åŠ è½½PPOæ¨¡å‹
    try:
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
            
        print(f"ğŸ”„ åŠ è½½PPOæ¨¡å‹: {model_path}")
        model_data = torch.load(model_path, map_location='cpu')
        
        if 'actor_state_dict' in model_data:
            ppo.actor.load_state_dict(model_data['actor_state_dict'], strict=False)
            print("âœ… PPO Actor åŠ è½½æˆåŠŸ")
        
        if 'critic_state_dict' in model_data:
            ppo.critic.load_state_dict(model_data['critic_state_dict'], strict=False)
            print("âœ… PPO Critic åŠ è½½æˆåŠŸ")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šè®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        ppo.actor.eval()
        ppo.critic.eval()
        print("ğŸ¯ æ¨¡å‹å·²è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
        
        # æ¨¡å‹éªŒè¯
        print(f"ğŸ” PPOæ¨¡å‹éªŒè¯:")
        print(f"   Actorå‚æ•°æ•°é‡: {sum(p.numel() for p in ppo.actor.parameters())}")
        print(f"   Criticå‚æ•°æ•°é‡: {sum(p.numel() for p in ppo.critic.parameters())}")
        
        print(f"ğŸ“‹ PPOæ¨¡å‹ä¿¡æ¯:")
        print(f"   è®­ç»ƒæ­¥æ•°: {model_data.get('step', 'N/A')}")
        print(f"   æ—¶é—´æˆ³: {model_data.get('timestamp', 'N/A')}")
        print(f"   æ¨¡å‹ç±»å‹: {model_data.get('model_type', 'N/A')}")
        if 'success_rate' in model_data:
            print(f"   è®­ç»ƒæ—¶æˆåŠŸç‡: {model_data.get('success_rate', 'N/A'):.3f}")
        if 'min_distance' in model_data:
            print(f"   è®­ç»ƒæ—¶æœ€å°è·ç¦»: {model_data.get('min_distance', 'N/A'):.1f}")
            
    except Exception as e:
        print(f"âŒ åŠ è½½PPOæ¨¡å‹å¤±è´¥: {e}")
        return None
    
    # æµ‹è¯•å¤šä¸ªepisode
    success_count = 0
    total_rewards = []
    min_distances = []
    episode_lengths = []
    
    print(f"\nğŸ® å¼€å§‹æµ‹è¯•PPOæ¨¡å‹ {num_episodes} ä¸ªepisodes...")
    print(f"ğŸ¯ ç›®æ ‡é˜ˆå€¼: {GOAL_THRESHOLD} pixels")
    
    for episode in range(num_episodes):
        obs = env.reset()
        episode_reward = 0
        step_count = 0
        max_steps = 5000
        min_distance_this_episode = float('inf')
        episode_success = False
        
        print(f"\nğŸ“ Episode {episode + 1}/{num_episodes}")
        print(f"   åˆå§‹è§‚å¯Ÿå½¢çŠ¶: {obs.shape}")
        print(f"   åˆå§‹æœ«ç«¯ä½ç½®: {env._get_end_effector_position()}")
        
        # è®¡ç®—åˆå§‹è·ç¦»
        initial_distance = np.linalg.norm(np.array(env._get_end_effector_position()) - env.goal_pos)
        print(f"   åˆå§‹ç›®æ ‡è·ç¦»: {initial_distance:.1f}px")
        
        while step_count < max_steps:
            # æ·»åŠ åŠ¨ä½œè°ƒè¯•
            if step_count % 100 == 0 or step_count < 5:
                # åœ¨æµ‹è¯•å¾ªç¯ä¸­ä½¿ç”¨PPOè·å–åŠ¨ä½œ
                action, _, _ = ppo.get_action(
                    torch.from_numpy(obs).float(),
                    gnn_embed.squeeze(0),
                    num_joints=num_joints,
                    deterministic=True  # æµ‹è¯•æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
                )
                print(f"   Step {step_count}: PPO Action = {action.detach().cpu().numpy()}")
                print(f"   Step {step_count}: æœ«ç«¯ä½ç½® = {env._get_end_effector_position()}")
                
                # è®¡ç®—è·ç¦»
                end_pos = env._get_end_effector_position()
                goal_pos = env.goal_pos
                distance = np.linalg.norm(np.array(end_pos) - goal_pos)
                print(f"   Step {step_count}: è·ç¦» = {distance:.1f}px")
            else:
                # è·å–åŠ¨ä½œï¼ˆä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼‰
                action, _, _ = ppo.get_action(
                    torch.from_numpy(obs).float(),
                    gnn_embed.squeeze(0),
                    num_joints=num_joints,
                    deterministic=True
                )
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = env.step(action.cpu().numpy())
            episode_reward += reward
            step_count += 1
            
            # æ£€æŸ¥è·ç¦»
            end_pos = env._get_end_effector_position()
            goal_pos = env.goal_pos
            distance = np.linalg.norm(np.array(end_pos) - goal_pos)
            min_distance_this_episode = min(min_distance_this_episode, distance)
            
            # æ¸²æŸ“
            if render:
                env.render()
                time.sleep(0.02)
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
            if done:
                if not episode_success:
                    success_count += 1
                    episode_success = True
                    print(f"  ğŸ‰ PPOç›®æ ‡åˆ°è¾¾! è·ç¦»: {distance:.1f} pixels, æ­¥éª¤: {step_count}")
                break
        
        total_rewards.append(episode_reward)
        min_distances.append(min_distance_this_episode)
        episode_lengths.append(step_count)
        
        print(f"  ğŸ“Š Episode {episode + 1} ç»“æœ:")
        print(f"    å¥–åŠ±: {episode_reward:.2f}")
        print(f"    æœ€å°è·ç¦»: {min_distance_this_episode:.1f} pixels")
        print(f"    æ­¥éª¤æ•°: {step_count}")
        print(f"    æˆåŠŸ: {'âœ… æ˜¯' if episode_success else 'âŒ å¦'}")
        print(f"    è·ç¦»æ”¹å–„: {initial_distance - min_distance_this_episode:.1f}px")
    
    # æµ‹è¯•æ€»ç»“
    success_rate = success_count / num_episodes
    avg_reward = np.mean(total_rewards)
    avg_min_distance = np.mean(min_distances)
    avg_episode_length = np.mean(episode_lengths)
    
    print(f"\n{'='*60}")
    print(f"ğŸ† PPOæµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"  æµ‹è¯•Episodes: {num_episodes}")
    print(f"  æˆåŠŸæ¬¡æ•°: {success_count}")
    print(f"  æˆåŠŸç‡: {success_rate:.1%}")
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"  å¹³å‡æœ€å°è·ç¦»: {avg_min_distance:.1f} pixels")
    print(f"  å¹³å‡Episodeé•¿åº¦: {avg_episode_length:.1f} steps")
    print(f"  ç›®æ ‡é˜ˆå€¼: {GOAL_THRESHOLD:.1f} pixels")
    
    # æ€§èƒ½è¯„ä»·
    print(f"\nğŸ“‹ PPOæ€§èƒ½è¯„ä»·:")
    if success_rate >= 0.8:
        print(f"  ğŸ† ä¼˜ç§€! æˆåŠŸç‡ >= 80%")
    elif success_rate >= 0.5:
        print(f"  ğŸ‘ è‰¯å¥½! æˆåŠŸç‡ >= 50%")
    elif success_rate >= 0.2:
        print(f"  âš ï¸  ä¸€èˆ¬! æˆåŠŸç‡ >= 20%")
    else:
        print(f"  âŒ éœ€è¦æ”¹è¿›! æˆåŠŸç‡ < 20%")
        
    if avg_min_distance <= GOAL_THRESHOLD:
        print(f"  âœ… å¹³å‡æœ€å°è·ç¦»è¾¾åˆ°ç›®æ ‡é˜ˆå€¼")
    else:
        print(f"  âš ï¸  å¹³å‡æœ€å°è·ç¦»è¶…å‡ºç›®æ ‡é˜ˆå€¼ {avg_min_distance - GOAL_THRESHOLD:.1f} pixels")
    
    print(f"{'='*60}")
    
    env.close()
    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward, 
        'avg_min_distance': avg_min_distance,
        'avg_episode_length': avg_episode_length,
        'success_count': success_count,
        'total_episodes': num_episodes
    }

def find_latest_model():
    """æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶"""
    base_path = "./trained_models/reacher2d/enhanced_test"
    
    if not os.path.exists(base_path):
        print(f"âŒ è®­ç»ƒæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {base_path}")
        return None
    
    model_candidates = []
    
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.pth') and ('final_model' in file or 'checkpoint' in file or 'ppo' in file):
                full_path = os.path.join(root, file)
                mtime = os.path.getmtime(full_path)
                model_candidates.append((full_path, mtime))
    
    if not model_candidates:
        print(f"âŒ åœ¨ {base_path} ä¸­æœªæ‰¾åˆ°PPOæ¨¡å‹æ–‡ä»¶")
        return None
    
    model_candidates.sort(key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ” æ‰¾åˆ° {len(model_candidates)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
    for i, (path, mtime) in enumerate(model_candidates[:5]):
        import datetime
        time_str = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        print(f"  {i+1}. {os.path.basename(path)} ({time_str})")
    
    latest_model = model_candidates[0][0]
    print(f"âœ… é€‰æ‹©æœ€æ–°PPOæ¨¡å‹: {latest_model}")
    return latest_model

# === ä¸»ç¨‹åºå…¥å£ ===
if __name__ == "__main__":
    torch.set_default_dtype(torch.float64)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == '--test':
        print("ğŸ§ª è¿›å…¥PPOæµ‹è¯•æ¨¡å¼")
        
        model_path = None
        num_episodes = 5
        render = True
        
        for i, arg in enumerate(sys.argv):
            if arg == '--model-path' and i + 1 < len(sys.argv):
                model_path = sys.argv[i + 1]
            elif arg == '--episodes' and i + 1 < len(sys.argv):
                num_episodes = int(sys.argv[i + 1])
            elif arg == '--no-render':
                render = False
            elif arg == '--latest':
                model_path = 'latest'
        
        if model_path is None or model_path == 'latest':
            print("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°PPOæ¨¡å‹...")
            model_path = find_latest_model()
        
        if model_path:
            print(f"ğŸ¯ PPOæµ‹è¯•å‚æ•°: episodes={num_episodes}, render={render}")
            result = test_trained_model(model_path, num_episodes, render)
            
            if result:
                print(f"\nğŸ¯ PPOå¿«é€Ÿç»“è®º:")
                if result['success_rate'] >= 0.8:
                    print(f"  âœ… PPOæ¨¡å‹è¡¨ç°ä¼˜ç§€! ç»§ç»­å½“å‰è®­ç»ƒç­–ç•¥")
                elif result['success_rate'] >= 0.3:
                    print(f"  âš ï¸  PPOæ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´å‚æ•°")
                else:
                    print(f"  âŒ PPOæ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦é‡æ–°å®¡è§†å¥–åŠ±å‡½æ•°æˆ–ç½‘ç»œç»“æ„")
        else:
            print("âŒ æœªæ‰¾åˆ°å¯æµ‹è¯•çš„PPOæ¨¡å‹")
        
        exit(0)
    
    # è®­ç»ƒæ¨¡å¼ - å‚æ•°è§£æ
    parser = create_training_parser()
    args = parser.parse_args()

    args.cuda = not args.no_cuda and torch.cuda.is_available()
    args.save_dir = os.path.join(args.save_dir, get_time_stamp())
    os.makedirs(args.save_dir, exist_ok=True)
    
    # ä¿å­˜å‚æ•°
    with open(os.path.join(args.save_dir, 'args.txt'), 'w') as f:
        f.write(str(sys.argv))

    main(args)