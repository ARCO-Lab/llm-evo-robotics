#!/usr/bin/env python3
"""
æ·±åº¦ä¿®å¤PPO - è§£å†³æŒç»­çš„Critic Lossè¿‡é«˜é—®é¢˜
"""

import torch
import sys
import os

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(__file__), "../../")
sys.path.append(base_dir)

from sac.universal_ppo_model import UniversalPPOWithBuffer

def deep_fix_ppo_model():
    """æ·±åº¦ä¿®å¤PPOæ¨¡å‹ - è§£å†³Critic Lossè¿‡é«˜é—®é¢˜"""
    print("ğŸ”§ å¼€å§‹æ·±åº¦ä¿®å¤PPOæ¨¡å‹...")
    
    # ä½¿ç”¨æ›´æç«¯çš„ä¿å®ˆå‚æ•°
    ppo = UniversalPPOWithBuffer(
        buffer_size=2048,
        batch_size=32,      # å‡å°æ‰¹æ¬¡å¤§å°
        lr=1e-5,           # æ›´ä½çš„åŸºç¡€å­¦ä¹ ç‡
        gamma=0.99,
        gae_lambda=0.95,
        clip_epsilon=0.1,   # æ›´å°çš„clipèŒƒå›´
        entropy_coef=0.001, # æ›´ä½çš„ç†µç³»æ•°
        value_coef=1.0,     # æé«˜value lossæƒé‡
        device='cpu',
        env_type='reacher2d'
    )
    
    print("âœ… åˆ›å»ºè¶…ä¿å®ˆPPOæ¨¡å‹å®Œæˆ")
    
    # å¼ºåˆ¶é‡ç½®æ‰€æœ‰å…³é”®å‚æ•°
    with torch.no_grad():
        # 1. é‡ç½®Actorçš„log_stdåˆ°æä½å€¼
        ppo.actor.log_std_base.data.fill_(-2.5)  # std â‰ˆ 0.08
        print(f"ğŸ”§ å¼ºåˆ¶é‡ç½®log_std_baseåˆ°-2.5 (std â‰ˆ 0.08)")
        
        # 2. é‡æ–°åˆå§‹åŒ–Criticç½‘ç»œæƒé‡
        def reinit_weights(m):
            if isinstance(m, torch.nn.Linear):
                torch.nn.init.xavier_uniform_(m.weight, gain=0.1)  # å°å¢ç›Š
                if m.bias is not None:
                    torch.nn.init.constant_(m.bias, 0.0)
        
        ppo.critic.apply(reinit_weights)
        print("ğŸ”§ é‡æ–°åˆå§‹åŒ–Criticç½‘ç»œæƒé‡ (å°å¢ç›Š)")
        
        # 3. è®¾ç½®æä½çš„å­¦ä¹ ç‡
        for param_group in ppo.actor_optimizer.param_groups:
            param_group['lr'] = 5e-6  # æä½Actorå­¦ä¹ ç‡
        for param_group in ppo.critic_optimizer.param_groups:
            param_group['lr'] = 1e-5  # æä½Criticå­¦ä¹ ç‡
        print("ğŸ”§ è®¾ç½®æä½å­¦ä¹ ç‡: Actor=5e-6, Critic=1e-5")
        
        # 4. è°ƒæ•´ä¼˜åŒ–å™¨çš„åŠ¨é‡å‚æ•°
        ppo.actor_optimizer.param_groups[0]['betas'] = (0.9, 0.999)
        ppo.critic_optimizer.param_groups[0]['betas'] = (0.9, 0.999)
        print("ğŸ”§ è°ƒæ•´Adamä¼˜åŒ–å™¨å‚æ•°")
    
    # ä¿å­˜æ·±åº¦ä¿®å¤åçš„æ¨¡å‹
    save_path = "deep_fixed_ppo_model.pth"
    ppo.save_model(save_path)
    print(f"ğŸ’¾ æ·±åº¦ä¿®å¤æ¨¡å‹å·²ä¿å­˜: {save_path}")
    
    # éªŒè¯ä¿®å¤ç»“æœ
    with torch.no_grad():
        current_log_std = ppo.actor.log_std_base.item()
        current_std = torch.exp(torch.clamp(torch.tensor(current_log_std), -2.3, -0.5)).item()
        actor_lr = ppo.actor_optimizer.param_groups[0]['lr']
        critic_lr = ppo.critic_optimizer.param_groups[0]['lr']
        
        print(f"\nğŸ” æ·±åº¦ä¿®å¤éªŒè¯:")
        print(f"   log_std_base: {current_log_std:.4f}")
        print(f"   std: {current_std:.4f}")
        print(f"   Actorå­¦ä¹ ç‡: {actor_lr:.2e}")
        print(f"   Criticå­¦ä¹ ç‡: {critic_lr:.2e}")
        print(f"   æ‰¹æ¬¡å¤§å°: 32")
        print(f"   ClipèŒƒå›´: 0.1")
        print(f"   ç†µç³»æ•°: 0.001")
        print(f"   Valueç³»æ•°: 1.0")
        
        print(f"\nğŸ¯ æ·±åº¦ä¿®å¤ç›®æ ‡:")
        print(f"   âœ… Entropy: é¢„æœŸ 0.1-0.8 (æä½æ¢ç´¢)")
        print(f"   âœ… Critic Loss: é¢„æœŸ < 5.0 (ç¨³å®š)")
        print(f"   âœ… Actor Loss: é¢„æœŸ 0.1-0.5 (ç¨³å®š)")
        print(f"   âœ… æ•´ä½“ç¨³å®šæ€§: æ— å¼‚å¸¸æ³¢åŠ¨")
    
    print(f"\nğŸš€ ä½¿ç”¨å»ºè®®:")
    print(f"   python enhanced_train.py --env-name reacher2d \\")
    print(f"       --lr 1e-5 --entropy-coef 0.001 --batch-size 32 \\")
    print(f"       --resume-checkpoint {save_path}")
    
    return save_path

if __name__ == "__main__":
    deep_fix_ppo_model()







