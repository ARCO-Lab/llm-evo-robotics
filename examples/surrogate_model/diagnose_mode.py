#!/usr/bin/env python3
"""
ğŸ” Enhanced Train æ·±åº¦åˆ†æå·¥å…· - å®Œå…¨ä¿®å¤ç‰ˆ
åˆ†ææ¨¡å‹å­¦ä¹ çŠ¶æ€ã€ç“¶é¢ˆå’Œä¼˜åŒ–å»ºè®®
"""
import numpy as np
import torch
import sys
import os
import time
from collections import deque, defaultdict
import json

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../..')
sys.path.extend([
    base_dir,
    os.path.join(base_dir, 'examples/2d_reacher/envs'),
    os.path.join(base_dir, 'examples/2d_reacher/utils'),
    os.path.join(base_dir, 'examples/surrogate_model/gnn_encoder'),
    os.path.join(base_dir, 'examples/rl/train'),
    os.path.join(base_dir, 'examples/rl/common'),
    os.path.join(base_dir, 'examples/rl/environments'),
    os.path.join(base_dir, 'examples/rl')
])

from reacher2d_env import Reacher2DEnv
from attn_model.attn_model import AttnModel
from sac.sac_model import AttentionSACWithBuffer

class TrainingAnalyzer:
    """è®­ç»ƒè¿‡ç¨‹æ·±åº¦åˆ†æå™¨ - å®Œå…¨ä¿®å¤ç‰ˆ"""
    
    def __init__(self):
        self.metrics = defaultdict(list)
        self.analysis_results = {}
        
    def analyze_model_capacity(self):
        """åˆ†ææ¨¡å‹å®¹é‡æ˜¯å¦åˆé€‚"""
        print("ğŸ” === æ¨¡å‹å®¹é‡åˆ†æ ===")
        
        # åˆ›å»ºæ¨¡å‹
        attn_model = AttnModel(128, 130, 130, 4)
        sac = AttentionSACWithBuffer(
            attn_model, 3,
            buffer_capacity=1000,
            batch_size=32,
            lr=1e-4,
            env_type='reacher2d'
        )
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in attn_model.parameters())
        trainable_params = sum(p.numel() for p in attn_model.parameters() if p.requires_grad)
        
        actor_params = sum(p.numel() for p in sac.actor.parameters())
        critic_params = sum(p.numel() for p in sac.critic1.parameters())
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"   AttnModelæ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   Actorå‚æ•°: {actor_params:,}")
        print(f"   Criticå‚æ•°: {critic_params:,}")
        print(f"   æ€»SACå‚æ•°: {actor_params + critic_params * 2:,}")
        
        # åˆ†æå¤æ‚åº¦
        task_complexity = 3  # 3å…³èŠ‚ä»»åŠ¡
        param_per_joint = total_params / task_complexity
        
        print(f"\nğŸ¯ å¤æ‚åº¦åˆ†æ:")
        print(f"   ä»»åŠ¡å¤æ‚åº¦: {task_complexity}å…³èŠ‚")
        print(f"   æ¯å…³èŠ‚å‚æ•°: {param_per_joint:,.0f}")
        
        if param_per_joint > 50000:
            print(f"   âš ï¸  æ¨¡å‹å¯èƒ½è¿‡äºå¤æ‚ (å»ºè®®<50kå‚æ•°/å…³èŠ‚)")
            complexity_verdict = "è¿‡äºå¤æ‚"
        elif param_per_joint < 5000:
            print(f"   âš ï¸  æ¨¡å‹å¯èƒ½è¿‡äºç®€å• (å»ºè®®>5kå‚æ•°/å…³èŠ‚)")
            complexity_verdict = "è¿‡äºç®€å•"
        else:
            print(f"   âœ… æ¨¡å‹å¤æ‚åº¦åˆé€‚")
            complexity_verdict = "åˆé€‚"
            
        self.analysis_results['model_complexity'] = {
            'total_params': total_params,
            'param_per_joint': param_per_joint,
            'verdict': complexity_verdict
        }
        
        return complexity_verdict
    
    def analyze_gradient_flow_fixed(self, steps=200):
        """å®Œå…¨ä¿®å¤ç‰ˆæ¢¯åº¦æµåŠ¨åˆ†æ"""
        print(f"\nğŸ” === æ¢¯åº¦æµåŠ¨åˆ†æ ({steps}æ­¥) ===")
        
        # åˆ›å»ºç¯å¢ƒå’Œæ¨¡å‹
        env = Reacher2DEnv(num_links=3, link_lengths=[90, 90, 90], config_path=None, render_mode=None)
        attn_model = AttnModel(128, 130, 130, 4)
        sac = AttentionSACWithBuffer(attn_model, 3, buffer_capacity=5000, batch_size=32, lr=1e-4, env_type='reacher2d')
        
        # ğŸ”§ å…³é”®ä¿®å¤1: æ­£ç¡®çš„GNNåµŒå…¥ç»´åº¦
        # reacher2déœ€è¦ [3, 128] è€Œä¸æ˜¯ [1, 20, 128]
        gnn_embed = torch.randn(3, 128)  # 3ä¸ªå…³èŠ‚ï¼Œæ¯ä¸ª128ç»´
        
        # æ”¶é›†æ¢¯åº¦ç»Ÿè®¡
        gradient_norms = {'actor': [], 'critic1': [], 'critic2': []}
        loss_history = {'actor': [], 'critic': [], 'alpha': []}
        reward_history = []
        distance_history = []
        
        obs = env.reset()
        episode_reward = 0
        
        print("ğŸ¯ å¼€å§‹æ¢¯åº¦åˆ†æè®­ç»ƒ...")
        
        for step in range(steps):
            # ğŸ”§ å…³é”®ä¿®å¤2: æ­£ç¡®çš„è§‚å¯Ÿç©ºé—´å¤„ç†
            # reacher2dçš„è§‚å¯Ÿç©ºé—´æ˜¯ [angles(3) + angular_vels(3) + end_pos(2)] = 8ç»´
            obs_tensor = torch.from_numpy(obs).float()
            
            # è·å–åŠ¨ä½œ
            if step < 20:  # çŸ­æš‚warmup
                action = env.action_space.sample()
            else:
                try:
                    action = sac.get_action(obs_tensor, gnn_embed, num_joints=3, deterministic=False)
                    action = action.detach().cpu().numpy()
                except Exception as e:
                    print(f"   âš ï¸  åŠ¨ä½œç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œ: {e}")
                    action = env.action_space.sample()
            
            # æ‰§è¡Œæ­¥éª¤
            next_obs, reward, done, info = env.step(action)
            episode_reward += reward
            
            # è®°å½•è·ç¦»
            end_pos = env._get_end_effector_position()
            distance = np.linalg.norm(np.array(end_pos) - env.goal_pos)
            distance_history.append(distance)
            
            # å­˜å‚¨ç»éªŒ
            if step >= 10:  # å‡å°‘warmupæ—¶é—´
                next_obs_tensor = torch.from_numpy(next_obs).float()
                action_tensor = torch.from_numpy(action).float()
                
                try:
                    sac.store_experience(
                        obs_tensor, gnn_embed, action_tensor, reward,
                        next_obs_tensor, gnn_embed, done, num_joints=3
                    )
                except Exception as e:
                    print(f"   âš ï¸  ç»éªŒå­˜å‚¨å¤±è´¥: {e}")
            
            # æ›´æ–°å’Œåˆ†ææ¢¯åº¦
            if step > 20 and step % 4 == 0 and sac.memory.can_sample(sac.batch_size):
                try:
                    metrics = sac.update()
                    
                    if metrics:
                        loss_history['actor'].append(metrics.get('actor_loss', 0))
                        loss_history['critic'].append(metrics.get('critic_loss', 0))
                        loss_history['alpha'].append(metrics.get('alpha_loss', 0))
                        
                        # ç®€åŒ–æ¢¯åº¦åˆ†æ
                        actor_grad_norm = 0
                        for param in sac.actor.parameters():
                            if param.grad is not None:
                                actor_grad_norm += param.grad.norm().item() ** 2
                        
                        if actor_grad_norm > 0:
                            gradient_norms['actor'].append(np.sqrt(actor_grad_norm))
                
                except Exception as e:
                    print(f"   âš ï¸  æ¨¡å‹æ›´æ–°å¤±è´¥: {e}")
            
            obs = next_obs
            
            if done:
                reward_history.append(episode_reward)
                episode_reward = 0
                obs = env.reset()
            
            if step % 50 == 0:  # å‡å°‘æ‰“å°é¢‘ç‡
                print(f"   æ­¥éª¤ {step}: è·ç¦»={distance:.1f}px, ç¼“å†²åŒº={len(sac.memory)}")
        
        # åˆ†æç»“æœ
        self._analyze_gradient_statistics(gradient_norms, loss_history, distance_history, reward_history)
        env.close()
    
    def _analyze_gradient_statistics(self, gradient_norms, loss_history, distance_history, reward_history):
        """åˆ†ææ¢¯åº¦ç»Ÿè®¡æ•°æ®"""
        print(f"\nğŸ“Š æ¢¯åº¦åˆ†æç»“æœ:")
        
        # æ¢¯åº¦èŒƒæ•°åˆ†æ
        for network, norms in gradient_norms.items():
            if norms:
                avg_norm = np.mean(norms)
                std_norm = np.std(norms)
                print(f"   {network} å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_norm:.6f} Â± {std_norm:.6f}")
                
                if avg_norm < 1e-6:
                    print(f"   âš ï¸  {network} æ¢¯åº¦è¿‡å°ï¼Œå¯èƒ½å­¦ä¹ åœæ»")
                elif avg_norm > 1.0:
                    print(f"   âš ï¸  {network} æ¢¯åº¦è¿‡å¤§ï¼Œå¯èƒ½ä¸ç¨³å®š")
                else:
                    print(f"   âœ… {network} æ¢¯åº¦èŒƒæ•°æ­£å¸¸")
        
        # æŸå¤±åˆ†æ
        print(f"\nğŸ“ˆ æŸå¤±è¶‹åŠ¿åˆ†æ:")
        for loss_type, losses in loss_history.items():
            if losses and len(losses) > 5:
                recent_losses = losses[-10:] if len(losses) > 10 else losses
                if len(recent_losses) > 1:
                    trend = np.polyfit(range(len(recent_losses)), recent_losses, 1)[0]
                    
                    if abs(trend) < 1e-6:
                        trend_desc = "å¹³ç¨³"
                    elif trend < 0:
                        trend_desc = "ä¸‹é™ âœ…"
                    else:
                        trend_desc = "ä¸Šå‡ âš ï¸"
                    
                    print(f"   {loss_type}_loss: æœ€è¿‘è¶‹åŠ¿ {trend_desc} (æ–œç‡: {trend:.6f})")
        
        # æ€§èƒ½åˆ†æ
        if distance_history and len(distance_history) > 20:
            initial_dist = np.mean(distance_history[:10])
            final_dist = np.mean(distance_history[-10:])
            improvement = initial_dist - final_dist
            
            print(f"\nğŸ¯ æ€§èƒ½æ”¹å–„åˆ†æ:")
            print(f"   åˆå§‹è·ç¦»: {initial_dist:.1f}px")
            print(f"   æœ€ç»ˆè·ç¦»: {final_dist:.1f}px")
            print(f"   æ”¹å–„ç¨‹åº¦: {improvement:.1f}px")
            
            if improvement > 20:
                print(f"   âœ… æ˜¾è‘—æ”¹å–„")
                learning_verdict = "æ­£å¸¸å­¦ä¹ "
            elif improvement > 5:
                print(f"   âš ï¸  è½»å¾®æ”¹å–„")
                learning_verdict = "å­¦ä¹ ç¼“æ…¢"
            else:
                print(f"   âŒ æ— æ˜æ˜¾æ”¹å–„")
                learning_verdict = "å­¦ä¹ åœæ»"
        else:
            print(f"   âš ï¸  æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æ")
            learning_verdict = "æ•°æ®ä¸è¶³"
            improvement = 0
        
        self.analysis_results['learning_progress'] = {
            'distance_improvement': improvement,
            'verdict': learning_verdict
        }
    
    def analyze_action_diversity_fixed(self):
        """å®Œå…¨ä¿®å¤ç‰ˆåŠ¨ä½œå¤šæ ·æ€§åˆ†æ"""
        print(f"\nğŸ” === åŠ¨ä½œå¤šæ ·æ€§åˆ†æ ===")
        
        try:
            # åˆ›å»ºæ¨¡å‹
            attn_model = AttnModel(128, 130, 130, 4)
            sac = AttentionSACWithBuffer(attn_model, 3, buffer_capacity=1000, batch_size=32, lr=1e-4, env_type='reacher2d')
            
            # ğŸ”§ å…³é”®ä¿®å¤3: æ­£ç¡®çš„GNNåµŒå…¥å’Œè§‚å¯Ÿç©ºé—´
            gnn_embed = torch.randn(3, 128)  # 3å…³èŠ‚ï¼Œæ¯ä¸ª128ç»´
            
            # æ”¶é›†åŠ¨ä½œæ ·æœ¬
            actions = []
            dummy_obs = torch.zeros(8)  # reacher2dè§‚å¯Ÿç©ºé—´æ˜¯8ç»´
            
            for i in range(100):  # å‡å°‘æ ·æœ¬æ•°é‡ä»¥åŠ å¿«æµ‹è¯•
                try:
                    action = sac.get_action(dummy_obs, gnn_embed, num_joints=3, deterministic=False)
                    actions.append(action.detach().cpu().numpy())
                except Exception as e:
                    if i == 0:  # åªæ‰“å°ç¬¬ä¸€æ¬¡é”™è¯¯
                        print(f"   âš ï¸  åŠ¨ä½œç”Ÿæˆå¤±è´¥: {e}")
                    # ä½¿ç”¨éšæœºåŠ¨ä½œä½œä¸ºæ›¿ä»£
                    action = np.random.uniform(-100, 100, 3)
                    actions.append(action)
            
            actions = np.array(actions)
            
            # åˆ†æå¤šæ ·æ€§
            action_std = np.std(actions, axis=0)
            action_range = np.max(actions, axis=0) - np.min(actions, axis=0)
            action_mean = np.mean(actions, axis=0)
            
            print(f"ğŸ“Š åŠ¨ä½œç»Ÿè®¡:")
            for i in range(3):
                print(f"   å…³èŠ‚{i+1}: å‡å€¼={action_mean[i]:+.1f}, æ ‡å‡†å·®={action_std[i]:.1f}, èŒƒå›´={action_range[i]:.1f}")
            
            # å¤šæ ·æ€§è¯„ä¼°
            avg_std = np.mean(action_std)
            if avg_std < 5:
                print(f"   âš ï¸  åŠ¨ä½œå¤šæ ·æ€§ä¸è¶³ (å¹³å‡æ ‡å‡†å·®: {avg_std:.1f})")
                diversity_verdict = "å¤šæ ·æ€§ä¸è¶³"
            elif avg_std > 50:
                print(f"   âš ï¸  åŠ¨ä½œè¿‡äºéšæœº (å¹³å‡æ ‡å‡†å·®: {avg_std:.1f})")
                diversity_verdict = "è¿‡äºéšæœº"
            else:
                print(f"   âœ… åŠ¨ä½œå¤šæ ·æ€§åˆé€‚ (å¹³å‡æ ‡å‡†å·®: {avg_std:.1f})")
                diversity_verdict = "åˆé€‚"
            
            self.analysis_results['action_diversity'] = {
                'avg_std': avg_std,
                'verdict': diversity_verdict
            }
            
        except Exception as e:
            print(f"   âŒ åŠ¨ä½œåˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.analysis_results['action_diversity'] = {
                'avg_std': 0,
                'verdict': "åˆ†æå¤±è´¥"
            }
    
    def analyze_learning_bottlenecks(self):
        """åˆ†æå­¦ä¹ ç“¶é¢ˆ"""
        print(f"\nğŸ” === å­¦ä¹ ç“¶é¢ˆåˆ†æ ===")
        
        bottlenecks = []
        
        # æ£€æŸ¥æ¨¡å‹å¤æ‚åº¦ç“¶é¢ˆ
        if self.analysis_results.get('model_complexity', {}).get('verdict') == 'è¿‡äºå¤æ‚':
            bottlenecks.append("æ¨¡å‹è¿‡äºå¤æ‚ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
        elif self.analysis_results.get('model_complexity', {}).get('verdict') == 'è¿‡äºç®€å•':
            bottlenecks.append("æ¨¡å‹è¿‡äºç®€å•ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ")
        
        # æ£€æŸ¥å­¦ä¹ è¿›åº¦ç“¶é¢ˆ
        learning_verdict = self.analysis_results.get('learning_progress', {}).get('verdict')
        if learning_verdict == 'å­¦ä¹ åœæ»':
            bottlenecks.append("å­¦ä¹ åœæ»ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡æˆ–æ¢ç´¢ç­–ç•¥")
        elif learning_verdict == 'å­¦ä¹ ç¼“æ…¢':
            bottlenecks.append("å­¦ä¹ ç¼“æ…¢ï¼Œå»ºè®®å¢åŠ å­¦ä¹ ç‡æˆ–å‡å°‘æ‰¹æ¬¡å¤§å°")
        
        # æ£€æŸ¥åŠ¨ä½œå¤šæ ·æ€§ç“¶é¢ˆ
        diversity_verdict = self.analysis_results.get('action_diversity', {}).get('verdict')
        if diversity_verdict == 'å¤šæ ·æ€§ä¸è¶³':
            bottlenecks.append("æ¢ç´¢ä¸è¶³ï¼Œå»ºè®®å¢åŠ alphaå€¼æˆ–å‡å°‘warmupæ­¥æ•°")
        elif diversity_verdict == 'è¿‡äºéšæœº':
            bottlenecks.append("æ¢ç´¢è¿‡åº¦ï¼Œå»ºè®®å‡å°‘alphaå€¼")
        
        if bottlenecks:
            print("âŒ å‘ç°çš„ç“¶é¢ˆ:")
            for i, bottleneck in enumerate(bottlenecks, 1):
                print(f"   {i}. {bottleneck}")
        else:
            print("âœ… æœªå‘ç°æ˜æ˜¾ç“¶é¢ˆ")
        
        return bottlenecks
    
    def generate_optimization_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print(f"\nğŸ’¡ === ä¼˜åŒ–å»ºè®® ===")
        
        recommendations = []
        
        # ğŸ”§ æ·»åŠ ç»´åº¦ä¿®å¤å»ºè®®
        recommendations.append("ğŸ”§ ä¿®å¤ç»´åº¦é—®é¢˜: ç¡®ä¿GNNåµŒå…¥ä¸º[3, 128]è€Œä¸æ˜¯[1, 20, 128]")
        recommendations.append("ğŸ”§ ä¿®å¤è§‚å¯Ÿç©ºé—´: ç¡®ä¿obsä¸º8ç»´è€Œä¸æ˜¯å…¶ä»–ç»´åº¦")
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        model_complexity = self.analysis_results.get('model_complexity', {}).get('verdict')
        if model_complexity == 'è¿‡äºå¤æ‚':
            recommendations.append("å‡å°‘æ¨¡å‹å¤æ‚åº¦: AttnModel(64, 66, 66, 2)")
        elif model_complexity == 'è¿‡äºç®€å•':
            recommendations.append("å¢åŠ æ¨¡å‹å¤æ‚åº¦: AttnModel(256, 260, 260, 8)")
        
        learning_progress = self.analysis_results.get('learning_progress', {}).get('verdict')
        if learning_progress == 'å­¦ä¹ åœæ»':
            recommendations.append("å¢åŠ å­¦ä¹ ç‡: --lr 5e-4")
            recommendations.append("å‡å°‘æ‰¹æ¬¡å¤§å°: --batch-size 16")
            recommendations.append("å¢åŠ æ¢ç´¢: --alpha 0.4")
        elif learning_progress == 'å­¦ä¹ ç¼“æ…¢':
            recommendations.append("é€‚åº¦å¢åŠ å­¦ä¹ ç‡: --lr 2e-4")
            recommendations.append("å¢åŠ æ¢ç´¢: --alpha 0.25")
        
        action_diversity = self.analysis_results.get('action_diversity', {}).get('verdict')
        if action_diversity == 'å¤šæ ·æ€§ä¸è¶³':
            recommendations.append("å¢åŠ æ¢ç´¢: --alpha 0.4")
            recommendations.append("å‡å°‘warmup: --warmup-steps 1000")
        elif action_diversity == 'è¿‡äºéšæœº':
            recommendations.append("å‡å°‘æ¢ç´¢: --alpha 0.15")
            recommendations.append("å¢åŠ warmup: --warmup-steps 3000")
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "ä½¿ç”¨å¯è¾¾é…ç½®: ä¿®æ”¹enhanced_train.pyç¬¬54è¡Œä¸ºreacher_easy.yaml",
            "ç›‘æ§è®­ç»ƒ: æ¯1000æ­¥æ£€æŸ¥è·ç¦»å˜åŒ–",
            "æ—©æœŸåœæ­¢: è¿ç»­5000æ­¥æ— æ”¹å–„æ—¶åœæ­¢"
        ])
        
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        return recommendations
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹æ·±åº¦è®­ç»ƒåˆ†æ...")
        print("="*60)
        
        try:
            # 1. æ¨¡å‹å®¹é‡åˆ†æ
            self.analyze_model_capacity()
            
            # 2. ä¿®å¤ç‰ˆæ¢¯åº¦æµåŠ¨åˆ†æ
            self.analyze_gradient_flow_fixed(steps=100)  # è¿›ä¸€æ­¥å‡å°‘æ­¥æ•°
            
            # 3. ä¿®å¤ç‰ˆåŠ¨ä½œå¤šæ ·æ€§åˆ†æ
            self.analyze_action_diversity_fixed()
            
            # 4. ç“¶é¢ˆåˆ†æ
            bottlenecks = self.analyze_learning_bottlenecks()
            
            # 5. ä¼˜åŒ–å»ºè®®
            recommendations = self.generate_optimization_recommendations()
            
            print("\n" + "="*60)
            print("ğŸ“‹ === åˆ†ææ€»ç»“ ===")
            print(f"æ¨¡å‹å¤æ‚åº¦: {self.analysis_results.get('model_complexity', {}).get('verdict', 'æœªçŸ¥')}")
            print(f"å­¦ä¹ è¿›åº¦: {self.analysis_results.get('learning_progress', {}).get('verdict', 'æœªçŸ¥')}")
            print(f"åŠ¨ä½œå¤šæ ·æ€§: {self.analysis_results.get('action_diversity', {}).get('verdict', 'æœªçŸ¥')}")
            print(f"å‘ç°ç“¶é¢ˆ: {len(bottlenecks)}ä¸ª")
            print(f"ä¼˜åŒ–å»ºè®®: {len(recommendations)}æ¡")
            
            return {
                'bottlenecks': bottlenecks,
                'recommendations': recommendations,
                'analysis_results': self.analysis_results
            }
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return {
                'bottlenecks': ["åˆ†æå¤±è´¥"],
                'recommendations': ["ä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜"],
                'analysis_results': self.analysis_results
            }

def main():
    analyzer = TrainingAnalyzer()
    results = analyzer.run_full_analysis()
    
    # ä¿å­˜åˆ†æç»“æœ
    try:
        with open('training_analysis_results_fixed.json', 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: training_analysis_results_fixed.json")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜ç»“æœå¤±è´¥: {e}")

if __name__ == "__main__":
    main()