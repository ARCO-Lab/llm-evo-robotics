#!/usr/bin/env python3
"""
ä»è®­ç»ƒè¾“å‡ºä¸­æå–æŸå¤±æ•°æ®
å®æ—¶è§£æè®­ç»ƒè¾“å‡ºå¹¶è®°å½•æŸå¤±æ•°æ®
"""

import re
import os
import csv
import json
import time
from datetime import datetime
from collections import defaultdict

class LossExtractor:
    """ä»è®­ç»ƒè¾“å‡ºä¸­æå–æŸå¤±æ•°æ®"""
    
    def __init__(self, experiment_name, log_dir="extracted_loss_logs"):
        self.experiment_name = experiment_name
        self.log_dir = log_dir
        self.experiment_dir = os.path.join(log_dir, f"{experiment_name}_extracted_loss")
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.experiment_dir, exist_ok=True)
        
        # æŸå¤±æ•°æ®å­˜å‚¨
        self.loss_data = defaultdict(list)
        
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.patterns = {
            'ppo_update': re.compile(r'ğŸ”¥ PPOç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'actor_loss': re.compile(r'ğŸ“Š Actor Loss: ([\d\.-]+)'),
            'critic_loss': re.compile(r'ğŸ“Š Critic Loss: ([\d\.-]+)'),
            'total_loss': re.compile(r'ğŸ“Š æ€»Loss: ([\d\.-]+)'),
            'entropy': re.compile(r'ğŸ­ Entropy: ([\d\.-]+)'),
            'learning_rate': re.compile(r'ğŸ“ˆ å­¦ä¹ ç‡: ([\d\.-]+e?[\d\.-]*)'),
            'update_count': re.compile(r'ğŸ”„ æ›´æ–°æ¬¡æ•°: (\d+)'),
            'buffer_size': re.compile(r'ğŸ’¾ Bufferå¤§å°: (\d+)')
        }
        
        print(f"ğŸ“Š æŸå¤±æå–å™¨åˆå§‹åŒ–")
        print(f"   å®éªŒåç§°: {experiment_name}")
        print(f"   æ—¥å¿—ç›®å½•: {self.experiment_dir}")
        
    def extract_from_line(self, line, step_context=None):
        """ä»å•è¡Œæ–‡æœ¬ä¸­æå–æŸå¤±æ•°æ®"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯PPOæ›´æ–°è¡Œ
        step_match = self.patterns['ppo_update'].search(line)
        if step_match:
            return int(step_match.group(1))
        
        # å¦‚æœæœ‰æ­¥æ•°ä¸Šä¸‹æ–‡ï¼Œæå–æŸå¤±å€¼
        if step_context is not None:
            losses = {}
            
            for loss_type, pattern in self.patterns.items():
                if loss_type == 'ppo_update':
                    continue
                    
                match = pattern.search(line)
                if match:
                    try:
                        value = float(match.group(1))
                        losses[loss_type] = value
                    except ValueError:
                        pass
            
            if losses:
                self.record_loss(step_context, losses)
                
        return None
        
    def record_loss(self, step, losses):
        """è®°å½•æŸå¤±æ•°æ®"""
        timestamp = time.time()
        
        entry = {
            'step': step,
            'timestamp': timestamp,
            'datetime': datetime.now().isoformat(),
            **losses
        }
        
        self.loss_data['ppo'].append(entry)
        actor_loss = losses.get('actor_loss', 'N/A')
        critic_loss = losses.get('critic_loss', 'N/A')
        actor_str = f"{actor_loss:.3f}" if isinstance(actor_loss, (int, float)) else str(actor_loss)
        critic_str = f"{critic_loss:.3f}" if isinstance(critic_loss, (int, float)) else str(critic_loss)
        print(f"ğŸ“Š è®°å½•PPOæŸå¤± [Step {step}]: Actor={actor_str}, Critic={critic_str}")
        
    def save_data(self):
        """ä¿å­˜æŸå¤±æ•°æ®"""
        if not self.loss_data:
            print("âš ï¸ æ²¡æœ‰æŸå¤±æ•°æ®å¯ä¿å­˜")
            return
            
        for network, data in self.loss_data.items():
            if data:
                # ä¿å­˜CSVæ–‡ä»¶
                csv_path = os.path.join(self.experiment_dir, f"{network}_losses.csv")
                
                with open(csv_path, 'w', newline='') as csvfile:
                    if data:
                        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„å­—æ®µå
                        all_fieldnames = set()
                        for entry in data:
                            all_fieldnames.update(entry.keys())
                        fieldnames = sorted(list(all_fieldnames))
                        
                        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                        writer.writeheader()
                        writer.writerows(data)
                
                print(f"ğŸ’¾ ä¿å­˜ {network} æŸå¤±æ•°æ®: {len(data)} æ¡è®°å½• -> {csv_path}")
                
                # ä¿å­˜JSONæ–‡ä»¶
                json_path = os.path.join(self.experiment_dir, f"{network}_losses.json")
                with open(json_path, 'w') as jsonfile:
                    json.dump(data, jsonfile, indent=2)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = self.get_statistics()
        stats_path = os.path.join(self.experiment_dir, "loss_statistics.json")
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"ğŸ“ˆ æŸå¤±ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_path}")
        
    def get_statistics(self):
        """è·å–æŸå¤±ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for network, data in self.loss_data.items():
            if data:
                # æå–æ•°å€¼æ•°æ®
                numeric_fields = {}
                for entry in data:
                    for key, value in entry.items():
                        if isinstance(value, (int, float)) and key not in ['step', 'timestamp']:
                            if key not in numeric_fields:
                                numeric_fields[key] = []
                            numeric_fields[key].append(value)
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                network_stats = {}
                for field, values in numeric_fields.items():
                    if values:
                        network_stats[field] = {
                            'count': len(values),
                            'min': min(values),
                            'max': max(values),
                            'avg': sum(values) / len(values),
                            'first': values[0],
                            'last': values[-1],
                            'trend': 'decreasing' if values[-1] < values[0] else 'increasing'
                        }
                
                stats[network] = network_stats
        
        return stats

def extract_from_file(log_file_path, experiment_name):
    """ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–æŸå¤±æ•°æ®"""
    extractor = LossExtractor(experiment_name)
    
    current_step = None
    
    try:
        with open(log_file_path, 'r') as f:
            for line in f:
                line = line.strip()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æ­¥æ•°
                step = extractor.extract_from_line(line)
                if step is not None:
                    current_step = step
                
                # æå–æŸå¤±æ•°æ®
                extractor.extract_from_line(line, current_step)
        
        # ä¿å­˜æ•°æ®
        extractor.save_data()
        
        print("âœ… æŸå¤±æå–å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")

if __name__ == "__main__":
    # æµ‹è¯•æå–å™¨
    print("ğŸ§ª æµ‹è¯•æŸå¤±æå–å™¨")
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¾“å‡º
    test_lines = [
        "ğŸ”¥ PPOç½‘ç»œLossæ›´æ–° [Step 1063]:",
        "   ğŸ“Š Actor Loss: 0.357086",
        "   ğŸ“Š Critic Loss: 19.341845", 
        "   ğŸ“Š æ€»Loss: 19.698931",
        "   ğŸ­ Entropy: -0.405246",
        "   ğŸ“ˆ å­¦ä¹ ç‡: 8.24e-06",
        "ğŸ”¥ PPOç½‘ç»œLossæ›´æ–° [Step 1127]:",
        "   ğŸ“Š Actor Loss: 0.367207",
        "   ğŸ“Š Critic Loss: 35.931519",
        "   ğŸ“Š æ€»Loss: 36.298727",
        "   ğŸ­ Entropy: -0.405081"
    ]
    
    extractor = LossExtractor("test_extract")
    
    current_step = None
    for line in test_lines:
        step = extractor.extract_from_line(line)
        if step is not None:
            current_step = step
        extractor.extract_from_line(line, current_step)
    
    extractor.save_data()
    print("âœ… æµ‹è¯•å®Œæˆ")
