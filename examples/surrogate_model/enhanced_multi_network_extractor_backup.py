#!/usr/bin/env python3
"""
å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨
æ”¯æŒå®žæ—¶æå–attentionã€GNNã€PPOç­‰æ‰€æœ‰ç½‘ç»œçš„æŸå¤±æ•°æ®
"""

import os
import sys
import subprocess
import threading
import time
import re
import json
import csv
import random
from datetime import datetime
from collections import defaultdict

class EnhancedMultiNetworkExtractor:
    """å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨"""
    
    def __init__(self, experiment_name, log_dir="enhanced_multi_network_logs"):
        self.experiment_name = experiment_name
        
        # ðŸ”§ ç¡®ä¿æ—¥å¿—ä¿å­˜åœ¨æ­£ç¡®çš„ä½ç½®ï¼ˆsurrogate_modelç›®å½•ä¸‹ï¼‰
        script_dir = os.path.dirname(os.path.abspath(__file__))
        self.log_dir = os.path.join(script_dir, log_dir)
        self.experiment_dir = os.path.join(self.log_dir, f"{experiment_name}_multi_network_loss")
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.experiment_dir, exist_ok=True)
        
        # ðŸ†• IndividualæˆåŠŸæ¬¡æ•°è®°å½•ï¼ˆç‹¬ç«‹æ–‡ä»¶ï¼‰
        self.individual_success_files = {}  # individual_id -> æ–‡ä»¶è·¯å¾„
        self.current_individual_success = 0  # å½“å‰individualçš„æˆåŠŸæ¬¡æ•°
        
        # åªè®°å½•çœŸå®žå­˜åœ¨çš„ç½‘ç»œæŸå¤±æ•°æ®
        self.loss_data = {
            'sac': [],           # SACç½‘ç»œæœ‰çœŸå®žæŸå¤±è¾“å‡º
            'performance': []    # æ€§èƒ½æŒ‡æ ‡æœ‰çœŸå®žè¾“å‡ºï¼ˆæˆåŠŸçŽ‡ã€è·ç¦»ç­‰ï¼‰
            # æ³¨æ„ï¼šåªæœ‰åœ¨è®­ç»ƒè¾“å‡ºä¸­çœŸå®žå­˜åœ¨æ—¶æ‰ä¼šåŠ¨æ€æ·»åŠ å…¶ä»–ç½‘ç»œ
        }
        self.running = False
        
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ - åªåŒ¹é…çœŸå®žå­˜åœ¨çš„è¾“å‡º
        self.patterns = {
            # SACç½‘ç»œæŸå¤±ï¼ˆçœŸå®žå­˜åœ¨ï¼‰
            'sac_update': re.compile(r'ðŸ”¥ SACç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'actor_loss': re.compile(r'ðŸ“Š Actor Loss: ([\d\.-]+)'),
            'critic_loss': re.compile(r'ðŸ“Š Critic Loss: ([\d\.-]+)'),
            'sac_total_loss': re.compile(r'ðŸ“Š æ€»Loss: ([\d\.-]+)'),
            'alpha_loss': re.compile(r'ðŸ“Š Alpha Loss: ([\d\.-]+)'),
            'alpha': re.compile(r'ðŸ“Š Alpha: ([\d\.-]+)'),
            'learning_rate': re.compile(r'ðŸ“ˆ å­¦ä¹ çŽ‡: ([\d\.-]+e?[\d\.-]*)'),
            'update_count': re.compile(r'ðŸ”„ æ›´æ–°æ¬¡æ•°: (\d+)'),
            'buffer_size': re.compile(r'ðŸ’¾ Bufferå¤§å°: (\d+)'),
            'q1_mean': re.compile(r'ðŸ“Š Q1å‡å€¼: ([\d\.-]+)'),
            'q2_mean': re.compile(r'ðŸ“Š Q2å‡å€¼: ([\d\.-]+)'),
            'q1_std': re.compile(r'ðŸ“Š Q1æ ‡å‡†å·®: ([\d\.-]+)'),
            'q2_std': re.compile(r'ðŸ“Š Q2æ ‡å‡†å·®: ([\d\.-]+)'),
            'entropy_term': re.compile(r'ðŸ“Š ç†µé¡¹: ([\d\.-]+)'),
            'q_term': re.compile(r'ðŸ“Š Qå€¼é¡¹: ([\d\.-]+)'),
            
            # æ€§èƒ½æŒ‡æ ‡ï¼ˆçœŸå®žå­˜åœ¨ï¼‰
            'success_rate': re.compile(r'âœ… å½“å‰æˆåŠŸçŽ‡: ([\d\.]+)%'),
            'best_distance': re.compile(r'ðŸ† å½“å‰æœ€ä½³è·ç¦»: ([\d\.]+)px'),
            'episode_best_distance': re.compile(r'ðŸ“Š å½“å‰Episodeæœ€ä½³è·ç¦»: ([\d\.]+)px'),
            'consecutive_success': re.compile(r'ðŸ”„ è¿žç»­æˆåŠŸæ¬¡æ•°: (\d+)'),
            'completed_episodes': re.compile(r'ðŸ“‹ å·²å®ŒæˆEpisodes: (\d+)'),
            'training_progress_report': re.compile(r'ðŸ“Š PPOè®­ç»ƒè¿›åº¦æŠ¥å‘Š \[Step (\d+)\]'),
            
            # ðŸ†• æˆåŠŸäº‹ä»¶æ£€æµ‹ï¼ˆç‹¬ç«‹è®°å½•ï¼‰
            'goal_reached': re.compile(r'ðŸŽ¯ \[DEBUG\] åˆ°è¾¾ç›®æ ‡ä½†éœ€ç»§ç»­ç»´æŒï¼Œç»§ç»­å½“å‰.*episode'),
            'goal_reached_with_distance': re.compile(r'ðŸŽ¯ åˆ°è¾¾ç›®æ ‡! è·ç¦»: ([\d\.]+)pxï¼Œè¿›å…¥ä¸‹ä¸€ä¸ªepisode'),
            
            # çœŸå®žçš„attentionç½‘ç»œæŸå¤±æ¨¡å¼ï¼ˆçŽ°åœ¨å·²å®žçŽ°ï¼‰
            'attention_update': re.compile(r'ðŸ”¥ Attentionç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            # ðŸ†• ç‹¬ç«‹çš„attentionç½‘ç»œlossæ¨¡å¼
            'attention_actor_loss': re.compile(r'ðŸ“Š Actor Attention Loss: ([\d\.-]+)'),
            'attention_critic_main_loss': re.compile(r'ðŸ“Š Critic Main Attention Loss: ([\d\.-]+)'),
            'attention_critic_value_loss': re.compile(r'ðŸ“Š Critic Value Attention Loss: ([\d\.-]+)'),
            
            # æ¢¯åº¦èŒƒæ•°ä¿¡æ¯
            'attention_actor_grad_norm': re.compile(r'ðŸ” Actor Attentionæ¢¯åº¦èŒƒæ•°: ([\d\.-]+)'),
            'attention_critic_main_grad_norm': re.compile(r'ðŸ” Critic Main Attentionæ¢¯åº¦èŒƒæ•°: ([\d\.-]+)'),
            'attention_critic_value_grad_norm': re.compile(r'ðŸ” Critic Value Attentionæ¢¯åº¦èŒƒæ•°: ([\d\.-]+)'),
            'attention_total_loss': re.compile(r'ðŸ“Š Attentionæ€»æŸå¤±: ([\d\.-]+)'),
            'attention_param_mean': re.compile(r'ðŸ“Š Attentionå‚æ•°å‡å€¼: ([\d\.-]+)'),
            'attention_param_std': re.compile(r'ðŸ“Š Attentionå‚æ•°æ ‡å‡†å·®: ([\d\.-]+)'),
            
            # ðŸ†• åˆ†ç¦»çš„Actorå’ŒCritic attentionç½‘ç»œå‚æ•°
            'attention_actor_param_mean': re.compile(r'ðŸ“Š Actor Attentionå‚æ•°: å‡å€¼=([\d\.-]+), æ ‡å‡†å·®=([\d\.-]+)'),
            'attention_critic_param_mean': re.compile(r'ðŸ“Š Critic Attentionå‚æ•°: å‡å€¼=([\d\.-]+), æ ‡å‡†å·®=([\d\.-]+)'),
            
            # ðŸ†• å…³èŠ‚æ³¨æ„åŠ›åˆ†å¸ƒæ¨¡å¼
            'most_attended_joint': re.compile(r'ðŸŽ¯ æœ€å…³æ³¨å…³èŠ‚: Joint (\d+)'),
            'most_important_joint': re.compile(r'ðŸŽ¯ æœ€é‡è¦å…³èŠ‚: Joint (\d+)'),
            'max_joint_importance': re.compile(r'æœ€é‡è¦å…³èŠ‚: Joint \d+ \(é‡è¦æ€§: ([\d\.-]+)\)'),
            'importance_concentration': re.compile(r'ðŸ“Š é‡è¦æ€§é›†ä¸­åº¦: ([\d\.-]+)'),
            'importance_entropy': re.compile(r'ðŸ“Š é‡è¦æ€§ç†µå€¼: ([\d\.-]+)'),
            'robot_num_joints': re.compile(r'ðŸ¤– æœºå™¨äººç»“æž„: (\d+)å…³èŠ‚'),
            'robot_structure_info': re.compile(r'ðŸ¤– æœºå™¨äººç»“æž„: \d+å…³èŠ‚ \((.+?)\)'),
            'joint_usage_ranking': re.compile(r'ðŸ† å…³èŠ‚ä½¿ç”¨æŽ’å: (.+)'),
            
            # ðŸ†• åŠ¨æ€å…³èŠ‚æ•°æ®æ¨¡å¼ï¼ˆæ”¯æŒä»»æ„å…³èŠ‚æ•°ï¼‰
            'joint_activity': re.compile(r'ðŸ” å…³èŠ‚æ´»è·ƒåº¦: (.+)'),
            'joint_importance': re.compile(r'ðŸŽ¯ å…³èŠ‚é‡è¦æ€§: (.+)'),  # ðŸ†• æ·»åŠ å…³èŠ‚é‡è¦æ€§æ¨¡å¼
            'joint_angles': re.compile(r'ðŸ“ å…³èŠ‚è§’åº¦å¹…åº¦: (.+)'),
            'joint_velocities': re.compile(r'âš¡ å…³èŠ‚é€Ÿåº¦å¹…åº¦: (.+)'),
            'link_lengths': re.compile(r'ðŸ“ Linké•¿åº¦: (.+)'),
            
            # GNNç½‘ç»œæŸå¤±æ¨¡å¼ï¼ˆå¦‚æžœå°†æ¥å®žçŽ°ï¼‰
            'gnn_update': re.compile(r'ðŸ”¥ GNNç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'gnn_loss': re.compile(r'ðŸ“Š GNN Loss: ([\d\.-]+)'),
            'node_accuracy': re.compile(r'ðŸ“Š èŠ‚ç‚¹å‡†ç¡®çŽ‡: ([\d\.-]+)'),
            
            # PPOç½‘ç»œæŸå¤±æ¨¡å¼ï¼ˆå¦‚æžœå°†æ¥å®žçŽ°ï¼‰
            'ppo_update': re.compile(r'ðŸ”¥ PPOç½‘ç»œLossæ›´æ–° \[Step (\d+)\]:'),
            'ppo_critic_loss': re.compile(r'ðŸ“Š PPO Critic Loss: ([\d\.-]+)'),
            'ppo_actor_loss': re.compile(r'ðŸ“Š PPO Actor Loss: ([\d\.-]+)'),
            'ppo_total_loss': re.compile(r'ðŸ“Š PPOæ€»Loss: ([\d\.-]+)'),
            'entropy': re.compile(r'ðŸŽ­ Entropy: ([\d\.-]+)'),
            
            # ðŸ†• ä¸ªä½“å’Œä»£æ•°ä¿¡æ¯æå–
            'individual_evaluation': re.compile(r'ðŸ§¬ è¯„ä¼°ä¸ªä½“ (.+)'),
            'individual_id_setting': re.compile(r'ðŸ†” è®¾ç½®Individual ID: (.+)'),
            'generation_info': re.compile(r'ç¬¬(\d+)ä»£'),
        }
        
        # å½“å‰æŸå¤±æ•°æ®ç¼“å­˜
        self.current_step = None
        self.current_network = 'sac'
        self.current_losses = {}
        
        # æ€§èƒ½æŒ‡æ ‡ç¼“å­˜
        self.current_performance = {}
        
        # ðŸ†• å½“å‰ä¸ªä½“å’Œä»£æ•°ä¿¡æ¯
        self.current_individual_id = None
        self.current_generation = 0
        self.individual_count = 0
        self.individuals_per_generation = 10  # é»˜è®¤å€¼ï¼Œå¯ä»¥ä»Žå‘½ä»¤è¡Œå‚æ•°èŽ·å–
        
        print(f"ðŸ“Š çœŸå®žæ•°æ®æŸå¤±æå–å™¨åˆå§‹åŒ–")
        print(f"   å®žéªŒåç§°: {experiment_name}")
        print(f"   æ—¥å¿—ç›®å½•: {self.experiment_dir}")
        print(f"   ðŸŽ¯ åªè®°å½•çœŸå®žå­˜åœ¨çš„ç½‘ç»œæŸå¤±ï¼Œç»ä¸ç”Ÿæˆå‡æ•°æ®")
        print(f"   ðŸ“Š å½“å‰æ”¯æŒ: SACç½‘ç»œæŸå¤± + Individual Reacheræ€§èƒ½æŒ‡æ ‡")
        
    def start_training_with_extraction(self, training_command):
        """å¯åŠ¨è®­ç»ƒå¹¶å®žæ—¶æå–å¤šç½‘ç»œæŸå¤±"""
        print(f"ðŸš€ å¯åŠ¨è®­ç»ƒå¹¶å®žæ—¶æå–å¤šç½‘ç»œæŸå¤±...")
        print(f"   è®­ç»ƒå‘½ä»¤: {' '.join(training_command)}")
        
        self.running = True
        
        try:
            # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
            process = subprocess.Popen(
                training_command,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            print(f"âœ… è®­ç»ƒè¿›ç¨‹å·²å¯åŠ¨ (PID: {process.pid})")
            
            # å¯åŠ¨è‡ªåŠ¨ä¿å­˜çº¿ç¨‹
            save_thread = threading.Thread(target=self._auto_save_loop, daemon=True)
            save_thread.start()
            
            # ä¸å†å¯åŠ¨æ¨¡æ‹ŸæŸå¤±ç”Ÿæˆçº¿ç¨‹ - åªè®°å½•çœŸå®žæ•°æ®
            
            # å®žæ—¶è¯»å–å¹¶å¤„ç†è¾“å‡º
            for line in process.stdout:
                if not self.running:
                    break
                    
                # æ˜¾ç¤ºè®­ç»ƒè¾“å‡º
                print(f"[è®­ç»ƒ] {line.rstrip()}")
                
                # å®žæ—¶æå–æŸå¤±æ•°æ®
                self._process_line(line.strip())
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            process.wait()
            
            print(f"ðŸ è®­ç»ƒè¿›ç¨‹ç»“æŸï¼Œè¿”å›žç : {process.returncode}")
            
        except KeyboardInterrupt:
            print("\nðŸ›‘ æŽ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·...")
            if 'process' in locals():
                process.terminate()
                process.wait()
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        finally:
            self.running = False
            # æœ€ç»ˆä¿å­˜æ•°æ®
            self._save_all_data()
            print("ðŸ§¹ å¢žå¼ºç‰ˆæå–å™¨å·²åœæ­¢")
    
    def _process_line(self, line):
        """å¤„ç†å•è¡Œè¾“å‡ºï¼Œåªæå–çœŸå®žå­˜åœ¨çš„æŸå¤±æ•°æ®"""
        
        # ðŸ†• é¦–å…ˆæ£€æŸ¥ä¸ªä½“å’Œä»£æ•°ä¿¡æ¯
        individual_match = self.patterns['individual_evaluation'].search(line)
        if individual_match:
            self.current_individual_id = individual_match.group(1).strip()
            self.individual_count += 1
            
            # æ ¹æ®ä¸ªä½“è®¡æ•°æŽ¨ç®—generationï¼ˆæ¯10ä¸ªä¸ªä½“ä¸€ä»£ï¼‰
            self.current_generation = (self.individual_count - 1) // self.individuals_per_generation
            
            print(f"   ðŸ“‹ æ£€æµ‹åˆ°ä¸ªä½“: {self.current_individual_id}")
            print(f"   ðŸ“Š ä¸ªä½“è®¡æ•°: {self.individual_count}, æŽ¨ç®—ä»£æ•°: {self.current_generation}")
            return
        
        # ðŸ†• æ£€æŸ¥æˆåŠŸäº‹ä»¶ï¼ˆç‹¬ç«‹è®°å½•åˆ°individualä¸“ç”¨æ–‡ä»¶ï¼‰
        goal_reached_match = self.patterns['goal_reached'].search(line)
        goal_reached_with_distance_match = self.patterns['goal_reached_with_distance'].search(line)
        
        if goal_reached_match or goal_reached_with_distance_match:
            # å¦‚æžœæœ‰è·ç¦»ä¿¡æ¯å°±ç”¨ï¼Œå¦åˆ™è®¾ä¸º0
            if goal_reached_with_distance_match:
                distance = float(goal_reached_with_distance_match.group(1))
            else:
                distance = 0.0  # DEBUGæ ¼å¼æ²¡æœ‰è·ç¦»ä¿¡æ¯
                
            self.current_individual_success += 1
            
            print(f"   ðŸŽ‰ æ£€æµ‹åˆ°æˆåŠŸäº‹ä»¶! è·ç¦»: {distance}px")
            print(f"   ðŸ“Š Individual {self.current_individual_id} æˆåŠŸæ¬¡æ•°: {self.current_individual_success}")
            
            # è®°å½•åˆ°individualä¸“ç”¨æ–‡ä»¶
            self._record_individual_success(distance)
            return
        
        # ðŸ†• æ£€æŸ¥Individual IDè®¾ç½®
        individual_id_match = self.patterns['individual_id_setting'].search(line)
        if individual_id_match:
            new_individual_id = individual_id_match.group(1).strip()
            
            # å¦‚æžœæ˜¯æ–°çš„individualï¼Œé‡ç½®æˆåŠŸè®¡æ•°
            if self.current_individual_id != new_individual_id:
                self.current_individual_success = 0
                print(f"   ðŸ”„ åˆ‡æ¢åˆ°æ–°Individual: {new_individual_id}")
            
            self.current_individual_id = new_individual_id
            print(f"   ðŸ†” æ£€æµ‹åˆ°Individual IDè®¾ç½®: {self.current_individual_id}")
            return
        
        generation_match = self.patterns['generation_info'].search(line)
        if generation_match:
            self.current_generation = int(generation_match.group(1))
            print(f"   ðŸ“‹ æ£€æµ‹åˆ°ä»£æ•°: {self.current_generation}")
            return
        
        # æ£€æŸ¥SACç½‘ç»œæ›´æ–°ï¼ˆå”¯ä¸€ç¡®è®¤å­˜åœ¨çš„çœŸå®žç½‘ç»œæŸå¤±ï¼‰
        step_match = self.patterns['sac_update'].search(line)
        if step_match:
            # ä¿å­˜ä¹‹å‰çš„æ•°æ®
            if self.current_step is not None and self.current_losses:
                self._record_current_loss()
            
            # å¼€å§‹æ–°çš„SACæŸå¤±è®°å½•
            self.current_step = int(step_match.group(1))
            self.current_network = 'sac'
            self.current_losses = {}
            return
        
        # æ£€æŸ¥å…¶ä»–ç½‘ç»œæ›´æ–°ï¼ˆå¦‚æžœè®­ç»ƒè¾“å‡ºä¸­çœŸå®žå­˜åœ¨ï¼‰
        for network_type in ['attention', 'gnn', 'ppo']:
            update_pattern = f'{network_type}_update'
            if update_pattern in self.patterns:
                step_match = self.patterns[update_pattern].search(line)
                if step_match:
                    # ä¿å­˜ä¹‹å‰çš„æ•°æ®
                    if self.current_step is not None and self.current_losses:
                        self._record_current_loss()
                    
                    # å¼€å§‹æ–°çš„ç½‘ç»œæŸå¤±è®°å½•
                    self.current_step = int(step_match.group(1))
                    self.current_network = network_type
                    self.current_losses = {}
                    
                    # åŠ¨æ€æ·»åŠ ç½‘ç»œåˆ°æ•°æ®å­˜å‚¨
                    if network_type not in self.loss_data:
                        self.loss_data[network_type] = []
                        print(f"   ðŸŽ¯ æ£€æµ‹åˆ°çœŸå®ž{network_type.upper()}ç½‘ç»œæŸå¤±ï¼Œå¼€å§‹è®°å½•")
                    return
        
        # æ£€æŸ¥è®­ç»ƒè¿›åº¦æŠ¥å‘Š
        progress_match = self.patterns['training_progress_report'].search(line)
        if progress_match:
            # ä¿å­˜ä¹‹å‰çš„æ€§èƒ½æ•°æ®
            if self.current_performance:
                self._record_performance_metrics()
            
            # å¼€å§‹æ–°çš„æ€§èƒ½æŠ¥å‘Š
            self.current_performance = {'report_step': int(progress_match.group(1))}
            return
        
        # æå–æ€§èƒ½æŒ‡æ ‡
        performance_extracted = False
        for perf_type in ['success_rate', 'best_distance', 'episode_best_distance', 
                         'consecutive_success', 'completed_episodes']:
            if perf_type in self.patterns:
                match = self.patterns[perf_type].search(line)
                if match:
                    try:
                        value = float(match.group(1))
                        self.current_performance[perf_type] = value
                        print(f"   ðŸ“Š æå–åˆ°æ€§èƒ½æŒ‡æ ‡ {perf_type}: {value}")
                        performance_extracted = True
                    except ValueError:
                        pass
        
        # æ£€æŸ¥æ˜¯å¦æ”¶é›†åˆ°å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚æžœæ˜¯åˆ™è®°å½•
        if ('report_step' in self.current_performance and 
            len(self.current_performance) >= 4):  # è‡³å°‘æœ‰report_step + 3ä¸ªæ€§èƒ½æŒ‡æ ‡
            self._record_performance_metrics()
        
        # æå–çœŸå®žæŸå¤±å€¼
        if self.current_step is not None:
            # æ ¹æ®å½“å‰ç½‘ç»œç±»åž‹æå–å¯¹åº”çš„æŸå¤±å€¼
            loss_patterns_to_check = []
            
            if self.current_network == 'sac':
                loss_patterns_to_check = ['actor_loss', 'critic_loss', 'sac_total_loss', 'alpha_loss', 'alpha',
                                        'learning_rate', 'update_count', 'buffer_size', 'q1_mean', 'q2_mean',
                                        'q1_std', 'q2_std', 'entropy_term', 'q_term']
            elif self.current_network == 'ppo':
                loss_patterns_to_check = ['ppo_actor_loss', 'ppo_critic_loss', 'ppo_total_loss', 'entropy', 
                                        'learning_rate', 'update_count', 'buffer_size']
            elif self.current_network == 'attention':
                loss_patterns_to_check = [
                    # ðŸ†• ç‹¬ç«‹çš„attentionç½‘ç»œloss
                    'attention_actor_loss', 'attention_critic_main_loss', 'attention_critic_value_loss',
                    'attention_total_loss',
                    # æ¢¯åº¦èŒƒæ•°
                    'attention_actor_grad_norm', 'attention_critic_main_grad_norm', 'attention_critic_value_grad_norm',
                    # å‚æ•°ç»Ÿè®¡
                    'attention_param_mean', 'attention_param_std',
                    'attention_actor_param_mean', 'attention_actor_param_std',
                    'attention_critic_param_mean', 'attention_critic_param_std',
                    # å…³èŠ‚åˆ†æž
                    'most_important_joint', 'max_joint_importance', 'importance_concentration',
                    'importance_entropy', 'robot_num_joints', 'robot_structure_info',
                    'joint_usage_ranking', 'joint_activity', 'joint_importance', 'joint_angles', 
                    'joint_velocities', 'link_lengths'
                ]
            elif self.current_network == 'gnn':
                loss_patterns_to_check = ['gnn_loss', 'node_accuracy']
            
            for loss_type in loss_patterns_to_check:
                if loss_type in self.patterns:
                    match = self.patterns[loss_type].search(line)
                    if match:
                        try:
                            # ç‰¹æ®Šå¤„ç†å…³èŠ‚åˆ†å¸ƒå­—ç¬¦ä¸²
                            if loss_type in ['joint_activity', 'joint_importance', 'joint_angles', 'joint_velocities', 'link_lengths', 'joint_usage_ranking']:
                                distribution_str = match.group(1)
                                # è§£æžå…³èŠ‚åˆ†å¸ƒå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "J0:1.000, J1:1.000, J2:1.000"
                                joint_values = self._parse_joint_distribution(distribution_str, loss_type)
                                self.current_losses.update(joint_values)
                                print(f"   ðŸŽ¯ æå–çœŸå®ž{self.current_network.upper()} {loss_type}: {distribution_str[:50]}...")
                            elif loss_type in ['robot_structure_info']:
                                # å­—ç¬¦ä¸²ç±»åž‹æ•°æ®
                                value_str = match.group(1)
                                self.current_losses[loss_type] = value_str
                                print(f"   ðŸŽ¯ æå–çœŸå®ž{self.current_network.upper()} {loss_type}: {value_str}")
                            elif loss_type in ['attention_actor_param_mean', 'attention_critic_param_mean']:
                                # ç‰¹æ®Šå¤„ç†ï¼šæå–å‡å€¼å’Œæ ‡å‡†å·®
                                mean_value = float(match.group(1))
                                std_value = float(match.group(2))
                                if loss_type == 'attention_actor_param_mean':
                                    self.current_losses['attention_actor_param_mean'] = mean_value
                                    self.current_losses['attention_actor_param_std'] = std_value
                                    print(f"   ðŸŽ¯ æå–çœŸå®ž{self.current_network.upper()} Actorå‚æ•°: å‡å€¼={mean_value:.6f}, æ ‡å‡†å·®={std_value:.6f}")
                                else:
                                    self.current_losses['attention_critic_param_mean'] = mean_value
                                    self.current_losses['attention_critic_param_std'] = std_value
                                    print(f"   ðŸŽ¯ æå–çœŸå®ž{self.current_network.upper()} Criticå‚æ•°: å‡å€¼={mean_value:.6f}, æ ‡å‡†å·®={std_value:.6f}")
                            else:
                                value = float(match.group(1))
                                self.current_losses[loss_type] = value
                                print(f"   ðŸŽ¯ æå–çœŸå®ž{self.current_network.upper()} {loss_type}: {value}")
                        except ValueError:
                            pass
    
    def _record_current_loss(self):
        """è®°å½•å½“å‰æ­¥éª¤çš„æŸå¤±æ•°æ®"""
        if not self.current_losses:
            return
            
        timestamp = time.time()
        
        entry = {
            'step': self.current_step,
            'timestamp': timestamp,
            'datetime': datetime.now().isoformat(),
            'generation': self.current_generation,
            'individual_id': self.current_individual_id,
            **self.current_losses
        }
        
        # æ ¹æ®ç½‘ç»œç±»åž‹è®°å½•
        self.loss_data[self.current_network].append(entry)
        
        # æ˜¾ç¤ºè®°å½•çš„æ•°æ®
        self._display_recorded_loss()
        
        # æ¸…ç©ºå½“å‰ç¼“å­˜
        self.current_losses = {}
    
    def _record_performance_metrics(self):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡æ•°æ®"""
        if not self.current_performance:
            return
            
        timestamp = time.time()
        
        entry = {
            'step': self.current_performance.get('report_step', 0),
            'timestamp': timestamp,
            'datetime': datetime.now().isoformat(),
            'generation': self.current_generation,
            'individual_id': self.current_individual_id,
            **self.current_performance
        }
        
        self.loss_data['performance'].append(entry)
        
        # æ˜¾ç¤ºè®°å½•çš„æ€§èƒ½æ•°æ®
        success_rate = self.current_performance.get('success_rate', 'N/A')
        best_distance = self.current_performance.get('best_distance', 'N/A')
        consecutive_success = self.current_performance.get('consecutive_success', 'N/A')
        completed_episodes = self.current_performance.get('completed_episodes', 'N/A')
        
        print(f"ðŸ“Š âœ… è®°å½•æ€§èƒ½æŒ‡æ ‡ [Step {self.current_performance.get('report_step', 0)}]:")
        print(f"     æˆåŠŸçŽ‡: {success_rate}%, æœ€ä½³è·ç¦»: {best_distance}px")
        print(f"     è¿žç»­æˆåŠŸ: {consecutive_success}, å®ŒæˆEpisodes: {completed_episodes}")
        
        # æ¸…ç©ºæ€§èƒ½ç¼“å­˜
        self.current_performance = {}
    
    def _record_individual_success(self, distance):
        """è®°å½•individualçš„æˆåŠŸäº‹ä»¶åˆ°ä¸“ç”¨æ–‡ä»¶"""
        if not self.current_individual_id:
            return
            
        # ä¸ºindividualåˆ›å»ºä¸“ç”¨æˆåŠŸè®°å½•æ–‡ä»¶
        if self.current_individual_id not in self.individual_success_files:
            # ðŸ”§ IndividualæˆåŠŸè®°å½•ä¹Ÿä¿å­˜åœ¨åŒä¸€ä¸ªå®žéªŒç›®å½•ä¸‹
            success_file = os.path.join(self.experiment_dir, f"individual_{self.current_individual_id}_success.csv")
            self.individual_success_files[self.current_individual_id] = success_file
            
            # åˆ›å»ºæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
            with open(success_file, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['step', 'timestamp', 'datetime', 'distance_to_goal', 'success_count', 'individual_id', 'generation'])
            
            print(f"   ðŸ“ ä¸ºIndividual {self.current_individual_id} åˆ›å»ºæˆåŠŸè®°å½•æ–‡ä»¶: {success_file}")
        
        # è®°å½•æˆåŠŸäº‹ä»¶
        success_file = self.individual_success_files[self.current_individual_id]
        timestamp = time.time()
        
        with open(success_file, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                self.current_step if self.current_step is not None else 0,
                timestamp,
                datetime.now().isoformat(),
                distance,
                self.current_individual_success,
                self.current_individual_id,
                self.current_generation
            ])
        
        print(f"   ðŸ’¾ æˆåŠŸäº‹ä»¶å·²è®°å½•åˆ°Individualä¸“ç”¨æ–‡ä»¶")
    
    def _display_recorded_loss(self):
        """æ˜¾ç¤ºè®°å½•çš„çœŸå®žæŸå¤±æ•°æ®"""
        if self.current_network == 'sac':
            actor_loss = self.current_losses.get('actor_loss', 'N/A')
            critic_loss = self.current_losses.get('critic_loss', 'N/A')
            alpha_loss = self.current_losses.get('alpha_loss', 'N/A')
            alpha = self.current_losses.get('alpha', 'N/A')
            q1_mean = self.current_losses.get('q1_mean', 'N/A')
            q2_mean = self.current_losses.get('q2_mean', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•çœŸå®žSACæŸå¤± [Step {self.current_step}]:")
            print(f"     Actor: {actor_loss}, Critic: {critic_loss}, Alpha: {alpha_loss}")
            print(f"     Alphaå€¼: {alpha}, Q1å‡å€¼: {q1_mean}, Q2å‡å€¼: {q2_mean}")
            
        elif self.current_network == 'ppo':
            actor_loss = self.current_losses.get('ppo_actor_loss', 'N/A')
            critic_loss = self.current_losses.get('ppo_critic_loss', 'N/A')
            total_loss = self.current_losses.get('ppo_total_loss', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•çœŸå®žPPOæŸå¤± [Step {self.current_step}]:")
            print(f"     Actor: {actor_loss}, Critic: {critic_loss}, Total: {total_loss}")
            
        elif self.current_network == 'attention':
            actor_grad = self.current_losses.get('attention_actor_grad_norm', 'N/A')
            critic_grad = self.current_losses.get('attention_critic_grad_norm', 'N/A')
            total_loss = self.current_losses.get('attention_total_loss', 'N/A')
            most_attended = self.current_losses.get('most_attended_joint', 'N/A')
            concentration = self.current_losses.get('attention_concentration', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•çœŸå®žAttentionæŸå¤± [Step {self.current_step}]:")
            print(f"     Actoræ¢¯åº¦: {actor_grad}, Criticæ¢¯åº¦: {critic_grad}, æ€»æŸå¤±: {total_loss}")
            print(f"     ðŸŽ¯ æœ€å…³æ³¨å…³èŠ‚: Joint {most_attended}, é›†ä¸­åº¦: {concentration}")
            
        elif self.current_network == 'gnn':
            gnn_loss = self.current_losses.get('gnn_loss', 'N/A')
            node_acc = self.current_losses.get('node_accuracy', 'N/A')
            print(f"ðŸ“Š âœ… è®°å½•çœŸå®žGNNæŸå¤± [Step {self.current_step}]:")
            print(f"     Loss: {gnn_loss}, Node Acc: {node_acc}")
            
    
    def _parse_joint_distribution(self, distribution_str, data_type):
        """è§£æžå…³èŠ‚åˆ†å¸ƒå­—ç¬¦ä¸² - æ”¯æŒå¤šç§æ•°æ®ç±»åž‹"""
        joint_values = {}
        
        try:
            # è§£æžæ ¼å¼: "J0:1.000, J1:1.000, J2:1.000" æˆ– "L0:80.0px, L1:70.0px"
            parts = distribution_str.split(', ')
            for part in parts:
                if ':' in part:
                    joint_name, value_str = part.split(':')
                    joint_id = joint_name.strip()  # ä¾‹å¦‚ "J0" æˆ– "L0"
                    
                    # æ¸…ç†æ•°å€¼å­—ç¬¦ä¸²ï¼ˆç§»é™¤pxç­‰å•ä½ï¼‰
                    clean_value_str = value_str.strip().replace('px', '').replace('Â°', '')
                    value = float(clean_value_str)
                    
                    # æ ¹æ®æ•°æ®ç±»åž‹è®¾ç½®å­—æ®µå
                    if data_type == 'joint_activity':
                        joint_values[f'{joint_id}_activity'] = value
                    elif data_type == 'joint_importance':
                        joint_values[f'{joint_id}_importance'] = value
                    elif data_type == 'joint_angles':
                        joint_values[f'{joint_id}_angle_magnitude'] = value
                    elif data_type == 'joint_velocities':
                        joint_values[f'{joint_id}_velocity_magnitude'] = value
                    elif data_type == 'link_lengths':
                        joint_values[f'{joint_id}_length'] = value
                    elif data_type == 'joint_usage_ranking':
                        joint_values[f'{joint_id}_usage_rank'] = value
                    else:
                        joint_values[f'{joint_id}_attention'] = value
                        
        except Exception as e:
            joint_values[f'{data_type}_parse_error'] = str(e)
        
        return joint_values
    
    def _auto_save_loop(self):
        """è‡ªåŠ¨ä¿å­˜å¾ªçŽ¯"""
        while self.running:
            time.sleep(30)  # æ¯30ç§’ä¿å­˜ä¸€æ¬¡
            if any(self.loss_data.values()):
                self._save_all_data()
                print("ðŸ’¾ è‡ªåŠ¨ä¿å­˜çœŸå®žæŸå¤±æ•°æ®å®Œæˆ")
    
    def _save_all_data(self):
        """ä¿å­˜æ‰€æœ‰ç½‘ç»œçš„æŸå¤±æ•°æ®"""
        # å…ˆè®°å½•å½“å‰æœªå®Œæˆçš„æŸå¤±
        if self.current_step is not None and self.current_losses:
            self._record_current_loss()
        
        saved_networks = []
        
        for network, data in self.loss_data.items():
            if data:
                # ä¿å­˜CSVæ–‡ä»¶
                csv_path = os.path.join(self.experiment_dir, f"{network}_losses.csv")
                
                with open(csv_path, 'w', newline='') as csvfile:
                    # ðŸ†• å¯¹äºŽattentionç½‘ç»œï¼Œå¼ºåˆ¶åŒ…å«20ä¸ªå…³èŠ‚çš„ç»Ÿä¸€æ ¼å¼
                    if network == 'attention':
                        # å¼ºåˆ¶åŒ…å«æ‰€æœ‰20ä¸ªå…³èŠ‚å’Œlinkçš„å­—æ®µ
                        base_fieldnames = set()
                        for entry in data:
                            base_fieldnames.update(entry.keys())
                        
                        # ðŸ†• å¼ºåˆ¶åŒ…å«ç‹¬ç«‹çš„attention losså­—æ®µ
                        base_fieldnames.add('attention_actor_loss')
                        base_fieldnames.add('attention_critic_main_loss')
                        base_fieldnames.add('attention_critic_value_loss')
                        base_fieldnames.add('attention_critic_main_grad_norm')
                        
                        # æ·»åŠ 20ä¸ªå…³èŠ‚çš„æ‰€æœ‰å­—æ®µ
                        for i in range(20):
                            base_fieldnames.add(f'joint_{i}_activity')
                            base_fieldnames.add(f'joint_{i}_importance') 
                            base_fieldnames.add(f'joint_{i}_angle_magnitude')
                            base_fieldnames.add(f'joint_{i}_velocity_magnitude')
                            base_fieldnames.add(f'link_{i}_length')
                        
                        fieldnames = sorted(list(base_fieldnames))
                        
                        # ç¡®ä¿æ‰€æœ‰æ•°æ®æ¡ç›®éƒ½åŒ…å«ç‹¬ç«‹attention losså­—æ®µå’Œ20ä¸ªå…³èŠ‚çš„å­—æ®µ
                        for entry in data:
                            # ðŸ†• ç¡®ä¿ç‹¬ç«‹attention losså­—æ®µå­˜åœ¨ï¼ˆä¸å­˜åœ¨çš„å¡«0.0ï¼‰
                            if 'attention_actor_loss' not in entry:
                                entry['attention_actor_loss'] = 0.0
                            if 'attention_critic_main_loss' not in entry:
                                entry['attention_critic_main_loss'] = 0.0
                            if 'attention_critic_value_loss' not in entry:
                                entry['attention_critic_value_loss'] = 0.0
                            if 'attention_critic_main_grad_norm' not in entry:
                                entry['attention_critic_main_grad_norm'] = 0.0
                            
                            # ç¡®ä¿20ä¸ªå…³èŠ‚çš„å­—æ®µå­˜åœ¨ï¼ˆä¸å­˜åœ¨çš„å¡«-1ï¼‰
                            for i in range(20):
                                if f'joint_{i}_activity' not in entry:
                                    entry[f'joint_{i}_activity'] = -1.0
                                if f'joint_{i}_importance' not in entry:
                                    entry[f'joint_{i}_importance'] = -1.0
                                if f'joint_{i}_angle_magnitude' not in entry:
                                    entry[f'joint_{i}_angle_magnitude'] = -1.0
                                if f'joint_{i}_velocity_magnitude' not in entry:
                                    entry[f'joint_{i}_velocity_magnitude'] = -1.0
                                if f'link_{i}_length' not in entry:
                                    entry[f'link_{i}_length'] = -1.0
                    else:
                        # å…¶ä»–ç½‘ç»œä½¿ç”¨åŽŸæœ‰çš„åŠ¨æ€å­—æ®µæ”¶é›†
                        all_fieldnames = set()
                        for entry in data:
                            all_fieldnames.update(entry.keys())
                        fieldnames = sorted(list(all_fieldnames))
                    
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(data)
                
                # ä¿å­˜JSONæ–‡ä»¶
                json_path = os.path.join(self.experiment_dir, f"{network}_losses.json")
                with open(json_path, 'w') as jsonfile:
                    json.dump(data, jsonfile, indent=2)
                
                saved_networks.append(network)
                print(f"ðŸ’¾ ä¿å­˜çœŸå®ž {network.upper()} æ•°æ®: {len(data)} æ¡è®°å½•")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        if saved_networks:
            stats = self._get_comprehensive_statistics()
            stats_path = os.path.join(self.experiment_dir, "comprehensive_loss_statistics.json")
            with open(stats_path, 'w') as f:
                json.dump(stats, f, indent=2)
            
            print(f"ðŸ“ˆ çœŸå®žæŸå¤±ç»Ÿè®¡å·²ä¿å­˜: {len(saved_networks)} ä¸ªç½‘ç»œ")
    
    def _get_comprehensive_statistics(self):
        """èŽ·å–æ‰€æœ‰ç½‘ç»œçš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'experiment_info': {
                'experiment_name': self.experiment_name,
                'data_type': 'real_only',  # æ ‡æ˜Žè¿™æ˜¯çœŸå®žæ•°æ®
                'total_networks': len([n for n, d in self.loss_data.items() if d]),
                'total_records': sum(len(d) for d in self.loss_data.values()),
                'generation_time': datetime.now().isoformat(),
                'note': 'Only real loss data from training output, no simulated data'
            },
            'network_stats': {}
        }
        
        for network, data in self.loss_data.items():
            if data:
                # æå–æ•°å€¼æ•°æ®
                numeric_fields = {}
                for entry in data:
                    for key, value in entry.items():
                        if isinstance(value, (int, float)) and key not in ['step', 'timestamp']:
                            if key not in numeric_fields:
                                numeric_fields[key] = []
                            numeric_fields[key].append(value)
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                network_stats = {}
                for field, values in numeric_fields.items():
                    if values:
                        network_stats[field] = {
                            'count': len(values),
                            'min': min(values),
                            'max': max(values),
                            'avg': sum(values) / len(values),
                            'first': values[0],
                            'last': values[-1],
                            'trend': 'decreasing' if len(values) > 1 and values[-1] < values[0] else 'increasing',
                            'std': self._calculate_std(values) if len(values) > 1 else 0
                        }
                
                stats['network_stats'][network] = {
                    'total_records': len(data),
                    'metrics': network_stats
                }
        
        return stats
    
    def _calculate_std(self, values):
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(values) <= 1:
            return 0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5

def run_enhanced_multi_network_training(experiment_name, mode='basic', training_steps=2000, 
                                        num_generations=None, individuals_per_generation=None, **kwargs):
    """è¿è¡Œå¢žå¼ºç‰ˆå¤šç½‘ç»œè®­ç»ƒ"""
    
    # åˆ›å»ºå¢žå¼ºç‰ˆæå–å™¨
    extractor = EnhancedMultiNetworkExtractor(experiment_name)
    
    # ðŸ†• è®¾ç½®æ¯ä»£ä¸ªä½“æ•°ï¼Œç”¨äºŽgenerationè®¡ç®—
    if individuals_per_generation is not None:
        extractor.individuals_per_generation = individuals_per_generation
    
    # æž„å»ºè®­ç»ƒå‘½ä»¤ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    training_script = os.path.join(script_dir, 'map_elites_with_loss_logger.py')
    
    training_command = [
        sys.executable,
        training_script,
        '--mode', mode,
        '--experiment-name', experiment_name,
        '--training-steps-per-individual', str(training_steps)
    ]
    
    # æ·»åŠ é¢å¤–çš„MAP-Eliteså‚æ•°
    if num_generations is not None:
        training_command.extend(['--num-generations', str(num_generations)])
    
    if individuals_per_generation is not None:
        training_command.extend(['--individuals-per-generation', str(individuals_per_generation)])
    
    # æ·»åŠ å…¶ä»–å‚æ•°
    for key, value in kwargs.items():
        if value is not None:
            if isinstance(value, bool):
                if value:  # åªæœ‰Trueæ—¶æ‰æ·»åŠ æ ‡å¿—
                    training_command.append(f'--{key.replace("_", "-")}')
            else:
                training_command.extend([f'--{key.replace("_", "-")}', str(value)])
    
    # è®¾ç½®çŽ¯å¢ƒå˜é‡
    os.environ['LOSS_EXPERIMENT_NAME'] = experiment_name
    
    # å¯åŠ¨è®­ç»ƒå¹¶æå–
    extractor.start_training_with_extraction(training_command)
    
    return extractor.experiment_dir

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨')
    parser.add_argument('--experiment-name', type=str, required=True, help='å®žéªŒåç§°')
    parser.add_argument('--mode', type=str, default='basic', help='è®­ç»ƒæ¨¡å¼')
    parser.add_argument('--training-steps', type=int, default=2000, help='æ¯ä¸ªä¸ªä½“è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--num-generations', type=int, help='è¿›åŒ–ä»£æ•°')
    parser.add_argument('--individuals-per-generation', type=int, help='æ¯ä»£ä¸ªä½“æ•°')
    parser.add_argument('--enable-rendering', action='store_true', help='å¯ç”¨çŽ¯å¢ƒæ¸²æŸ“')
    parser.add_argument('--silent-mode', action='store_true', help='é™é»˜æ¨¡å¼')
    parser.add_argument('--use-genetic-fitness', action='store_true', help='ä½¿ç”¨é—ä¼ ç®—æ³•fitness')
    
    args = parser.parse_args()
    
    print("ðŸŽ¯ å¢žå¼ºç‰ˆå¤šç½‘ç»œæŸå¤±æå–å™¨")
    print("=" * 60)
    print(f"å®žéªŒåç§°: {args.experiment_name}")
    print(f"è®­ç»ƒæ¨¡å¼: {args.mode}")
    print(f"æ¯ä¸ªä½“è®­ç»ƒæ­¥æ•°: {args.training_steps}")
    if args.num_generations:
        print(f"è¿›åŒ–ä»£æ•°: {args.num_generations}")
    if args.individuals_per_generation:
        print(f"æ¯ä»£ä¸ªä½“æ•°: {args.individuals_per_generation}")
    
    try:
        # å‡†å¤‡é¢å¤–å‚æ•°
        extra_kwargs = {}
        if args.enable_rendering:
            extra_kwargs['enable_rendering'] = True
        if args.silent_mode:
            extra_kwargs['silent_mode'] = True
        if args.use_genetic_fitness:
            extra_kwargs['use_genetic_fitness'] = True
        
        log_dir = run_enhanced_multi_network_training(
            experiment_name=args.experiment_name, 
            mode=args.mode, 
            training_steps=args.training_steps,
            num_generations=args.num_generations,
            individuals_per_generation=args.individuals_per_generation,
            **extra_kwargs
        )
        
        print(f"\nðŸŽ‰ çœŸå®žæŸå¤±æå–å®Œæˆï¼")
        print(f"ðŸ“ çœŸå®žæŸå¤±æ•°æ®ä¿å­˜åœ¨: {log_dir}")
        print(f"ðŸ“Š åªåŒ…å«è®­ç»ƒè¾“å‡ºä¸­çœŸå®žå­˜åœ¨çš„ç½‘ç»œæŸå¤±ï¼Œæ— ä»»ä½•æ¨¡æ‹Ÿæ•°æ®")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ å¤šç½‘ç»œæŸå¤±æå–è¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å¤šç½‘ç»œæŸå¤±æå–å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
