import torch
import torch.nn.functional as F
def prepare_joint_q_input(obs_batch, gnn_embeds, num_joints):
    """
    obs_batch: Tensor of shape [B, 40]  â†’ B ä¸ªæœºå™¨äººä¸ªä½“
    gnn_embeds: Tensor of shape [B, 12, D] â†’ æ¯ä¸ªä¸ªä½“çš„æ¯ä¸ªå…³èŠ‚çš„ GNN embedding
    Returns:
        joint_q_input: Tensor of shape [B, 12, 2 + D]
    """
    joint_q_input = []

    for b in range(obs_batch.size(0)):
        obs = obs_batch[b]
        joint_pos_start = 16
        joint_pos_end = joint_pos_start + num_joints
        joint_vel_start = joint_pos_end  
        joint_vel_end = joint_vel_start + num_joints
        
        joint_pos = obs[joint_pos_start:joint_pos_end]  # [num_joints]
        joint_vel = obs[joint_vel_start:joint_vel_end]  # [num_joints]
        gnn_embed = gnn_embeds[b]  # [num_joints, D]

        # Expand joint_pos and joint_vel to [12, 1]
        pos = joint_pos.unsqueeze(1)
        vel = joint_vel.unsqueeze(1)
        # print(pos.shape, vel.shape, gnn_embed.shape)
        # Concatenate: [12, 1] + [12, 1] + [12, D] â†’ [12, 2 + D]
        q_input = torch.cat([pos, vel, gnn_embed], dim=1)
        joint_q_input.append(q_input)

    joint_q_input = torch.stack(joint_q_input, dim=0)  # [B, 12, 2 + D]
    return joint_q_input

def vertex_attention(vertex_k, vertex_v, joint_q, mask=None):
    """
    vertex_k: [B, N, D_k]          
    vertex_v: [B, N, D_v]          
    joint_q : [B, J, H, D_k]       
    """
    B, N, D_k = vertex_k.shape
    _, J, H, _ = joint_q.shape
    _, _, D_v = vertex_v.shape

    # é‡å¡‘ä¸ºä¾¿äºè®¡ç®—çš„å½¢çŠ¶
    k = vertex_k.view(B, 1, 1, N, D_k).expand(B, J, H, N, D_k)
    v = vertex_v.view(B, 1, 1, N, D_v).expand(B, J, H, N, D_v)
    q = joint_q.view(B, J, H, 1, D_k).expand(B, J, H, N, D_k)

    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    attn_scores = torch.sum(q * k, dim=-1)  # [B, J, H, N]
    
    if mask is not None:
        mask_expanded = mask.view(B, 1, 1, N)
        attn_scores = attn_scores.masked_fill(~mask_expanded, float('-inf'))

    # è®¡ç®—æ³¨æ„åŠ›æƒé‡å¹¶åº”ç”¨
    attn_weights = F.softmax(attn_scores, dim=-1)  # [B, J, H, N]
    attn_weights_expanded = attn_weights.view(B, J, H, N, 1)  # [B, J, H, N, 1]
    weighted_values = attn_weights_expanded * v  # [B, J, H, N, D_v]
    output = torch.sum(weighted_values, dim=3)   # [B, J, H, D_v]
    
    return output

def create_vertex_mask(vertex_lengths, max_vertices):
    """
    vertex_lengths: List[int] of length B (çœŸå®çš„èŠ‚ç‚¹æ•°é‡)
    max_vertices: intï¼Œæœ€å¤§èŠ‚ç‚¹æ•°é‡ï¼Œç”¨äºpadding
    Returns:
        mask: Tensor of shape [B, 1, 1, N]ï¼Œbool ç±»å‹ï¼ŒTrue è¡¨ç¤ºä½ç½®éœ€è¦maskæ‰
    """
    batch_size = len(vertex_lengths)
    mask = torch.zeros(batch_size, max_vertices, dtype=torch.bool)
    
    for i, l in enumerate(vertex_lengths):
        if l < max_vertices:
            mask[i, l:] = True  # å°† padding çš„éƒ¨åˆ†æ ‡è®°ä¸º Trueï¼ˆå³è¢« mask æ‰ï¼‰

    # reshape æˆä¸º [B, 1, 1, N]ï¼Œæ–¹ä¾¿å¹¿æ’­åˆ° multi-head attention
    return mask.unsqueeze(1).unsqueeze(1)  # [B, 1, 1, N]


def prepare_reacher2d_joint_q_input(obs_batch, gnn_embeds, num_joints):
    """
    ç›´æ¥é€‚é… Reacher2Dï¼Œä¸éœ€è¦å¡«å……åˆ°12å…³èŠ‚
    
    Args:
        obs_batch: Tensor [B, 2*num_joints + 2] 
        gnn_embeds: Tensor [B, N, 128]
        num_joints: int â†’ å®é™…å…³èŠ‚æ•° (3, 4, 5)
    
    Returns:
        joint_q_input: Tensor [B, num_joints, 130] ğŸ‘ˆ åŠ¨æ€ç»´åº¦ï¼
    """
    joint_q_input = []
    
    for b in range(obs_batch.size(0)):
        obs = obs_batch[b]
        
        # æå– Reacher2D æ•°æ®
        joint_angles = obs[:num_joints]                    # [num_joints]
        joint_angular_vels = obs[num_joints:2*num_joints]  # [num_joints]
        # end_effector_pos = obs[2*num_joints:2*num_joints+2]  # æš‚ä¸ä½¿ç”¨
        
        # è·å– GNN embedding
        gnn_embed = gnn_embeds[b]  # [N, 128]
        
        # ğŸ¯ å…³é”®ï¼šåªå¤„ç†å®é™…å…³èŠ‚æ•°ï¼Œä¸å¡«å……
        if gnn_embed.size(0) > num_joints:
            gnn_embed = gnn_embed[:num_joints]  # å–å‰num_jointsä¸ª
        elif gnn_embed.size(0) < num_joints:
            # ç”¨é›¶å‘é‡å¡«å……åˆ°num_joints
            padding_size = num_joints - gnn_embed.size(0)
            padding = torch.zeros(padding_size, gnn_embed.size(1), 
                                dtype=gnn_embed.dtype, device=gnn_embed.device)
            gnn_embed = torch.cat([gnn_embed, padding], dim=0)
        
        # æ„å»ºè¾“å…¥: [num_joints, 130]
        pos = joint_angles.unsqueeze(1)      # [num_joints, 1] 
        vel = joint_angular_vels.unsqueeze(1) # [num_joints, 1]
        
        q_input = torch.cat([pos, vel, gnn_embed], dim=1)  # [num_joints, 130]
        joint_q_input.append(q_input)
    
    joint_q_input = torch.stack(joint_q_input, dim=0)  # [B, num_joints, 130]
    return joint_q_input