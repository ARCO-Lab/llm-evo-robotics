#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ”¹è¿›SACæµ‹è¯• - éªŒè¯ç†µæƒé‡è°ƒåº¦å’Œè¯¾ç¨‹å­¦ä¹ æ•ˆæœ
ä½¿ç”¨ç®€å•SACæ¨¡å‹ï¼Œé¿å…å¤æ‚çš„attentionæœºåˆ¶
"""

import sys
import os
import numpy as np
import torch
import time
from datetime import datetime

# æ·»åŠ è·¯å¾„
base_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.')
sys.path.extend([
    base_dir,
    os.path.join(base_dir, 'sac'),
    os.path.join(base_dir, '../2d_reacher/envs')
])

# å¯¼å…¥æ¨¡å—
from sac.simple_sac_model import SimpleSAC
from reacher2d_env_improved import ImprovedReacher2DEnv

class SimpleSACTester:
    """ç®€åŒ–ç‰ˆSACæµ‹è¯•å™¨ - ä¸“æ³¨äºéªŒè¯æ”¹è¿›æ–¹æ³•"""
    
    def __init__(self, num_joints=3, curriculum_stage=0):
        self.num_joints = num_joints
        self.curriculum_stage = curriculum_stage
        
        # åˆ›å»ºæ”¹è¿›çš„ç¯å¢ƒ
        self.env = ImprovedReacher2DEnv(
            num_links=num_joints,
            link_lengths=[90.0] * num_joints,
            curriculum_stage=curriculum_stage,
            debug_level='INFO'
        )
        
        # åˆ›å»ºç®€å•SACæ¨¡å‹
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # è®¡ç®—çŠ¶æ€ç»´åº¦
        state_dim = self.env.observation_space.shape[0]
        
        # åˆ›å»ºç®€å•SACæ¨¡å‹
        self.sac = SimpleSAC(
            state_dim=state_dim,
            action_dim=num_joints,
            buffer_capacity=5000,
            batch_size=64,
            lr=1e-4,
            gamma=0.99,
            tau=0.005,
            alpha=0.2,  # åˆå§‹alphaå€¼
            device=self.device
        )
        
        # ğŸ†• æ·»åŠ ç†µæƒé‡è°ƒåº¦åŠŸèƒ½
        self.sac.alpha_start = 0.2
        self.sac.alpha_end = 0.05
        self.sac.entropy_schedule_enabled = True
        self.sac.exploration_phase_ratio = 0.7
        
        print(f"ğŸ”„ ç†µæƒé‡è°ƒåº¦: å¯ç”¨ ({self.sac.alpha_start:.3f} â†’ {self.sac.alpha_end:.3f})")
        print(f"ğŸ“Š çŠ¶æ€ç»´åº¦: {state_dim}, åŠ¨ä½œç»´åº¦: {num_joints}")
        
        self.stats = {
            'episodes': 0,
            'total_steps': 0,
            'successful_episodes': 0,
            'maintain_times': [],
            'alpha_history': [],
            'reward_history': [],
            'curriculum_upgrades': 0,
            'exploration_phase_end': 0
        }
    
    def update_alpha_schedule(self, current_step, total_steps):
        """ğŸ†• æ›´æ–°ç†µæƒé‡è°ƒåº¦"""
        if not self.sac.entropy_schedule_enabled:
            return
            
        # è®¡ç®—è®­ç»ƒè¿›åº¦
        progress = min(current_step / (total_steps * self.sac.exploration_phase_ratio), 1.0)
        
        # çº¿æ€§è¡°å‡alpha
        old_alpha = self.sac.alpha
        scheduled_alpha = self.sac.alpha_start * (1 - progress) + self.sac.alpha_end * progress
        
        # æ›´æ–°alphaå€¼
        self.sac.alpha = scheduled_alpha
        
        # åŒæ­¥æ›´æ–°log_alpha
        if hasattr(self.sac, 'log_alpha'):
            self.sac.log_alpha.data.fill_(torch.log(torch.tensor(scheduled_alpha)).item())
        
        # è®°å½•é˜¶æ®µè½¬æ¢
        if progress >= 1.0 and self.stats['exploration_phase_end'] == 0:
            self.stats['exploration_phase_end'] = current_step
            print(f"ğŸ¯ è¿›å…¥ç¨³å®šé˜¶æ®µ! Alphaå›ºå®šåœ¨ {scheduled_alpha:.3f}")
        
        # æ¯100æ­¥è¾“å‡ºä¸€æ¬¡è°ƒåº¦ä¿¡æ¯
        if current_step % 100 == 0 and abs(float(old_alpha) - float(scheduled_alpha)) > 0.001:
            phase = "æ¢ç´¢é˜¶æ®µ" if progress < 1.0 else "ç¨³å®šé˜¶æ®µ"
            print(f"ğŸ”„ Alphaè°ƒåº¦æ›´æ–° [Step {current_step}]: {float(old_alpha):.3f} â†’ {float(scheduled_alpha):.3f} ({phase})")
            
        return scheduled_alpha
    
    def run_test(self, total_steps=1500, max_episodes=25):
        """è¿è¡Œç®€åŒ–ç‰ˆSACæµ‹è¯•"""
        print(f"\nğŸš€ å¼€å§‹ç®€åŒ–ç‰ˆæ”¹è¿›SACç»´æŒä»»åŠ¡æµ‹è¯•")
        print(f"ğŸ“Š å‚æ•°: æ€»æ­¥æ•°={total_steps}, æœ€å¤§episodes={max_episodes}")
        print(f"ğŸ“ åˆå§‹è¯¾ç¨‹é˜¶æ®µ: {self.curriculum_stage}")
        print("="*60)
        
        obs, info = self.env.reset()
        episode_reward = 0
        episode_steps = 0
        step = 0
        
        start_time = time.time()
        
        while step < total_steps and self.stats['episodes'] < max_episodes:
            # è·å–åŠ¨ä½œ
            if step < 100:  # åˆå§‹éšæœºæ¢ç´¢
                action = np.random.uniform(-0.3, 0.3, self.num_joints)
            else:
                # SACç­–ç•¥ï¼Œæ ¹æ®é˜¶æ®µè°ƒæ•´ç¡®å®šæ€§
                is_stable_phase = step > total_steps * self.sac.exploration_phase_ratio
                obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
                
                if is_stable_phase:
                    # ç¨³å®šé˜¶æ®µï¼šæ›´ç¡®å®šæ€§çš„åŠ¨ä½œ
                    action_tensor, _, mean = self.sac.actor.sample(obs_tensor)
                    action = mean.squeeze(0).detach().cpu().numpy()  # ä½¿ç”¨å‡å€¼è€Œä¸æ˜¯é‡‡æ ·
                else:
                    # æ¢ç´¢é˜¶æ®µï¼šæ­£å¸¸é‡‡æ ·
                    action_tensor, _, _ = self.sac.actor.sample(obs_tensor)
                    action = action_tensor.squeeze(0).detach().cpu().numpy()
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, terminated, truncated, info = self.env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            if step >= 100:
                obs_tensor = torch.FloatTensor(obs).to(self.device)
                next_obs_tensor = torch.FloatTensor(next_obs).to(self.device)
                action_tensor = torch.FloatTensor(action).to(self.device)
                reward_tensor = torch.FloatTensor([reward]).to(self.device)
                done_tensor = torch.FloatTensor([float(terminated or truncated)]).to(self.device)
                
                self.sac.memory.push(obs_tensor, action_tensor, reward_tensor, 
                                   next_obs_tensor, done_tensor)
            
            # æ›´æ–°ç½‘ç»œ
            if step >= 200 and step % 4 == 0 and self.sac.memory.can_sample(self.sac.batch_size):
                loss_info = self.sac.update()
                
                # æ›´æ–°ç†µæƒé‡è°ƒåº¦
                if step % 50 == 0:  # æ›´é¢‘ç¹çš„è°ƒåº¦æ›´æ–°
                    alpha = self.update_alpha_schedule(step, total_steps)
                    self.stats['alpha_history'].append(alpha)
                    
                    # è¾“å‡ºè®­ç»ƒçŠ¶æ€
                    if loss_info and step % 200 == 0:
                        print(f"ğŸ“Š [Step {step}] Actor Loss: {loss_info['actor_loss']:.4f}, "
                              f"Critic Loss: {loss_info['critic_loss']:.4f}, "
                              f"Alpha: {alpha:.3f}")
            
            episode_reward += reward
            episode_steps += 1
            step += 1
            
            # Episodeç»“æŸå¤„ç†
            if terminated or truncated or episode_steps >= 400:
                self.stats['episodes'] += 1
                self.stats['total_steps'] += episode_steps
                self.stats['reward_history'].append(episode_reward)
                
                # è®°å½•ç»´æŒæ—¶é—´
                if 'maintain_progress' in info:
                    maintain_time = int(info['maintain_progress'] * self.env.maintain_target_steps)
                    self.stats['maintain_times'].append(maintain_time)
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                if info.get('maintain_completed', False):
                    self.stats['successful_episodes'] += 1
                    print(f"ğŸ‰ Episode {self.stats['episodes']}: ç»´æŒä»»åŠ¡æˆåŠŸ! "
                          f"å¥–åŠ±: {episode_reward:.1f}, æ­¥æ•°: {episode_steps}")
                else:
                    max_maintain = max(self.stats['maintain_times'][-5:]) if self.stats['maintain_times'] else 0
                    print(f"ğŸ“Š Episode {self.stats['episodes']}: æœªå®Œæˆç»´æŒ "
                          f"(æœ€å¤§: {max_maintain}æ­¥/{self.env.maintain_target_steps}æ­¥) "
                          f"å¥–åŠ±: {episode_reward:.1f}")
                
                # ğŸ†• è¯¾ç¨‹å­¦ä¹ å‡çº§æ£€æŸ¥
                if self.stats['episodes'] % 8 == 0:
                    self.check_curriculum_upgrade()
                
                # é‡ç½®episode
                obs, info = self.env.reset()
                episode_reward = 0
                episode_steps = 0
            else:
                obs = next_obs
        
        # æµ‹è¯•ç»“æŸï¼Œè¾“å‡ºç»Ÿè®¡
        self.print_final_stats(time.time() - start_time)
    
    def check_curriculum_upgrade(self):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å‡çº§è¯¾ç¨‹"""
        if self.stats['episodes'] < 8:
            return
            
        # æ£€æŸ¥æœ€è¿‘8ä¸ªepisodeçš„æˆåŠŸç‡
        recent_episodes = min(8, len(self.stats['maintain_times']))
        if recent_episodes < 4:
            return
        
        recent_maintains = self.stats['maintain_times'][-recent_episodes:]
        recent_success_rate = sum(1 for m in recent_maintains 
                                 if m >= self.env.maintain_target_steps) / recent_episodes
        
        # å¦‚æœæˆåŠŸç‡ >= 50% ä¸”å½“å‰ä¸æ˜¯æœ€é«˜é˜¶æ®µï¼Œåˆ™å‡çº§
        if recent_success_rate >= 0.5 and self.curriculum_stage < 2:
            old_stage = self.curriculum_stage
            old_threshold = self.env.maintain_threshold
            old_target = self.env.maintain_target_steps
            
            self.curriculum_stage += 1
            self.env.set_curriculum_stage(self.curriculum_stage)
            self.stats['curriculum_upgrades'] += 1
            
            print(f"\nğŸ“ è¯¾ç¨‹å‡çº§! {old_stage} â†’ {self.curriculum_stage}")
            print(f"   æœ€è¿‘æˆåŠŸç‡: {recent_success_rate:.1%}")
            print(f"   è¦æ±‚å˜åŒ–: {old_threshold}px/{old_target}æ­¥ â†’ {self.env.maintain_threshold}px/{self.env.maintain_target_steps}æ­¥\n")
    
    def print_final_stats(self, elapsed_time):
        """è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ† ç®€åŒ–ç‰ˆæ”¹è¿›SACç»´æŒä»»åŠ¡æµ‹è¯•å®Œæˆ!")
        print("="*60)
        
        print(f"â±ï¸  æ€»ç”¨æ—¶: {elapsed_time:.1f}ç§’")
        print(f"ğŸ“Š Episodes: {self.stats['episodes']}")
        print(f"ğŸ¯ æˆåŠŸEpisodes: {self.stats['successful_episodes']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {self.stats['successful_episodes']/max(self.stats['episodes'],1):.1%}")
        
        if self.stats['maintain_times']:
            print(f"â³ å¹³å‡ç»´æŒæ—¶é—´: {np.mean(self.stats['maintain_times']):.1f}æ­¥")
            print(f"ğŸ† æœ€é•¿ç»´æŒæ—¶é—´: {max(self.stats['maintain_times'])}æ­¥")
            print(f"ğŸ¯ ç›®æ ‡ç»´æŒæ—¶é—´: {self.env.maintain_target_steps}æ­¥")
            
            # åˆ†æç»´æŒæ—¶é—´è¶‹åŠ¿
            if len(self.stats['maintain_times']) >= 10:
                early = np.mean(self.stats['maintain_times'][:5])
                recent = np.mean(self.stats['maintain_times'][-5:])
                improvement = recent - early
                print(f"ğŸ“ˆ ç»´æŒæ—¶é—´æ”¹è¿›: {early:.1f} â†’ {recent:.1f} (æå‡{improvement:+.1f}æ­¥)")
        
        if self.stats['alpha_history']:
            print(f"ğŸ”„ Alphaå˜åŒ–: {self.stats['alpha_history'][0]:.3f} â†’ {self.stats['alpha_history'][-1]:.3f}")
            if self.stats['exploration_phase_end'] > 0:
                print(f"ğŸ¯ æ¢ç´¢â†’ç¨³å®šè½¬æ¢: Step {self.stats['exploration_phase_end']}")
        
        if self.stats['reward_history']:
            recent_rewards = self.stats['reward_history'][-5:]
            print(f"ğŸ’° æœ€è¿‘5æ¬¡å¹³å‡å¥–åŠ±: {np.mean(recent_rewards):.1f}")
        
        print(f"ğŸ“ è¯¾ç¨‹å‡çº§æ¬¡æ•°: {self.stats['curriculum_upgrades']}")
        print(f"ğŸ“š æœ€ç»ˆè¯¾ç¨‹é˜¶æ®µ: {self.curriculum_stage}")
        
        # ğŸ¯ æ”¹è¿›æ•ˆæœåˆ†æ
        print("\nğŸ“‹ æ”¹è¿›æ–¹æ³•æ•ˆæœåˆ†æ:")
        
        # 1. ç†µæƒé‡è°ƒåº¦æ•ˆæœ
        if self.stats['alpha_history']:
            alpha_reduction = self.stats['alpha_history'][0] - self.stats['alpha_history'][-1]
            print(f"âœ… ç†µæƒé‡è°ƒåº¦: Alphaé™ä½äº† {alpha_reduction:.3f} (ä»æ¢ç´¢è½¬å‘ç¨³å®š)")
        
        # 2. è¯¾ç¨‹å­¦ä¹ æ•ˆæœ
        if self.stats['curriculum_upgrades'] > 0:
            print(f"âœ… è¯¾ç¨‹å­¦ä¹ : æˆåŠŸå‡çº§ {self.stats['curriculum_upgrades']} æ¬¡")
        else:
            print(f"âš ï¸ è¯¾ç¨‹å­¦ä¹ : æœªè§¦å‘å‡çº§ (éœ€è¦æ›´å¤šè®­ç»ƒ)")
        
        # 3. ç»´æŒä»»åŠ¡æ•ˆæœ
        if self.stats['successful_episodes'] > 0:
            print(f"âœ… ç»´æŒä»»åŠ¡: æˆåŠŸè§£å†³ SAC æ¢ç´¢vsåˆ©ç”¨å†²çª!")
            print(f"   æˆåŠŸå­¦ä¼šäº†åœ¨ç›®æ ‡ä½ç½®ç¨³å®šç»´æŒ")
        else:
            if self.stats['maintain_times']:
                best_maintain = max(self.stats['maintain_times'])
                target = self.env.maintain_target_steps
                progress = best_maintain / target * 100
                print(f"ğŸ“Š ç»´æŒä»»åŠ¡: éƒ¨åˆ†æˆåŠŸï¼Œæœ€ä½³è¿›åº¦ {progress:.1f}%")
                
                if progress > 50:
                    print(f"   âœ… æ˜¾è‘—æ”¹è¿›: èƒ½å¤Ÿç»´æŒè¶…è¿‡ç›®æ ‡çš„ä¸€åŠæ—¶é—´")
                elif progress > 20:
                    print(f"   âš ï¸ æœ‰æ”¹è¿›: èƒ½å¤ŸçŸ­æœŸç»´æŒï¼Œä½†éœ€è¦æ›´å¤šè®­ç»ƒ")
                else:
                    print(f"   âŒ æ•ˆæœæœ‰é™: éœ€è¦è°ƒæ•´å‚æ•°æˆ–å»¶é•¿è®­ç»ƒ")
            else:
                print(f"âŒ ç»´æŒä»»åŠ¡: æœªèƒ½æ¥è¿‘ç›®æ ‡åŒºåŸŸ")
        
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç®€åŒ–ç‰ˆæ”¹è¿›SACç»´æŒä»»åŠ¡æµ‹è¯•")
    print("éªŒè¯æ”¹è¿›æ–¹æ³•:")
    print("  ğŸ”„ ç†µæƒé‡è‡ªé€‚åº”è°ƒåº¦ (0.2 â†’ 0.05)")
    print("  ğŸ“ è¯¾ç¨‹å­¦ä¹  (3ä¸ªéš¾åº¦é˜¶æ®µ)")
    print("  ğŸ† æ”¹è¿›å¥–åŠ±è®¾è®¡ (é‡Œç¨‹ç¢‘+æ¸©å’Œæƒ©ç½š)")
    print("  ğŸ§  ç®€åŒ–SACæ¨¡å‹ (é¿å…å¤æ‚attention)")
    print()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = SimpleSACTester(
        num_joints=3,  # ä½¿ç”¨3å…³èŠ‚ç®€åŒ–æµ‹è¯•
        curriculum_stage=0  # ä»æœ€ç®€å•çš„é˜¶æ®µå¼€å§‹
    )
    
    # è¿è¡Œæµ‹è¯•
    tester.run_test(
        total_steps=1500,  # æ€»è®­ç»ƒæ­¥æ•°
        max_episodes=25    # æœ€å¤§episodeæ•°
    )

if __name__ == "__main__":
    main()
