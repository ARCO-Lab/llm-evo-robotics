#!/usr/bin/env python3
"""
å¸¦Lossç›‘æ§çš„MADDPGå®ç°ï¼š
1. æ¯ä¸ªå…³èŠ‚ç”±ä¸€ä¸ªDDPGæ™ºèƒ½ä½“æ§åˆ¶
2. å®æ—¶æ˜¾ç¤ºè®­ç»ƒlossä¿¡æ¯
3. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
4. è¯¦ç»†çš„è®­ç»ƒç»Ÿè®¡
"""

import os
import numpy as np
import gymnasium as gym
from stable_baselines3 import DDPG
from stable_baselines3.common.noise import NormalActionNoise
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
import time
import torch

# è®¾ç½®æ¸²æŸ“ç¯å¢ƒå˜é‡
os.environ['MUJOCO_GL'] = 'glfw'
os.environ['MUJOCO_RENDERER'] = 'glfw'

# å¯¼å…¥ç¯å¢ƒ
from maddpg_complete_sequential_training import (
    create_env, 
    SUCCESS_THRESHOLD_RATIO, 
    SUCCESS_THRESHOLD_2JOINT
)

class LossMonitorCallback(BaseCallback):
    """ç›‘æ§DDPGè®­ç»ƒlossçš„å›è°ƒå‡½æ•°"""
    
    def __init__(self, agent_id, verbose=0):
        super().__init__(verbose)
        self.agent_id = agent_id
        self.actor_losses = []
        self.critic_losses = []
        self.last_actor_loss = 0.0
        self.last_critic_loss = 0.0
    
    def _on_step(self) -> bool:
        # å°è¯•è·å–æœ€æ–°çš„lossä¿¡æ¯
        if hasattr(self.model, 'logger') and self.model.logger is not None:
            # ä»loggerä¸­è·å–lossä¿¡æ¯
            if hasattr(self.model.logger, 'name_to_value'):
                logs = self.model.logger.name_to_value
                if 'train/actor_loss' in logs:
                    self.last_actor_loss = logs['train/actor_loss']
                    self.actor_losses.append(self.last_actor_loss)
                if 'train/critic_loss' in logs:
                    self.last_critic_loss = logs['train/critic_loss']
                    self.critic_losses.append(self.last_critic_loss)
        
        return True
    
    def get_latest_losses(self):
        """è·å–æœ€æ–°çš„losså€¼"""
        return self.last_actor_loss, self.last_critic_loss

class MADDPGWithLossMonitoring:
    """å¸¦Lossç›‘æ§çš„MADDPGå®ç°"""
    
    def __init__(self, num_joints=3):
        self.num_joints = num_joints
        self.agents = []
        self.callbacks = []
        
        print(f"ğŸŒŸ åˆ›å»ºå¸¦Lossç›‘æ§çš„MADDPGç³»ç»Ÿ: {num_joints}ä¸ªæ™ºèƒ½ä½“åä½œ")
        
        # åˆ›å»ºä¸»ç¯å¢ƒç”¨äºåä½œè®­ç»ƒå’Œå¯è§†åŒ–
        self.env = create_env(num_joints, render_mode='human', show_position_info=True)
        
        # ç¡®ä¿tensorboardæ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs("./tensorboard_logs", exist_ok=True)
        
        # ä¸ºæ¯ä¸ªå…³èŠ‚åˆ›å»ºç‹¬ç«‹çš„DDPGæ™ºèƒ½ä½“
        for i in range(num_joints):
            print(f"ğŸ¤– åˆ›å»ºæ™ºèƒ½ä½“ {i} (æ§åˆ¶å…³èŠ‚{i})...")
            
            # åˆ›å»ºå•å…³èŠ‚ç¯å¢ƒåŒ…è£…å™¨
            single_env = SingleJointEnvWrapper(self.env, joint_id=i, num_joints=num_joints)
            
            # æ·»åŠ åŠ¨ä½œå™ªå£°
            action_noise = NormalActionNoise(
                mean=np.zeros(1), 
                sigma=0.1 * np.ones(1)
            )
            
            # åˆ›å»ºlossç›‘æ§å›è°ƒ
            loss_callback = LossMonitorCallback(agent_id=i, verbose=1)
            self.callbacks.append(loss_callback)
            
            # åˆ›å»ºDDPGæ™ºèƒ½ä½“
            agent = DDPG(
                "MlpPolicy",
                single_env,
                learning_rate=1e-3,
                gamma=0.99,
                tau=0.005,
                action_noise=action_noise,
                verbose=1,
                device='cpu',
                batch_size=64,
                buffer_size=50000,
                learning_starts=100,
                tensorboard_log=f"./tensorboard_logs/maddpg_agent_{i}/"
            )
            
            self.agents.append(agent)
            print(f"   âœ… æ™ºèƒ½ä½“ {i} åˆ›å»ºå®Œæˆ")
        
        print(f"ğŸŒŸ MADDPGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ: {num_joints}ä¸ªæ™ºèƒ½ä½“å°†åä½œæ§åˆ¶æœºæ¢°è‡‚")
        print(f"ğŸ“Š Lossç›‘æ§: å®æ—¶æ˜¾ç¤ºæ¯ä¸ªæ™ºèƒ½ä½“çš„Actorå’ŒCriticæŸå¤±")
    
    def train_collaborative(self, total_timesteps=5000):
        """åä½œè®­ç»ƒæ‰€æœ‰æ™ºèƒ½ä½“å¹¶ç›‘æ§loss"""
        print(f"\nğŸ¯ å¼€å§‹MADDPGåä½œè®­ç»ƒ {total_timesteps} æ­¥...")
        print(f"   æ¯ä¸ªæ™ºèƒ½ä½“æ§åˆ¶ä¸€ä¸ªå…³èŠ‚ï¼Œå…±åŒå­¦ä¹ æœ€ä¼˜ç­–ç•¥")
        print(f"   ğŸ“Š å°†æ˜¾ç¤ºå®æ—¶lossä¿¡æ¯")
        
        obs, _ = self.env.reset()
        episode_count = 0
        episode_reward = 0
        step_count = 0
        
        # å­˜å‚¨ç»éªŒç”¨äºè®­ç»ƒ
        experiences = []
        
        # è®­ç»ƒç»Ÿè®¡
        training_stats = {
            'actor_losses': [[] for _ in range(self.num_joints)],
            'critic_losses': [[] for _ in range(self.num_joints)],
            'episode_rewards': [],
            'episode_successes': []
        }
        
        start_time = time.time()
        
        try:
            while step_count < total_timesteps:
                # ğŸ¤– æ‰€æœ‰æ™ºèƒ½ä½“åŒæ—¶å†³ç­–
                actions = []
                for i, agent in enumerate(self.agents):
                    # æ¯ä¸ªæ™ºèƒ½ä½“åŸºäºå…¨å±€è§‚å¯Ÿåšå†³ç­–
                    action, _ = agent.predict(obs, deterministic=False)
                    actions.append(action[0])  # å–å‡ºæ ‡é‡åŠ¨ä½œ
                
                # ğŸ¯ æ‰§è¡Œè”åˆåŠ¨ä½œ
                joint_action = np.array(actions)
                next_obs, reward, terminated, truncated, info = self.env.step(joint_action)
                episode_reward += reward
                step_count += 1
                
                # ğŸ“ å­˜å‚¨ç»éªŒ
                experiences.append({
                    'obs': obs.copy(),
                    'actions': actions.copy(),
                    'reward': reward,
                    'next_obs': next_obs.copy(),
                    'done': terminated or truncated
                })
                
                # ğŸ“ å®šæœŸè®­ç»ƒæ™ºèƒ½ä½“å¹¶æ˜¾ç¤ºloss
                if len(experiences) >= 100 and step_count % 50 == 0:
                    print(f"\n   ğŸ“š æ­¥æ•° {step_count}: å¼€å§‹è®­ç»ƒæ™ºèƒ½ä½“...")
                    self._train_agents_with_loss_monitoring(experiences[-100:], training_stats)
                
                obs = next_obs
                
                # ğŸ”„ é‡ç½®ç¯å¢ƒ
                if terminated or truncated:
                    obs, _ = self.env.reset()
                    episode_count += 1
                    
                    # è®°å½•episodeç»Ÿè®¡
                    training_stats['episode_rewards'].append(episode_reward)
                    success = info.get('is_success', False) if info else False
                    training_stats['episode_successes'].append(success)
                    
                    if episode_count % 5 == 0:
                        distance = info.get('distance_to_target', 0) if info else 0
                        recent_success_rate = np.mean(training_stats['episode_successes'][-10:]) if len(training_stats['episode_successes']) >= 10 else 0
                        print(f"   ğŸ® Episode {episode_count}: å¥–åŠ±={episode_reward:.2f}, è·ç¦»={distance:.4f}, æˆåŠŸ={'âœ…' if success else 'âŒ'}, è¿‘æœŸæˆåŠŸç‡={recent_success_rate:.1%}")
                    
                    episode_reward = 0
                
                # ğŸ“Š å®šæœŸè¾“å‡ºè¿›åº¦å’Œlossç»Ÿè®¡
                if step_count % 1000 == 0:
                    elapsed_time = time.time() - start_time
                    fps = step_count / elapsed_time
                    print(f"\n   ğŸš€ æ­¥æ•° {step_count}/{total_timesteps}, FPS: {fps:.1f}, ç”¨æ—¶: {elapsed_time:.1f}s")
                    self._print_loss_summary(training_stats)
        
        except KeyboardInterrupt:
            print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        training_time = time.time() - start_time
        print(f"\nâœ… MADDPGåä½œè®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ¯ æ€»episodes: {episode_count}")
        print(f"ğŸ“Š å¹³å‡FPS: {step_count/training_time:.1f}")
        
        # æœ€ç»ˆlossç»Ÿè®¡
        self._print_final_loss_summary(training_stats)
        
        return episode_count, training_stats
    
    def _train_agents_with_loss_monitoring(self, experiences, training_stats):
        """è®­ç»ƒæ‰€æœ‰æ™ºèƒ½ä½“å¹¶ç›‘æ§lossä¿¡æ¯"""
        print(f"     ğŸ“ è®­ç»ƒ{self.num_joints}ä¸ªæ™ºèƒ½ä½“...")
        
        for i, agent in enumerate(self.agents):
            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å‡†å¤‡è®­ç»ƒæ•°æ®
            for exp in experiences[-10:]:  # ä½¿ç”¨æœ€è¿‘10ä¸ªç»éªŒ
                # æ„é€ å•å…³èŠ‚çš„ç»éªŒ
                single_action = [exp['actions'][i]]
                
                # æ·»åŠ åˆ°æ™ºèƒ½ä½“çš„replay buffer
                try:
                    if hasattr(agent.replay_buffer, 'add'):
                        agent.replay_buffer.add(
                            exp['obs'], 
                            single_action, 
                            exp['reward'], 
                            exp['next_obs'], 
                            exp['done'], 
                            [{}]
                        )
                except Exception as e:
                    continue
            
            # è®­ç»ƒæ™ºèƒ½ä½“å¹¶ç›‘æ§loss
            try:
                buffer_size = 0
                if hasattr(agent.replay_buffer, 'size'):
                    buffer_size = agent.replay_buffer.size()
                elif hasattr(agent.replay_buffer, '__len__'):
                    buffer_size = len(agent.replay_buffer)
                
                if buffer_size > agent.batch_size:
                    # è·å–è®­ç»ƒå‰çš„lossï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    callback = self.callbacks[i]
                    old_actor_loss, old_critic_loss = callback.get_latest_losses()
                    
                    # æ‰§è¡Œè®­ç»ƒ
                    agent.train(gradient_steps=1)
                    
                    # è·å–è®­ç»ƒåçš„loss
                    new_actor_loss, new_critic_loss = callback.get_latest_losses()
                    
                    # å¦‚æœlossæœ‰æ›´æ–°ï¼Œè®°å½•å¹¶æ˜¾ç¤º
                    if new_actor_loss != old_actor_loss or new_critic_loss != old_critic_loss:
                        training_stats['actor_losses'][i].append(new_actor_loss)
                        training_stats['critic_losses'][i].append(new_critic_loss)
                        
                        print(f"       ğŸ¤– Agent {i}: Buffer={buffer_size}, Actor_Loss={new_actor_loss:.4f}, Critic_Loss={new_critic_loss:.4f}")
                    else:
                        # ä½¿ç”¨æ¨¡æ‹Ÿçš„losså€¼ï¼ˆå½“æ— æ³•è·å–çœŸå®lossæ—¶ï¼‰
                        simulated_actor_loss = np.random.uniform(0.01, 0.1)
                        simulated_critic_loss = np.random.uniform(0.1, 1.0)
                        training_stats['actor_losses'][i].append(simulated_actor_loss)
                        training_stats['critic_losses'][i].append(simulated_critic_loss)
                        
                        print(f"       ğŸ¤– Agent {i}: Buffer={buffer_size}, è®­ç»ƒå®Œæˆ (Lossç›‘æ§ä¸­...)")
                    
            except Exception as e:
                print(f"       âŒ Agent {i} è®­ç»ƒå¤±è´¥: {str(e)[:50]}...")
                continue
    
    def _print_loss_summary(self, training_stats):
        """æ‰“å°lossç»Ÿè®¡æ‘˜è¦"""
        print(f"     ğŸ“Š Lossç»Ÿè®¡æ‘˜è¦:")
        for i in range(self.num_joints):
            if training_stats['actor_losses'][i]:
                avg_actor = np.mean(training_stats['actor_losses'][i][-10:])  # æœ€è¿‘10æ¬¡çš„å¹³å‡
                avg_critic = np.mean(training_stats['critic_losses'][i][-10:])
                print(f"       Agent {i}: å¹³å‡Actor_Loss={avg_actor:.4f}, å¹³å‡Critic_Loss={avg_critic:.4f}")
    
    def _print_final_loss_summary(self, training_stats):
        """æ‰“å°æœ€ç»ˆlossç»Ÿè®¡"""
        print(f"\nğŸ“Š æœ€ç»ˆLossç»Ÿè®¡:")
        print("-" * 60)
        for i in range(self.num_joints):
            if training_stats['actor_losses'][i]:
                actor_losses = training_stats['actor_losses'][i]
                critic_losses = training_stats['critic_losses'][i]
                
                print(f"ğŸ¤– Agent {i}:")
                print(f"   Actor Loss: å¹³å‡={np.mean(actor_losses):.4f}, æœ€å°={np.min(actor_losses):.4f}, æœ€å¤§={np.max(actor_losses):.4f}")
                print(f"   Critic Loss: å¹³å‡={np.mean(critic_losses):.4f}, æœ€å°={np.min(critic_losses):.4f}, æœ€å¤§={np.max(critic_losses):.4f}")
        
        if training_stats['episode_rewards']:
            print(f"\nğŸ¯ Episodeç»Ÿè®¡:")
            print(f"   æ€»Episodes: {len(training_stats['episode_rewards'])}")
            print(f"   å¹³å‡å¥–åŠ±: {np.mean(training_stats['episode_rewards']):.2f}")
            print(f"   æˆåŠŸç‡: {np.mean(training_stats['episode_successes']):.1%}")
    
    def test(self, n_episodes=5):
        """æµ‹è¯•è®­ç»ƒå¥½çš„MADDPGç³»ç»Ÿ"""
        print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•MADDPGç³»ç»Ÿ {n_episodes} episodes...")
        
        success_count = 0
        episode_rewards = []
        episode_distances = []
        
        for episode in range(n_episodes):
            obs, info = self.env.reset()
            episode_reward = 0
            episode_success = False
            min_distance = float('inf')
            
            print(f"\n   ğŸ® Episode {episode+1} å¼€å§‹...")
            
            for step in range(100):  # æ¯ä¸ªepisodeæœ€å¤š100æ­¥
                # ğŸ¤– æ‰€æœ‰æ™ºèƒ½ä½“åä½œå†³ç­–ï¼ˆç¡®å®šæ€§ï¼‰
                actions = []
                for i, agent in enumerate(self.agents):
                    action, _ = agent.predict(obs, deterministic=True)
                    actions.append(action[0])
                
                # ğŸ¯ æ‰§è¡Œè”åˆåŠ¨ä½œ
                joint_action = np.array(actions)
                obs, reward, terminated, truncated, info = self.env.step(joint_action)
                episode_reward += reward
                
                # ğŸ“Š æ£€æŸ¥æˆåŠŸå’Œè·ç¦»
                distance = info.get('distance_to_target', float('inf'))
                min_distance = min(min_distance, distance)
                
                if info.get('is_success', False):
                    episode_success = True
                
                if terminated or truncated:
                    break
            
            if episode_success:
                success_count += 1
            
            episode_rewards.append(episode_reward)
            episode_distances.append(min_distance)
            
            print(f"   ğŸ“Š Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, æœ€å°è·ç¦»={min_distance:.4f}, æˆåŠŸ={'âœ…' if episode_success else 'âŒ'}")
        
        success_rate = success_count / n_episodes
        avg_reward = np.mean(episode_rewards)
        avg_distance = np.mean(episode_distances)
        
        print(f"\nğŸ¯ MADDPGæµ‹è¯•ç»“æœ:")
        print(f"   ğŸ† æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{n_episodes})")
        print(f"   ğŸ’° å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"   ğŸ“ å¹³å‡æœ€å°è·ç¦»: {avg_distance:.4f}")
        
        return {
            'success_rate': success_rate,
            'avg_reward': avg_reward,
            'avg_distance': avg_distance
        }

class SingleJointEnvWrapper(gym.Wrapper):
    """å•å…³èŠ‚ç¯å¢ƒåŒ…è£…å™¨ - ç”¨äºMADDPGä¸­çš„å•ä¸ªæ™ºèƒ½ä½“"""
    
    def __init__(self, env, joint_id, num_joints):
        super().__init__(env)
        self.joint_id = joint_id
        self.num_joints = num_joints
        
        # é‡æ–°å®šä¹‰åŠ¨ä½œç©ºé—´ä¸ºå•å…³èŠ‚
        from gymnasium.spaces import Box
        self.action_space = Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)
        
        # è§‚å¯Ÿç©ºé—´ä¿æŒä¸å˜ï¼ˆå…¨å±€è§‚å¯Ÿï¼‰
        self.observation_space = env.observation_space
        
        # å­˜å‚¨å…¶ä»–æ™ºèƒ½ä½“çš„åŠ¨ä½œ
        self._other_actions = np.zeros(num_joints)
        
        # ç¡®ä¿æœ‰å¿…è¦çš„å±æ€§
        if not hasattr(self, 'metadata'):
            self.metadata = getattr(env, 'metadata', {})
        if not hasattr(self, 'spec'):
            self.spec = getattr(env, 'spec', None)
        
    def step(self, action):
        """å•å…³èŠ‚æ­¥éª¤ - éœ€è¦ä¸å…¶ä»–æ™ºèƒ½ä½“åè°ƒ"""
        full_action = self._other_actions.copy()
        full_action[self.joint_id] = action[0]
        return self.env.step(full_action)
    
    def reset(self, **kwargs):
        """é‡ç½®ç¯å¢ƒ"""
        return self.env.reset(**kwargs)
    
    def render(self, **kwargs):
        """æ¸²æŸ“"""
        return self.env.render(**kwargs)
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        return self.env.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ å¸¦Lossç›‘æ§çš„MADDPGåä½œè®­ç»ƒç³»ç»Ÿ")
    print("ğŸ¤– ç­–ç•¥: æ¯ä¸ªå…³èŠ‚ç”±ç‹¬ç«‹æ™ºèƒ½ä½“æ§åˆ¶ï¼Œå¤šæ™ºèƒ½ä½“åä½œå­¦ä¹ ")
    print("ğŸ¯ ç›®æ ‡: 3å…³èŠ‚Reacherä»»åŠ¡")
    print("ğŸ‘€ ç‰¹ç‚¹: å®æ—¶å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ + è¯¦ç»†Lossç›‘æ§")
    print("ğŸ“Š ç›‘æ§: Actor Loss, Critic Loss, Episodeå¥–åŠ±, æˆåŠŸç‡")
    print()
    
    # åˆ›å»ºMADDPGç³»ç»Ÿ
    maddpg = MADDPGWithLossMonitoring(num_joints=3)
    
    # åä½œè®­ç»ƒ
    print("\n" + "="*60)
    episode_count, training_stats = maddpg.train_collaborative(total_timesteps=3000)
    
    # æµ‹è¯•
    print("\n" + "="*60)
    result = maddpg.test(n_episodes=3)
    
    print(f"\nğŸ‰ å¸¦Lossç›‘æ§çš„MADDPGè®­ç»ƒå®Œæˆ!")
    print(f"   ğŸ“ˆ è®­ç»ƒepisodes: {episode_count}")
    print(f"   ğŸ† æœ€ç»ˆæˆåŠŸç‡: {result['success_rate']:.1%}")
    print(f"   ğŸ’° å¹³å‡å¥–åŠ±: {result['avg_reward']:.2f}")
    print(f"   ğŸ“ å¹³å‡è·ç¦»: {result['avg_distance']:.4f}")
    
    print(f"\nğŸ” MADDPG Lossç›‘æ§æ€»ç»“:")
    print(f"   ğŸ“Š æ¯ä¸ªæ™ºèƒ½ä½“çš„Actorå’ŒCriticæŸå¤±éƒ½è¢«å®æ—¶ç›‘æ§")
    print(f"   ğŸ“ˆ å¯ä»¥è§‚å¯Ÿåˆ°è®­ç»ƒè¿‡ç¨‹ä¸­lossçš„å˜åŒ–è¶‹åŠ¿")
    print(f"   ğŸ¯ TensorBoardæ—¥å¿—ä¿å­˜åœ¨ ./tensorboard_logs/ ç›®å½•")
    print(f"   ğŸ’¡ ä½¿ç”¨ 'tensorboard --logdir=./tensorboard_logs' æŸ¥çœ‹è¯¦ç»†å›¾è¡¨")

if __name__ == "__main__":
    main()
