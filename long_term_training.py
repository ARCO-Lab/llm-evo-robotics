#!/usr/bin/env python3
"""
é•¿æœŸè®­ç»ƒ SACï¼Œéšæœºç›®æ ‡ä½ç½®ï¼Œç›´åˆ°ç¨³å®šåˆ°è¾¾ç›®æ ‡
"""

import sys
import os
import numpy as np
import torch
import time
import random
from collections import deque

# æ·»åŠ è·¯å¾„
sys.path.append('examples/surrogate_model/sac')
sys.path.append('examples/2d_reacher/envs')

from sb3_sac_adapter import SB3SACFactory
from reacher_env_factory import create_reacher_env

def long_term_training():
    print("ğŸš€ é•¿æœŸè®­ç»ƒ SAC - éšæœºç›®æ ‡ä½ç½®")
    print("ğŸ¯ è®­ç»ƒç›®æ ‡:")
    print("   - æ¯æ¬¡åˆ°è¾¾ç›®æ ‡åéšæœºç”Ÿæˆæ–°ä½ç½®")
    print("   - è®­ç»ƒç›´åˆ°èƒ½ç¨³å®šåˆ°è¾¾å„ç§ç›®æ ‡")
    print("   - æˆåŠŸæ ‡å‡†: è·ç¦» < 20px")
    print("   - ç¨³å®šæ ‡å‡†: è¿ç»­10ä¸ªç›®æ ‡æˆåŠŸç‡ > 80%")
    print("=" * 70)
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_reacher_env(version='mujoco', render_mode='human')
    
    # åˆ›å»º SAC
    sac = SB3SACFactory.create_reacher_sac(
        action_dim=2,
        buffer_capacity=20000,  # å¢å¤§ç¼“å†²åŒº
        batch_size=128,         # å¢å¤§æ‰¹æ¬¡å¤§å°
        lr=1e-3,
        device='cpu'
    )
    sac.set_env(env)
    
    # é‡ç½®ç¯å¢ƒ
    reset_result = env.reset()
    if isinstance(reset_result, tuple):
        obs, info = reset_result
    else:
        obs = reset_result
    
    print("âœ… ç¯å¢ƒå’Œ SAC åˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {env.action_space}")
    print("=" * 70)
    
    # è®­ç»ƒç»Ÿè®¡
    total_episodes = 0
    successful_episodes = 0
    recent_success_rates = deque(maxlen=10)  # æœ€è¿‘10ä¸ªç›®æ ‡çš„æˆåŠŸç‡
    
    # å½“å‰ç›®æ ‡ç»Ÿè®¡
    current_target_attempts = 0
    current_target_successes = 0
    
    # è®­ç»ƒè¿›åº¦
    total_steps = 0
    start_time = time.time()
    
    # ç›®æ ‡ä½ç½®ç”Ÿæˆå‡½æ•°
    def generate_random_goal():
        """ç”Ÿæˆéšæœºç›®æ ‡ä½ç½®"""
        # åœ¨åˆç†èŒƒå›´å†…ç”Ÿæˆç›®æ ‡
        # åŸºäº MuJoCo Reacher çš„å·¥ä½œç©ºé—´
        angle = random.uniform(0, 2 * np.pi)
        radius = random.uniform(0.05, 0.2)  # MuJoCo å•ä½æ˜¯ç±³
        
        # è½¬æ¢ä¸ºåƒç´ åæ ‡ï¼ˆç›¸å¯¹äº anchor_pointï¼‰
        goal_x = 480 + radius * np.cos(angle) * 600  # ç¼©æ”¾åˆ°åƒç´ 
        goal_y = 620 + radius * np.sin(angle) * 600
        
        # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        goal_x = np.clip(goal_x, 400, 700)
        goal_y = np.clip(goal_y, 450, 750)
        
        return [goal_x, goal_y]
    
    # è®¾ç½®åˆå§‹ç›®æ ‡
    current_goal = generate_random_goal()
    if hasattr(env, 'set_goal_position'):
        env.set_goal_position(current_goal)
    
    print(f"ğŸ¯ å¼€å§‹é•¿æœŸè®­ç»ƒ...")
    print(f"ğŸ“ åˆå§‹ç›®æ ‡ä½ç½®: [{current_goal[0]:.1f}, {current_goal[1]:.1f}]")
    print("=" * 70)
    
    # å½“å‰ episode ç»Ÿè®¡
    current_reward = 0
    current_length = 0
    best_distance = float('inf')
    
    while True:  # æŒç»­è®­ç»ƒç›´åˆ°è¾¾åˆ°ç¨³å®šæ ‡å‡†
        # è·å–åŠ¨ä½œ
        action = sac.get_action(obs, deterministic=False)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        step_result = env.step(action.cpu().numpy())
        if len(step_result) == 5:
            obs, reward, terminated, truncated, info = step_result
        else:
            obs, reward, done, info = step_result
            terminated = done
            truncated = False
        
        current_reward += reward
        current_length += 1
        total_steps += 1
        
        # è·å–è·ç¦»ä¿¡æ¯
        distance = obs[8] if len(obs) > 8 else 999
        best_distance = min(best_distance, distance)
        
        # Episode ç»“æŸå¤„ç†
        if terminated or truncated:
            total_episodes += 1
            current_target_attempts += 1
            
            # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
            success = best_distance < 20.0  # 20px å†…ç®—æˆåŠŸ
            if success:
                successful_episodes += 1
                current_target_successes += 1
                print(f"ğŸ¯ Episode {total_episodes} æˆåŠŸ! è·ç¦»: {best_distance:.1f}px, å¥–åŠ±: {current_reward:.2f}, æ­¥æ•°: {current_length}")
            else:
                print(f"âŒ Episode {total_episodes} å¤±è´¥. è·ç¦»: {best_distance:.1f}px, å¥–åŠ±: {current_reward:.2f}, æ­¥æ•°: {current_length}")
            
            # æ£€æŸ¥å½“å‰ç›®æ ‡æ˜¯å¦å®Œæˆè®­ç»ƒ
            if current_target_attempts >= 10:  # æ¯ä¸ªç›®æ ‡å°è¯•10æ¬¡
                target_success_rate = current_target_successes / current_target_attempts
                recent_success_rates.append(target_success_rate)
                
                print(f"\nğŸ“Š ç›®æ ‡å®Œæˆç»Ÿè®¡:")
                print(f"   ç›®æ ‡ä½ç½®: [{current_goal[0]:.1f}, {current_goal[1]:.1f}]")
                print(f"   æˆåŠŸç‡: {target_success_rate:.1%} ({current_target_successes}/{current_target_attempts})")
                
                # ç”Ÿæˆæ–°ç›®æ ‡
                current_goal = generate_random_goal()
                if hasattr(env, 'set_goal_position'):
                    env.set_goal_position(current_goal)
                
                print(f"ğŸ¯ æ–°ç›®æ ‡ä½ç½®: [{current_goal[0]:.1f}, {current_goal[1]:.1f}]")
                
                # é‡ç½®å½“å‰ç›®æ ‡ç»Ÿè®¡
                current_target_attempts = 0
                current_target_successes = 0
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç¨³å®šæ ‡å‡†
                if len(recent_success_rates) >= 10:
                    avg_success_rate = np.mean(recent_success_rates)
                    print(f"ğŸ“ˆ æœ€è¿‘10ä¸ªç›®æ ‡å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1%}")
                    
                    if avg_success_rate >= 0.8:  # 80% æˆåŠŸç‡
                        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! è¾¾åˆ°ç¨³å®šæ ‡å‡†!")
                        print(f"   æœ€è¿‘10ä¸ªç›®æ ‡å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1%}")
                        break
                
                print("=" * 50)
            
            # é‡ç½®
            reset_result = env.reset()
            if isinstance(reset_result, tuple):
                obs, info = reset_result
            else:
                obs = reset_result
            
            current_reward = 0
            current_length = 0
            best_distance = float('inf')
        
        # æ¯1000æ­¥è¿›è¡Œè¿›åº¦æŠ¥å‘Š
        if total_steps % 1000 == 0:
            elapsed_time = time.time() - start_time
            overall_success_rate = successful_episodes / total_episodes if total_episodes > 0 else 0
            
            print(f"\nğŸ“ˆ è®­ç»ƒè¿›åº¦æŠ¥å‘Š - Step {total_steps}")
            print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {elapsed_time/60:.1f} åˆ†é’Ÿ")
            print(f"ğŸ“Š æ€»Episodes: {total_episodes}")
            print(f"ğŸ¯ æ€»ä½“æˆåŠŸç‡: {overall_success_rate:.1%} ({successful_episodes}/{total_episodes})")
            print(f"ğŸ“ å½“å‰ç›®æ ‡: [{current_goal[0]:.1f}, {current_goal[1]:.1f}]")
            print(f"ğŸ¯ å½“å‰ç›®æ ‡æˆåŠŸç‡: {current_target_successes}/{current_target_attempts}")
            
            # è·å–æŸå¤±ä¿¡æ¯
            if total_steps >= 100:
                losses = sac.update()
                print(f"ğŸ§  ç½‘ç»œæŸå¤±: Actor={losses['actor_loss']:.4f}, Critic={losses['critic_loss']:.4f}")
            
            print("-" * 50)
        
        # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœè®­ç»ƒæ—¶é—´è¿‡é•¿ï¼Œæä¾›é€‰é¡¹é€€å‡º
        if total_steps >= 50000:  # 50k æ­¥åè¯¢é—®æ˜¯å¦ç»§ç»­
            elapsed_time = time.time() - start_time
            print(f"\nâ° è®­ç»ƒå·²è¿›è¡Œ {elapsed_time/60:.1f} åˆ†é’Ÿï¼Œ{total_steps} æ­¥")
            print(f"ğŸ“Š å½“å‰æ•´ä½“æˆåŠŸç‡: {successful_episodes/total_episodes:.1%}")
            if len(recent_success_rates) > 0:
                print(f"ğŸ“ˆ æœ€è¿‘ç›®æ ‡å¹³å‡æˆåŠŸç‡: {np.mean(recent_success_rates):.1%}")
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ ç”¨æˆ·äº¤äº’ï¼Œä½†ä¸ºäº†è‡ªåŠ¨åŒ–ï¼Œæˆ‘ä»¬ç»§ç»­è®­ç»ƒ
            if total_steps >= 100000:  # 100k æ­¥åå¼ºåˆ¶åœæ­¢
                print("âš ï¸ è¾¾åˆ°æœ€å¤§è®­ç»ƒæ­¥æ•°é™åˆ¶ï¼Œåœæ­¢è®­ç»ƒ")
                break
    
    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    print("\n" + "=" * 70)
    print("ğŸ† é•¿æœŸè®­ç»ƒå®Œæˆ!")
    print("=" * 70)
    
    overall_success_rate = successful_episodes / total_episodes if total_episodes > 0 else 0
    
    print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ”¢ æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    print(f"ğŸ“Š æ€»Episodes: {total_episodes}")
    print(f"ğŸ¯ æ€»ä½“æˆåŠŸç‡: {overall_success_rate:.1%} ({successful_episodes}/{total_episodes})")
    
    if len(recent_success_rates) > 0:
        final_avg_success_rate = np.mean(recent_success_rates)
        print(f"ğŸ“ˆ æœ€ç»ˆç¨³å®šæˆåŠŸç‡: {final_avg_success_rate:.1%}")
        
        if final_avg_success_rate >= 0.8:
            print("ğŸ‰ è®­ç»ƒæˆåŠŸ! Reacher èƒ½å¤Ÿç¨³å®šåˆ°è¾¾å„ç§ç›®æ ‡ä½ç½®!")
        elif final_avg_success_rate >= 0.6:
            print("ğŸ‘ è®­ç»ƒè‰¯å¥½! Reacher æœ‰è¾ƒå¥½çš„ç›®æ ‡åˆ°è¾¾èƒ½åŠ›!")
        else:
            print("âš ï¸ éœ€è¦ç»§ç»­è®­ç»ƒä»¥æé«˜ç¨³å®šæ€§")
    
    print(f"\nâœ… é•¿æœŸè®­ç»ƒæµ‹è¯•å®Œæˆ!")
    env.close()
    
    return {
        'total_steps': total_steps,
        'total_episodes': total_episodes,
        'overall_success_rate': overall_success_rate,
        'final_stable_rate': np.mean(recent_success_rates) if len(recent_success_rates) > 0 else 0,
        'training_time_minutes': total_time / 60
    }

if __name__ == "__main__":
    results = long_term_training()
    print(f"\nğŸ“‹ æœ€ç»ˆè®­ç»ƒç»“æœ:")
    print(f"   è®­ç»ƒæ­¥æ•°: {results['total_steps']}")
    print(f"   æ€»Episodes: {results['total_episodes']}")
    print(f"   æ•´ä½“æˆåŠŸç‡: {results['overall_success_rate']:.1%}")
    print(f"   ç¨³å®šæˆåŠŸç‡: {results['final_stable_rate']:.1%}")
    print(f"   è®­ç»ƒæ—¶é—´: {results['training_time_minutes']:.1f}åˆ†é’Ÿ")
