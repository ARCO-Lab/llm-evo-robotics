#!/usr/bin/env python3
"""
ç®€åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶ SAC
è½»é‡çº§è®¾è®¡ï¼Œæ›´é€‚åˆ Reacher ä»»åŠ¡
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import time
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.evaluation import evaluate_policy
from typing import Dict, List, Tuple, Type, Union

class SimpleAttentionLayer(nn.Module):
    """
    ç®€åŒ–çš„æ³¨æ„åŠ›å±‚ - ä¸“é—¨ä¸º Reacher ä»»åŠ¡è®¾è®¡
    ä¸ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼Œè€Œæ˜¯ä½¿ç”¨ç®€å•çš„ç‰¹å¾åŠ æƒæœºåˆ¶
    """
    def __init__(self, input_dim: int, hidden_dim: int = 64):
        super(SimpleAttentionLayer, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶ï¼šåªä½¿ç”¨ä¸€ä¸ªçº¿æ€§å±‚è®¡ç®—æƒé‡
        self.attention_weights = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),  # è¾“å‡ºä¸è¾“å…¥ç»´åº¦ç›¸åŒ
            nn.Softmax(dim=-1)  # å½’ä¸€åŒ–æƒé‡
        )
        
        # ç‰¹å¾å˜æ¢å±‚
        self.feature_transform = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        print(f"ğŸ§  SimpleAttentionLayer åˆå§‹åŒ–: input_dim={input_dim}, hidden_dim={hidden_dim}")
        print(f"   å‚æ•°é‡å¤§å¹…å‡å°‘ï¼Œè®¡ç®—æ›´é«˜æ•ˆ")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ç®€åŒ–çš„å‰å‘ä¼ æ’­
        x: [batch_size, input_dim]
        """
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = self.attention_weights(x)  # [batch_size, input_dim]
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°è¾“å…¥ç‰¹å¾
        weighted_features = x * attention_weights  # å…ƒç´ çº§åˆ«çš„åŠ æƒ
        
        # ç‰¹å¾å˜æ¢
        output = self.feature_transform(weighted_features)
        
        return output

class ReacherSpecificAttention(nn.Module):
    """
    ä¸“é—¨ä¸º Reacher ä»»åŠ¡è®¾è®¡çš„æ³¨æ„åŠ›æœºåˆ¶
    è€ƒè™‘ Reacher è§‚å¯Ÿç©ºé—´çš„ç‰¹å®šç»“æ„
    """
    def __init__(self, obs_dim: int = 10, hidden_dim: int = 64):
        super(ReacherSpecificAttention, self).__init__()
        self.obs_dim = obs_dim
        self.hidden_dim = hidden_dim
        
        # MuJoCo Reacher-v5 è§‚å¯Ÿç©ºé—´ç»“æ„ (10ç»´):
        # [0:2] - cos/sin of joint angles (å…³èŠ‚è§’åº¦)
        # [2:4] - joint velocities (å…³èŠ‚é€Ÿåº¦)  
        # [4:6] - end effector position (æœ«ç«¯ä½ç½® x,y)
        # [6:8] - target position (ç›®æ ‡ä½ç½® x,y)
        # [8:10] - vector from target to end effector (ç›®æ ‡åˆ°æœ«ç«¯çš„å‘é‡)
        
        # ä¸ºä¸åŒç±»å‹çš„ç‰¹å¾è®¾è®¡ä¸åŒçš„æ³¨æ„åŠ›æƒé‡
        self.joint_attention = nn.Linear(4, 4)      # å…³èŠ‚ç›¸å…³ (è§’åº¦+é€Ÿåº¦)
        self.position_attention = nn.Linear(4, 4)   # ä½ç½®ç›¸å…³ (æœ«ç«¯+ç›®æ ‡)
        self.vector_attention = nn.Linear(2, 2)     # å‘é‡ç›¸å…³ (ç›®æ ‡åˆ°æœ«ç«¯)
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim),
            nn.Dropout(0.1)
        )
        
        print(f"ğŸ¯ ReacherSpecificAttention åˆå§‹åŒ–:")
        print(f"   ä¸“é—¨é’ˆå¯¹ Reacher {obs_dim}ç»´è§‚å¯Ÿç©ºé—´è®¾è®¡")
        print(f"   åˆ†åˆ«å¤„ç†å…³èŠ‚ã€ä½ç½®ã€å‘é‡ä¿¡æ¯")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        é’ˆå¯¹ Reacher ç»“æ„çš„æ³¨æ„åŠ›å¤„ç†
        """
        batch_size = x.size(0)
        
        # åˆ†è§£è§‚å¯Ÿç©ºé—´ (10ç»´)
        joint_features = x[:, :4]      # å…³èŠ‚è§’åº¦å’Œé€Ÿåº¦
        position_features = x[:, 4:8]  # æœ«ç«¯å’Œç›®æ ‡ä½ç½®
        vector_features = x[:, 8:10]   # ç›®æ ‡åˆ°æœ«ç«¯çš„å‘é‡
        
        # åˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›æƒé‡
        joint_weights = torch.sigmoid(self.joint_attention(joint_features))
        position_weights = torch.sigmoid(self.position_attention(position_features))
        vector_weights = torch.sigmoid(self.vector_attention(vector_features))
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        weighted_joint = joint_features * joint_weights
        weighted_position = position_features * position_weights
        weighted_vector = vector_features * vector_weights
        
        # é‡æ–°ç»„åˆ
        weighted_obs = torch.cat([weighted_joint, weighted_position, weighted_vector], dim=1)
        
        # èåˆå¤„ç†
        output = self.fusion(weighted_obs)
        
        return output

class SimplifiedAttentionExtractor(BaseFeaturesExtractor):
    """
    ç®€åŒ–çš„æ³¨æ„åŠ›ç‰¹å¾æå–å™¨
    """
    def __init__(self, observation_space: gym.Space, features_dim: int = 128, attention_type: str = "simple"):
        super(SimplifiedAttentionExtractor, self).__init__(observation_space, features_dim)
        
        obs_dim = observation_space.shape[0]
        self.attention_type = attention_type
        
        print(f"ğŸ” SimplifiedAttentionExtractor åˆå§‹åŒ–:")
        print(f"   è§‚å¯Ÿç©ºé—´ç»´åº¦: {obs_dim}")
        print(f"   è¾“å‡ºç‰¹å¾ç»´åº¦: {features_dim}")
        print(f"   æ³¨æ„åŠ›ç±»å‹: {attention_type}")
        
        if attention_type == "simple":
            # ç®€å•æ³¨æ„åŠ›æœºåˆ¶
            self.attention = SimpleAttentionLayer(obs_dim, features_dim)
        elif attention_type == "reacher_specific":
            # Reacher ä¸“ç”¨æ³¨æ„åŠ›æœºåˆ¶
            self.attention = ReacherSpecificAttention(obs_dim, features_dim)
        else:
            raise ValueError(f"Unknown attention type: {attention_type}")
        
        # è¾“å‡ºå±‚ (å¦‚æœéœ€è¦è°ƒæ•´ç»´åº¦)
        if attention_type == "simple":
            self.output_layer = nn.Identity()  # SimpleAttentionLayer å·²ç»è¾“å‡ºæ­£ç¡®ç»´åº¦
        else:
            self.output_layer = nn.Sequential(
                nn.Linear(features_dim, features_dim),
                nn.ReLU()
            )
        
        print(f"âœ… SimplifiedAttentionExtractor æ„å»ºå®Œæˆ")
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        """
        # æ³¨æ„åŠ›å¤„ç†
        features = self.attention(observations)
        
        # è¾“å‡ºå¤„ç†
        output = self.output_layer(features)
        
        return output

def sac_with_simplified_attention_training(attention_type: str = "reacher_specific"):
    print("ğŸš€ SAC + ç®€åŒ–æ³¨æ„åŠ›æœºåˆ¶è®­ç»ƒ")
    print(f"ğŸ§  æ³¨æ„åŠ›ç±»å‹: {attention_type}")
    print("âš¡ è½»é‡çº§è®¾è®¡ï¼Œå‡å°‘è®¡ç®—å¼€é”€ï¼Œæé«˜ç¨³å®šæ€§")
    print("=" * 70)
    
    # åˆ›å»ºåŸç”Ÿ MuJoCo Reacher ç¯å¢ƒ
    print("ğŸ­ åˆ›å»º MuJoCo Reacher-v5 ç¯å¢ƒ...")
    env = gym.make('Reacher-v5', render_mode='human')
    env = Monitor(env)
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"ğŸ‘ï¸ è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"ğŸ“ è§‚å¯Ÿç»´åº¦: {env.observation_space.shape}")
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = gym.make('Reacher-v5')
    eval_env = Monitor(eval_env)
    
    print("=" * 70)
    
    # åˆ›å»ºå¸¦ç®€åŒ–æ³¨æ„åŠ›çš„ SAC æ¨¡å‹
    print("ğŸ¤– åˆ›å»º SAC + ç®€åŒ–æ³¨æ„åŠ›æ¨¡å‹...")
    
    # å®šä¹‰ç­–ç•¥å‚æ•°
    policy_kwargs = {
        "features_extractor_class": SimplifiedAttentionExtractor,
        "features_extractor_kwargs": {
            "features_dim": 128,
            "attention_type": attention_type
        },
        "net_arch": [256, 256],  # ä¿æŒä¸ä¹‹å‰ç›¸åŒçš„ç½‘ç»œæ¶æ„
        "activation_fn": torch.nn.ReLU,
    }
    
    model = SAC(
        "MlpPolicy",
        env,
        learning_rate=3e-4,          # ä¸ baseline ç›¸åŒ
        buffer_size=1000000,         # ä¸ baseline ç›¸åŒ
        learning_starts=100,         # ä¸ baseline ç›¸åŒ
        batch_size=256,              # ä¸ baseline ç›¸åŒ
        tau=0.005,                   # ä¸ baseline ç›¸åŒ
        gamma=0.99,                  # ä¸ baseline ç›¸åŒ
        train_freq=1,                # ä¸ baseline ç›¸åŒ
        gradient_steps=1,            # ä¸ baseline ç›¸åŒ
        ent_coef='auto',             # ä¸ baseline ç›¸åŒ
        target_update_interval=1,    # ä¸ baseline ç›¸åŒ
        use_sde=False,               # ä¸ baseline ç›¸åŒ
        policy_kwargs=policy_kwargs, # ç®€åŒ–æ³¨æ„åŠ›æœºåˆ¶
        verbose=1,
        device='cpu'
    )
    
    print("âœ… SAC + ç®€åŒ–æ³¨æ„åŠ›æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°:")
    print(f"   ç­–ç•¥: MlpPolicy + SimplifiedAttentionExtractor")
    print(f"   æ³¨æ„åŠ›ç±»å‹: {attention_type}")
    print(f"   ç‰¹å¾ç»´åº¦: 128")
    print(f"   ç½‘ç»œæ¶æ„: [256, 256]")
    print(f"   å‚æ•°é‡: å¤§å¹…å‡å°‘")
    print(f"   è®¡ç®—å¼€é”€: æ˜¾è‘—é™ä½")
    
    print("=" * 70)
    
    # åˆ›å»ºè¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f'./sac_simple_attention_{attention_type}_best/',
        log_path=f'./sac_simple_attention_{attention_type}_logs/',
        eval_freq=5000,
        n_eval_episodes=10,
        deterministic=True,
        render=False
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print("ğŸ“Š è®­ç»ƒé…ç½®:")
    print("   æ€»æ­¥æ•°: 50,000")
    print("   è¯„ä¼°é¢‘ç‡: æ¯ 5,000 æ­¥")
    print("   é¢„æœŸ: æ›´å¿«æ”¶æ•›ï¼Œæ›´ç¨³å®šæ€§èƒ½")
    print("=" * 70)
    
    start_time = time.time()
    
    # è®­ç»ƒæ¨¡å‹
    model.learn(
        total_timesteps=50000,
        callback=eval_callback,
        log_interval=10,
        progress_bar=True
    )
    
    training_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("ğŸ† è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
    print("=" * 70)
    
    # ä¿å­˜æ¨¡å‹
    model.save(f"sac_simple_attention_{attention_type}_final")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º: sac_simple_attention_{attention_type}_final.zip")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ” æœ€ç»ˆè¯„ä¼° (20 episodes)...")
    mean_reward, std_reward = evaluate_policy(
        model, 
        eval_env, 
        n_eval_episodes=20,
        deterministic=True,
        render=False
    )
    
    print(f"ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    # ä¸ä¹‹å‰ç»“æœå¯¹æ¯”
    baseline_reward = -4.86
    complex_attention_reward = -4.45
    
    improvement_vs_baseline = mean_reward - baseline_reward
    improvement_vs_complex = mean_reward - complex_attention_reward
    
    print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
    print(f"   Baseline SAC: {baseline_reward:.2f}")
    print(f"   å¤æ‚æ³¨æ„åŠ›: {complex_attention_reward:.2f}")
    print(f"   ç®€åŒ–æ³¨æ„åŠ›: {mean_reward:.2f}")
    print(f"   vs Baseline: {improvement_vs_baseline:+.2f}")
    print(f"   vs å¤æ‚æ³¨æ„åŠ›: {improvement_vs_complex:+.2f}")
    
    if improvement_vs_baseline > 0.3 and improvement_vs_complex > 0:
        print("   ğŸ‰ ç®€åŒ–æ³¨æ„åŠ›æ•ˆæœæœ€å¥½!")
    elif improvement_vs_baseline > 0.1:
        print("   ğŸ‘ ç®€åŒ–æ³¨æ„åŠ›æœ‰æ•ˆæ”¹è¿›!")
    elif improvement_vs_baseline > -0.1:
        print("   âš–ï¸ ç®€åŒ–æ³¨æ„åŠ›æ•ˆæœç›¸å½“")
    else:
        print("   âš ï¸ ç®€åŒ–æ³¨æ„åŠ›ä»éœ€ä¼˜åŒ–")
    
    # æ¼”ç¤ºè®­ç»ƒå¥½çš„æ¨¡å‹
    print("\nğŸ® æ¼”ç¤ºè®­ç»ƒå¥½çš„æ¨¡å‹ (10 episodes)...")
    demo_env = gym.make('Reacher-v5', render_mode='human')
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(10):
        obs, info = demo_env.reset()
        episode_reward = 0
        episode_length = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = demo_env.step(action)
            
            episode_reward += reward
            episode_length += 1
            
            if terminated or truncated:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # åˆ¤æ–­æˆåŠŸ
        if episode_reward > -5:
            success_count += 1
            print(f"ğŸ¯ Episode {episode+1}: æˆåŠŸ! å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
        else:
            print(f"ğŸ“Š Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
    
    demo_env.close()
    
    # æ¼”ç¤ºç»Ÿè®¡
    demo_success_rate = success_count / 10
    demo_avg_reward = np.mean(episode_rewards)
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¼”ç¤ºç»Ÿè®¡:")
    print(f"   æˆåŠŸç‡: {demo_success_rate:.1%} ({success_count}/10)")
    print(f"   å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    print(f"   å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.1f}")
    print(f"   å¥–åŠ±æ ‡å‡†å·®: {np.std(episode_rewards):.2f}")
    
    # ä¸ä¹‹å‰æ¼”ç¤ºå¯¹æ¯”
    baseline_demo_success = 0.9
    complex_demo_success = 0.4
    baseline_demo_reward = -4.82
    complex_demo_reward = -5.18
    
    print(f"\nğŸ“ˆ æ¼”ç¤ºæ•ˆæœå¯¹æ¯”:")
    print(f"   Baseline æˆåŠŸç‡: {baseline_demo_success:.1%}")
    print(f"   å¤æ‚æ³¨æ„åŠ›æˆåŠŸç‡: {complex_demo_success:.1%}")
    print(f"   ç®€åŒ–æ³¨æ„åŠ›æˆåŠŸç‡: {demo_success_rate:.1%}")
    print(f"   ")
    print(f"   Baseline å¹³å‡å¥–åŠ±: {baseline_demo_reward:.2f}")
    print(f"   å¤æ‚æ³¨æ„åŠ›å¹³å‡å¥–åŠ±: {complex_demo_reward:.2f}")
    print(f"   ç®€åŒ–æ³¨æ„åŠ›å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    
    # è®­ç»ƒæ—¶é—´å¯¹æ¯”
    baseline_time = 14.3
    complex_time = 19.3
    time_vs_baseline = training_time/60 - baseline_time
    time_vs_complex = training_time/60 - complex_time
    
    print(f"\nâ±ï¸ è®­ç»ƒæ—¶é—´å¯¹æ¯”:")
    print(f"   Baseline: {baseline_time:.1f} åˆ†é’Ÿ")
    print(f"   å¤æ‚æ³¨æ„åŠ›: {complex_time:.1f} åˆ†é’Ÿ")
    print(f"   ç®€åŒ–æ³¨æ„åŠ›: {training_time/60:.1f} åˆ†é’Ÿ")
    print(f"   vs Baseline: {time_vs_baseline:+.1f} åˆ†é’Ÿ")
    print(f"   vs å¤æ‚æ³¨æ„åŠ›: {time_vs_complex:+.1f} åˆ†é’Ÿ")
    
    if abs(time_vs_baseline) < 2:
        print("   âœ… è®­ç»ƒæ—¶é—´ä¸ Baseline ç›¸å½“ï¼Œå¼€é”€å¯æ¥å—")
    elif time_vs_complex < -2:
        print("   ğŸš€ è®­ç»ƒæ—¶é—´æ˜¾è‘—å‡å°‘ï¼Œç®€åŒ–æ•ˆæœæ˜æ˜¾")
    
    print("\nâœ… SAC + ç®€åŒ–æ³¨æ„åŠ›è®­ç»ƒå®Œæˆ!")
    
    # æ¸…ç†
    env.close()
    eval_env.close()
    
    return {
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'training_time': training_time,
        'demo_success_rate': demo_success_rate,
        'demo_avg_reward': demo_avg_reward,
        'improvement_vs_baseline': improvement_vs_baseline,
        'improvement_vs_complex': improvement_vs_complex,
        'time_vs_baseline': time_vs_baseline,
        'time_vs_complex': time_vs_complex
    }

if __name__ == "__main__":
    print("ğŸ”¥ å¼€å§‹ SAC + ç®€åŒ–æ³¨æ„åŠ›æœºåˆ¶è®­ç»ƒ")
    print("âš¡ è½»é‡çº§è®¾è®¡ï¼Œä¸“é—¨é’ˆå¯¹ Reacher ä»»åŠ¡ä¼˜åŒ–")
    print("ğŸ¯ ç›®æ ‡: ä¿æŒæ€§èƒ½æå‡çš„åŒæ—¶æé«˜ç¨³å®šæ€§å’Œæ•ˆç‡")
    print()
    
    # å¯ä»¥é€‰æ‹©ä¸åŒçš„æ³¨æ„åŠ›ç±»å‹
    attention_types = ["reacher_specific", "simple"]
    
    for attention_type in attention_types[:1]:  # å…ˆæµ‹è¯• reacher_specific
        print(f"\n{'='*50}")
        print(f"ğŸ§  æµ‹è¯•æ³¨æ„åŠ›ç±»å‹: {attention_type}")
        print(f"{'='*50}")
        
        try:
            results = sac_with_simplified_attention_training(attention_type)
            
            print(f"\nğŸŠ {attention_type} æ³¨æ„åŠ›è®­ç»ƒç»“æœæ€»ç»“:")
            print(f"   æœ€ç»ˆè¯„ä¼°å¥–åŠ±: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
            print(f"   è®­ç»ƒæ—¶é—´: {results['training_time']/60:.1f} åˆ†é’Ÿ")
            print(f"   æ¼”ç¤ºæˆåŠŸç‡: {results['demo_success_rate']:.1%}")
            print(f"   æ¼”ç¤ºå¹³å‡å¥–åŠ±: {results['demo_avg_reward']:.2f}")
            print(f"   vs Baseline æ”¹è¿›: {results['improvement_vs_baseline']:+.2f}")
            print(f"   vs å¤æ‚æ³¨æ„åŠ›æ”¹è¿›: {results['improvement_vs_complex']:+.2f}")
            print(f"   è®­ç»ƒæ—¶é—´ vs Baseline: {results['time_vs_baseline']:+.1f} åˆ†é’Ÿ")
            print(f"   è®­ç»ƒæ—¶é—´ vs å¤æ‚æ³¨æ„åŠ›: {results['time_vs_complex']:+.1f} åˆ†é’Ÿ")
            
            # æ€»ä½“è¯„ä¼°
            if (results['improvement_vs_baseline'] > 0.2 and 
                results['demo_success_rate'] > 0.7 and 
                results['time_vs_complex'] < 0):
                print(f"\nğŸ† {attention_type} æ³¨æ„åŠ›æœºåˆ¶è¡¨ç°ä¼˜ç§€!")
                print("   æ€§èƒ½æå‡ + é«˜æˆåŠŸç‡ + è®­ç»ƒæ•ˆç‡æé«˜")
            elif results['improvement_vs_baseline'] > 0.1:
                print(f"\nğŸ‘ {attention_type} æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆ!")
            else:
                print(f"\nâš ï¸ {attention_type} æ³¨æ„åŠ›æœºåˆ¶éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            
        except Exception as e:
            print(f"âŒ {attention_type} è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
