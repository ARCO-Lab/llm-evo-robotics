#!/usr/bin/env python3
"""
å¯è§†åŒ–3å…³èŠ‚Reacherè®­ç»ƒè¿‡ç¨‹
å®æ—¶æ˜¾ç¤ºæœºæ¢°è‡‚å­¦ä¹ è¿‡ç¨‹
"""

import gymnasium as gym
import numpy as np
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback
from complete_sequential_training_with_evaluation import create_env
import time

class VisualTrainingCallback(BaseCallback):
    """å¯è§†åŒ–è®­ç»ƒå›è°ƒ"""
    
    def __init__(self, eval_env, eval_freq=1000, verbose=0):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.episode_count = 0
        
    def _on_step(self) -> bool:
        # æ¯éš”ä¸€å®šæ­¥æ•°è¿›è¡Œå¯è§†åŒ–è¯„ä¼°
        if self.n_calls % self.eval_freq == 0:
            print(f"\nğŸ® è®­ç»ƒæ­¥æ•°: {self.n_calls} - å¼€å§‹å¯è§†åŒ–è¯„ä¼°")
            self._visual_evaluation()
        return True
    
    def _visual_evaluation(self):
        """å¯è§†åŒ–è¯„ä¼°å½“å‰æ¨¡å‹"""
        obs, info = self.eval_env.reset()
        
        target_pos = info.get('target_pos', [0, 0])
        initial_distance = info.get('distance_to_target', 0)
        
        print(f"   ç›®æ ‡ä½ç½®: ({target_pos[0]:.3f}, {target_pos[1]:.3f})")
        print(f"   åˆå§‹è·ç¦»: {initial_distance:.4f}")
        
        episode_reward = 0
        min_distance = initial_distance
        success_achieved = False
        
        for step in range(100):
            action, _ = self.model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = self.eval_env.step(action)
            episode_reward += reward
            
            distance = info.get('distance_to_target', 0)
            min_distance = min(min_distance, distance)
            
            if info.get('is_success', False) and not success_achieved:
                print(f"   âœ… Step {step}: åˆ°è¾¾ç›®æ ‡! è·ç¦»={distance:.4f}")
                success_achieved = True
            
            # æ¯20æ­¥æ˜¾ç¤ºçŠ¶æ€
            if step % 20 == 0:
                fingertip_pos = info.get('fingertip_pos', [0, 0])
                print(f"   Step {step:2d}: pos=({fingertip_pos[0]:.3f},{fingertip_pos[1]:.3f}), dist={distance:.4f}")
            
            # æ§åˆ¶é€Ÿåº¦ä»¥ä¾¿è§‚å¯Ÿ
            time.sleep(0.02)
            
            if terminated or truncated:
                break
        
        improvement = initial_distance - min_distance
        print(f"   ç»“æœ: æœ€å°è·ç¦»={min_distance:.4f}, æ”¹å–„={improvement:.4f}, å¥–åŠ±={episode_reward:.1f}, æˆåŠŸ={'âœ…' if success_achieved else 'âŒ'}")

def visual_training():
    """å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"""
    print("ğŸ® å¼€å§‹å¯è§†åŒ–3å…³èŠ‚Reacherè®­ç»ƒ")
    print("ğŸ“‹ ä½ å°†çœ‹åˆ°:")
    print("  - è®­ç»ƒè¿‡ç¨‹ä¸­çš„å®æ—¶æœºæ¢°è‡‚è¿åŠ¨")
    print("  - æ¯1000æ­¥çš„æ€§èƒ½è¯„ä¼°")
    print("  - å­¦ä¹ è¿›åº¦çš„å¯è§†åŒ–å±•ç¤º")
    print("  - çº¢è‰²ç›®æ ‡çƒå’Œç»¿è‰²æœ«ç«¯æ‰§è¡Œå™¨")
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼ˆæ— æ¸²æŸ“ï¼‰
    train_env = create_env(3, render_mode=None)
    
    # åˆ›å»ºå¯è§†åŒ–è¯„ä¼°ç¯å¢ƒï¼ˆæœ‰æ¸²æŸ“ï¼‰
    eval_env = create_env(3, render_mode='human')
    
    print("âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºSACæ¨¡å‹
    model = SAC(
        'MlpPolicy',
        train_env,
        verbose=1,
        learning_rate=3e-4,
        buffer_size=50000,  # å‡å°bufferä»¥åŠ å¿«è®­ç»ƒ
        batch_size=128,     # å‡å°batch size
    )
    
    print("âœ… SACæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # åˆ›å»ºå¯è§†åŒ–å›è°ƒ
    visual_callback = VisualTrainingCallback(
        eval_env=eval_env,
        eval_freq=1000,  # æ¯1000æ­¥è¯„ä¼°ä¸€æ¬¡
        verbose=1
    )
    
    print("\nğŸ¯ å¼€å§‹å¯è§†åŒ–è®­ç»ƒ...")
    print("   æŒ‰Ctrl+Cå¯ä»¥åœæ­¢è®­ç»ƒ")
    
    try:
        # å¼€å§‹è®­ç»ƒ
        model.learn(
            total_timesteps=10000,
            callback=visual_callback,
            progress_bar=True
        )
        
        print("\nâœ… è®­ç»ƒå®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    
    # ä¿å­˜æ¨¡å‹
    model.save('models/visual_trained_3joint_sac')
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: models/visual_trained_3joint_sac.zip")
    
    # æœ€ç»ˆæµ‹è¯•
    print("\nğŸ æœ€ç»ˆæ€§èƒ½æµ‹è¯•...")
    final_test(model, eval_env)
    
    train_env.close()
    eval_env.close()

def final_test(model, env):
    """æœ€ç»ˆæ€§èƒ½æµ‹è¯•"""
    print("ğŸ§ª è¿›è¡Œ5ä¸ªepisodesçš„æœ€ç»ˆæµ‹è¯•...")
    
    success_count = 0
    rewards = []
    
    for i in range(5):
        print(f"\n--- æœ€ç»ˆæµ‹è¯• Episode {i+1} ---")
        obs, info = env.reset()
        
        target_pos = info.get('target_pos', [0, 0])
        initial_distance = info.get('distance_to_target', 0)
        
        print(f"ç›®æ ‡: ({target_pos[0]:.3f}, {target_pos[1]:.3f}), åˆå§‹è·ç¦»: {initial_distance:.4f}")
        
        episode_reward = 0
        min_distance = initial_distance
        success_achieved = False
        
        for step in range(100):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            
            distance = info.get('distance_to_target', 0)
            min_distance = min(min_distance, distance)
            
            if info.get('is_success', False):
                success_achieved = True
            
            # æ§åˆ¶é€Ÿåº¦
            time.sleep(0.03)
            
            if terminated or truncated:
                break
        
        if success_achieved:
            success_count += 1
        
        rewards.append(episode_reward)
        improvement = initial_distance - min_distance
        
        print(f"ç»“æœ: æœ€å°è·ç¦»={min_distance:.4f}, æ”¹å–„={improvement:.4f}, å¥–åŠ±={episode_reward:.1f}, æˆåŠŸ={'âœ…' if success_achieved else 'âŒ'}")
    
    print(f"\nğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœ:")
    print(f"æˆåŠŸç‡: {success_count/5:.1%}")
    print(f"å¹³å‡å¥–åŠ±: {np.mean(rewards):.1f}")
    print(f"å¥–åŠ±èŒƒå›´: [{min(rewards):.1f}, {max(rewards):.1f}]")

def quick_demo():
    """å¿«é€Ÿæ¼”ç¤ºç°æœ‰æ¨¡å‹"""
    print("ğŸ® å¿«é€Ÿæ¼”ç¤ºç°æœ‰3å…³èŠ‚æ¨¡å‹")
    
    try:
        # å°è¯•åŠ è½½ç°æœ‰æ¨¡å‹
        model = SAC.load('models/complete_sequential_3joint_reacher.zip')
        print("âœ… åŠ è½½ç°æœ‰æ¨¡å‹æˆåŠŸ")
    except:
        print("âŒ æ— æ³•åŠ è½½ç°æœ‰æ¨¡å‹ï¼Œå°†ä½¿ç”¨éšæœºåŠ¨ä½œæ¼”ç¤º")
        model = None
    
    # åˆ›å»ºå¯è§†åŒ–ç¯å¢ƒ
    env = create_env(3, render_mode='human')
    
    print("\nğŸ¯ å¼€å§‹æ¼”ç¤º...")
    print("   ä½ å°†çœ‹åˆ°3å…³èŠ‚æœºæ¢°è‡‚çš„è¿åŠ¨")
    
    try:
        for episode in range(3):
            print(f"\n--- Episode {episode+1} ---")
            obs, info = env.reset()
            
            target_pos = info.get('target_pos', [0, 0])
            initial_distance = info.get('distance_to_target', 0)
            
            print(f"ç›®æ ‡: ({target_pos[0]:.3f}, {target_pos[1]:.3f}), åˆå§‹è·ç¦»: {initial_distance:.4f}")
            
            for step in range(100):
                if model is not None:
                    action, _ = model.predict(obs, deterministic=True)
                else:
                    action = env.action_space.sample()  # éšæœºåŠ¨ä½œ
                
                obs, reward, terminated, truncated, info = env.step(action)
                
                distance = info.get('distance_to_target', 0)
                
                if step % 20 == 0:
                    fingertip_pos = info.get('fingertip_pos', [0, 0])
                    print(f"  Step {step:2d}: pos=({fingertip_pos[0]:.3f},{fingertip_pos[1]:.3f}), dist={distance:.4f}")
                
                # æ§åˆ¶é€Ÿåº¦
                time.sleep(0.05)
                
                if terminated or truncated:
                    break
            
            time.sleep(1.0)  # Episodeé—´æš‚åœ
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    
    env.close()

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "demo":
        # å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼
        quick_demo()
    else:
        # å®Œæ•´å¯è§†åŒ–è®­ç»ƒæ¨¡å¼
        visual_training()

