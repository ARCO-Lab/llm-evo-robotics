#!/usr/bin/env python3
"""
è¿ç»­ SAC è®­ç»ƒæµ‹è¯• - æ— å¼ºåˆ¶é‡ç½®
"""

import sys
import os
import numpy as np
import torch
import time
from collections import deque

# æ·»åŠ è·¯å¾„
sys.path.append('examples/surrogate_model/sac')
sys.path.append('examples/2d_reacher/envs')

from sb3_sac_adapter import SB3SACFactory
from reacher_env_factory import create_reacher_env

def continuous_sac_training():
    print("ğŸš€ è¿ç»­ SAC è®­ç»ƒæµ‹è¯• - å·²å–æ¶ˆå¼ºåˆ¶é‡ç½®")
    print("ğŸ¯ ç›®æ ‡: éªŒè¯ SAC è¿ç»­å­¦ä¹ æ•ˆæœ")
    print("ğŸ“Š è®­ç»ƒå‚æ•°: 2000 æ­¥ï¼Œæ¯ 400 æ­¥è¯„ä¼°ä¸€æ¬¡")
    print("=" * 70)
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_reacher_env(version='mujoco', render_mode='human')
    
    # åˆ›å»º SAC
    sac = SB3SACFactory.create_reacher_sac(
        action_dim=2,
        buffer_capacity=10000,
        batch_size=64,
        lr=1e-3,
        device='cpu'
    )
    sac.set_env(env)
    
    # é‡ç½®ç¯å¢ƒ
    reset_result = env.reset()
    if isinstance(reset_result, tuple):
        obs, info = reset_result
    else:
        obs = reset_result
    
    print("âœ… ç¯å¢ƒå’Œ SAC åˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {env.action_space}")
    print("=" * 70)
    
    # è®­ç»ƒç»Ÿè®¡
    episode_rewards = []
    episode_lengths = []
    episode_distances = []
    success_episodes = 0
    total_episodes = 0
    
    # å½“å‰ episode ç»Ÿè®¡
    current_reward = 0
    current_length = 0
    best_distance = float('inf')
    
    # å­¦ä¹ è¿›åº¦ç»Ÿè®¡
    recent_rewards = deque(maxlen=10)
    recent_distances = deque(maxlen=10)
    
    # åŠ¨ä½œå¤šæ ·æ€§ç»Ÿè®¡
    action_history = deque(maxlen=50)
    
    print("ğŸ¯ å¼€å§‹è¿ç»­è®­ç»ƒ...")
    start_time = time.time()
    
    for step in range(2000):  # è®­ç»ƒ 2000 æ­¥
        # è·å–åŠ¨ä½œ
        action = sac.get_action(obs, deterministic=False)
        action_history.append(action.cpu().numpy().copy())
        
        # æ‰§è¡ŒåŠ¨ä½œ
        step_result = env.step(action.cpu().numpy())
        if len(step_result) == 5:
            obs, reward, terminated, truncated, info = step_result
        else:
            obs, reward, done, info = step_result
            terminated = done
            truncated = False
        
        current_reward += reward
        current_length += 1
        
        # è·å–è·ç¦»ä¿¡æ¯
        distance = obs[8] if len(obs) > 8 else 999
        best_distance = min(best_distance, distance)
        
        # Episode ç»“æŸå¤„ç†
        if terminated or truncated:
            total_episodes += 1
            episode_rewards.append(current_reward)
            episode_lengths.append(current_length)
            episode_distances.append(best_distance)
            recent_rewards.append(current_reward)
            recent_distances.append(best_distance)
            
            # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
            if best_distance < 15.0:  # 15px å†…ç®—æˆåŠŸ
                success_episodes += 1
                print(f"ğŸ¯ Episode {total_episodes} æˆåŠŸ! è·ç¦»: {best_distance:.1f}px, å¥–åŠ±: {current_reward:.2f}, æ­¥æ•°: {current_length}")
            else:
                if total_episodes % 3 == 0:  # æ¯3ä¸ªå¤±è´¥episodeæ‰“å°ä¸€æ¬¡
                    print(f"ğŸ“Š Episode {total_episodes}: è·ç¦»: {best_distance:.1f}px, å¥–åŠ±: {current_reward:.2f}, æ­¥æ•°: {current_length}")
            
            # é‡ç½®
            reset_result = env.reset()
            if isinstance(reset_result, tuple):
                obs, info = reset_result
            else:
                obs = reset_result
            
            current_reward = 0
            current_length = 0
            best_distance = float('inf')
        
        # æ¯ 400 æ­¥è¿›è¡Œè¯¦ç»†è¯„ä¼°
        if (step + 1) % 400 == 0:
            elapsed_time = time.time() - start_time
            
            print(f"\nğŸ“ˆ è®­ç»ƒè¿›åº¦è¯„ä¼° - Step {step + 1}/2000 ({elapsed_time/60:.1f}åˆ†é’Ÿ)")
            print("=" * 50)
            
            # åŠ¨ä½œå¤šæ ·æ€§åˆ†æ
            if len(action_history) > 10:
                actions_array = np.array(list(action_history))
                action_std = np.std(actions_array, axis=0)
                action_mean = np.mean(actions_array, axis=0)
                print(f"ğŸ® åŠ¨ä½œåˆ†æ:")
                print(f"   å¹³å‡åŠ¨ä½œ: [{action_mean[0]:.4f}, {action_mean[1]:.4f}]")
                print(f"   åŠ¨ä½œæ ‡å‡†å·®: [{action_std[0]:.4f}, {action_std[1]:.4f}]")
                
                if max(action_std) > 0.002:
                    print(f"   âœ… åŠ¨ä½œæœ‰å¤šæ ·æ€§ï¼ŒSAC åœ¨æ¢ç´¢")
                else:
                    print(f"   âš ï¸ åŠ¨ä½œå˜åŒ–è¾ƒå°ï¼Œå¯èƒ½æ”¶æ•›æˆ–éœ€è¦æ›´å¤šæ¢ç´¢")
            
            if len(episode_rewards) > 0:
                avg_reward = np.mean(episode_rewards)
                avg_distance = np.mean(episode_distances)
                avg_length = np.mean(episode_lengths)
                success_rate = (success_episodes / total_episodes) * 100 if total_episodes > 0 else 0
                
                print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
                print(f"   Episodes: {total_episodes}")
                print(f"   æˆåŠŸç‡: {success_rate:.1f}% ({success_episodes}/{total_episodes})")
                print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
                print(f"   å¹³å‡è·ç¦»: {avg_distance:.1f}px")
                print(f"   å¹³å‡æ­¥æ•°: {avg_length:.1f}")
                
                # æœ€è¿‘è¡¨ç°
                if len(recent_rewards) > 0:
                    recent_avg_reward = np.mean(recent_rewards)
                    recent_avg_distance = np.mean(recent_distances)
                    print(f"ğŸ“ˆ æœ€è¿‘10ä¸ªEpisodes:")
                    print(f"   å¹³å‡å¥–åŠ±: {recent_avg_reward:.2f}")
                    print(f"   å¹³å‡è·ç¦»: {recent_avg_distance:.1f}px")
                
                # å­¦ä¹ è¿›æ­¥åˆ†æ
                if len(episode_rewards) >= 10:
                    early_rewards = episode_rewards[:5] if len(episode_rewards) >= 10 else episode_rewards[:len(episode_rewards)//2]
                    recent_rewards_list = episode_rewards[-5:]
                    if len(early_rewards) > 0 and len(recent_rewards_list) > 0:
                        improvement = np.mean(recent_rewards_list) - np.mean(early_rewards)
                        print(f"ğŸ¯ å­¦ä¹ è¿›æ­¥: {improvement:.2f} (æ­£æ•°è¡¨ç¤ºæ”¹å–„)")
                
                # è·å–æŸå¤±ä¿¡æ¯
                if step >= 100:  # å­¦ä¹ å¼€å§‹å
                    losses = sac.update()
                    print(f"ğŸ§  ç½‘ç»œæŸå¤±:")
                    print(f"   Actor Loss: {losses['actor_loss']:.4f}")
                    print(f"   Critic Loss: {losses['critic_loss']:.4f}")
                    print(f"   Alpha: {losses['alpha']:.4f}")
            
            print("=" * 50 + "\n")
    
    # æœ€ç»ˆè¯„ä¼°
    total_time = time.time() - start_time
    print("\n" + "=" * 70)
    print("ğŸ† è¿ç»­ SAC è®­ç»ƒæµ‹è¯•å®Œæˆ!")
    print("=" * 70)
    
    if len(episode_rewards) > 0:
        final_avg_reward = np.mean(episode_rewards)
        final_avg_distance = np.mean(episode_distances)
        final_success_rate = (success_episodes / total_episodes) * 100
        
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“Š æ€»Episodes: {total_episodes}")
        print(f"ğŸ¯ æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.1f}%")
        print(f"ğŸ æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_avg_reward:.2f}")
        print(f"ğŸ“ æœ€ç»ˆå¹³å‡è·ç¦»: {final_avg_distance:.1f}px")
        
        # è®­ç»ƒæ•ˆæœè¯„ä¼°
        print(f"\nğŸ” è®­ç»ƒæ•ˆæœè¯„ä¼°:")
        if final_success_rate >= 30:
            print("ğŸ¥‡ ä¼˜ç§€! SAC å·²ç»å­¦ä¼šäº† Reacher ä»»åŠ¡")
        elif final_success_rate >= 15:
            print("ğŸ¥ˆ è‰¯å¥½! SAC æœ‰æ˜æ˜¾å­¦ä¹ è¿›æ­¥")
        elif final_success_rate >= 5:
            print("ğŸ¥‰ æœ‰è¿›æ­¥! SAC å¼€å§‹å­¦ä¹ ä»»åŠ¡")
        elif final_avg_reward > -0.3:
            print("ğŸ“ˆ å­¦ä¹ ä¸­... å¥–åŠ±æœ‰æ”¹å–„")
        else:
            print("ğŸ”„ éœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´")
        
        # æœ€ä½³è¡¨ç°
        if len(episode_distances) > 0:
            best_distance_ever = min(episode_distances)
            best_reward_ever = max(episode_rewards)
            print(f"\nğŸ… æœ€ä½³è¡¨ç°:")
            print(f"   æœ€è¿‘è·ç¦»: {best_distance_ever:.1f}px")
            print(f"   æœ€é«˜å¥–åŠ±: {best_reward_ever:.2f}")
        
        # åŠ¨ä½œå¤šæ ·æ€§æœ€ç»ˆåˆ†æ
        if len(action_history) > 10:
            final_actions = np.array(list(action_history))
            final_action_std = np.std(final_actions, axis=0)
            print(f"\nğŸ® æœ€ç»ˆåŠ¨ä½œåˆ†æ:")
            print(f"   åŠ¨ä½œæ ‡å‡†å·®: [{final_action_std[0]:.4f}, {final_action_std[1]:.4f}]")
            if max(final_action_std) > 0.002:
                print("   âœ… SAC ä¿æŒäº†åŠ¨ä½œå¤šæ ·æ€§")
            else:
                print("   âš ï¸ åŠ¨ä½œè¶‹äºå›ºå®šï¼Œå¯èƒ½å·²æ”¶æ•›")
    
    print(f"\nâœ… è¿ç»­è®­ç»ƒæµ‹è¯•å®Œæˆ! æ— å¼ºåˆ¶é‡ç½®å¹²æ‰°ã€‚")
    env.close()
    
    return {
        'success_rate': final_success_rate if len(episode_rewards) > 0 else 0,
        'avg_reward': final_avg_reward if len(episode_rewards) > 0 else 0,
        'avg_distance': final_avg_distance if len(episode_rewards) > 0 else 999,
        'total_episodes': total_episodes
    }

if __name__ == "__main__":
    results = continuous_sac_training()
    print(f"\nğŸŠ è¿ç»­è®­ç»ƒç»“æœæ€»ç»“:")
    print(f"   æˆåŠŸç‡: {results['success_rate']:.1f}%")
    print(f"   å¹³å‡å¥–åŠ±: {results['avg_reward']:.2f}")
    print(f"   å¹³å‡è·ç¦»: {results['avg_distance']:.1f}px")
    print(f"   æ€»Episodes: {results['total_episodes']}")


