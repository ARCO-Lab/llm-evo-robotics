#!/usr/bin/env python3
"""
åŸºäºMADDPGçš„å®Œæ•´ä¾æ¬¡è®­ç»ƒå’Œæµ‹è¯•è„šæœ¬ï¼š
1. å°†SACæ›¿æ¢ä¸ºbaseline MADDPGå®ç°
2. æ¯ä¸ªå…³èŠ‚ä½œä¸ºä¸€ä¸ªæ™ºèƒ½ä½“ï¼ŒååŒå­¦ä¹ 
3. ä¸­å¿ƒåŒ–è®­ç»ƒï¼Œåˆ†å¸ƒå¼æ‰§è¡Œ
4. ä¿æŒç›¸åŒçš„ç¯å¢ƒå’Œå¥–åŠ±å‡½æ•°
"""

import os
import numpy as np
import gymnasium as gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from stable_baselines3 import DDPG
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.noise import NormalActionNoise
import time
import tempfile
from gymnasium.envs.mujoco import MujocoEnv
from gymnasium.spaces import Box
from collections import deque
import random
from copy import deepcopy

# è®¾ç½®æ¸²æŸ“ç¯å¢ƒå˜é‡
os.environ['MUJOCO_GL'] = 'glfw'
os.environ['MUJOCO_RENDERER'] = 'glfw'

# ğŸ¯ ç»Ÿä¸€ä¸ºæ ‡å‡†Reacher-v5å¥–åŠ±å‚æ•°
SUCCESS_THRESHOLD_2JOINT = 0.05  # 2å…³èŠ‚ä¿æŒåŸæœ‰é˜ˆå€¼ 5cm
SUCCESS_THRESHOLD_RATIO = 0.25   # 3+å…³èŠ‚ï¼šk = 0.25ï¼ŒæˆåŠŸé˜ˆå€¼ä¸å¯è¾¾åŠå¾„çš„æ¯”ä¾‹
# æ ‡å‡†Reacher-v5å¥–åŠ±å‚æ•°
REWARD_NEAR_WEIGHT = 1.0         # è·ç¦»å¥–åŠ±æƒé‡ï¼ˆæ ‡å‡†Reacher-v5ï¼‰
REWARD_CONTROL_WEIGHT = 0.1      # æ§åˆ¶æƒ©ç½šæƒé‡ï¼ˆæ ‡å‡†Reacher-v5ï¼‰
TARGET_MIN_RATIO = 0.15          # ç›®æ ‡æœ€å°è·ç¦»æ¯”ä¾‹ï¼ˆ3+å…³èŠ‚ï¼‰
TARGET_MAX_RATIO = 0.85          # ç›®æ ‡æœ€å¤§è·ç¦»æ¯”ä¾‹ï¼ˆ3+å…³èŠ‚ï¼‰

# ============================================================================
# ğŸ¤– åŸºäºStable-Baselines3 DDPGçš„MADDPGå®ç°
# ============================================================================

class SingleJointActionEnv(gym.Wrapper):
    """å•å…³èŠ‚åŠ¨ä½œç¯å¢ƒåŒ…è£…å™¨ - æ¯ä¸ªæ™ºèƒ½ä½“åªæ§åˆ¶ä¸€ä¸ªå…³èŠ‚"""
    
    def __init__(self, env, joint_id, num_joints):
        super().__init__(env)
        self.joint_id = joint_id
        self.num_joints = num_joints
        
        # é‡æ–°å®šä¹‰åŠ¨ä½œç©ºé—´ - åªæ§åˆ¶ä¸€ä¸ªå…³èŠ‚
        self.action_space = Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)
        
        # è§‚å¯Ÿç©ºé—´ä¿æŒä¸å˜
        self.observation_space = env.observation_space
        
        # å­˜å‚¨å…¶ä»–æ™ºèƒ½ä½“çš„åŠ¨ä½œï¼ˆç”¨äºåè°ƒï¼‰
        self._other_actions = np.zeros(num_joints)
        
        print(f"ğŸ¤– SingleJointActionEnv {joint_id}: æ§åˆ¶å…³èŠ‚{joint_id}, åŠ¨ä½œç»´åº¦=1")
    
    def step(self, action):
        # æ„å»ºå®Œæ•´çš„åŠ¨ä½œå‘é‡
        full_action = self._other_actions.copy()
        full_action[self.joint_id] = action[0]
        
        # æ‰§è¡Œç¯å¢ƒæ­¥éª¤
        obs, reward, terminated, truncated, info = self.env.step(full_action)
        
        return obs, reward, terminated, truncated, info
    
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self._other_actions = np.zeros(self.num_joints)
        return obs, info
    
    def set_other_actions(self, actions):
        """è®¾ç½®å…¶ä»–æ™ºèƒ½ä½“çš„åŠ¨ä½œ"""
        self._other_actions = actions.copy()

class MADDPG_SB3:
    """åŸºäºStable-Baselines3 DDPGçš„Multi-Agent DDPG - ç‹¬ç«‹å­¦ä¹ ç‰ˆæœ¬"""
    
    def __init__(self, env, num_agents, learning_rate=1e-3, gamma=0.99, tau=0.005):
        self.num_agents = num_agents
        self.env = env
        self.agents = []
        self.single_envs = []
        
        # ä¸ºæ¯ä¸ªå…³èŠ‚åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„DDPGæ™ºèƒ½ä½“å’Œç¯å¢ƒ
        for i in range(num_agents):
            # åˆ›å»ºå•å…³èŠ‚åŠ¨ä½œç©ºé—´çš„ç¯å¢ƒ
            single_joint_env = SingleJointActionEnv(env, joint_id=i, num_joints=num_agents)
            self.single_envs.append(single_joint_env)
            
            # æ·»åŠ åŠ¨ä½œå™ªå£°
            action_noise = NormalActionNoise(
                mean=np.zeros(1), 
                sigma=0.1 * np.ones(1)
            )
            
            # åˆ›å»ºDDPGæ™ºèƒ½ä½“
            agent = DDPG(
                "MlpPolicy",
                single_joint_env,
                learning_rate=learning_rate,
                gamma=gamma,
                tau=tau,
                action_noise=action_noise,
                verbose=0,
                device='cpu',
                batch_size=256,
                buffer_size=100000
            )
            
            self.agents.append(agent)
            
            print(f"ğŸ¤– DDPG Agent {i}: å·²åˆ›å»ºï¼Œæ§åˆ¶å…³èŠ‚{i}")
        
        print(f"ğŸŒŸ MADDPG_SB3åˆå§‹åŒ–å®Œæˆ: {num_agents}ä¸ªç‹¬ç«‹DDPGæ™ºèƒ½ä½“")
    
    def predict(self, obs, deterministic=True):
        """æ‰€æœ‰æ™ºèƒ½ä½“åŒæ—¶é¢„æµ‹åŠ¨ä½œ"""
        actions = []
        for i, agent in enumerate(self.agents):
            # æ¯ä¸ªæ™ºèƒ½ä½“åŸºäºå…¨å±€è§‚å¯Ÿåšå†³ç­–
            action, _ = agent.predict(obs, deterministic=deterministic)
            actions.append(action[0])  # å–å‡ºå•ä¸ªåŠ¨ä½œå€¼
        
        return np.array(actions)
    
    def learn(self, total_timesteps, log_interval=1000):
        """ä½¿ç”¨ç‹¬ç«‹å­¦ä¹ æ–¹å¼è®­ç»ƒæ‰€æœ‰æ™ºèƒ½ä½“"""
        print(f"ğŸ¯ å¼€å§‹MADDPG_SB3ç‹¬ç«‹å­¦ä¹ è®­ç»ƒ...")
        print(f"   æ¯ä¸ªæ™ºèƒ½ä½“å°†è®­ç»ƒ {total_timesteps // self.num_agents} æ­¥")
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹è®­ç»ƒ
        episode_rewards = []
        for i, agent in enumerate(self.agents):
            print(f"\n   ğŸ¤– è®­ç»ƒæ™ºèƒ½ä½“ {i} (æ§åˆ¶å…³èŠ‚{i})...")
            try:
                agent.learn(total_timesteps=total_timesteps // self.num_agents)
                episode_rewards.append(f"Agent_{i}_completed")
            except Exception as e:
                print(f"   âŒ æ™ºèƒ½ä½“ {i} è®­ç»ƒå¤±è´¥: {e}")
                episode_rewards.append(f"Agent_{i}_failed")
        
        print(f"\nâœ… MADDPG_SB3ç‹¬ç«‹å­¦ä¹ è®­ç»ƒå®Œæˆ!")
        return episode_rewards
    
    def save(self, filepath):
        """ä¿å­˜æ‰€æœ‰æ™ºèƒ½ä½“"""
        import os
        base_path = filepath.replace('.pth', '').replace('.zip', '')
        
        for i, agent in enumerate(self.agents):
            agent_path = f"{base_path}_agent_{i}.zip"
            agent.save(agent_path)
        
        print(f"ğŸ’¾ MADDPG_SB3æ¨¡å‹å·²ä¿å­˜: {base_path}_agent_*.zip")
    
    def load(self, filepath):
        """åŠ è½½æ‰€æœ‰æ™ºèƒ½ä½“"""
        base_path = filepath.replace('.pth', '').replace('.zip', '')
        
        for i, agent in enumerate(self.agents):
            agent_path = f"{base_path}_agent_{i}.zip"
            if os.path.exists(agent_path):
                agent.load(agent_path)
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ™ºèƒ½ä½“{i}çš„æ¨¡å‹æ–‡ä»¶: {agent_path}")
        
        print(f"ğŸ“‚ MADDPG_SB3æ¨¡å‹å·²åŠ è½½: {base_path}_agent_*.zip")

# ============================================================================
# ğŸŒ ç¯å¢ƒç›¸å…³ä»£ç ï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰
# ============================================================================

# ä¿®å¤ç›®æ ‡æ»šåŠ¨é—®é¢˜çš„åŸºç±»
class SequentialReacherEnv(MujocoEnv):
    """ä¾æ¬¡è®­ç»ƒç”¨çš„Reacherç¯å¢ƒåŸºç±»ï¼ˆ3+å…³èŠ‚åº”ç”¨ç»Ÿä¸€å¥–åŠ±è§„èŒƒï¼‰"""
    
    def __init__(self, xml_content, num_joints, link_lengths, render_mode=None, show_position_info=False, **kwargs):
        self.num_joints = num_joints
        self.link_lengths = link_lengths
        self.show_position_info = show_position_info  # ğŸ¯ æ§åˆ¶æ˜¯å¦æ˜¾ç¤ºå®æ—¶ä½ç½®ä¿¡æ¯
        
        # ğŸ¯ è®¡ç®—å¯è¾¾åŠå¾„Rå’Œç»Ÿä¸€çš„æˆåŠŸé˜ˆå€¼ï¼ˆä»…3+å…³èŠ‚ï¼‰
        self.max_reach = sum(link_lengths)
        if num_joints >= 3:
            self.success_threshold = SUCCESS_THRESHOLD_RATIO * self.max_reach
            self.use_unified_reward = True
        else:
            self.success_threshold = SUCCESS_THRESHOLD_2JOINT
            self.use_unified_reward = False
        
        # åˆ›å»ºä¸´æ—¶XMLæ–‡ä»¶
        self.xml_file = tempfile.NamedTemporaryFile(mode='w', suffix='.xml', delete=False)
        self.xml_file.write(xml_content)
        self.xml_file.flush()
        
        # è®¡ç®—è§‚å¯Ÿç©ºé—´ç»´åº¦
        obs_dim = num_joints * 3 + 4  # cos, sin, vel + ee_pos + target_pos
        observation_space = Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float64)
        
        super().__init__(
            self.xml_file.name,
            frame_skip=2,
            observation_space=observation_space,
            render_mode=render_mode,
            width=480,  # ä¸æ ‡å‡†Reacherä¸€è‡´
            height=480  # ä¸æ ‡å‡†Reacherä¸€è‡´
        )
        
        self.step_count = 0
        self.max_episode_steps = 100  # æµ‹è¯•æ—¶æ¯ä¸ªepisode 100æ­¥
        
        reward_type = "ç»Ÿä¸€å¥–åŠ±è§„èŒƒ" if self.use_unified_reward else "é»˜è®¤å¥–åŠ±"
        position_info_status = "å¼€å¯" if self.show_position_info else "å…³é—­"
        print(f"âœ… {num_joints}å…³èŠ‚Reacheråˆ›å»ºå®Œæˆ ({reward_type}, ä½ç½®ä¿¡æ¯æ˜¾ç¤º: {position_info_status})")
        print(f"   é“¾é•¿: {link_lengths}")
        print(f"   ğŸ¯ å¯è¾¾åŠå¾„R: {self.max_reach:.3f}")
        print(f"   ğŸ¯ æˆåŠŸé˜ˆå€¼: {self.success_threshold:.3f}")
        if self.use_unified_reward:
            print(f"   ğŸ¯ æˆåŠŸé˜ˆå€¼æ¯”ä¾‹: {SUCCESS_THRESHOLD_RATIO:.1%} * R")
            print(f"   ğŸ¯ ç›®æ ‡ç”ŸæˆèŒƒå›´: {self.calculate_unified_target_min():.3f} ~ {self.calculate_unified_target_max():.3f}")
        else:
            print(f"   ğŸ¯ ç›®æ ‡ç”ŸæˆèŒƒå›´: {self.calculate_target_range():.3f}")
        
        if self.show_position_info:
            print(f"   ğŸ“ å®æ—¶ä½ç½®ä¿¡æ¯: æ¯10æ­¥æ˜¾ç¤ºä¸€æ¬¡end-effectorä½ç½®")
    
    def calculate_max_reach(self):
        """è®¡ç®—ç†è®ºæœ€å¤§å¯è¾¾è·ç¦»"""
        return sum(self.link_lengths)
    
    def calculate_target_range(self):
        """è®¡ç®—ç›®æ ‡ç”Ÿæˆçš„æœ€å¤§è·ç¦»ï¼ˆ2å…³èŠ‚ç”¨ï¼‰"""
        max_reach = self.calculate_max_reach()
        return max_reach * 0.85  # 85%çš„å¯è¾¾èŒƒå›´ï¼Œç•™15%æŒ‘æˆ˜æ€§
    
    def calculate_unified_target_min(self):
        """è®¡ç®—ç»Ÿä¸€ç›®æ ‡ç”Ÿæˆçš„æœ€å°è·ç¦»ï¼ˆ3+å…³èŠ‚ç”¨ï¼‰"""
        return TARGET_MIN_RATIO * self.max_reach
    
    def calculate_unified_target_max(self):
        """è®¡ç®—ç»Ÿä¸€ç›®æ ‡ç”Ÿæˆçš„æœ€å¤§è·ç¦»ï¼ˆ3+å…³èŠ‚ç”¨ï¼‰"""
        return TARGET_MAX_RATIO * self.max_reach
    
    def generate_unified_target(self):
        """ğŸ¯ ç»Ÿä¸€çš„ç›®æ ‡ç”Ÿæˆç­–ç•¥ - åŸºäºå¯è¾¾èŒƒå›´çš„æ™ºèƒ½ç”Ÿæˆ"""
        if self.use_unified_reward:
            # 3+å…³èŠ‚ï¼šä½¿ç”¨ç»Ÿä¸€çš„ç›®æ ‡ç”Ÿæˆç­–ç•¥
            min_distance = self.calculate_unified_target_min()
            max_distance = self.calculate_unified_target_max()
        else:
            # 2å…³èŠ‚ï¼šä¿æŒåŸæœ‰ç­–ç•¥
            max_distance = self.calculate_target_range()
            min_distance = 0.05  # æœ€å°è·ç¦»ï¼Œé¿å…å¤ªå®¹æ˜“
        
        # ä½¿ç”¨æåæ ‡ç”Ÿæˆç›®æ ‡
        target_distance = self.np_random.uniform(min_distance, max_distance)
        target_angle = self.np_random.uniform(-np.pi, np.pi)
        
        target_x = target_distance * np.cos(target_angle)
        target_y = target_distance * np.sin(target_angle)
        
        return target_x, target_y
    
    def step(self, action):
        # ä½¿ç”¨æ ‡å‡†MuJoCoæ­¥éª¤ï¼Œè®©å†…ç½®çš„V-Syncå¤„ç†FPS
        self.do_simulation(action, self.frame_skip)
        
        observation = self._get_obs()
        reward = self.reward(action)
        
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåƒæ ‡å‡†Reacherä¸€æ ·åœ¨stepä¸­æ¸²æŸ“
        if self.render_mode == "human":
            self.render()
        
        # è®¡ç®—è·ç¦»
        fingertip_pos = self.get_body_com("fingertip")[:2]
        target_pos = self.get_body_com("target")[:2]
        distance = np.linalg.norm(fingertip_pos - target_pos)
        
        # ğŸ¯ ç§»é™¤terminateé€‰é¡¹ï¼šä¸å†å› ä¸ºåˆ°è¾¾ç›®æ ‡è€Œæå‰ç»“æŸ
        terminated = False
        
        self.step_count += 1
        truncated = self.step_count >= self.max_episode_steps
        
        # è®¡ç®—å½’ä¸€åŒ–è·ç¦»ï¼ˆä»…3+å…³èŠ‚ï¼‰
        normalized_distance = distance / self.max_reach if self.use_unified_reward else None
        
        info = {
            'distance_to_target': distance,
            'normalized_distance': normalized_distance,
            'is_success': distance < self.success_threshold,  # ğŸ”§ å…³é”®ï¼šç»Ÿä¸€çš„æˆåŠŸåˆ¤æ–­ï¼ˆä»…ç”¨äºç»Ÿè®¡ï¼‰
            'max_reach': self.max_reach,
            'success_threshold': self.success_threshold,
            'use_unified_reward': self.use_unified_reward,
            'fingertip_pos': fingertip_pos.copy(),
            'target_pos': target_pos.copy()
        }
        
        return observation, reward, terminated, truncated, info
    
    def reward(self, action):
        fingertip_pos = self.get_body_com("fingertip")[:2]
        target_pos = self.get_body_com("target")[:2]
        distance = np.linalg.norm(fingertip_pos - target_pos)
        
        # ğŸ¯ ç»Ÿä¸€å¥–åŠ±å°ºåº¦ï¼š3+å…³èŠ‚ä½¿ç”¨å½’ä¸€åŒ–è·ç¦»ï¼Œ2å…³èŠ‚ä¿æŒæ ‡å‡†Reacher-v5
        if self.use_unified_reward:  # 3+å…³èŠ‚ä½¿ç”¨å½’ä¸€åŒ–è·ç¦»
            normalized_distance = distance / self.max_reach
            distance_reward = -REWARD_NEAR_WEIGHT * normalized_distance
        else:  # 2å…³èŠ‚ä¿æŒæ ‡å‡†Reacher-v5å¥–åŠ±
            distance_reward = -REWARD_NEAR_WEIGHT * distance
        
        # æ§åˆ¶æƒ©ç½šä¿æŒä¸å˜
        control_penalty = -REWARD_CONTROL_WEIGHT * np.sum(np.square(action))
        
        total_reward = distance_reward + control_penalty
        
        return total_reward
    
    def _get_obs(self):
        # Nå…³èŠ‚çš„è§‚å¯Ÿï¼šcos, sin, vel, fingertip_pos, target_pos
        theta = self.data.qpos.flat[:self.num_joints]
        obs = np.concatenate([
            np.cos(theta),                    # Nä¸ªcoså€¼
            np.sin(theta),                    # Nä¸ªsinå€¼
            self.data.qvel.flat[:self.num_joints],  # Nä¸ªå…³èŠ‚é€Ÿåº¦
            self.get_body_com("fingertip")[:2],     # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½® (x,y)
            self.get_body_com("target")[:2],        # ç›®æ ‡ä½ç½® (x,y)
        ])
        return obs
    
    def reset_model(self):
        # ğŸ”§ ä¿®å¤ç›®æ ‡æ»šåŠ¨é—®é¢˜çš„é‡ç½®ç­–ç•¥
        qpos = self.init_qpos + self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq)
        qvel = self.init_qvel.copy()  # ä»åˆå§‹é€Ÿåº¦å¼€å§‹
        
        # ğŸ¯ åªç»™æœºæ¢°è‡‚å…³èŠ‚æ·»åŠ éšæœºé€Ÿåº¦ï¼Œç›®æ ‡å…³èŠ‚é€Ÿåº¦ä¿æŒä¸º0
        qvel[:self.num_joints] += self.np_random.standard_normal(self.num_joints) * 0.1
        # qvel[-2:] ä¿æŒä¸º0ï¼Œè¿™æ ·ç›®æ ‡å°±ä¸ä¼šæ»šåŠ¨äº†ï¼
        
        # ğŸ¯ ä½¿ç”¨ç»Ÿä¸€çš„ç›®æ ‡ç”Ÿæˆç­–ç•¥
        target_x, target_y = self.generate_unified_target()
        qpos[-2:] = [target_x, target_y]
        
        self.set_state(qpos, qvel)
        self.step_count = 0
        return self._get_obs()

# XMLé…ç½®ç”Ÿæˆå‡½æ•°ï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰
def get_2joint_xml():
    """2å…³èŠ‚XMLé…ç½®ï¼ˆä½¿ç”¨æ ‡å‡†Reacher-v5çš„ç»“æ„ï¼‰"""
    return """
<mujoco model="reacher">
  <compiler angle="radian" inertiafromgeom="true"/>
  <default>
    <joint armature="1" damping="1" limited="true"/>
    <geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
  </default>
  <option gravity="0 0 -9.81" integrator="RK4" timestep="0.01"/>
  <worldbody>
    <geom conaffinity="0" contype="0" name="ground" pos="0 0 0" rgba="0.9 0.9 0.9 1" size="1 1 10" type="plane"/>
    <geom conaffinity="0" fromto="-.3 -.3 .01 .3 -.3 .01" name="sideS" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" fromto=" .3 -.3 .01 .3  .3 .01" name="sideE" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" fromto="-.3  .3 .01 .3  .3 .01" name="sideN" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" fromto="-.3 -.3 .01 -.3 .3 .01" name="sideW" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto="0 0 0 0 0 0.02" name="root" rgba="0.9 0.4 0.6 1" size=".011" type="cylinder"/>
    <body name="body0" pos="0 0 .01">
      <geom fromto="0 0 0 0.1 0 0" name="link0" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
      <joint axis="0 0 1" limited="false" name="joint0" pos="0 0 0" type="hinge"/>
      <body name="body1" pos="0.1 0 0">
        <joint axis="0 0 1" limited="true" name="joint1" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
        <geom fromto="0 0 0 0.1 0 0" name="link1" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
        <body name="fingertip" pos="0.11 0 0">
          <geom contype="0" name="fingertip" pos="0 0 0" rgba="0.0 0.8 0.6 1" size=".01" type="sphere"/>
        </body>
      </body>
    </body>
    <body name="target" pos=".2 -.2 .01">
      <joint armature="0" axis="1 0 0" damping="0" limited="true" name="target_x" pos="0 0 0" range="-.27 .27" ref=".2" stiffness="0" type="slide"/>
      <joint armature="0" axis="0 1 0" damping="0" limited="true" name="target_y" pos="0 0 0" range="-.27 .27" ref="-.2" stiffness="0" type="slide"/>
      <geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
    </body>
  </worldbody>
  <actuator>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint0"/>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint1"/>
  </actuator>
</mujoco>
"""

def get_3joint_xml():
    """3å…³èŠ‚XMLé…ç½®ï¼ˆç»Ÿä¸€åŠ¨åŠ›å­¦å‚æ•° + è‡ªç¢°æ’æ£€æµ‹ï¼‰"""
    return """
<mujoco model="3joint_reacher">
  <compiler angle="radian" inertiafromgeom="true"/>
  <default>
    <joint armature="1" damping="1" limited="true"/>
    <geom contype="1" conaffinity="1" friction="1 0.1 0.1" rgba="0.7 0.7 0 1" density="1000"/>
  </default>
  <contact>
    <!-- é“¾èŠ‚ä¹‹é—´çš„è‡ªç¢°æ’æ£€æµ‹ -->
    <pair geom1="link0" geom2="link2" condim="3"/>
    <!-- End-effectorä¸æ‰€æœ‰é“¾èŠ‚çš„ç¢°æ’æ£€æµ‹ -->
    <pair geom1="fingertip" geom2="link0" condim="3"/>
    <pair geom1="fingertip" geom2="link1" condim="3"/>
  </contact>
  <option gravity="0 0 -9.81" integrator="RK4" timestep="0.01"/>
  <worldbody>
    <geom conaffinity="0" contype="0" name="ground" pos="0 0 0" rgba="0.9 0.9 0.9 1" size="0.5 0.5 10" type="plane"/>
    <geom conaffinity="0" contype="0" fromto="-.5 -.5 .01 .5 -.5 .01" name="sideS" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto=" .5 -.5 .01 .5  .5 .01" name="sideE" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto="-.5  .5 .01 .5  .5 .01" name="sideN" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto="-.5 -.5 .01 -.5 .5 .01" name="sideW" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
    <geom conaffinity="0" contype="0" fromto="0 0 0 0 0 0.02" name="root" rgba="0.9 0.4 0.6 1" size=".011" type="cylinder"/>
    <body name="body0" pos="0 0 .01">
      <geom fromto="0 0 0 0.1 0 0" name="link0" rgba="0.0 0.4 0.6 1" size=".01" type="capsule" contype="1" conaffinity="1"/>
      <joint axis="0 0 1" limited="false" name="joint0" pos="0 0 0" type="hinge"/>
      <body name="body1" pos="0.1 0 0">
        <joint axis="0 0 1" limited="true" name="joint1" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
        <geom fromto="0 0 0 0.1 0 0" name="link1" rgba="0.0 0.4 0.6 1" size=".01" type="capsule" contype="2" conaffinity="2"/>
        <body name="body2" pos="0.1 0 0">
          <joint axis="0 0 1" limited="true" name="joint2" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
          <geom fromto="0 0 0 0.1 0 0" name="link2" rgba="0.0 0.4 0.6 1" size=".01" type="capsule" contype="4" conaffinity="4"/>
          <body name="fingertip" pos="0.11 0 0">
            <geom contype="16" conaffinity="16" name="fingertip" pos="0 0 0" rgba="0.0 0.8 0.6 1" size=".01" type="sphere"/>
          </body>
        </body>
      </body>
    </body>
    <body name="target" pos=".2 -.2 .01">
      <joint armature="0" axis="1 0 0" damping="0" limited="true" name="target_x" pos="0 0 0" range="-.5 .5" ref=".2" stiffness="0" type="slide"/>
      <joint armature="0" axis="0 1 0" damping="0" limited="true" name="target_y" pos="0 0 0" range="-.5 .5" ref="-.2" stiffness="0" type="slide"/>
      <geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
    </body>
  </worldbody>
  <actuator>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint0"/>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint1"/>
    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint2"/>
  </actuator>
</mujoco>
"""

# ç¯å¢ƒç±»
class Sequential3JointReacherEnv(SequentialReacherEnv):
    """ä¾æ¬¡è®­ç»ƒçš„3å…³èŠ‚Reacherç¯å¢ƒ"""
    
    def __init__(self, render_mode=None, show_position_info=False, **kwargs):
        print("ğŸŒŸ Sequential3JointReacherEnv åˆå§‹åŒ–")
        
        super().__init__(
            xml_content=get_3joint_xml(),
            num_joints=3,
            link_lengths=[0.1, 0.1, 0.1],
            render_mode=render_mode,
            show_position_info=show_position_info,
            **kwargs
        )

def create_env(num_joints, render_mode=None, show_position_info=False):
    """åˆ›å»ºæŒ‡å®šå…³èŠ‚æ•°çš„ç¯å¢ƒ"""
    if num_joints == 3:
        env = Sequential3JointReacherEnv(render_mode=render_mode, show_position_info=show_position_info)
    else:
        raise ValueError(f"MADDPGç‰ˆæœ¬ç›®å‰åªæ”¯æŒ3å…³èŠ‚: {num_joints}")
    
    env = Monitor(env)
    return env

# ============================================================================
# ğŸš€ MADDPGè®­ç»ƒå’Œæµ‹è¯•å‡½æ•°
# ============================================================================

class MADDPG_SB3_Wrapper:
    """MADDPG_SB3åŒ…è£…å™¨ï¼Œæä¾›ç»Ÿä¸€æ¥å£"""
    
    def __init__(self, maddpg_model):
        self.maddpg = maddpg_model
        self.num_agents = maddpg_model.num_agents
    
    def predict(self, obs, deterministic=True):
        """é¢„æµ‹åŠ¨ä½œï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        actions = self.maddpg.predict(obs, deterministic=deterministic)
        return actions, None
    
    def save(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        self.maddpg.save(filepath)
    
    def load(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        self.maddpg.load(filepath)

def train_single_joint_model(num_joints, total_timesteps=50000):
    """è®­ç»ƒå•ä¸ªå…³èŠ‚æ•°çš„MADDPG_SB3æ¨¡å‹"""
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ{num_joints}å…³èŠ‚MADDPG_SB3 Reacheræ¨¡å‹")
    print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {total_timesteps}")
    print(f"ğŸ¤– ç®—æ³•: Stable-Baselines3 DDPG Multi-Agent (æ¯ä¸ªå…³èŠ‚ä¸€ä¸ªDDPGæ™ºèƒ½ä½“)")
    if num_joints >= 3:
        print(f"ğŸ¯ æˆåŠŸé˜ˆå€¼: {SUCCESS_THRESHOLD_RATIO:.1%} * R (ç»Ÿä¸€å¥–åŠ±è§„èŒƒ)")
    else:
        print(f"ğŸ¯ æˆåŠŸé˜ˆå€¼: {SUCCESS_THRESHOLD_2JOINT} ({SUCCESS_THRESHOLD_2JOINT*100}cm) (é»˜è®¤å¥–åŠ±)")
    print("="*60)
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼ˆå¼€å¯æ¸²æŸ“ä»¥è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹ï¼‰
    train_env = create_env(num_joints, render_mode='human', show_position_info=True)
    
    print(f"ğŸ”§ ç¯å¢ƒé…ç½®:")
    print(f"   è§‚å¯Ÿç©ºé—´: {train_env.observation_space}")
    print(f"   åŠ¨ä½œç©ºé—´: {train_env.action_space}")
    print(f"   æ™ºèƒ½ä½“æ•°é‡: {num_joints}")
    
    # åˆ›å»ºMADDPG_SB3æ¨¡å‹
    maddpg = MADDPG_SB3(
        env=train_env,
        num_agents=num_joints,
        learning_rate=1e-3,
        gamma=0.99,
        tau=0.005
    )
    
    # åŒ…è£…ä¸ºå…¼å®¹æ¥å£
    model = MADDPG_SB3_Wrapper(maddpg)
    
    print(f"âœ… {num_joints}å…³èŠ‚MADDPG_SB3æ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹{num_joints}å…³èŠ‚MADDPG_SB3è®­ç»ƒ...")
    
    try:
        start_time = time.time()
        
        # ä½¿ç”¨MADDPG_SB3çš„learnæ–¹æ³•è¿›è¡Œè®­ç»ƒ
        episode_rewards = maddpg.learn(total_timesteps=total_timesteps, log_interval=1000)
        
        training_time = time.time() - start_time
        
        print(f"\nâœ… {num_joints}å…³èŠ‚MADDPG_SB3è®­ç»ƒå®Œæˆ!")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        print(f"ğŸ“Š å¹³å‡FPS: {total_timesteps/training_time:.1f}")
        print(f"ğŸ¯ æ€»episodes: {len(episode_rewards)}")
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"models/maddpg_sb3_sequential_{num_joints}joint_reacher"
        model.save(model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        return model, training_time
        
    except KeyboardInterrupt:
        training_time = time.time() - start_time
        print(f"\nâš ï¸ {num_joints}å…³èŠ‚MADDPG_SB3è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print(f"â±ï¸ å·²è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        
        model_path = f"models/maddpg_sb3_sequential_{num_joints}joint_reacher_interrupted"
        model.save(model_path)
        print(f"ğŸ’¾ ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {model_path}")
        return model, training_time
    
    finally:
        train_env.close()

def test_single_joint_model(num_joints, model, n_eval_episodes=10):
    """æµ‹è¯•å•ä¸ªå…³èŠ‚æ•°çš„MADDPG_SB3æ¨¡å‹"""
    print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•{num_joints}å…³èŠ‚MADDPG_SB3æ¨¡å‹")
    print(f"ğŸ“Š æµ‹è¯•episodes: {n_eval_episodes}, æ¯ä¸ªepisode: 100æ­¥")
    if num_joints >= 3:
        print(f"ğŸ¯ æˆåŠŸæ ‡å‡†: è·ç¦»ç›®æ ‡ < {SUCCESS_THRESHOLD_RATIO:.1%} * R (ç»Ÿä¸€å¥–åŠ±è§„èŒƒ)")
    else:
        print(f"ğŸ¯ æˆåŠŸæ ‡å‡†: è·ç¦»ç›®æ ‡ < {SUCCESS_THRESHOLD_2JOINT} ({SUCCESS_THRESHOLD_2JOINT*100}cm) (é»˜è®¤å¥–åŠ±)")
    print("-"*40)
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒï¼ˆå¸¦æ¸²æŸ“ï¼‰
    test_env = create_env(num_joints, render_mode='human', show_position_info=True)
    
    try:
        # æ‰‹åŠ¨è¿è¡Œepisodesæ¥è®¡ç®—æˆåŠŸç‡
        success_episodes = 0
        total_episodes = n_eval_episodes
        episode_rewards = []
        episode_distances = []
        episode_normalized_distances = []
        
        for episode in range(n_eval_episodes):
            obs, info = test_env.reset()
            episode_reward = 0
            episode_success = False
            min_distance = float('inf')
            min_normalized_distance = float('inf')
            
            # è·å–ç¯å¢ƒä¿¡æ¯
            max_reach = info.get('max_reach', 1.0) if info else 0.3
            success_threshold = info.get('success_threshold', 0.05) if info else 0.075
            use_unified_reward = info.get('use_unified_reward', False) if info else True
            
            for step in range(100):  # æ¯ä¸ªepisode 100æ­¥
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = test_env.step(action)
                episode_reward += reward
                
                # è·å–è·ç¦»å’ŒæˆåŠŸä¿¡æ¯
                distance = info.get('distance_to_target', float('inf'))
                normalized_distance = info.get('normalized_distance', None)
                is_success = info.get('is_success', False)
                
                min_distance = min(min_distance, distance)
                if normalized_distance is not None:
                    min_normalized_distance = min(min_normalized_distance, normalized_distance)
                
                if is_success and not episode_success:
                    episode_success = True
                
                if terminated or truncated:
                    break
            
            if episode_success:
                success_episodes += 1
            
            episode_rewards.append(episode_reward)
            episode_distances.append(min_distance)
            episode_normalized_distances.append(min_normalized_distance if min_normalized_distance != float('inf') else None)
            
            normalized_dist_str = f", å½’ä¸€åŒ–è·ç¦»={min_normalized_distance:.3f}" if min_normalized_distance != float('inf') else ""
            print(f"   Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, æœ€å°è·ç¦»={min_distance:.4f}{normalized_dist_str}, æˆåŠŸ={'âœ…' if episode_success else 'âŒ'}")
        
        success_rate = success_episodes / total_episodes
        avg_reward = np.mean(episode_rewards)
        avg_min_distance = np.mean(episode_distances)
        avg_normalized_distance = np.mean([d for d in episode_normalized_distances if d is not None]) if any(d is not None for d in episode_normalized_distances) else None
        
        print(f"\nğŸ¯ {num_joints}å…³èŠ‚MADDPGæ¨¡å‹æµ‹è¯•ç»“æœ:")
        print(f"   æˆåŠŸç‡: {success_rate:.1%} ({success_episodes}/{total_episodes})")
        print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"   å¹³å‡æœ€å°è·ç¦»: {avg_min_distance:.4f}")
        if avg_normalized_distance is not None:
            print(f"   å¹³å‡å½’ä¸€åŒ–è·ç¦»: {avg_normalized_distance:.3f}")
        print(f"   ğŸ¯ å¯è¾¾åŠå¾„R: {max_reach:.3f}")
        print(f"   ğŸ¯ æˆåŠŸé˜ˆå€¼: {success_threshold:.3f}")
        
        return {
            'num_joints': num_joints,
            'success_rate': success_rate,
            'success_episodes': success_episodes,
            'total_episodes': total_episodes,
            'avg_reward': avg_reward,
            'avg_min_distance': avg_min_distance,
            'avg_normalized_distance': avg_normalized_distance,
            'max_reach': max_reach,
            'success_threshold': success_threshold,
            'use_unified_reward': use_unified_reward,
            'episode_rewards': episode_rewards,
            'episode_distances': episode_distances,
            'episode_normalized_distances': episode_normalized_distances
        }
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ {num_joints}å…³èŠ‚MADDPGæµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return None
    
    finally:
        test_env.close()

def main():
    """ä¸»å‡½æ•°ï¼šMADDPG_SB3ç‰ˆæœ¬çš„è®­ç»ƒå’Œæµ‹è¯•"""
    print("ğŸŒŸ MADDPG_SB3ç‰ˆæœ¬ï¼šåŸºäºStable-Baselines3 DDPGçš„å¤šæ™ºèƒ½ä½“Reacherè®­ç»ƒç³»ç»Ÿ")
    print("ğŸ¤– ç­–ç•¥: æ¯ä¸ªå…³èŠ‚ä½¿ç”¨ç‹¬ç«‹çš„SB3 DDPGæ™ºèƒ½ä½“ï¼ŒååŒå­¦ä¹ æœ€ä¼˜ç­–ç•¥")
    print("ğŸ”§ MADDPG_SB3é…ç½®:")
    print(f"   1. æ¯ä¸ªå…³èŠ‚ä¸€ä¸ªStable-Baselines3 DDPGæ™ºèƒ½ä½“")
    print(f"   2. ä½¿ç”¨æ ‡å‡†DDPGç®—æ³•å’Œç½‘ç»œç»“æ„")
    print(f"   3. è‡ªåŠ¨å¤„ç†ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œæ›´æ–°")
    print(f"   4. ç»Ÿä¸€æ ‡å‡†Reacher-v5å¥–åŠ±: -1.0*distance - 0.1*sum(actionÂ²)")
    print(f"   5. æˆåŠŸé˜ˆå€¼: 3+å…³èŠ‚={SUCCESS_THRESHOLD_RATIO:.1%}*R")
    print(f"ğŸ›¡ï¸ è‡ªç¢°æ’æ£€æµ‹: é˜²æ­¢æœºæ¢°è‡‚ç©¿é€è‡ªå·±ï¼Œæé«˜ç‰©ç†çœŸå®æ€§")
    print(f"ğŸ“Š é…ç½®: è®­ç»ƒ50000æ­¥ï¼Œæµ‹è¯•10ä¸ªepisodes")
    print("ğŸ’¾ è¾“å‡º: ä¿å­˜MADDPG_SB3æ¨¡å‹æ–‡ä»¶")
    print()
    
    # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
    os.makedirs("models", exist_ok=True)
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    training_times = []
    
    # åªè®­ç»ƒ3å…³èŠ‚ï¼ˆMADDPGç‰ˆæœ¬çš„æ¼”ç¤ºï¼‰
    joint_numbers = [3]
    
    for num_joints in joint_numbers:
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ”„ å½“å‰è¿›åº¦: {num_joints}å…³èŠ‚ MADDPG Reacher ({joint_numbers.index(num_joints)+1}/{len(joint_numbers)})")
            print(f"{'='*60}")
            
            # è®­ç»ƒæ¨¡å‹
            model, training_time = train_single_joint_model(num_joints, total_timesteps=50000)
            training_times.append(training_time)
            
            # æµ‹è¯•æ¨¡å‹
            test_result = test_single_joint_model(num_joints, model, n_eval_episodes=10)
            
            if test_result:
                all_results.append(test_result)
            
            print(f"\nâœ… {num_joints}å…³èŠ‚ MADDPG Reacher å®Œæˆ!")
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸ åœ¨{num_joints}å…³èŠ‚è®­ç»ƒæ—¶è¢«ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ {num_joints}å…³èŠ‚è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # è¾“å‡ºæœ€ç»ˆæ€»ç»“
    print(f"\n{'='*80}")
    print("ğŸ‰ MADDPGç‰ˆæœ¬è®­ç»ƒå’Œæµ‹è¯•å®Œæˆ!")
    print(f"{'='*80}")
    
    if all_results:
        print("\nğŸ“Š MADDPG_SB3æ¨¡å‹æ€§èƒ½æ€»ç»“:")
        print("-"*100)
        print(f"{'å…³èŠ‚æ•°':<8} {'ç®—æ³•':<12} {'æˆåŠŸç‡':<10} {'å¹³å‡å¥–åŠ±':<12} {'å¹³å‡è·ç¦»':<12} {'å½’ä¸€åŒ–è·ç¦»':<12} {'è®­ç»ƒæ—¶é—´':<10}")
        print("-"*100)
        
        for i, result in enumerate(all_results):
            training_time_min = training_times[i] / 60 if i < len(training_times) else 0
            normalized_dist = result.get('avg_normalized_distance', 'N/A')
            normalized_dist_str = f"{normalized_dist:.3f}" if normalized_dist != 'N/A' and normalized_dist is not None else 'N/A'
            print(f"{result['num_joints']:<8} {'MADDPG_SB3':<12} {result['success_rate']:<10.1%} {result['avg_reward']:<12.2f} {result['avg_min_distance']:<12.4f} {normalized_dist_str:<12} {training_time_min:<10.1f}åˆ†é’Ÿ")
        
        print("-"*100)
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(all_results, key=lambda x: x['success_rate'])
        print(f"\nğŸ† æœ€ä½³MADDPG_SB3æ¨¡å‹: {best_model['num_joints']}å…³èŠ‚")
        print(f"   æˆåŠŸç‡: {best_model['success_rate']:.1%}")
        print(f"   å¹³å‡å¥–åŠ±: {best_model['avg_reward']:.2f}")
        print(f"   å¹³å‡æœ€å°è·ç¦»: {best_model['avg_min_distance']:.4f}")
        
        # æ¨¡å‹æ–‡ä»¶æ€»ç»“
        print(f"\nğŸ’¾ æ‰€æœ‰MADDPG_SB3æ¨¡å‹å·²ä¿å­˜åˆ° models/ ç›®å½•:")
        for result in all_results:
            print(f"   - models/maddpg_sb3_sequential_{result['num_joints']}joint_reacher_agent_*.zip")
        
        # è¯¦ç»†ç»Ÿè®¡
        success_rates = [r['success_rate'] for r in all_results]
        rewards = [r['avg_reward'] for r in all_results]
        
        print(f"\nğŸ“‹ MADDPG_SB3è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»è®­ç»ƒæ—¶é—´: {sum(training_times)/60:.1f} åˆ†é’Ÿ")
        print(f"   å¹³å‡æˆåŠŸç‡: {np.mean(success_rates):.1%}")
        print(f"   å¹³å‡å¥–åŠ±: {np.mean(rewards):.2f}")
        print(f"   ğŸ¯ æˆåŠŸé˜ˆå€¼æ¯”ä¾‹: {SUCCESS_THRESHOLD_RATIO:.1%} * R")
        
        # ç»“è®º
        print(f"\nğŸ¯ MADDPG_SB3ç»“è®º:")
        if best_model['success_rate'] > 0.5:
            print(f"   ğŸ† MADDPG_SB3è®­ç»ƒæˆåŠŸï¼å¤šæ™ºèƒ½ä½“åä½œæ•ˆæœè‰¯å¥½")
        elif best_model['success_rate'] > 0.3:
            print(f"   âš ï¸ MADDPG_SB3éƒ¨åˆ†æˆåŠŸï¼ŒæˆåŠŸç‡ä¸º{best_model['success_rate']:.1%}")
        else:
            print(f"   âŒ MADDPG_SB3è¡¨ç°è¾ƒå·®ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°æˆ–ç½‘ç»œç»“æ„")
        
        print(f"   ğŸ¤– å¤šæ™ºèƒ½ä½“åä½œ: æ¯ä¸ªå…³èŠ‚ä½¿ç”¨ç‹¬ç«‹çš„SB3 DDPG")
        print(f"   ğŸ¯ ä¸SACå¯¹æ¯”: å¯ä»¥æ¯”è¾ƒDDPG vs SACçš„æ•ˆæœ")
    
    print(f"\nğŸ¯ MADDPG_SB3ç‰ˆæœ¬å®Œæˆï¼")
    print(f"   - å¤šæ™ºèƒ½ä½“åä½œ: æ¯ä¸ªå…³èŠ‚ä½¿ç”¨ç‹¬ç«‹çš„Stable-Baselines3 DDPG")
    print(f"   - æ ‡å‡†åŒ–å®ç°: ä½¿ç”¨æˆç†Ÿçš„SB3åº“ï¼Œæ›´ç¨³å®šå¯é ")
    print(f"   - æ˜“äºè°ƒè¯•: åˆ©ç”¨SB3çš„å®Œå–„å·¥å…·å’Œæ–‡æ¡£")
    print(f"   - å¯ä»¥ä¸SACç‰ˆæœ¬è¿›è¡Œæ€§èƒ½å¯¹æ¯”")

if __name__ == "__main__":
    main()
