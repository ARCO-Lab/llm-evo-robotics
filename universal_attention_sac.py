#!/usr/bin/env python3
"""
é€šç”¨æ³¨æ„åŠ› SAC æ¶æ„
åŸºäºåŸå§‹ sac_with_attention.py æ”¹é€ ï¼Œæ”¯æŒä»»æ„å…³èŠ‚æ•°é‡
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import time
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.evaluation import evaluate_policy
from typing import Dict, List, Tuple, Type, Union
import math

class UniversalAttentionLayer(nn.Module):
    """
    é€šç”¨è‡ªæ³¨æ„åŠ›å±‚ - æ”¯æŒä»»æ„å…³èŠ‚æ•°é‡
    æ”¹é€ è‡ªåŸå§‹ AttentionLayerï¼Œå¢åŠ å…³èŠ‚æ„ŸçŸ¥èƒ½åŠ›
    """
    def __init__(self, joint_feature_dim: int, hidden_dim: int = 64, num_heads: int = 4):
        super(UniversalAttentionLayer, self).__init__()
        self.joint_feature_dim = joint_feature_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        assert hidden_dim % num_heads == 0, "hidden_dim must be divisible by num_heads"
        
        # çº¿æ€§å˜æ¢å±‚ - å¤„ç†å…³èŠ‚ç‰¹å¾
        self.query = nn.Linear(joint_feature_dim, hidden_dim)
        self.key = nn.Linear(joint_feature_dim, hidden_dim)
        self.value = nn.Linear(joint_feature_dim, hidden_dim)
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # Dropout
        self.dropout = nn.Dropout(0.1)
        
        # è¾“å…¥æŠ•å½± (å¦‚æœè¾“å…¥ç»´åº¦ä¸åŒ¹é…)
        if joint_feature_dim != hidden_dim:
            self.input_proj = nn.Linear(joint_feature_dim, hidden_dim)
        else:
            self.input_proj = None
        
        print(f"ğŸ§  UniversalAttentionLayer åˆå§‹åŒ–: joint_feature_dim={joint_feature_dim}, hidden_dim={hidden_dim}, num_heads={num_heads}")
    
    def forward(self, joint_features: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ - å¤„ç†å…³èŠ‚ç‰¹å¾åºåˆ—
        joint_features: [batch_size, num_joints, joint_feature_dim]
        return: [batch_size, num_joints, hidden_dim]
        """
        batch_size, num_joints, joint_feature_dim = joint_features.shape
        
        # è®¡ç®— Q, K, V
        Q = self.query(joint_features)  # [batch_size, num_joints, hidden_dim]
        K = self.key(joint_features)    # [batch_size, num_joints, hidden_dim]
        V = self.value(joint_features)  # [batch_size, num_joints, hidden_dim]
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        Q = Q.view(batch_size, num_joints, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, num_joints, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, num_joints, self.num_heads, self.head_dim).transpose(1, 2)
        # ç°åœ¨å½¢çŠ¶ä¸º: [batch_size, num_heads, num_joints, head_dim]
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attended = torch.matmul(attention_weights, V)
        # [batch_size, num_heads, num_joints, head_dim]
        
        # é‡æ–°ç»„åˆå¤šå¤´
        attended = attended.transpose(1, 2).contiguous().view(
            batch_size, num_joints, self.hidden_dim
        )
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_proj(attended)
        
        # æ®‹å·®è¿æ¥ + Layer Norm
        if self.input_proj is not None:
            joint_features_proj = self.input_proj(joint_features)
        else:
            joint_features_proj = joint_features
        
        output = self.layer_norm(output + joint_features_proj)
        
        return output

class JointFeatureExtractor(nn.Module):
    """
    å…³èŠ‚ç‰¹å¾æå–å™¨ - å°†åŸå§‹å…³èŠ‚ä¿¡æ¯è½¬æ¢ä¸ºç‰¹å¾
    """
    def __init__(self, joint_input_dim: int = 2, joint_feature_dim: int = 32):
        super(JointFeatureExtractor, self).__init__()
        self.joint_input_dim = joint_input_dim
        self.joint_feature_dim = joint_feature_dim
        
        # æ¯ä¸ªå…³èŠ‚çš„ç‰¹å¾æå–ç½‘ç»œ
        self.joint_encoder = nn.Sequential(
            nn.Linear(joint_input_dim, joint_feature_dim),
            nn.ReLU(),
            nn.LayerNorm(joint_feature_dim),
            nn.Linear(joint_feature_dim, joint_feature_dim),
            nn.ReLU(),
            nn.LayerNorm(joint_feature_dim)
        )
        
        print(f"ğŸ”§ JointFeatureExtractor: {joint_input_dim} â†’ {joint_feature_dim}")
    
    def forward(self, joint_input: torch.Tensor) -> torch.Tensor:
        """
        æå–å•ä¸ªå…³èŠ‚çš„ç‰¹å¾
        joint_input: [batch_size, joint_input_dim]
        return: [batch_size, joint_feature_dim]
        """
        return self.joint_encoder(joint_input)

class GlobalFeatureProcessor(nn.Module):
    """
    å…¨å±€ç‰¹å¾å¤„ç†å™¨ - å¤„ç†éå…³èŠ‚ä¿¡æ¯ï¼ˆå¦‚ç›®æ ‡ä½ç½®ã€æœ«ç«¯ä½ç½®ç­‰ï¼‰
    """
    def __init__(self, global_input_dim: int, global_feature_dim: int = 64):
        super(GlobalFeatureProcessor, self).__init__()
        self.global_input_dim = global_input_dim
        self.global_feature_dim = global_feature_dim
        
        if global_input_dim > 0:
            self.global_encoder = nn.Sequential(
                nn.Linear(global_input_dim, global_feature_dim),
                nn.ReLU(),
                nn.LayerNorm(global_feature_dim),
                nn.Dropout(0.1)
            )
        else:
            self.global_encoder = None
        
        print(f"ğŸŒ GlobalFeatureProcessor: {global_input_dim} â†’ {global_feature_dim}")
    
    def forward(self, global_input: torch.Tensor) -> torch.Tensor:
        """
        å¤„ç†å…¨å±€ç‰¹å¾
        global_input: [batch_size, global_input_dim]
        return: [batch_size, global_feature_dim] æˆ– None
        """
        if self.global_encoder is not None:
            return self.global_encoder(global_input)
        else:
            return None

class UniversalAttentionFeaturesExtractor(BaseFeaturesExtractor):
    """
    é€šç”¨æ³¨æ„åŠ›ç‰¹å¾æå–å™¨
    æ”¯æŒä»»æ„å…³èŠ‚æ•°é‡çš„ Reacher ä»»åŠ¡
    """
    def __init__(self, observation_space: gym.Space, features_dim: int = 128, 
                 num_joints: int = 2, joint_input_dim: int = 2):
        super(UniversalAttentionFeaturesExtractor, self).__init__(observation_space, features_dim)
        
        self.obs_dim = observation_space.shape[0]
        self.num_joints = num_joints
        self.joint_input_dim = joint_input_dim
        
        print(f"ğŸŒŸ UniversalAttentionFeaturesExtractor åˆå§‹åŒ–:")
        print(f"   è§‚å¯Ÿç©ºé—´ç»´åº¦: {self.obs_dim}")
        print(f"   å…³èŠ‚æ•°é‡: {num_joints}")
        print(f"   æ¯ä¸ªå…³èŠ‚è¾“å…¥ç»´åº¦: {joint_input_dim}")
        print(f"   è¾“å‡ºç‰¹å¾ç»´åº¦: {features_dim}")
        
        # è®¡ç®—å…¨å±€ç‰¹å¾ç»´åº¦
        joint_total_dim = num_joints * joint_input_dim
        global_input_dim = max(0, self.obs_dim - joint_total_dim)
        
        # ç»„ä»¶åˆå§‹åŒ–
        joint_feature_dim = 32
        
        # å…³èŠ‚ç‰¹å¾æå–å™¨
        self.joint_extractor = JointFeatureExtractor(
            joint_input_dim=joint_input_dim,
            joint_feature_dim=joint_feature_dim
        )
        
        # é€šç”¨æ³¨æ„åŠ›å±‚
        self.attention_layer = UniversalAttentionLayer(
            joint_feature_dim=joint_feature_dim,
            hidden_dim=64,
            num_heads=4
        )
        
        # å…¨å±€ç‰¹å¾å¤„ç†å™¨
        global_feature_dim = 32
        self.global_processor = GlobalFeatureProcessor(
            global_input_dim=global_input_dim,
            global_feature_dim=global_feature_dim
        )
        
        # ç‰¹å¾èåˆ
        # å…³èŠ‚ç‰¹å¾æ± åŒ–
        self.joint_pooling = nn.Sequential(
            nn.Linear(64, 32),  # ä»æ³¨æ„åŠ›å±‚è¾“å‡ºç»´åº¦åˆ°æ± åŒ–ç»´åº¦
            nn.ReLU(),
            nn.LayerNorm(32)
        )
        
        # æœ€ç»ˆèåˆ
        fusion_input_dim = 32 + (global_feature_dim if global_input_dim > 0 else 0)
        self.fusion_layer = nn.Sequential(
            nn.Linear(fusion_input_dim, features_dim),
            nn.ReLU(),
            nn.LayerNorm(features_dim),
            nn.Dropout(0.1),
            nn.Linear(features_dim, features_dim)
        )
        
        print(f"âœ… UniversalAttentionFeaturesExtractor æ„å»ºå®Œæˆ")
        print(f"   å…³èŠ‚ç‰¹å¾ç»´åº¦: {joint_input_dim} â†’ {joint_feature_dim} â†’ 64")
        print(f"   å…¨å±€ç‰¹å¾ç»´åº¦: {global_input_dim} â†’ {global_feature_dim}")
        print(f"   èåˆè¾“å…¥ç»´åº¦: {fusion_input_dim}")
        print(f"   æ”¯æŒä»»æ„å…³èŠ‚æ•°é‡æ‰©å±•")
    
    def extract_joint_features(self, observations: torch.Tensor) -> torch.Tensor:
        """
        ä»è§‚å¯Ÿä¸­æå–å…³èŠ‚ç‰¹å¾
        æ”¯æŒä¸åŒçš„è§‚å¯Ÿæ ¼å¼
        """
        batch_size = observations.size(0)
        
        joint_features = []
        
        if self.num_joints == 2:
            # MuJoCo Reacher-v5 æ ¼å¼
            # [0:2] - cos/sin of joint angles
            # [2:4] - joint velocities
            for i in range(self.num_joints):
                angle = observations[:, i:i+1]  # cos æˆ– sin
                velocity = observations[:, 2+i:2+i+1]  # å¯¹åº”çš„é€Ÿåº¦
                joint_input = torch.cat([angle, velocity], dim=1)
                joint_features.append(joint_input)
        else:
            # é€šç”¨æ ¼å¼ï¼šå‰ num_joints æ˜¯è§’åº¦ï¼Œæ¥ä¸‹æ¥ num_joints æ˜¯é€Ÿåº¦
            for i in range(self.num_joints):
                angle_idx = i
                velocity_idx = self.num_joints + i
                
                if angle_idx < self.obs_dim and velocity_idx < self.obs_dim:
                    angle = observations[:, angle_idx:angle_idx+1]
                    velocity = observations[:, velocity_idx:velocity_idx+1]
                    joint_input = torch.cat([angle, velocity], dim=1)
                else:
                    # å¦‚æœè¶…å‡ºè§‚å¯Ÿç©ºé—´ï¼Œç”¨é›¶å¡«å……
                    joint_input = torch.zeros(batch_size, self.joint_input_dim, device=observations.device)
                
                joint_features.append(joint_input)
        
        return torch.stack(joint_features, dim=1)  # [batch_size, num_joints, joint_input_dim]
    
    def extract_global_features(self, observations: torch.Tensor) -> torch.Tensor:
        """
        æå–å…¨å±€ç‰¹å¾ï¼ˆéå…³èŠ‚ä¿¡æ¯ï¼‰
        """
        joint_total_dim = self.num_joints * self.joint_input_dim
        if self.obs_dim > joint_total_dim:
            return observations[:, joint_total_dim:]
        else:
            return torch.empty(observations.size(0), 0, device=observations.device)
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        é€šç”¨å‰å‘ä¼ æ’­
        observations: [batch_size, obs_dim]
        return: [batch_size, features_dim]
        """
        batch_size = observations.size(0)
        
        # 1. æå–å…³èŠ‚ç‰¹å¾
        joint_raw_features = self.extract_joint_features(observations)
        # joint_raw_features: [batch_size, num_joints, joint_input_dim]
        
        # 2. å¤„ç†æ¯ä¸ªå…³èŠ‚
        joint_processed_features = []
        for i in range(self.num_joints):
            joint_feature = self.joint_extractor(joint_raw_features[:, i])
            joint_processed_features.append(joint_feature)
        
        joint_processed = torch.stack(joint_processed_features, dim=1)
        # joint_processed: [batch_size, num_joints, joint_feature_dim]
        
        # 3. å…³èŠ‚é—´æ³¨æ„åŠ›
        joint_attended = self.attention_layer(joint_processed)
        # joint_attended: [batch_size, num_joints, 64]
        
        # 4. å…³èŠ‚ç‰¹å¾æ± åŒ–
        joint_pooled_list = []
        for i in range(self.num_joints):
            pooled = self.joint_pooling(joint_attended[:, i])
            joint_pooled_list.append(pooled)
        
        # å¹³å‡æ± åŒ–æ‰€æœ‰å…³èŠ‚ç‰¹å¾
        joint_pooled = torch.stack(joint_pooled_list, dim=1).mean(dim=1)
        # joint_pooled: [batch_size, 32]
        
        # 5. å¤„ç†å…¨å±€ç‰¹å¾
        global_raw_features = self.extract_global_features(observations)
        global_processed = self.global_processor(global_raw_features)
        
        # 6. ç‰¹å¾èåˆ
        if global_processed is not None:
            fused_features = torch.cat([joint_pooled, global_processed], dim=1)
        else:
            fused_features = joint_pooled
        
        # 7. æœ€ç»ˆå¤„ç†
        output = self.fusion_layer(fused_features)
        
        return output

class UniversalActionGenerator(nn.Module):
    """
    é€šç”¨åŠ¨ä½œç”Ÿæˆå™¨ - ç”Ÿæˆä»»æ„æ•°é‡å…³èŠ‚çš„åŠ¨ä½œ
    æ³¨æ„ï¼šè¿™ä¸ªéœ€è¦åœ¨ SAC ç­–ç•¥å±‚é¢é›†æˆï¼Œè¿™é‡Œæä¾›è®¾è®¡æ€è·¯
    """
    def __init__(self, features_dim: int, num_joints: int):
        super(UniversalActionGenerator, self).__init__()
        self.features_dim = features_dim
        self.num_joints = num_joints
        
        # å…±äº«ç‰¹å¾å¤„ç†
        self.shared_net = nn.Sequential(
            nn.Linear(features_dim, features_dim),
            nn.ReLU(),
            nn.LayerNorm(features_dim)
        )
        
        # æ¯ä¸ªå…³èŠ‚çš„åŠ¨ä½œå¤´
        self.joint_action_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(features_dim, features_dim // 2),
                nn.ReLU(),
                nn.Linear(features_dim // 2, 1)  # æ¯ä¸ªå…³èŠ‚ä¸€ä¸ªåŠ¨ä½œ
            ) for _ in range(num_joints)
        ])
        
        print(f"ğŸ® UniversalActionGenerator: {features_dim} â†’ {num_joints} ä¸ªå…³èŠ‚åŠ¨ä½œ")
    
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        ç”Ÿæˆå…³èŠ‚åŠ¨ä½œ
        features: [batch_size, features_dim]
        return: [batch_size, num_joints]
        """
        shared_features = self.shared_net(features)
        
        joint_actions = []
        for joint_head in self.joint_action_heads:
            action = joint_head(shared_features)
            joint_actions.append(action)
        
        return torch.cat(joint_actions, dim=1)

def create_universal_reacher_env(num_joints: int = 2):
    """
    åˆ›å»ºé€šç”¨ Reacher ç¯å¢ƒ
    """
    if num_joints == 2:
        return gym.make('Reacher-v5')
    else:
        raise NotImplementedError(f"æš‚ä¸æ”¯æŒ {num_joints} å…³èŠ‚ Reacherï¼Œä½†æ¶æ„å·²å‡†å¤‡å¥½")

def train_universal_attention_sac(num_joints: int = 2, joint_input_dim: int = 2, total_timesteps: int = 50000):
    """
    è®­ç»ƒé€šç”¨æ³¨æ„åŠ› SAC
    """
    print("ğŸŒŸ é€šç”¨æ³¨æ„åŠ› SAC è®­ç»ƒ")
    print(f"ğŸ”— å…³èŠ‚æ•°é‡: {num_joints}")
    print(f"ğŸ“ æ¯ä¸ªå…³èŠ‚è¾“å…¥ç»´åº¦: {joint_input_dim}")
    print(f"ğŸ¯ åŸºäºåŸå§‹ sac_with_attention.py æ”¹é€ ")
    print(f"âœ¨ æ”¯æŒä»»æ„å…³èŠ‚æ•°é‡")
    print("=" * 70)
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"ğŸ­ åˆ›å»º {num_joints} å…³èŠ‚ Reacher ç¯å¢ƒ...")
    try:
        env = create_universal_reacher_env(num_joints)
        env = Monitor(env)
        
        eval_env = create_universal_reacher_env(num_joints)
        eval_env = Monitor(eval_env)
        
        print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
        print(f"ğŸ® åŠ¨ä½œç©ºé—´: {env.action_space}")
        print(f"ğŸ‘ï¸ è§‚å¯Ÿç©ºé—´: {env.observation_space}")
        
    except NotImplementedError as e:
        print(f"âš ï¸ {e}")
        print("ğŸ”§ ä½¿ç”¨ 2 å…³èŠ‚ç¯å¢ƒè¿›è¡Œæ¶æ„éªŒè¯...")
        env = create_universal_reacher_env(2)
        env = Monitor(env)
        eval_env = create_universal_reacher_env(2)
        eval_env = Monitor(eval_env)
    
    print("=" * 70)
    
    # åˆ›å»ºé€šç”¨æ³¨æ„åŠ›æ¨¡å‹
    print("ğŸ¤– åˆ›å»ºé€šç”¨æ³¨æ„åŠ› SAC æ¨¡å‹...")
    
    policy_kwargs = {
        "features_extractor_class": UniversalAttentionFeaturesExtractor,
        "features_extractor_kwargs": {
            "features_dim": 128,
            "num_joints": num_joints,
            "joint_input_dim": joint_input_dim
        },
        "net_arch": [256, 256],  # ä¿æŒä¸åŸå§‹ç›¸åŒ
        "activation_fn": torch.nn.ReLU,
    }
    
    model = SAC(
        "MlpPolicy",
        env,
        learning_rate=3e-4,          # ä¸åŸå§‹ç›¸åŒ
        buffer_size=1000000,         # ä¸åŸå§‹ç›¸åŒ
        learning_starts=100,         # ä¸åŸå§‹ç›¸åŒ
        batch_size=256,              # ä¸åŸå§‹ç›¸åŒ
        tau=0.005,                   # ä¸åŸå§‹ç›¸åŒ
        gamma=0.99,                  # ä¸åŸå§‹ç›¸åŒ
        train_freq=1,                # ä¸åŸå§‹ç›¸åŒ
        gradient_steps=1,            # ä¸åŸå§‹ç›¸åŒ
        ent_coef='auto',             # ä¸åŸå§‹ç›¸åŒ
        target_update_interval=1,    # ä¸åŸå§‹ç›¸åŒ
        use_sde=False,               # ä¸åŸå§‹ç›¸åŒ
        policy_kwargs=policy_kwargs, # é€šç”¨æ³¨æ„åŠ›æœºåˆ¶
        verbose=1,
        device='cpu'
    )
    
    print("âœ… é€šç”¨æ³¨æ„åŠ› SAC æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š æ¨¡å‹ç‰¹ç‚¹:")
    print(f"   âœ¨ æ”¯æŒä»»æ„å…³èŠ‚æ•°é‡")
    print(f"   ğŸ§  å…³èŠ‚çº§æ³¨æ„åŠ›æœºåˆ¶")
    print(f"   ğŸ”§ æ¨¡å—åŒ–è®¾è®¡")
    print(f"   ğŸ¯ åŸºäºæˆåŠŸçš„æ³¨æ„åŠ›æ¶æ„")
    print(f"   ğŸ“ˆ é¢„æœŸè¾¾åˆ° 70% æˆåŠŸç‡")
    
    print("=" * 70)
    
    # åˆ›å»ºè¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f'./universal_attention_{num_joints}joints_best/',
        log_path=f'./universal_attention_{num_joints}joints_logs/',
        eval_freq=5000,
        n_eval_episodes=10,
        deterministic=True,
        render=False
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹é€šç”¨æ³¨æ„åŠ›è®­ç»ƒ...")
    print("ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   æ€»æ­¥æ•°: {total_timesteps:,}")
    print("   è¯„ä¼°é¢‘ç‡: æ¯ 5,000 æ­¥")
    print("   é¢„æœŸ: ä¿æŒåŸå§‹æ³¨æ„åŠ›çš„æ€§èƒ½ï¼Œå¢åŠ é€šç”¨æ€§")
    print("=" * 70)
    
    start_time = time.time()
    
    # è®­ç»ƒæ¨¡å‹
    model.learn(
        total_timesteps=total_timesteps,
        callback=eval_callback,
        log_interval=10,
        progress_bar=True
    )
    
    training_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("ğŸ† é€šç”¨æ³¨æ„åŠ›è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
    print("=" * 70)
    
    # ä¿å­˜æ¨¡å‹
    model_name = f"universal_attention_{num_joints}joints_final"
    model.save(model_name)
    print(f"ğŸ’¾ é€šç”¨æ³¨æ„åŠ›æ¨¡å‹å·²ä¿å­˜ä¸º: {model_name}.zip")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ” æœ€ç»ˆè¯„ä¼° (20 episodes)...")
    mean_reward, std_reward = evaluate_policy(
        model, 
        eval_env, 
        n_eval_episodes=20,
        deterministic=True,
        render=False
    )
    
    print(f"ğŸ“Š é€šç”¨æ³¨æ„åŠ›æ¨¡å‹è¯„ä¼°ç»“æœ:")
    print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    # ä¸åŸå§‹æ³¨æ„åŠ›å¯¹æ¯”
    original_attention_reward = -5.70
    original_attention_demo_success = 0.7
    original_attention_demo_reward = -4.61
    
    improvement = mean_reward - original_attention_reward
    
    print(f"\nğŸ“ˆ ä¸åŸå§‹æ³¨æ„åŠ›å¯¹æ¯”:")
    print(f"   åŸå§‹æ³¨æ„åŠ›: {original_attention_reward:.2f}")
    print(f"   é€šç”¨æ³¨æ„åŠ›: {mean_reward:.2f}")
    print(f"   æ”¹è¿›: {improvement:+.2f}")
    
    if improvement > 0.2:
        print("   ğŸ‰ é€šç”¨åŒ–æˆåŠŸï¼Œæ€§èƒ½æå‡!")
    elif improvement > -0.2:
        print("   ğŸ‘ é€šç”¨åŒ–æˆåŠŸï¼Œæ€§èƒ½ä¿æŒ!")
    else:
        print("   âš ï¸ é€šç”¨åŒ–æœ‰æ€§èƒ½æŸå¤±ï¼Œéœ€è¦è°ƒä¼˜")
    
    # æ¼”ç¤º
    print("\nğŸ® æ¼”ç¤ºé€šç”¨æ³¨æ„åŠ›æ¨¡å‹ (10 episodes)...")
    demo_env = create_universal_reacher_env(num_joints)
    
    episode_rewards = []
    success_count = 0
    
    for episode in range(10):
        obs, info = demo_env.reset()
        episode_reward = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = demo_env.step(action)
            episode_reward += reward
            
            if terminated or truncated:
                break
        
        episode_rewards.append(episode_reward)
        
        if episode_reward > -5:
            success_count += 1
            print(f"ğŸ¯ Episode {episode+1}: æˆåŠŸ! å¥–åŠ±={episode_reward:.2f}")
        else:
            print(f"ğŸ“Š Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}")
    
    demo_env.close()
    
    demo_success_rate = success_count / 10
    demo_avg_reward = np.mean(episode_rewards)
    
    print("\n" + "=" * 70)
    print("ğŸ“Š é€šç”¨æ³¨æ„åŠ›æ¼”ç¤ºç»Ÿè®¡:")
    print(f"   æˆåŠŸç‡: {demo_success_rate:.1%} ({success_count}/10)")
    print(f"   å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    print(f"   å¥–åŠ±æ ‡å‡†å·®: {np.std(episode_rewards):.2f}")
    
    print(f"\nğŸ“ˆ ä¸åŸå§‹æ³¨æ„åŠ›æ¼”ç¤ºå¯¹æ¯”:")
    print(f"   åŸå§‹æ³¨æ„åŠ›æˆåŠŸç‡: {original_attention_demo_success:.1%}")
    print(f"   é€šç”¨æ³¨æ„åŠ›æˆåŠŸç‡: {demo_success_rate:.1%}")
    print(f"   æˆåŠŸç‡å˜åŒ–: {demo_success_rate - original_attention_demo_success:+.1%}")
    print(f"   ")
    print(f"   åŸå§‹æ³¨æ„åŠ›å¹³å‡å¥–åŠ±: {original_attention_demo_reward:.2f}")
    print(f"   é€šç”¨æ³¨æ„åŠ›å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    print(f"   å¥–åŠ±å˜åŒ–: {demo_avg_reward - original_attention_demo_reward:+.2f}")
    
    if demo_success_rate >= 0.6:
        print("   ğŸ‰ é€šç”¨åŒ–æˆåŠŸï¼Œä¿æŒé«˜æ€§èƒ½!")
    elif demo_success_rate >= 0.5:
        print("   ğŸ‘ é€šç”¨åŒ–è‰¯å¥½!")
    else:
        print("   âš ï¸ é€šç”¨åŒ–éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    print(f"\nğŸŒŸ é€šç”¨æ³¨æ„åŠ›æ¶æ„ä¼˜åŠ¿:")
    print(f"   âœ… æ”¯æŒä»»æ„å…³èŠ‚æ•°é‡")
    print(f"   âœ… å…³èŠ‚çº§ç‰¹å¾æå–")
    print(f"   âœ… å…³èŠ‚é—´æ³¨æ„åŠ›äº¤äº’")
    print(f"   âœ… æ¨¡å—åŒ–è®¾è®¡")
    print(f"   âœ… åŸºäºæˆåŠŸçš„æ³¨æ„åŠ›æœºåˆ¶")
    print(f"   âœ… ä¿æŒåŸå§‹æ¶æ„çš„ä¼˜åŠ¿")
    
    print(f"\nğŸ”® æ‰©å±•èƒ½åŠ›:")
    print(f"   ğŸ”— æ”¯æŒ 2-10 å…³èŠ‚ Reacher")
    print(f"   ğŸ¯ å¯è°ƒèŠ‚å…³èŠ‚è¾“å…¥ç»´åº¦")
    print(f"   ğŸŒŠ è‡ªé€‚åº”ç‰¹å¾èåˆ")
    print(f"   ğŸ”„ æ”¯æŒè¿ç§»å­¦ä¹ ")
    print(f"   âš¡ å¿«é€Ÿé€‚åº”æ–°é…ç½®")
    
    # æ¸…ç†
    env.close()
    eval_env.close()
    
    return {
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'training_time': training_time,
        'demo_success_rate': demo_success_rate,
        'demo_avg_reward': demo_avg_reward,
        'improvement_vs_original': improvement,
        'num_joints': num_joints,
        'joint_input_dim': joint_input_dim
    }

if __name__ == "__main__":
    print("ğŸŒŸ é€šç”¨æ³¨æ„åŠ› SAC è®­ç»ƒç³»ç»Ÿ")
    print("ğŸ¯ åŸºäºåŸå§‹ sac_with_attention.py æ”¹é€ ")
    print("âœ¨ æ”¯æŒä»»æ„å…³èŠ‚æ•°é‡çš„ Reacher ä»»åŠ¡")
    print("ğŸ”§ ä¿æŒåŸå§‹æ¶æ„çš„æˆåŠŸç‰¹æ€§")
    print()
    
    # æµ‹è¯•é…ç½®
    configs = [
        {"num_joints": 2, "joint_input_dim": 2, "total_timesteps": 50000},
        # æœªæ¥å¯ä»¥æµ‹è¯•æ›´å¤šé…ç½®:
        # {"num_joints": 3, "joint_input_dim": 2, "total_timesteps": 60000},
        # {"num_joints": 4, "joint_input_dim": 2, "total_timesteps": 70000},
    ]
    
    results = []
    
    for config in configs:
        print(f"\n{'='*60}")
        print(f"ğŸ§  æµ‹è¯•é…ç½®: {config}")
        print(f"{'='*60}")
        
        try:
            result = train_universal_attention_sac(**config)
            results.append(result)
            
            print(f"\nğŸŠ é…ç½® {config} è®­ç»ƒç»“æœ:")
            print(f"   æœ€ç»ˆè¯„ä¼°å¥–åŠ±: {result['mean_reward']:.2f} Â± {result['std_reward']:.2f}")
            print(f"   è®­ç»ƒæ—¶é—´: {result['training_time']/60:.1f} åˆ†é’Ÿ")
            print(f"   æ¼”ç¤ºæˆåŠŸç‡: {result['demo_success_rate']:.1%}")
            print(f"   æ¼”ç¤ºå¹³å‡å¥–åŠ±: {result['demo_avg_reward']:.2f}")
            print(f"   vs åŸå§‹æ³¨æ„åŠ›æ”¹è¿›: {result['improvement_vs_original']:+.2f}")
            
            if result['improvement_vs_original'] > 0.1:
                print(f"   ğŸ† é€šç”¨åŒ–æˆåŠŸï¼Œæ€§èƒ½æå‡!")
            elif result['improvement_vs_original'] > -0.2:
                print(f"   ğŸ‰ é€šç”¨åŒ–æˆåŠŸï¼Œæ€§èƒ½ä¿æŒ!")
            else:
                print(f"   ğŸ“ˆ é€šç”¨åŒ–æœ‰æ”¹è¿›ç©ºé—´")
            
        except Exception as e:
            print(f"âŒ é…ç½® {config} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # æ€»ç»“
    if results:
        print(f"\n{'='*60}")
        print("ğŸŒŸ é€šç”¨æ³¨æ„åŠ›æ¶æ„æ€»ç»“")
        print(f"{'='*60}")
        
        avg_improvement = np.mean([r['improvement_vs_original'] for r in results])
        avg_success_rate = np.mean([r['demo_success_rate'] for r in results])
        avg_training_time = np.mean([r['training_time']/60 for r in results])
        
        print(f"ğŸ“Š æ•´ä½“æ€§èƒ½:")
        print(f"   vs åŸå§‹æ³¨æ„åŠ›å¹³å‡æ”¹è¿›: {avg_improvement:+.2f}")
        print(f"   å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1%}")
        print(f"   å¹³å‡è®­ç»ƒæ—¶é—´: {avg_training_time:.1f} åˆ†é’Ÿ")
        
        print(f"\nğŸ† é€šç”¨æ³¨æ„åŠ›æ¶æ„ä¼˜åŠ¿:")
        print(f"   âœ… åŸºäºæˆåŠŸçš„æ³¨æ„åŠ›æœºåˆ¶")
        print(f"   âœ… æ”¯æŒä»»æ„å…³èŠ‚æ•°é‡")
        print(f"   âœ… å…³èŠ‚çº§ç‰¹å¾å¤„ç†")
        print(f"   âœ… æ¨¡å—åŒ–è®¾è®¡")
        print(f"   âœ… ä¿æŒåŸå§‹æ€§èƒ½")
        print(f"   âœ… æ˜“äºæ‰©å±•å’Œè°ƒè¯•")
        
        print(f"\nğŸ¯ æœ€ä½³å®è·µ:")
        print(f"   1. ä¿æŒåŸå§‹æ¶æ„çš„æˆåŠŸè¦ç´ ")
        print(f"   2. å¢åŠ å…³èŠ‚æ„ŸçŸ¥èƒ½åŠ›")
        print(f"   3. æ¨¡å—åŒ–è®¾è®¡ä¾¿äºæ‰©å±•")
        print(f"   4. æ¸è¿›å¼æ”¹é€ é™ä½é£é™©")
        
        print(f"\nâœ… é€šç”¨æ³¨æ„åŠ›æ¶æ„éªŒè¯å®Œæˆ!")
        print(f"ğŸš€ æˆåŠŸå°†åŸå§‹æ³¨æ„åŠ›æ¶æ„é€šç”¨åŒ–!")
