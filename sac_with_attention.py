#!/usr/bin/env python3
"""
åœ¨ Baseline SAC åŸºç¡€ä¸Šæ·»åŠ  Attention Layer
é€æ­¥é›†æˆè‡ªå®šä¹‰æ¶æ„
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import time
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.evaluation import evaluate_policy
from typing import Dict, List, Tuple, Type, Union

class AttentionLayer(nn.Module):
    """
    è‡ªæ³¨æ„åŠ›å±‚ - ç”¨äºå¤„ç†è§‚å¯Ÿç‰¹å¾
    """
    def __init__(self, input_dim: int, hidden_dim: int = 64, num_heads: int = 4):
        super(AttentionLayer, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        assert hidden_dim % num_heads == 0, "hidden_dim must be divisible by num_heads"
        
        # çº¿æ€§å˜æ¢å±‚
        self.query = nn.Linear(input_dim, hidden_dim)
        self.key = nn.Linear(input_dim, hidden_dim)
        self.value = nn.Linear(input_dim, hidden_dim)
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # Dropout
        self.dropout = nn.Dropout(0.1)
        
        print(f"ğŸ§  AttentionLayer åˆå§‹åŒ–: input_dim={input_dim}, hidden_dim={hidden_dim}, num_heads={num_heads}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        x: [batch_size, input_dim] æˆ– [batch_size, seq_len, input_dim]
        """
        batch_size = x.size(0)
        
        # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œæ‰©å±•ä¸º3D (æ·»åŠ åºåˆ—ç»´åº¦)
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [batch_size, 1, input_dim]
            squeeze_output = True
        else:
            squeeze_output = False
        
        seq_len = x.size(1)
        
        # è®¡ç®— Q, K, V
        Q = self.query(x)  # [batch_size, seq_len, hidden_dim]
        K = self.key(x)    # [batch_size, seq_len, hidden_dim]
        V = self.value(x)  # [batch_size, seq_len, hidden_dim]
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        # ç°åœ¨å½¢çŠ¶ä¸º: [batch_size, num_heads, seq_len, head_dim]
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attended = torch.matmul(attention_weights, V)
        # [batch_size, num_heads, seq_len, head_dim]
        
        # é‡æ–°ç»„åˆå¤šå¤´
        attended = attended.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_dim
        )
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_proj(attended)
        
        # æ®‹å·®è¿æ¥ + Layer Norm (éœ€è¦å…ˆæŠ•å½±è¾“å…¥åˆ°ç›¸åŒç»´åº¦)
        if x.size(-1) != self.hidden_dim:
            # å¦‚æœè¾“å…¥ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨ä¸€ä¸ªæŠ•å½±å±‚
            if not hasattr(self, 'input_proj'):
                self.input_proj = nn.Linear(x.size(-1), self.hidden_dim).to(x.device)
            x_proj = self.input_proj(x)
        else:
            x_proj = x
        
        output = self.layer_norm(output + x_proj)
        
        # å¦‚æœåŸå§‹è¾“å…¥æ˜¯2Dï¼Œå‹ç¼©å›2D
        if squeeze_output:
            output = output.squeeze(1)
        
        return output

class AttentionFeaturesExtractor(BaseFeaturesExtractor):
    """
    å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹å¾æå–å™¨
    """
    def __init__(self, observation_space: gym.Space, features_dim: int = 128):
        super(AttentionFeaturesExtractor, self).__init__(observation_space, features_dim)
        
        # è·å–è§‚å¯Ÿç©ºé—´ç»´åº¦
        obs_dim = observation_space.shape[0]
        
        print(f"ğŸ” AttentionFeaturesExtractor åˆå§‹åŒ–:")
        print(f"   è§‚å¯Ÿç©ºé—´ç»´åº¦: {obs_dim}")
        print(f"   è¾“å‡ºç‰¹å¾ç»´åº¦: {features_dim}")
        
        # è¾“å…¥é¢„å¤„ç†å±‚
        self.input_layer = nn.Sequential(
            nn.Linear(obs_dim, 64),
            nn.ReLU(),
            nn.LayerNorm(64)
        )
        
        # æ³¨æ„åŠ›å±‚
        self.attention = AttentionLayer(
            input_dim=64,
            hidden_dim=features_dim,
            num_heads=4
        )
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(features_dim, features_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        print(f"âœ… AttentionFeaturesExtractor æ„å»ºå®Œæˆ")
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        observations: [batch_size, obs_dim]
        return: [batch_size, features_dim]
        """
        # è¾“å…¥é¢„å¤„ç†
        x = self.input_layer(observations)
        
        # æ³¨æ„åŠ›å¤„ç†
        x = self.attention(x)
        
        # è¾“å‡ºå¤„ç†
        features = self.output_layer(x)
        
        return features

def sac_with_attention_training():
    print("ğŸš€ SAC + Attention Layer è®­ç»ƒ")
    print("ğŸ§  åœ¨æˆåŠŸçš„ Baseline SAC åŸºç¡€ä¸Šæ·»åŠ æ³¨æ„åŠ›æœºåˆ¶")
    print("ğŸ“Š å¯¹æ¯”è®­ç»ƒæ•ˆæœå’Œæ€§èƒ½å˜åŒ–")
    print("=" * 70)
    
    # åˆ›å»ºåŸç”Ÿ MuJoCo Reacher ç¯å¢ƒ
    print("ğŸ­ åˆ›å»º MuJoCo Reacher-v5 ç¯å¢ƒ...")
    env = gym.make('Reacher-v5', render_mode='human')
    env = Monitor(env)
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"ğŸ‘ï¸ è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"ğŸ“ è§‚å¯Ÿç»´åº¦: {env.observation_space.shape}")
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = gym.make('Reacher-v5')
    eval_env = Monitor(eval_env)
    
    print("=" * 70)
    
    # åˆ›å»ºå¸¦æ³¨æ„åŠ›çš„ SAC æ¨¡å‹
    print("ğŸ¤– åˆ›å»º SAC + Attention æ¨¡å‹...")
    
    # å®šä¹‰ç­–ç•¥å‚æ•°ï¼Œä½¿ç”¨è‡ªå®šä¹‰ç‰¹å¾æå–å™¨
    policy_kwargs = {
        "features_extractor_class": AttentionFeaturesExtractor,
        "features_extractor_kwargs": {"features_dim": 128},
        "net_arch": [256, 256],  # Actor å’Œ Critic ç½‘ç»œæ¶æ„
        "activation_fn": torch.nn.ReLU,
    }
    
    model = SAC(
        "MlpPolicy",
        env,
        learning_rate=3e-4,          # ä¸ baseline ç›¸åŒ
        buffer_size=1000000,         # ä¸ baseline ç›¸åŒ
        learning_starts=100,         # ä¸ baseline ç›¸åŒ
        batch_size=256,              # ä¸ baseline ç›¸åŒ
        tau=0.005,                   # ä¸ baseline ç›¸åŒ
        gamma=0.99,                  # ä¸ baseline ç›¸åŒ
        train_freq=1,                # ä¸ baseline ç›¸åŒ
        gradient_steps=1,            # ä¸ baseline ç›¸åŒ
        ent_coef='auto',             # ä¸ baseline ç›¸åŒ
        target_update_interval=1,    # ä¸ baseline ç›¸åŒ
        use_sde=False,               # ä¸ baseline ç›¸åŒ
        policy_kwargs=policy_kwargs, # æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
        verbose=1,
        device='cpu'
    )
    
    print("âœ… SAC + Attention æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°:")
    print(f"   ç­–ç•¥: MlpPolicy + AttentionFeaturesExtractor")
    print(f"   ç‰¹å¾ç»´åº¦: 128")
    print(f"   æ³¨æ„åŠ›å¤´æ•°: 4")
    print(f"   ç½‘ç»œæ¶æ„: [256, 256]")
    print(f"   å…¶ä»–å‚æ•°ä¸ baseline ç›¸åŒ")
    
    print("=" * 70)
    
    # åˆ›å»ºè¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path='./sac_attention_best/',
        log_path='./sac_attention_logs/',
        eval_freq=5000,              # æ¯5000æ­¥è¯„ä¼°ä¸€æ¬¡
        n_eval_episodes=10,          # æ¯æ¬¡è¯„ä¼°10ä¸ªepisodes
        deterministic=True,          # è¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        render=False                 # è¯„ä¼°æ—¶ä¸æ¸²æŸ“
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print("ğŸ“Š è®­ç»ƒé…ç½®:")
    print("   æ€»æ­¥æ•°: 50,000 (ä¸ baseline ç›¸åŒ)")
    print("   è¯„ä¼°é¢‘ç‡: æ¯ 5,000 æ­¥")
    print("   æ—¥å¿—é—´éš”: æ¯ 1,000 æ­¥")
    print("=" * 70)
    
    start_time = time.time()
    
    # è®­ç»ƒæ¨¡å‹
    model.learn(
        total_timesteps=50000,       # ä¸ baseline ç›¸åŒçš„è®­ç»ƒæ­¥æ•°
        callback=eval_callback,
        log_interval=10,
        progress_bar=True
    )
    
    training_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("ğŸ† è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
    print("=" * 70)
    
    # ä¿å­˜æ¨¡å‹
    model.save("sac_attention_final")
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º: sac_attention_final.zip")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ” æœ€ç»ˆè¯„ä¼° (20 episodes)...")
    mean_reward, std_reward = evaluate_policy(
        model, 
        eval_env, 
        n_eval_episodes=20,
        deterministic=True,
        render=False
    )
    
    print(f"ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    # ä¸ baseline å¯¹æ¯”
    baseline_reward = -4.86  # baseline SAC çš„ç»“æœ
    improvement = mean_reward - baseline_reward
    
    print(f"\nğŸ“ˆ ä¸ Baseline SAC å¯¹æ¯”:")
    print(f"   Baseline SAC: {baseline_reward:.2f}")
    print(f"   SAC + Attention: {mean_reward:.2f}")
    print(f"   æ”¹è¿›å¹…åº¦: {improvement:+.2f}")
    
    if improvement > 0.5:
        print("   ğŸ‰ æ˜¾è‘—æ”¹è¿›! æ³¨æ„åŠ›æœºåˆ¶æ•ˆæœå¾ˆå¥½")
    elif improvement > 0.1:
        print("   ğŸ‘ æœ‰æ”¹è¿›! æ³¨æ„åŠ›æœºåˆ¶æœ‰ç§¯æä½œç”¨")
    elif improvement > -0.1:
        print("   âš–ï¸ æ•ˆæœç›¸å½“ï¼Œæ³¨æ„åŠ›æœºåˆ¶æ²¡æœ‰è´Ÿé¢å½±å“")
    else:
        print("   âš ï¸ æ€§èƒ½ä¸‹é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶å‚æ•°")
    
    # æ¼”ç¤ºè®­ç»ƒå¥½çš„æ¨¡å‹
    print("\nğŸ® æ¼”ç¤ºè®­ç»ƒå¥½çš„æ¨¡å‹ (10 episodes)...")
    demo_env = gym.make('Reacher-v5', render_mode='human')
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(10):
        obs, info = demo_env.reset()
        episode_reward = 0
        episode_length = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = demo_env.step(action)
            
            episode_reward += reward
            episode_length += 1
            
            if terminated or truncated:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # åˆ¤æ–­æˆåŠŸ (ä¸ baseline ç›¸åŒçš„æ ‡å‡†)
        if episode_reward > -5:
            success_count += 1
            print(f"ğŸ¯ Episode {episode+1}: æˆåŠŸ! å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
        else:
            print(f"ğŸ“Š Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
    
    demo_env.close()
    
    # æ¼”ç¤ºç»Ÿè®¡
    demo_success_rate = success_count / 10
    demo_avg_reward = np.mean(episode_rewards)
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¼”ç¤ºç»Ÿè®¡:")
    print(f"   æˆåŠŸç‡: {demo_success_rate:.1%} ({success_count}/10)")
    print(f"   å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    print(f"   å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.1f}")
    print(f"   å¥–åŠ±æ ‡å‡†å·®: {np.std(episode_rewards):.2f}")
    
    # ä¸ baseline æ¼”ç¤ºå¯¹æ¯”
    baseline_demo_success = 0.9  # baseline çš„æ¼”ç¤ºæˆåŠŸç‡
    baseline_demo_reward = -4.82  # baseline çš„æ¼”ç¤ºå¹³å‡å¥–åŠ±
    
    print(f"\nğŸ“ˆ æ¼”ç¤ºæ•ˆæœå¯¹æ¯”:")
    print(f"   Baseline æˆåŠŸç‡: {baseline_demo_success:.1%}")
    print(f"   SAC + Attention æˆåŠŸç‡: {demo_success_rate:.1%}")
    print(f"   æˆåŠŸç‡å˜åŒ–: {demo_success_rate - baseline_demo_success:+.1%}")
    print(f"   ")
    print(f"   Baseline å¹³å‡å¥–åŠ±: {baseline_demo_reward:.2f}")
    print(f"   SAC + Attention å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    print(f"   å¥–åŠ±å˜åŒ–: {demo_avg_reward - baseline_demo_reward:+.2f}")
    
    # è®­ç»ƒæ—¶é—´å¯¹æ¯”
    baseline_time = 14.3  # baseline çš„è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ)
    time_change = training_time/60 - baseline_time
    
    print(f"\nâ±ï¸ è®­ç»ƒæ—¶é—´å¯¹æ¯”:")
    print(f"   Baseline è®­ç»ƒæ—¶é—´: {baseline_time:.1f} åˆ†é’Ÿ")
    print(f"   SAC + Attention è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
    print(f"   æ—¶é—´å˜åŒ–: {time_change:+.1f} åˆ†é’Ÿ")
    
    if abs(time_change) < 2:
        print("   âœ… è®­ç»ƒæ—¶é—´åŸºæœ¬ç›¸åŒï¼Œæ³¨æ„åŠ›æœºåˆ¶å¼€é”€å¯æ¥å—")
    elif time_change > 0:
        print("   âš ï¸ è®­ç»ƒæ—¶é—´å¢åŠ ï¼Œæ³¨æ„åŠ›æœºåˆ¶æœ‰ä¸€å®šè®¡ç®—å¼€é”€")
    else:
        print("   ğŸš€ è®­ç»ƒæ—¶é—´å‡å°‘ï¼Œå¯èƒ½æ˜¯éšæœºå› ç´ ")
    
    print("\nâœ… SAC + Attention è®­ç»ƒå®Œæˆ!")
    
    # æ¸…ç†
    env.close()
    eval_env.close()
    
    return {
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'training_time': training_time,
        'demo_success_rate': demo_success_rate,
        'demo_avg_reward': demo_avg_reward,
        'improvement_vs_baseline': improvement,
        'time_vs_baseline': time_change
    }

if __name__ == "__main__":
    print("ğŸ”¥ å¼€å§‹ SAC + Attention Layer è®­ç»ƒ")
    print("ğŸ§  åœ¨æˆåŠŸçš„ Baseline SAC åŸºç¡€ä¸Šæ·»åŠ æ³¨æ„åŠ›æœºåˆ¶")
    print("ğŸ“ˆ è§‚å¯Ÿæ³¨æ„åŠ›æœºåˆ¶å¯¹è®­ç»ƒæ•ˆæœçš„å½±å“")
    print()
    
    try:
        results = sac_with_attention_training()
        
        print(f"\nğŸŠ SAC + Attention è®­ç»ƒç»“æœæ€»ç»“:")
        print(f"   æœ€ç»ˆè¯„ä¼°å¥–åŠ±: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
        print(f"   è®­ç»ƒæ—¶é—´: {results['training_time']/60:.1f} åˆ†é’Ÿ")
        print(f"   æ¼”ç¤ºæˆåŠŸç‡: {results['demo_success_rate']:.1%}")
        print(f"   æ¼”ç¤ºå¹³å‡å¥–åŠ±: {results['demo_avg_reward']:.2f}")
        print(f"   ç›¸æ¯” Baseline æ”¹è¿›: {results['improvement_vs_baseline']:+.2f}")
        print(f"   è®­ç»ƒæ—¶é—´å˜åŒ–: {results['time_vs_baseline']:+.1f} åˆ†é’Ÿ")
        
        # æ€»ä½“è¯„ä¼°
        if results['improvement_vs_baseline'] > 0.1:
            print(f"\nğŸ‰ æ³¨æ„åŠ›æœºåˆ¶é›†æˆæˆåŠŸ! æ€§èƒ½æœ‰æ˜æ˜¾æå‡")
        elif results['improvement_vs_baseline'] > -0.1:
            print(f"\nğŸ‘ æ³¨æ„åŠ›æœºåˆ¶é›†æˆè‰¯å¥½! æ€§èƒ½ä¿æŒç¨³å®š")
        else:
            print(f"\nâš ï¸ æ³¨æ„åŠ›æœºåˆ¶å¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ³¨æ„åŠ›æœºåˆ¶å®ç°å’Œå‚æ•°è®¾ç½®")
        import traceback
        traceback.print_exc()
