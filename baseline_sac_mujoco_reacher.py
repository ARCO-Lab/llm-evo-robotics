#!/usr/bin/env python3
"""
çº¯å‡€çš„ Stable Baselines3 SAC è®­ç»ƒ MuJoCo Reacher
ç›´æ¥ä½¿ç”¨å®˜æ–¹å®ç°ï¼Œæ— ä»»ä½•è‡ªå®šä¹‰é€‚é…å™¨
"""

import gymnasium as gym
import numpy as np
import torch
import time
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.evaluation import evaluate_policy

def baseline_sac_training():
    print("ğŸš€ çº¯å‡€çš„ Stable Baselines3 SAC è®­ç»ƒ MuJoCo Reacher")
    print("ğŸ“š å‚è€ƒæ–‡æ¡£: https://stable-baselines3.readthedocs.io/en/master/modules/sac.html")
    print("ğŸ¯ ç¯å¢ƒ: https://gymnasium.farama.org/environments/mujoco/reacher/")
    print("=" * 70)
    
    # åˆ›å»ºåŸç”Ÿ MuJoCo Reacher ç¯å¢ƒ
    print("ğŸ­ åˆ›å»º MuJoCo Reacher-v5 ç¯å¢ƒ...")
    env = gym.make('Reacher-v5', render_mode='human')
    env = Monitor(env)  # æ·»åŠ ç›‘æ§åŒ…è£…å™¨
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"ğŸ‘ï¸ è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"ğŸ“ è§‚å¯Ÿç»´åº¦: {env.observation_space.shape}")
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = gym.make('Reacher-v5')
    eval_env = Monitor(eval_env)
    
    print("=" * 70)
    
    # åˆ›å»º SAC æ¨¡å‹ - ä½¿ç”¨å®˜æ–¹æ¨èå‚æ•°
    print("ğŸ¤– åˆ›å»º SAC æ¨¡å‹...")
    model = SAC(
        "MlpPolicy",
        env,
        learning_rate=3e-4,          # å®˜æ–¹é»˜è®¤å­¦ä¹ ç‡
        buffer_size=1000000,         # 1M ç¼“å†²åŒº
        learning_starts=100,         # 100 æ­¥åå¼€å§‹å­¦ä¹ 
        batch_size=256,              # æ‰¹æ¬¡å¤§å°
        tau=0.005,                   # è½¯æ›´æ–°ç³»æ•°
        gamma=0.99,                  # æŠ˜æ‰£å› å­
        train_freq=1,                # æ¯æ­¥è®­ç»ƒ
        gradient_steps=1,            # æ¯æ¬¡è®­ç»ƒ1ä¸ªæ¢¯åº¦æ­¥
        ent_coef='auto',             # è‡ªåŠ¨è°ƒæ•´ç†µç³»æ•°
        target_update_interval=1,    # ç›®æ ‡ç½‘ç»œæ›´æ–°é—´éš”
        use_sde=False,               # ä¸ä½¿ç”¨çŠ¶æ€ä¾èµ–æ¢ç´¢
        verbose=1,                   # è¯¦ç»†è¾“å‡º
        device='cpu'                 # ä½¿ç”¨ CPU
    )
    
    print("âœ… SAC æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°:")
    print(f"   ç­–ç•¥: MlpPolicy")
    print(f"   å­¦ä¹ ç‡: 3e-4")
    print(f"   ç¼“å†²åŒºå¤§å°: 1,000,000")
    print(f"   æ‰¹æ¬¡å¤§å°: 256")
    print(f"   ç†µç³»æ•°: auto")
    
    print("=" * 70)
    
    # åˆ›å»ºè¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path='./sac_reacher_best/',
        log_path='./sac_reacher_logs/',
        eval_freq=5000,              # æ¯5000æ­¥è¯„ä¼°ä¸€æ¬¡
        n_eval_episodes=10,          # æ¯æ¬¡è¯„ä¼°10ä¸ªepisodes
        deterministic=True,          # è¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        render=False                 # è¯„ä¼°æ—¶ä¸æ¸²æŸ“
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print("ğŸ“Š è®­ç»ƒé…ç½®:")
    print("   æ€»æ­¥æ•°: 50,000")
    print("   è¯„ä¼°é¢‘ç‡: æ¯ 5,000 æ­¥")
    print("   æ—¥å¿—é—´éš”: æ¯ 1,000 æ­¥")
    print("=" * 70)
    
    start_time = time.time()
    
    # è®­ç»ƒæ¨¡å‹
    model.learn(
        total_timesteps=50000,       # è®­ç»ƒ50kæ­¥
        callback=eval_callback,      # è¯„ä¼°å›è°ƒ
        log_interval=10,             # æ¯10ä¸ªepisodesè®°å½•ä¸€æ¬¡
        progress_bar=True            # æ˜¾ç¤ºè¿›åº¦æ¡
    )
    
    training_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("ğŸ† è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
    print("=" * 70)
    
    # ä¿å­˜æ¨¡å‹
    model.save("sac_reacher_final")
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º: sac_reacher_final.zip")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ” æœ€ç»ˆè¯„ä¼° (20 episodes)...")
    mean_reward, std_reward = evaluate_policy(
        model, 
        eval_env, 
        n_eval_episodes=20,
        deterministic=True,
        render=False
    )
    
    print(f"ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    # æ€§èƒ½è¯„ä¼°
    if mean_reward > -5:
        print("ğŸ¥‡ ä¼˜ç§€! SAC å­¦ä¼šäº† Reacher ä»»åŠ¡")
    elif mean_reward > -10:
        print("ğŸ¥ˆ è‰¯å¥½! SAC æœ‰ä¸é”™çš„è¡¨ç°")
    elif mean_reward > -20:
        print("ğŸ¥‰ ä¸€èˆ¬! SAC æœ‰ä¸€å®šå­¦ä¹ æ•ˆæœ")
    else:
        print("âš ï¸ éœ€è¦æ›´å¤šè®­ç»ƒæˆ–å‚æ•°è°ƒæ•´")
    
    # æ¼”ç¤ºè®­ç»ƒå¥½çš„æ¨¡å‹
    print("\nğŸ® æ¼”ç¤ºè®­ç»ƒå¥½çš„æ¨¡å‹ (10 episodes)...")
    demo_env = gym.make('Reacher-v5', render_mode='human')
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(10):
        obs, info = demo_env.reset()
        episode_reward = 0
        episode_length = 0
        min_distance = float('inf')
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = demo_env.step(action)
            
            episode_reward += reward
            episode_length += 1
            
            # è®¡ç®—åˆ°ç›®æ ‡çš„è·ç¦» (MuJoCo Reacher çš„å¥–åŠ±å‡½æ•°åŒ…å«è·ç¦»ä¿¡æ¯)
            # è·ç¦»å¯ä»¥ä»å¥–åŠ±æ¨ç®—ï¼Œæˆ–è€…ä»è§‚å¯Ÿä¸­è·å–
            if hasattr(info, 'distance') or 'distance' in info:
                distance = info.get('distance', 0)
            else:
                # MuJoCo Reacher çš„è§‚å¯ŸåŒ…å«ç›®æ ‡å‘é‡ï¼Œå¯ä»¥è®¡ç®—è·ç¦»
                target_vector = obs[-3:-1]  # æœ€åå‡ ä¸ªç»´åº¦æ˜¯ç›®æ ‡å‘é‡
                distance = np.linalg.norm(target_vector)
            
            min_distance = min(min_distance, distance)
            
            if terminated or truncated:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # åˆ¤æ–­æˆåŠŸ (è·ç¦»å°äºæŸä¸ªé˜ˆå€¼æˆ–å¥–åŠ±è¶³å¤Ÿé«˜)
        if episode_reward > -5 or min_distance < 0.05:  # MuJoCo å•ä½æ˜¯ç±³
            success_count += 1
            print(f"ğŸ¯ Episode {episode+1}: æˆåŠŸ! å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
        else:
            print(f"ğŸ“Š Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
    
    demo_env.close()
    
    # æ¼”ç¤ºç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¼”ç¤ºç»Ÿè®¡:")
    print(f"   æˆåŠŸç‡: {success_count/10:.1%} ({success_count}/10)")
    print(f"   å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f}")
    print(f"   å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.1f}")
    print(f"   å¥–åŠ±æ ‡å‡†å·®: {np.std(episode_rewards):.2f}")
    
    # ä¸ MuJoCo åŸºå‡†æ¯”è¾ƒ
    print(f"\nğŸ… æ€§èƒ½è¯„ä¼°:")
    print(f"   MuJoCo Reacher åŸºå‡†å¥–åŠ±é€šå¸¸åœ¨ -5 åˆ° -50 ä¹‹é—´")
    print(f"   æ‚¨çš„æ¨¡å‹å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f}")
    
    if np.mean(episode_rewards) > -10:
        print("   ğŸ‰ è¡¨ç°ä¼˜ç§€ï¼Œè¶…è¿‡äº†å¤§å¤šæ•°åŸºå‡†!")
    elif np.mean(episode_rewards) > -20:
        print("   ğŸ‘ è¡¨ç°è‰¯å¥½ï¼Œè¾¾åˆ°äº†åˆç†æ°´å¹³!")
    else:
        print("   ğŸ“ˆ æœ‰æ”¹è¿›ç©ºé—´ï¼Œå¯ä»¥å°è¯•æ›´é•¿æ—¶é—´è®­ç»ƒ")
    
    print("\nâœ… Baseline SAC è®­ç»ƒå®Œæˆ!")
    
    # æ¸…ç†
    env.close()
    eval_env.close()
    
    return {
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'training_time': training_time,
        'demo_success_rate': success_count / 10,
        'demo_avg_reward': np.mean(episode_rewards)
    }

if __name__ == "__main__":
    print("ğŸ”¥ å¼€å§‹ Baseline SAC + MuJoCo Reacher è®­ç»ƒ")
    print("ğŸ“– è¿™æ˜¯ä¸€ä¸ªçº¯å‡€çš„å®ç°ï¼Œç›´æ¥ä½¿ç”¨å®˜æ–¹åº“")
    print()
    
    try:
        results = baseline_sac_training()
        
        print(f"\nğŸŠ è®­ç»ƒç»“æœæ€»ç»“:")
        print(f"   æœ€ç»ˆè¯„ä¼°å¥–åŠ±: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
        print(f"   è®­ç»ƒæ—¶é—´: {results['training_time']/60:.1f} åˆ†é’Ÿ")
        print(f"   æ¼”ç¤ºæˆåŠŸç‡: {results['demo_success_rate']:.1%}")
        print(f"   æ¼”ç¤ºå¹³å‡å¥–åŠ±: {results['demo_avg_reward']:.2f}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ˜¯å¦æ­£ç¡®å®‰è£…äº† MuJoCo å’Œç›¸å…³ä¾èµ–")
        print("   pip install gymnasium[mujoco]")
        print("   pip install stable-baselines3[extra]")
