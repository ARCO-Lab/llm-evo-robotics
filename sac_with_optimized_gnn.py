#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆ SAC + GNN + æ³¨æ„åŠ›æœºåˆ¶
è§£å†³æ€§èƒ½ä¸‹é™é—®é¢˜
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import time
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.evaluation import evaluate_policy
from typing import Dict, List, Tuple, Type, Union, Optional

class SimpleGraphProcessor(nn.Module):
    """
    ç®€åŒ–çš„å›¾å¤„ç†å™¨ - ä¸“é—¨ä¸º 2 å…³èŠ‚ Reacher ä¼˜åŒ–
    é¿å…è¿‡åº¦å¤æ‚åŒ–
    """
    def __init__(self, input_dim: int = 2, hidden_dim: int = 32):
        super(SimpleGraphProcessor, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # ç®€å•çš„å…³èŠ‚é—´äº¤äº’å±‚
        self.joint_interaction = nn.Sequential(
            nn.Linear(input_dim * 2, hidden_dim),  # ä¸¤ä¸ªå…³èŠ‚çš„ç‰¹å¾æ‹¼æ¥
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim * 2),  # è¾“å‡ºå¢å¼ºçš„å…³èŠ‚ç‰¹å¾
        )
        
        # æ®‹å·®è¿æ¥çš„æƒé‡
        self.residual_weight = nn.Parameter(torch.tensor(0.5))
        
        print(f"ğŸ”— SimpleGraphProcessor åˆå§‹åŒ–: {input_dim}â†’{hidden_dim}â†’{input_dim*2}")
    
    def forward(self, joint1_features: torch.Tensor, joint2_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å¤„ç†ä¸¤ä¸ªå…³èŠ‚ä¹‹é—´çš„äº¤äº’
        joint1_features: [batch_size, input_dim]
        joint2_features: [batch_size, input_dim]
        return: (enhanced_joint1, enhanced_joint2)
        """
        # æ‹¼æ¥ä¸¤ä¸ªå…³èŠ‚çš„ç‰¹å¾
        combined = torch.cat([joint1_features, joint2_features], dim=1)
        
        # é€šè¿‡äº¤äº’å±‚
        enhanced_combined = self.joint_interaction(combined)
        
        # åˆ†ç¦»å¢å¼ºåçš„ç‰¹å¾
        enhanced_joint1 = enhanced_combined[:, :self.input_dim]
        enhanced_joint2 = enhanced_combined[:, self.input_dim:]
        
        # æ®‹å·®è¿æ¥
        output_joint1 = self.residual_weight * enhanced_joint1 + (1 - self.residual_weight) * joint1_features
        output_joint2 = self.residual_weight * enhanced_joint2 + (1 - self.residual_weight) * joint2_features
        
        return output_joint1, output_joint2

class LightweightAttention(nn.Module):
    """
    è½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶ - å‡å°‘å‚æ•°æ•°é‡
    """
    def __init__(self, feature_dim: int = 32):
        super(LightweightAttention, self).__init__()
        self.feature_dim = feature_dim
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›è®¡ç®—
        self.attention = nn.Linear(feature_dim, 1, bias=False)
        
        print(f"ğŸ¯ LightweightAttention åˆå§‹åŒ–: {feature_dim}â†’1")
    
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—è½»é‡çº§æ³¨æ„åŠ›
        features: [batch_size, feature_dim]
        return: [batch_size, feature_dim]
        """
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weight = torch.sigmoid(self.attention(features))  # [batch_size, 1]
        
        # åº”ç”¨æ³¨æ„åŠ›
        attended_features = features * attention_weight
        
        return attended_features

class OptimizedGNNExtractor(BaseFeaturesExtractor):
    """
    ä¼˜åŒ–ç‰ˆ GNN ç‰¹å¾æå–å™¨
    è§£å†³æ€§èƒ½ä¸‹é™é—®é¢˜
    """
    def __init__(self, observation_space: gym.Space, features_dim: int = 128):
        super(OptimizedGNNExtractor, self).__init__(observation_space, features_dim)
        
        self.obs_dim = observation_space.shape[0]
        
        print(f"ğŸ” OptimizedGNNExtractor åˆå§‹åŒ–:")
        print(f"   è§‚å¯Ÿç©ºé—´ç»´åº¦: {self.obs_dim}")
        print(f"   è¾“å‡ºç‰¹å¾ç»´åº¦: {features_dim}")
        print(f"   ä¼˜åŒ–ç›®æ ‡: å‡å°‘å¤æ‚åº¦ï¼Œæå‡æ€§èƒ½")
        
        # å…³èŠ‚ç‰¹å¾å¤„ç† (ç®€åŒ–ç‰ˆ)
        joint_feature_dim = 16  # å‡å°‘ç‰¹å¾ç»´åº¦
        
        self.joint1_encoder = nn.Sequential(
            nn.Linear(2, joint_feature_dim),  # angle + velocity
            nn.ReLU(),
            nn.LayerNorm(joint_feature_dim)
        )
        
        self.joint2_encoder = nn.Sequential(
            nn.Linear(2, joint_feature_dim),  # angle + velocity
            nn.ReLU(),
            nn.LayerNorm(joint_feature_dim)
        )
        
        # ç®€åŒ–çš„å›¾å¤„ç†
        self.graph_processor = SimpleGraphProcessor(
            input_dim=joint_feature_dim,
            hidden_dim=32
        )
        
        # è½»é‡çº§æ³¨æ„åŠ›
        self.joint1_attention = LightweightAttention(joint_feature_dim)
        self.joint2_attention = LightweightAttention(joint_feature_dim)
        
        # å…¨å±€ç‰¹å¾å¤„ç† (ä½ç½®ä¿¡æ¯)
        # MuJoCo Reacher-v5: [4:6] end effector, [6:8] target, [8:10] vector
        global_feature_dim = 6
        self.global_encoder = nn.Sequential(
            nn.Linear(global_feature_dim, 32),
            nn.ReLU(),
            nn.LayerNorm(32)
        )
        
        # ç‰¹å¾èåˆ (ç®€åŒ–ç‰ˆ)
        fusion_input_dim = joint_feature_dim * 2 + 32  # ä¸¤ä¸ªå…³èŠ‚ + å…¨å±€ç‰¹å¾
        self.fusion_net = nn.Sequential(
            nn.Linear(fusion_input_dim, features_dim),
            nn.ReLU(),
            nn.Dropout(0.1),  # å‡å°‘ dropout
            nn.Linear(features_dim, features_dim)
        )
        
        print(f"âœ… OptimizedGNNExtractor æ„å»ºå®Œæˆ")
        print(f"   å…³èŠ‚ç‰¹å¾ç»´åº¦: 2â†’{joint_feature_dim}")
        print(f"   å…¨å±€ç‰¹å¾ç»´åº¦: {global_feature_dim}â†’32")
        print(f"   èåˆè¾“å…¥ç»´åº¦: {fusion_input_dim}")
        print(f"   æ€»å‚æ•°æ•°é‡æ˜¾è‘—å‡å°‘")
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        ä¼˜åŒ–çš„å‰å‘ä¼ æ’­
        observations: [batch_size, obs_dim]
        return: [batch_size, features_dim]
        """
        batch_size = observations.size(0)
        
        # 1. æå–å…³èŠ‚ç‰¹å¾ (MuJoCo Reacher-v5 æ ¼å¼)
        # [0:2] - cos/sin of joint angles
        # [2:4] - joint velocities
        joint1_raw = torch.cat([observations[:, 0:1], observations[:, 2:3]], dim=1)  # [angle, velocity]
        joint2_raw = torch.cat([observations[:, 1:2], observations[:, 3:4]], dim=1)  # [angle, velocity]
        
        # 2. ç¼–ç å…³èŠ‚ç‰¹å¾
        joint1_encoded = self.joint1_encoder(joint1_raw)  # [batch_size, joint_feature_dim]
        joint2_encoded = self.joint2_encoder(joint2_raw)  # [batch_size, joint_feature_dim]
        
        # 3. å›¾å¤„ç† (å…³èŠ‚é—´äº¤äº’)
        joint1_enhanced, joint2_enhanced = self.graph_processor(joint1_encoded, joint2_encoded)
        
        # 4. è½»é‡çº§æ³¨æ„åŠ›
        joint1_attended = self.joint1_attention(joint1_enhanced)
        joint2_attended = self.joint2_attention(joint2_enhanced)
        
        # 5. å¤„ç†å…¨å±€ç‰¹å¾
        global_features = observations[:, 4:]  # [end_effector, target, vector]
        global_encoded = self.global_encoder(global_features)  # [batch_size, 32]
        
        # 6. ç‰¹å¾èåˆ
        fused_features = torch.cat([joint1_attended, joint2_attended, global_encoded], dim=1)
        output = self.fusion_net(fused_features)
        
        return output

def optimized_sac_training():
    print("ğŸš€ ä¼˜åŒ–ç‰ˆ SAC + GNN + æ³¨æ„åŠ›æœºåˆ¶è®­ç»ƒ")
    print("ğŸ¯ ç›®æ ‡: è§£å†³æ€§èƒ½ä¸‹é™é—®é¢˜")
    print("ğŸ”§ ä¼˜åŒ–ç­–ç•¥:")
    print("   - ç®€åŒ– GNN æ¶æ„")
    print("   - å‡å°‘å‚æ•°æ•°é‡")
    print("   - è½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶")
    print("   - é’ˆå¯¹ 2 å…³èŠ‚ Reacher ä¼˜åŒ–")
    print("=" * 70)
    
    # åˆ›å»ºç¯å¢ƒ
    print("ğŸ­ åˆ›å»º MuJoCo Reacher-v5 ç¯å¢ƒ...")
    env = gym.make('Reacher-v5', render_mode='human')
    env = Monitor(env)
    
    eval_env = gym.make('Reacher-v5')
    eval_env = Monitor(eval_env)
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"ğŸ‘ï¸ è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    
    print("=" * 70)
    
    # åˆ›å»ºä¼˜åŒ–ç‰ˆæ¨¡å‹
    print("ğŸ¤– åˆ›å»ºä¼˜åŒ–ç‰ˆ SAC æ¨¡å‹...")
    
    policy_kwargs = {
        "features_extractor_class": OptimizedGNNExtractor,
        "features_extractor_kwargs": {
            "features_dim": 128,
        },
        "net_arch": [128, 128],  # å‡å°‘ç½‘ç»œå¤§å°
        "activation_fn": torch.nn.ReLU,
    }
    
    model = SAC(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        buffer_size=100000,         # å‡å°‘ç¼“å†²åŒºå¤§å°
        learning_starts=100,        # æ›´æ—©å¼€å§‹å­¦ä¹ 
        batch_size=128,             # å‡å°‘æ‰¹æ¬¡å¤§å°
        tau=0.005,
        gamma=0.99,
        train_freq=1,
        gradient_steps=1,
        ent_coef='auto',
        target_update_interval=1,
        use_sde=False,
        policy_kwargs=policy_kwargs,
        verbose=1,
        device='cpu'
    )
    
    print("âœ… ä¼˜åŒ–ç‰ˆ SAC æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"ğŸ“Š ä¼˜åŒ–é…ç½®:")
    print(f"   ç‰¹å¾ç»´åº¦: 128 (vs ä¹‹å‰çš„å¤æ‚æ¶æ„)")
    print(f"   ç½‘ç»œæ¶æ„: [128, 128] (vs [256, 256])")
    print(f"   æ‰¹æ¬¡å¤§å°: 128 (vs 256)")
    print(f"   ç¼“å†²åŒº: 100K (vs 1M)")
    print(f"   å‚æ•°æ•°é‡æ˜¾è‘—å‡å°‘")
    
    print("=" * 70)
    
    # åˆ›å»ºè¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path='./sac_optimized_gnn_best/',
        log_path='./sac_optimized_gnn_logs/',
        eval_freq=5000,
        n_eval_episodes=10,
        deterministic=True,
        render=False
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹ä¼˜åŒ–è®­ç»ƒ...")
    print("ğŸ“Š è®­ç»ƒé…ç½®:")
    print("   æ€»æ­¥æ•°: 30,000 (å‡å°‘è®­ç»ƒæ­¥æ•°)")
    print("   è¯„ä¼°é¢‘ç‡: æ¯ 5,000 æ­¥")
    print("   é¢„æœŸ: æ›´å¿«æ”¶æ•›ï¼Œæ›´å¥½æ€§èƒ½")
    print("=" * 70)
    
    start_time = time.time()
    
    # è®­ç»ƒæ¨¡å‹ (å‡å°‘è®­ç»ƒæ­¥æ•°)
    model.learn(
        total_timesteps=30000,  # å‡å°‘åˆ° 30K
        callback=eval_callback,
        log_interval=10,
        progress_bar=True
    )
    
    training_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("ğŸ† ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
    print("=" * 70)
    
    # ä¿å­˜æ¨¡å‹
    model.save("sac_optimized_gnn_final")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º: sac_optimized_gnn_final.zip")
    
    # æœ€ç»ˆè¯„ä¼°
    print("\nğŸ” æœ€ç»ˆè¯„ä¼° (20 episodes)...")
    mean_reward, std_reward = evaluate_policy(
        model, 
        eval_env, 
        n_eval_episodes=20,
        deterministic=True,
        render=False
    )
    
    print(f"ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
    
    # ä¸ä¹‹å‰ç»“æœå¯¹æ¯”
    baseline_reward = -4.86
    simple_attention_reward = -4.69
    complex_gnn_reward = -5.56
    
    improvement_vs_baseline = mean_reward - baseline_reward
    improvement_vs_simple = mean_reward - simple_attention_reward
    improvement_vs_complex = mean_reward - complex_gnn_reward
    
    print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
    print(f"   Baseline SAC: {baseline_reward:.2f}")
    print(f"   ç®€åŒ–æ³¨æ„åŠ›: {simple_attention_reward:.2f}")
    print(f"   å¤æ‚ GNN: {complex_gnn_reward:.2f}")
    print(f"   ä¼˜åŒ– GNN: {mean_reward:.2f}")
    print(f"   vs Baseline: {improvement_vs_baseline:+.2f}")
    print(f"   vs ç®€åŒ–æ³¨æ„åŠ›: {improvement_vs_simple:+.2f}")
    print(f"   vs å¤æ‚ GNN: {improvement_vs_complex:+.2f}")
    
    if improvement_vs_baseline > -0.2:
        print("   ğŸ‰ ä¼˜åŒ– GNN æˆåŠŸæ¥è¿‘ Baseline æ€§èƒ½!")
    elif improvement_vs_complex > 0.5:
        print("   ğŸ‘ ä¼˜åŒ– GNN æ˜¾è‘—ä¼˜äºå¤æ‚ç‰ˆæœ¬!")
    else:
        print("   âš ï¸ ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # æ¼”ç¤ºè®­ç»ƒå¥½çš„æ¨¡å‹
    print("\nğŸ® æ¼”ç¤ºä¼˜åŒ–åçš„æ¨¡å‹ (10 episodes)...")
    demo_env = gym.make('Reacher-v5', render_mode='human')
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(10):
        obs, info = demo_env.reset()
        episode_reward = 0
        episode_length = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = demo_env.step(action)
            
            episode_reward += reward
            episode_length += 1
            
            if terminated or truncated:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # åˆ¤æ–­æˆåŠŸ
        if episode_reward > -5:
            success_count += 1
            print(f"ğŸ¯ Episode {episode+1}: æˆåŠŸ! å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
        else:
            print(f"ğŸ“Š Episode {episode+1}: å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
    
    demo_env.close()
    
    # æ¼”ç¤ºç»Ÿè®¡
    demo_success_rate = success_count / 10
    demo_avg_reward = np.mean(episode_rewards)
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¼”ç¤ºç»Ÿè®¡:")
    print(f"   æˆåŠŸç‡: {demo_success_rate:.1%} ({success_count}/10)")
    print(f"   å¹³å‡å¥–åŠ±: {demo_avg_reward:.2f}")
    print(f"   å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.1f}")
    print(f"   å¥–åŠ±æ ‡å‡†å·®: {np.std(episode_rewards):.2f}")
    
    # ä¸ä¹‹å‰æ¼”ç¤ºå¯¹æ¯”
    baseline_demo_success = 0.9
    simple_demo_success = 0.7
    complex_demo_success = 0.4
    
    print(f"\nğŸ“ˆ æ¼”ç¤ºæ•ˆæœå¯¹æ¯”:")
    print(f"   Baseline æˆåŠŸç‡: {baseline_demo_success:.1%}")
    print(f"   ç®€åŒ–æ³¨æ„åŠ›æˆåŠŸç‡: {simple_demo_success:.1%}")
    print(f"   å¤æ‚ GNN æˆåŠŸç‡: {complex_demo_success:.1%}")
    print(f"   ä¼˜åŒ– GNN æˆåŠŸç‡: {demo_success_rate:.1%}")
    
    if demo_success_rate >= 0.8:
        print("   ğŸ† ä¼˜åŒ– GNN æˆåŠŸæ¢å¤é«˜æ€§èƒ½!")
    elif demo_success_rate >= 0.6:
        print("   ğŸ‘ ä¼˜åŒ– GNN æ˜¾è‘—æ”¹å–„!")
    elif demo_success_rate > complex_demo_success:
        print("   ğŸ“ˆ ä¼˜åŒ– GNN æœ‰æ‰€æ”¹å–„")
    else:
        print("   âš ï¸ ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # è®­ç»ƒæ—¶é—´å¯¹æ¯”
    baseline_time = 14.3
    simple_time = 16.4
    complex_time = 35.7
    time_vs_baseline = training_time/60 - baseline_time
    time_vs_complex = training_time/60 - complex_time
    
    print(f"\nâ±ï¸ è®­ç»ƒæ—¶é—´å¯¹æ¯”:")
    print(f"   Baseline: {baseline_time:.1f} åˆ†é’Ÿ")
    print(f"   ç®€åŒ–æ³¨æ„åŠ›: {simple_time:.1f} åˆ†é’Ÿ")
    print(f"   å¤æ‚ GNN: {complex_time:.1f} åˆ†é’Ÿ")
    print(f"   ä¼˜åŒ– GNN: {training_time/60:.1f} åˆ†é’Ÿ")
    print(f"   vs Baseline: {time_vs_baseline:+.1f} åˆ†é’Ÿ")
    print(f"   vs å¤æ‚ GNN: {time_vs_complex:+.1f} åˆ†é’Ÿ")
    
    if abs(time_vs_baseline) < 5:
        print("   âœ… è®­ç»ƒæ—¶é—´æ¥è¿‘ Baselineï¼Œä¼˜åŒ–æˆåŠŸ!")
    elif time_vs_complex < -10:
        print("   ğŸš€ è®­ç»ƒæ—¶é—´æ˜¾è‘—å‡å°‘!")
    
    print("\nâœ… ä¼˜åŒ–ç‰ˆ SAC + GNN è®­ç»ƒå®Œæˆ!")
    
    # æ¸…ç†
    env.close()
    eval_env.close()
    
    return {
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'training_time': training_time,
        'demo_success_rate': demo_success_rate,
        'demo_avg_reward': demo_avg_reward,
        'improvement_vs_baseline': improvement_vs_baseline,
        'improvement_vs_complex': improvement_vs_complex,
        'time_vs_baseline': time_vs_baseline,
        'time_vs_complex': time_vs_complex
    }

if __name__ == "__main__":
    print("ğŸ”¥ å¼€å§‹ä¼˜åŒ–ç‰ˆ SAC + GNN è®­ç»ƒ")
    print("ğŸ¯ è§£å†³æ€§èƒ½ä¸‹é™é—®é¢˜")
    print("ğŸ”§ å…³é”®ä¼˜åŒ–:")
    print("   1. ç®€åŒ– GNN æ¶æ„")
    print("   2. å‡å°‘å‚æ•°æ•°é‡") 
    print("   3. è½»é‡çº§æ³¨æ„åŠ›")
    print("   4. é’ˆå¯¹ä»»åŠ¡ä¼˜åŒ–")
    print()
    
    try:
        results = optimized_sac_training()
        
        print(f"\nğŸŠ ä¼˜åŒ–ç‰ˆè®­ç»ƒç»“æœæ€»ç»“:")
        print(f"   æœ€ç»ˆè¯„ä¼°å¥–åŠ±: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
        print(f"   è®­ç»ƒæ—¶é—´: {results['training_time']/60:.1f} åˆ†é’Ÿ")
        print(f"   æ¼”ç¤ºæˆåŠŸç‡: {results['demo_success_rate']:.1%}")
        print(f"   æ¼”ç¤ºå¹³å‡å¥–åŠ±: {results['demo_avg_reward']:.2f}")
        print(f"   vs Baseline æ”¹è¿›: {results['improvement_vs_baseline']:+.2f}")
        print(f"   vs å¤æ‚ GNN æ”¹è¿›: {results['improvement_vs_complex']:+.2f}")
        print(f"   è®­ç»ƒæ—¶é—´ vs Baseline: {results['time_vs_baseline']:+.1f} åˆ†é’Ÿ")
        print(f"   è®­ç»ƒæ—¶é—´ vs å¤æ‚ GNN: {results['time_vs_complex']:+.1f} åˆ†é’Ÿ")
        
        # æ€»ä½“è¯„ä¼°
        if (results['improvement_vs_baseline'] > -0.3 and 
            results['demo_success_rate'] > 0.7 and 
            results['time_vs_baseline'] < 10):
            print(f"\nğŸ† ä¼˜åŒ–æˆåŠŸ!")
            print("   æ€§èƒ½æ¥è¿‘ Baseline + åˆç†è®­ç»ƒæ—¶é—´ + é«˜æˆåŠŸç‡")
        elif results['improvement_vs_complex'] > 0.5:
            print(f"\nğŸ‘ æ˜¾è‘—æ”¹å–„!")
            print("   ç›¸æ¯”å¤æ‚ GNN æœ‰æ˜æ˜¾æå‡")
        else:
            print(f"\nğŸ“ˆ æœ‰æ‰€æ”¹å–„ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        print(f"\nğŸ”— ä¼˜åŒ–ç‰ˆæ¨¡å‹å·²å‡†å¤‡å¥½æ‰©å±•åˆ°å¤šå…³èŠ‚ä»»åŠ¡!")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
