#!/usr/bin/env python3
"""
ä½¿ç”¨è‡ªç„¶3å…³èŠ‚Reacherç¯å¢ƒè®­ç»ƒSAC
åŸºäºæ ‡å‡†MuJoCo Reacherï¼Œæ¸²æŸ“æ•ˆæœæ›´å¥½
"""

import os
import time
import numpy as np
import torch
import torch.nn as nn
from typing import Dict
import gymnasium as gym
from gymnasium.spaces import Box
from stable_baselines3 import SAC
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

# å¯¼å…¥è‡ªç„¶3å…³èŠ‚ç¯å¢ƒ
from natural_3joint_reacher import Natural3JointReacherEnv

# ============================================================================
# ğŸ§© è‡ªç„¶3å…³èŠ‚ç‰¹å¾æå–å™¨
# ============================================================================

class Natural3JointExtractor(BaseFeaturesExtractor):
    """é’ˆå¯¹è‡ªç„¶3å…³èŠ‚Reacherä¼˜åŒ–çš„ç‰¹å¾æå–å™¨"""
    
    def __init__(self, observation_space: gym.Space, features_dim: int = 128):
        super(Natural3JointExtractor, self).__init__(observation_space, features_dim)
        
        obs_dim = observation_space.shape[0]  # 15ç»´
        print(f"ğŸ”§ Natural3JointExtractor: {obs_dim} -> {features_dim}")
        
        # é’ˆå¯¹15ç»´è§‚å¯Ÿç©ºé—´çš„ç½‘ç»œ
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, features_dim),
            nn.ReLU()
        )
    
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        return self.net(observations)

# ============================================================================
# ğŸ§© è®­ç»ƒå‡½æ•°
# ============================================================================

def train_natural_3joint_sac(total_timesteps: int = 20000, 
                            model_name: str = "natural_3joint_sac",
                            render_during_training: bool = True):
    """è®­ç»ƒè‡ªç„¶3å…³èŠ‚SACæ¨¡å‹"""
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ è‡ªç„¶3å…³èŠ‚ Reacher SAC è®­ç»ƒ")
    print(f"{'='*70}")
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    print(f"ğŸŒ åˆ›å»ºè‡ªç„¶3å…³èŠ‚è®­ç»ƒç¯å¢ƒ...")
    env = Natural3JointReacherEnv(
        render_mode='human' if render_during_training else None
    )
    env = Monitor(env)
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    print(f"   è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"   æ¸²æŸ“æ¨¡å¼: {'å¯ç”¨' if render_during_training else 'ç¦ç”¨'}")
    
    # åˆ›å»ºSACæ¨¡å‹
    print(f"ğŸ¤– åˆ›å»ºSACæ¨¡å‹...")
    
    policy_kwargs = {
        'features_extractor_class': Natural3JointExtractor,
        'features_extractor_kwargs': {
            'features_dim': 128
        },
        'net_arch': [256, 256]
    }
    
    model = SAC(
        'MlpPolicy',
        env,
        policy_kwargs=policy_kwargs,
        learning_rate=3e-4,
        buffer_size=50000,
        learning_starts=500,
        batch_size=128,
        tau=0.005,
        gamma=0.99,
        ent_coef='auto',
        verbose=2,
        device='cpu'
    )
    
    print(f"âœ… SACæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ ({total_timesteps} æ­¥)...")
    if render_during_training:
        print(f"ğŸ’¡ è¯·è§‚å¯ŸMuJoCoçª—å£ä¸­çš„3å…³èŠ‚æœºå™¨äººè®­ç»ƒè¿‡ç¨‹")
        print(f"ğŸ® åœºåœ°å¤§å°å·²ä¼˜åŒ–ï¼Œæœºæ¢°è‡‚ä¸ä¼šä¼¸å‡ºè¾¹ç•Œ")
    print("-" * 70)
    
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            log_interval=4,
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        
        # ä¿å­˜æ¨¡å‹
        model.save(model_name)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_name}")
        
        # ç«‹å³æµ‹è¯•
        print(f"\nğŸ® ç«‹å³æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹...")
        test_results = test_natural_3joint_model(model, env)
        
        return model, training_time, test_results
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        training_time = time.time() - start_time
        print(f"â±ï¸ å·²è®­ç»ƒæ—¶é—´: {training_time/60:.1f} åˆ†é’Ÿ")
        
        # ä¿å­˜ä¸­æ–­çš„æ¨¡å‹
        model.save(f"{model_name}_interrupted")
        print(f"ğŸ’¾ ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {model_name}_interrupted")
        
        return model, training_time, None
    
    finally:
        env.close()

# ============================================================================
# ğŸ§© æµ‹è¯•å‡½æ•°
# ============================================================================

def test_natural_3joint_model(model, env=None, num_episodes: int = 5):
    """æµ‹è¯•è‡ªç„¶3å…³èŠ‚æ¨¡å‹"""
    
    print(f"\n{'='*50}")
    print(f"ğŸ® æµ‹è¯•è‡ªç„¶3å…³èŠ‚æ¨¡å‹")
    print(f"{'='*50}")
    
    # å¦‚æœæ²¡æœ‰æä¾›ç¯å¢ƒï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
    if env is None:
        env = Natural3JointReacherEnv(render_mode='human')
        env = Monitor(env)
        should_close = True
    else:
        should_close = False
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(num_episodes):
        obs, info = env.reset()
        episode_reward = 0
        episode_length = 0
        min_distance = float('inf')
        
        print(f"\nğŸ“ Episode {episode + 1}/{num_episodes}")
        
        while True:
            # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            episode_length += 1
            
            # è®°å½•æœ€å°è·ç¦»
            distance = info['distance_to_target']
            min_distance = min(min_distance, distance)
            
            # æ¯10æ­¥æ˜¾ç¤ºä¸€æ¬¡ä¿¡æ¯
            if episode_length % 10 == 0:
                print(f"   Step {episode_length}: è·ç¦»={distance:.3f}, å¥–åŠ±={reward:.3f}")
            
            # ç¨å¾®æ…¢ä¸€ç‚¹è®©æ‚¨è§‚å¯Ÿ
            time.sleep(0.05)
            
            if terminated or truncated:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        # åˆ¤æ–­æˆåŠŸ (è·ç¦»å°äº2cm)
        success = min_distance < 0.02
        if success:
            success_count += 1
            print(f"   âœ… æˆåŠŸ! å¥–åŠ±={episode_reward:.3f}, æœ€å°è·ç¦»={min_distance:.3f}")
        else:
            print(f"   âŒ å¤±è´¥. å¥–åŠ±={episode_reward:.3f}, æœ€å°è·ç¦»={min_distance:.3f}")
    
    # ç»Ÿè®¡ç»“æœ
    avg_reward = np.mean(episode_rewards)
    success_rate = success_count / num_episodes
    
    print(f"\nğŸ“Š è‡ªç„¶3å…³èŠ‚æµ‹è¯•ç»“æœ:")
    print(f"   æˆåŠŸç‡: {success_rate:.1%} ({success_count}/{num_episodes})")
    print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.3f} Â± {np.std(episode_rewards):.3f}")
    print(f"   å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.1f}")
    
    if should_close:
        env.close()
    
    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward,
        'episode_rewards': episode_rewards
    }

# ============================================================================
# ğŸ§© ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ è‡ªç„¶3å…³èŠ‚ Reacher SAC è®­ç»ƒç³»ç»Ÿ")
    print("ğŸ’¡ åŸºäºæ ‡å‡†MuJoCo Reacherï¼Œä¼˜åŒ–çš„åœºåœ°å’Œæ¸²æŸ“æ•ˆæœ")
    print("ğŸ¯ 3å…³èŠ‚æœºæ¢°è‡‚ï¼Œæ€»é•¿åº¦çº¦24cmï¼Œé€‚åˆçš„å·¥ä½œç©ºé—´")
    print()
    
    try:
        # è®­ç»ƒæ¨¡å‹
        model, training_time, test_results = train_natural_3joint_sac(
            total_timesteps=20000,
            model_name="models/natural_3joint_sac",
            render_during_training=True  # è®­ç»ƒæ—¶æ˜¾ç¤ºæ¸²æŸ“
        )
        
        print(f"\nğŸ‰ è‡ªç„¶3å…³èŠ‚è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ æ€»ç”¨æ—¶: {training_time/60:.1f} åˆ†é’Ÿ")
        
        if test_results:
            print(f"ğŸ“Š æµ‹è¯•ç»“æœ: æˆåŠŸç‡ {test_results['success_rate']:.1%}")
            print(f"           å¹³å‡å¥–åŠ± {test_results['avg_reward']:.3f}")
            
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

# ============================================================================
# ğŸ§© ç‹¬ç«‹æµ‹è¯•å‡½æ•°
# ============================================================================

def test_saved_natural_3joint_model():
    """æµ‹è¯•å·²ä¿å­˜çš„è‡ªç„¶3å…³èŠ‚æ¨¡å‹"""
    print("ğŸ® æµ‹è¯•å·²ä¿å­˜çš„è‡ªç„¶3å…³èŠ‚æ¨¡å‹")
    
    model_path = "models/natural_3joint_sac"
    
    if os.path.exists(f"{model_path}.zip"):
        try:
            # åˆ›å»ºç¯å¢ƒ
            env = Natural3JointReacherEnv(render_mode='human')
            
            # åŠ è½½æ¨¡å‹
            model = SAC.load(model_path, env=env)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # æµ‹è¯•æ¨¡å‹
            results = test_natural_3joint_model(model, env, num_episodes=3)
            
            print(f"âœ… æµ‹è¯•å®Œæˆ")
            print(f"   æˆåŠŸç‡: {results['success_rate']:.1%}")
            print(f"   å¹³å‡å¥–åŠ±: {results['avg_reward']:.3f}")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    else:
        print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}.zip")
        print("   è¯·å…ˆè¿è¡Œè®­ç»ƒ: python train_natural_3joint_sac.py")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # ä»…æµ‹è¯•å·²ä¿å­˜çš„æ¨¡å‹
        test_saved_natural_3joint_model()
    else:
        # å®Œæ•´çš„è®­ç»ƒæµç¨‹
        main()


